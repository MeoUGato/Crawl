,description,evaluation,data_description,data_download_commmand,sols
0,"Goal of the Competition
The National Football League (NFL) is back with another Big Data Bowl, where contestants use Next Gen Stats player tracking data to generate actionable, creative, and novel stats. Previous iterations have considered running backs, defensive backs, and special teams, and have generated metrics that have been used on television and by NFL teams. In this year’s competition, you’ll have more subtle performances to consider—and potentially more players to measure.
2023 Theme: Linemen on Pass Plays
Quarterbacks may get the glory, but some of the most important work takes place a few feet in front of them. The offensive line protects the passer, providing precious seconds to find receivers downfield. At the same time, the opposing team’s defensive line attempts to find a disruptive path. If a defender sneaks through, it can mean a sack, a blocked pass, or even a turnover. Some of the game’s most important plays happen on the line and this competition examines the data behind the hardest workers in football.
In this competition, you’ll have access to the NFL’s Next Gen Stats data, including player tracking, play, game, and player information, as well as Pro Football Focus (PFF) scouting data for 2021 passing plays (Weeks 1-8 of the NFL season). You’ll create new metrics and stats for America's most popular sports league. Notebook submissions will be scored based on five components: innovation, accuracy, relevance, clarity, and data visualization.
Winners will be invited to present their results to the NFL, where one competition team will receive an additional prize. The most useful new metrics or analysis could be also used by NFL teams to evaluate their offensive and defensive lines.","Overview
Your challenge is to generate actionable, practical, and novel insights from player tracking data that corresponds to linemen performance on passing plays.
Participants will select one of three tracks in which to submit.
Undergraduate track. This is open only to groups or individuals composed entirely of undergraduate students. Verification may be required to prove eligibility.
Metric track. Create a metric to assess performance and/or strategy. This can be focused on offensive or defensive players, and on teams or individuals.
Coaching presentation track. The goal of this track is to analyze and present data in a submission designed for coaches. We encourage participants interested in this track to partner with a coach (or current/former player), though that is not required.
All submissions must explicitly state which track they are submitting to. Participants may not submit to more than one track.
Scoring
An entry to the competition consists of a Notebook submission that is evaluated on the following five components, where 0 is the low score and 10 is the high score. Submissions will be judged based on how well they address:
Innovation:
Are the proposed findings actionable?
Is this a way of looking at tracking data that is novel?
Is this project creative?
Accuracy:
Is the work correct?
Are claims backed up by data?
Are the statistical models appropriate given the data?
Relevance:
Would NFL teams (or the league office) be able to use these results on a week-to-week basis?
Does the analysis account for variables that make football data complex?
Clarity:
Evaluate the writing with respect to how clear the writer(s) make findings.
Data visualization/tables:
Are the charts and tables provided accessible, interesting, visually appealing, and accurate?
Judges will consist of analytics staffers that are working for either (i) NFL headquarters, (ii) the 32 NFL teams, or (iii) player tracking vendors that work with NFL teams. Scores will be averaged so that each of the components above are weighed equally.
The top eight scoring papers will be selected as finalists (at least two per track)
Notebook requirements
Notebooks should consist of no more than 2,000 words and no more than 10 tables/figures. Submissions will not be penalized for any number of words or figures under this limit. Participants are encouraged to show statistical code if it helps readers better understand their analyses; most, if not all code, however, should be hidden in the Appendix.
All notebooks submitted must be made public on or before the submission deadline to be eligible. If submitting as a team, all team members must be listed as collaborators on all notebooks submitted.
Any notebook that does not use the player tracking data will not be scored.","Here, you'll find a summary of each data set in the 2023 Data Bowl, a list of key variables to join on, and a description of each variable. The tracking data is provided by the NFL Next Gen Stats team. The Scouting data is provided by Pro Football Focus.
The 2023 Big Data Bowl allows participants to use supplemental NFL data as long as it is free and publicly available to all participants. Examples of sources that could be used include nflverse and Pro Football Reference. Please note that the gameId and playId of the Big Data Bowl data merge with the old_game_id and play_id of nflverse's play-by-play data.
File descriptions
Game data: The games.csv contains the teams playing in each game. The key variable is gameId.
Play data: The plays.csv file contains play-level information from each game. The key variables are gameId and playId.
Player data: The players.csv file contains player-level information from players that participated in any of the tracking data files. The key variable is nflId.
PFF Scouting data: The pffScoutingData.csv file contains player-level scouting information for each game and play. The key variables are gameId, playId, and nflId.
Tracking data: Files week[week].csv contain player tracking data from season [week]. The key variables are gameId, , and .",kaggle competitions download -c nfl-big-data-bowl-2023,"['https://www.kaggle.com/code/huntingdata11/animated-and-interactive-nfl-plays-in-plotly', 'https://www.kaggle.com/code/nickwan/creating-player-stats-using-tracking-data', 'https://www.kaggle.com/code/nickwan/animated-gif-for-plays-python', 'https://www.kaggle.com/code/iwanowtf/decision-time-matters', 'https://www.kaggle.com/code/nickwan/calculating-distance-from-other-players-football']"
1,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.
First, the series is getting upgraded branding. We've dropped ""Tabular"" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.
Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.
Regardless of these changes, the goals of the Playground Series remain the same—to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!
With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict the value for the target booking_status. The file should contain a header and have the following format:
id,booking_status
42100,0
42101,1
42102,0
etc.","The dataset for this competition (both train and test) was generated from a deep learning model trained on the Reservation Cancellation Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Files
train.csv - the training dataset; booking_status is the target (e.g., whether the reservation was cancelled)
test.csv - the test dataset; your objective is to predict booking_status
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e7,"['https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e7-2023-eda-and-submission', 'https://www.kaggle.com/code/jcaliz/ps-s03e07-a-complete-eda', 'https://www.kaggle.com/code/kimtaehun/simple-eda-with-lgmb-baseline', 'https://www.kaggle.com/code/alexandershumilin/ps-s3-e7-ensemble-model', 'https://www.kaggle.com/code/martynovandrey/ps-3-7-eda-cv-weights-optimizer-15th-place']"
2,"Goal of the Competition
More than a hundred AI papers are published every day, making it exceedingly hard to keep up with current innovations. The goal of this competition is to tap into the diverse expertise of the Kaggle community to centralize and summarize the rapid advancements in AI from the past two years. The Kaggle community has a breadth and depth of AI experience which extends beyond the reach of any single individual or research group. We aim to share your collective perspective with the broader research community.
Context
In this analytics competition, participants will write an essay on one of the following seven topics, with a prompt to describe what the community has learned over the past 2 years of working and experimenting with:
Text data
Image and/or video data
Tabular and/or time series data
Kaggle Competitions
Generative AI
AI ethics
Other (anything that does not fall into any other category)
Kaggle has provided a collection of writeups and a collection of scholarly articles in the official competition dataset, but participants are free to use any resources. Participants will also take part in a peer feedback process that involves reviewing three peer essays, and your submission will be assessed by an expert grading panel of seven Kaggle Grandmasters. Winning essays will be aggregated in a post-competition publication and given authorship credit.
In response to demand and feedback from prior analytics competitions, this competition also pilots a format in which all eligible participants will be ranked on the leaderboard and awarded points based on their performance. Results will show on your profile, just as in a standard competition.
See the submission instructions and evaluation pages for more detail.","Submissions to the 2023 Kaggle AI Report competition will be assessed by an expert grading panel of seven Kaggle Grandmasters. Each of the seven topic areas has a dedicated grader who will be responsible for applying the evaluation rubric to essays submitted to their section. The Kaggle Team will also participate in the evaluation process in order to ensure a fair and standardized evaluation process. In the event of tied scores, the judging panel will stack rank submissions within the tied group.
To learn more about how your notebook will be assessed, see the resources below:
The eligibility checklist - this checklist provides guidance on how to structure your essays. If you follow these guidelines, your submission will be considered for the prize.
The essay-grading rubric - this rubric will be used to score and rank eligible essays.
The eligibility checklist
Rules Compliance:
The submission was consistent with the guidelines and instructions.
The essay was published as a public notebook on Kaggle, and the notebook was attached to the official competition dataset [yes/no]
The essay was relevant to one of the 7x pre-defined topics [yes/no]
The author(s) provided high quality feedback on 3x peer essays [yes/no]
Narrative Quality:
The essay contained high quality writing with excellent composition.
The overall composition was high quality (e.g. articulate, concise, accurate, and easy-to-understand) [yes/no]
The essay did not contain any content that could be considered inappropriate (e.g. plagiarism, academic misconduct, or incorrect statements) [yes/no]
The way the essay was structured made the essay clear and accessible [yes/no]
Narrative Background:
The essay provided appropriate background and context concerning the topic of interest.
Contained an overview of the relevant topic of interest [yes/no]
The background context provided sufficient & accessible information to understand the main essay as a non expert [yes/no]
Narrative Accuracy:
The essay accurately summarized what we have learned over the past few years about the topic of interest.
Contained discussion of the most important recent advancements that are relevant to their specific topic [yes/no]
The discussion of the recent advances was thorough, detailed, and accurate [yes/no]
Narrative References:
The essay contained references and citations where appropriate.
The essay contained appropriate references (e.g. academic literature, Kaggle Competition writeups, and similar) [yes/no]
The references within this essay were thorough and detailed [yes/no]
Supplementary Materials:
The narrative was supported by supplemental tables, figures, and/or code samples.
Contained one or more tables, figures and/or code samples [yes/no]
The tables, figures, and/or code samples were high quality (e.g. information-dense, easy-to-understand, and informative) [yes/no]
The essay grading rubric
Rules Compliance:
The submission was consistent with the guidelines and instructions.
The author(s) provided high quality feedback on 3x peer essays [Required]
Narrative Quality:
The essay contained high quality writing with excellent composition.
The overall composition was high quality (e.g. articulate, concise, accurate, and easy-to-understand) [10pts]
Narrative Background:
The essay provided appropriate background and context concerning the topic of interest.
The background context provided sufficient & accessible information to understand the main essay as a non expert [10pts]
Narrative Accuracy:
The essay accurately summarized what we have learned over the past few years about the topic of interest.
The discussion of the recent advances was thorough, detailed, and accurate [10pts]
Narrative References:
The essay contained references and citations where appropriate.
The references within this essay were thorough and detailed [10pts]
Supplementary Materials:
The narrative was supported by supplemental tables, figures, and/or code samples.
The tables, figures, and/or code samples were high quality (e.g. information-dense, easy-to-understand, and informative) [10pts]","Participants are encouraged to reference resources such as winning solution writeups and scholarly articles. We have included a collection of writeups and scholarly articles for your convenience, but participants are free to use additional resources as well. There is no requirement to refer to any of these files using code.
kaggle_writeups.csv - metadata from recent Kaggle Competition Writeups
Source: https://www.kaggle.com/datasets/kaggle/meta-kaggle
arXiv_metadata.json - metadata for recent arXiv publications
Source: https://www.kaggle.com/datasets/Cornell-University/arxiv
sample_submission.csv -a template file that you can modify for making your submission",kaggle competitions download -c 2023-kaggle-ai-report,"['https://www.kaggle.com/code/leonidkulyk/kaggle-ai-report-topic-selection', 'https://www.kaggle.com/code/thedrcat/how-to-win-a-kaggle-competition', 'https://www.kaggle.com/code/iamleonie/towards-green-ai', 'https://www.kaggle.com/code/muhammadirfanakbar/data-centric-ai-the-underdog-game-changer-in-ai', 'https://www.kaggle.com/code/tamlhp/machine-unlearning-the-right-to-be-forgotten']"
3,"Goal of the Competition
Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our ninth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's college basketball tournaments. Unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.
Context
You are provided data of historical NCAA games to forecast the outcomes of the Division 1 Men's and Women's basketball tournaments. This competition is the official 2023 edition, with points, medals, prizes, and basketball glory at stake.
We have made several updates to the competition format compared to prior editions:
There is a change in evaluation metric from log loss to Brier scores. See the Evaluation Page for full details.
We are combining the Men's and Women's tournament into one single competition, instead of running separate tracks. The competition will award full points/medals as a result.
We have changed the prediction format so that you may forecast the 2023 tournaments right away, instead of having to wait to see which teams are selected for the tournament.
We have also launched a companion warmup competition, which is setup as a practice leaderboard covering the previous five tournaments. Because its only for practice and the ground historical game outcomes are public information, the warmup competition does not count for points/medals and will be taken down once it has served its purpose. Prior to the start of the tournaments, the leaderboard of this competition will reflect all zero scores. Kaggle will periodically fill in the outcomes and rescore once games begin.
Good luck and happy forecasting!","The evaluation methodology for 2023 has changed from prior editions of this competition. Submissions are now evaluated on the Brier score between the predicted probabilities and the actual game outcomes (this is equivalent to mean squared error in this context). This change was made to reduce the competitive ""distractions"" caused by the 0 and 1 boundaries of the previous log-loss metric (e.g. submitting rounded predictions to gamble on a given upset, or caring deeply about the 0.99 vs 0.999 distinction that log loss would reward/punish).
Submission File
The submission file format also has a revised format for 2023:
We have combined the Men's and Women's tournaments into one single competition. Your submission file should contain predictions for both.
You will now be predicting the hypothetical results for every possible team matchup, not just teams that are selected for the NCAA tournament. This change was enacted to provide a longer time window to submit predictions for the 2023 tournament. Previously, the short time between Selection Sunday and the tournament tipoffs would require participants to quickly turn around updated predictions. By forecasting every possible outcome between every team, you can now submit a valid prediction at any point leading up to the tournaments.
You may submit as many times as you wish before the tournaments start, but make sure to select the two submissions you want to count towards scoring. Do not rely on automatic selection to pick your submissions, as there is no public leaderboard score and the system will select your earliest two submissions.
As with prior years, each game has a unique ID created by concatenating the season in which the game was played and the two team's respective TeamIds. For example, ""2023_1101_1102"" indicates a hypothetical matchup between team 1101 and 1102 in the year 2023. You must predict the probability that the team with the lower TeamId beats the team with the higher TeamId. Note that the men's teams and women's TeamIds do not overlap.
The resulting submission format looks like the following, where Pred represents the predicted probability that the first team will win:
ID,Pred
2023_1101_1102,0.5
2023_1101_1103,0.5
2023_1101_1104,0.5
...
Your 2023 submissions will score 0.0 if you have submitted predictions in the right format. The leaderboard of this competition will be only meaningful once the 2023 tournaments begin and Kaggle rescores your predictions!","Each season there are thousands of NCAA basketball games played between Division I college basketball teams, culminating in March Madness®, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness® game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.
If you are unfamiliar with the format and intricacies of the NCAA® tournament, we encourage reading the wikipedia pages for the men's and women's tournaments before before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears.
Please note that in previous years, there were separate competitions for predicting the men's tournament games or the women's tournament games. In this year's competition, you will be submitting combined prediction files that include predictions for both the men's tournament and the women's tournament. Thus the data files incorporate both men's data and women's data. The files that pertain only to men's data will start with the letter prefix M, and the files that pertain only to women's data will start with the letter prefix W. Some files span both men's and women's data, such as Cities and Conferences, and these files do not start with an M prefix or a W prefix.
As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings and WTeamSpellings files, which are listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.",kaggle competitions download -c march-machine-learning-mania-2023,"['https://www.kaggle.com/code/theoviel/it-s-that-time-of-the-year-again', 'https://www.kaggle.com/code/benjenkins96/deep-learning-techniques-to-predict-march-madness', 'https://www.kaggle.com/code/nishaanamin/predicting-upsets', 'https://www.kaggle.com/code/benjenkins96/march-mania-finding-the-signal-before-the-buzzer', 'https://www.kaggle.com/code/toshimelonhead/ncaa-march-madness-sabermetric-spin-v2']"
4,"Goal of the Competition
Contrails are clouds of ice crystals that form in aircraft engine exhaust. They can contribute to global warming by trapping heat in the atmosphere. Researchers have developed models to predict when contrails will form and how much warming they will cause. However, they need to validate these models with satellite imagery.
Your work will help researchers improve the accuracy of their contrail models. This will help airlines avoid creating contrails and reduce their impact on climate change.
Context
Contrail avoidance is potentially one of the most scalable, cost-effective sustainability solutions available to airlines today. Contrails, short for ‘condensation trails’, are line-shaped clouds of ice crystals that form in aircraft engine exhaust, and are created by airplanes flying through super humid regions in the atmosphere. Persistent contrails contribute as much to global warming as the fuel they burn for flights. This phenomenon was first figured out 75 years ago by military airplanes with the goal to avoid leaving a visible trail. About 30 years ago, climate scientists in Europe began to understand that the contrails blocked heat that normally is released from the earth overnight. They have built powerful models, based on weather data, to identify when contrails will form and how warming they will be. Their research has been validated by other labs as well and it is now well accepted that contrails contribute approximately 1% of all human caused global warming. The motivation behind the use of satellite imagery is to empirically confirm the predictions from these models. With reliable verification, pilots can have confidence in the models and the airline industry can have a trusted way to measure successful contrail avoidance.

Image courtesy of Imperial College
Your work will quantifiably improve the confidence in prediction of contrail forming regions and the techniques to avoid creating them.
Google Research applies ML to opportunities to mitigate climate change and adapt to the changes we already see. We have run research projects in fusion energy plasma modeling, wildfire early detection, optimal car routing, and forecasts for climate disasters.
Acknowledgments
MIT Laboratory for Aviation and the Environment led by MIT Professor Steven Barrett
Satellite images are from NOAA GOES-16, see more in https://www.goes-r.gov/.

This is a Code Competition. Refer to Code Requirements for details.","This competition is evaluated on the global Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:
$$ \frac{2 * |X \cap Y|}{|X| + |Y|},$$
where X is the entire set of predicted contrail pixels for all observations in the test data and Y is the ground truth set of all contrail pixels in the test data.
Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.  Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length, e.g., '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
Note that, at the time of encoding, the mask should be binary, meaning the masks for all objects in an image are joined into a single large mask. A value of 0 should indicate pixels that are not masked, and a value of 1 will indicate pixels that are masked.
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
IMPORTANT: Empty predictions must be marked with '-' in the submission file. Code to correctly encode and decode run-lengh encoding can be found in this notebook.
The file should contain a header and have the following format:
record_id,encoded_pixels
1000834164244036115,1 1 5 1
1002653297254493116,-
etc.","In this competition you will be using geostationary satellite images to identify aviation contrails. The original satellite images were obtained from the GOES-16 Advanced Baseline Imager (ABI), which is publicly available on Google Cloud Storage. The original full-disk images were reprojected using bilinear resampling to generate a local scene image. Because contrails are easier to identify with temporal context, a sequence of images at 10-minute intervals are provided. Each example (record_id) contains exactly one labeled frame.
Learn more about the dataset from the preprint: OpenContrails: Benchmarking Contrail Detection on GOES-16 ABI. Labeling instructions can be found at in this supplementary material. Some key labeling guidance:
Contrails must contain at least 10 pixels
At some time in their life, Contrails must be at least 3x longer than they are wide
Contrails must either appear suddenly or enter from the sides of the image
Contrails should be visible in at least two image
Ground truth was determined by (generally) 4+ different labelers annotating each image. Pixels were considered a contrail when >50% of the labelers annotated it as such. Individual annotations (human_individual_masks.npy) as well as the aggregated ground truth annotations (human_pixel_masks.npy) are included in the training data. The validation data only includes the aggregated ground truth annotations.
This is an example of labeled contrails. Code to produce images like this can be found in this notebook.",kaggle competitions download -c google-research-identify-contrails-reduce-global-warming,"['https://www.kaggle.com/code/inversion/contrails-rle-submission', 'https://www.kaggle.com/code/inversion/visualizing-contrails', 'https://www.kaggle.com/code/bencetar/contrail-segmentation-tensorflow', 'https://www.kaggle.com/code/nberretti/identifying-contrails-initial-model-local', 'https://www.kaggle.com/code/robindiligent/exploring-dataset']"
5,"Goal of the Competition
The goal of this competition is to predict if a person has any of three medical conditions. You are being asked to predict if the person has one or more of any of the three medical conditions (Class 1), or none of the three medical conditions (Class 0). You will create a model trained on measurements of health characteristics.
To determine if someone has these medical conditions requires a long and intrusive process to collect information from patients. With predictive models, we can shorten this process and keep patient details private by collecting key characteristics relative to the conditions, then encoding these characteristics.
Your work will help researchers discover the relationship between measurements of certain characteristics and potential patient conditions.
Context
They say age is just a number but a whole host of health issues come with aging. From heart disease and dementia to hearing loss and arthritis, aging is a risk factor for numerous diseases and complications. The growing field of bioinformatics includes research into interventions that can help slow and reverse biological aging and prevent major age-related ailments. Data science could have a role to play in developing new methods to solve problems with diverse data, even if the number of samples is small.
Currently, models like XGBoost and random forest are used to predict medical conditions yet the models' performance is not good enough. Dealing with critical problems where lives are on the line, models need to make correct predictions reliably and consistently between different cases.
Founded in 2015, competition host InVitro Cell Research, LLC (ICR) is a privately funded company focused on regenerative and preventive personalized medicine. Their offices and labs in the greater New York City area offer state-of-the-art research space. InVitro Cell Research's Scientists are what set them apart, helping guide and defining their mission of researching how to repair aging people fast.
In this competition, you’ll work with measurements of health characteristic data to solve critical problems in bioinformatics. Based on minimal training, you’ll create a model to predict if a person has any of three medical conditions, with an aim to improve on existing methods.
You could help advance the growing field of bioinformatics and explore new methods to solve complex problems with diverse data.
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated using a balanced logarithmic loss. The overall effect is such that each class is roughly equally important for the final score.
Each observation is either of class 0 or of class 1. For each observation, you must submit a probability for each class. The formula is then:
$$\text{Log Loss} = \frac{-\frac{1}{N_{0}} \sum_{i=1}^{N_{0}} y_{0 i} \log p_{0 i} - \frac {1}{N_{1}} \sum_{i=1}^{N_{1}} y_{1 i} \log p_{1 i} } { 2 } $$
where (N_{c}) is the number of observations of class (c), (\log) is the natural logarithm, (y_{c i}) is 1 if observation (i) belongs to class (c) and 0 otherwise, (p_{c i}) is the predicted probability that observation (i) belongs to class (c).
The submitted probabilities for a given row are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, each predicted probability \(p\) is replaced with \(\max(\min(p,1-10^{-15}),10^{-15})\).
Submission File
For each id in the test set, you must predict a probability for each of the two classes. The file should contain a header and have the following format:
Id,class_0,class_1
00eed32682bb,0.5,0.5
010ebe33f668,0.5,0.5
02fa521e1838,0.5,0.5
040e15f562a2,0.5,0.5
046e85c7cc7f,0.5,0.5
...","The competition data comprises over fifty anonymized health characteristics linked to three age-related conditions. Your goal is to predict whether a subject has or has not been diagnosed with one of these conditions -- a binary classification problem.
Note that this is a Code Competition, in which the actual test set is hidden. In this version, we give some sample data in the correct format to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. There are about 400 rows in the full test set.
Files and Field Descriptions
train.csv - The training set.
Id Unique identifier for each observation.
AB-GL Fifty-six anonymized health characteristics. All are numeric except for EJ, which is categorical.
Class A binary target: 1 indicates the subject has been diagnosed with one of the three conditions, 0 indicates they have not.
test.csv - The test set. Your goal is to predict the probability that a subject in this set belongs to each of the two classes.
greeks.csv - Supplemental metadata, only available for the training set.
Alpha Identifies the type of age-related condition, if present.
A No age-related condition. Corresponds to class 0.",kaggle competitions download -c icr-identify-age-related-conditions,"['https://www.kaggle.com/code/gusthema/identifying-age-related-conditions-w-tfdf', 'https://www.kaggle.com/code/raddar/icr-competition-analysis-and-findings', 'https://www.kaggle.com/code/tetsutani/icr-iarc-eda-ensemble-and-stacking-baseline', 'https://www.kaggle.com/code/datafan07/icr-simple-eda-baseline', 'https://www.kaggle.com/code/mateuszk013/icr-eda-balanced-learning-with-lgbm-xgb']"
6,"Goal of the Competition
The goal of this competition is to reverse the typical direction of a generative text-to-image model: instead of generating an image from a text prompt, can you create a model which can predict the text prompt given a generated image? You will make predictions on a dataset containing a wide variety of (prompt, image) pairs generated by Stable Diffusion 2.0, in order to understand how reversible the latent relationship is.
Context
The popularity of text-to-image models has spurned an entire new field of prompt engineering. Part art and part unsettled science, ML practitioners and researchers are rapidly grappling with understanding the relationships between prompts and the images they generate. Is adding ""4k"" to a prompt the best way to make it more photographic? Do small perturbations in prompts lead to highly divergent images? How does the order of prompt keywords impact the resulting generated scene? This competition tasks you with creating a model that can reliably invert the diffusion process that generated to a given image.
In order to calculate prompt similarity in a robust way—meaning that ""epic cat"" is scored as similar to ""majestic kitten"" in spite of character-level differences—you will submit embeddings of your predicted prompts. Whether you model the embeddings directly or first predict prompts and then convert to embeddings is up to you! Good luck, and may you create ""highly quality, sharp focus, intricate, detailed, in the style of unreal robust cross validation"" models herein.
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated using the mean cosine similarity score between the predicted and actual prompt embedding vectors. The precise details of how embeddings are calculated for the ground truth prompts are found in this notebook.
Submission File
For each image in the test set, you must predict the prompt that was used to generate the image and convert the prompt into a 384-length embedding vector. Predictions should be flattened to rows of image (imgId) and embedding (eId) pairs (imgId_eId). The file should contain a header and have the following format:
imgId_eId,val
20057f34d_0,0.018848453
20057f34d_1,0.030189732
....
20057f34d_383,-0.007934112
227ef0887_0,0.017384542
etc.","Your task for this challenge is to predict the prompts that were used to generate target images. Prompts for this challenge were generated using a variety of (non disclosed) methods, and range from fairly simple to fairly complex with multiple objects and modifiers. Images were generated from the prompts using Stable Diffusion 2.0 (768-v-ema.ckpt) and were generated with 50 steps at 768x768 px and then downsized to 512x512 for the competition dataset. (This script was used, with the majority of default parameters unchanged.)
Files
images/ - images generated from prompts; your task it to predict the prompt that was used to generate each image in this folder. The hidden re-run test folder contains approximately 16,000 images.
prompts.csv - the prompts that were used to create the samples in the images/ folder. These are provided as illustrative examples only. It is up to each competitor to develop their own strategy of creating a training set of images, using pre-trained models, etc. Note that this file is not contained in the re-run test set, and thus referencing it in a Notebook submission will result in a failure.
sample_submission.csv - a sample submission file in the correct format. The values found in this file are embeddings of the prompts in the prompts.csv file and thus can be used validate your embedding pipeline. This notebook demonstrates how to calculate embeddings.",kaggle competitions download -c stable-diffusion-image-to-prompts,"['https://www.kaggle.com/code/inversion/calculating-stable-diffusion-prompt-embeddings', 'https://www.kaggle.com/code/inversion/stable-diffusion-sample-submission', 'https://www.kaggle.com/code/inversion/sd-infinite-loop', 'https://www.kaggle.com/code/leonidkulyk/lb-0-45836-blip-clip-clip-interrogator', 'https://www.kaggle.com/code/vad13irt/language-models-pre-training']"
7,"Goal of the Competition
The goal of this competition is to detect and translate American Sign Language (ASL) fingerspelling into text. You will create a model trained on the largest dataset of its kind, released specifically for this competition. The data includes more than three million fingerspelled characters produced by over 100 Deaf signers captured via the selfie camera of a smartphone with a variety of backgrounds and lighting conditions.
Your work may help move sign language recognition forward, making AI more accessible for the Deaf and Hard of Hearing community.
💡Getting Started Notebook
To get started quickly, feel free to take advantage of this starter notebook.
Context
Voice-enabled assistants open the world of useful and sometimes life-changing features of modern devices. These revolutionary AI solutions include automated speech recognition (ASR) and machine translation. Unfortunately, these technologies are often not accessible to the more than 70 million Deaf people around the world who use sign language to communicate, nor to the 1.5+ billion people affected by hearing loss globally.
Fingerspelling uses hand shapes that represent individual letters to convey words. While fingerspelling is only a part of ASL, it is often used for communicating names, addresses, phone numbers, and other information commonly entered on a mobile phone. Many Deaf smartphone users can fingerspell words faster than they can type on mobile keyboards. In fact, ASL fingerspelling can be substantially faster than typing on a smartphone’s virtual keyboard (57 words/minute average versus 36 words/minute US average). But sign language recognition AI for text entry lags far behind voice-to-text or even gesture-based typing, as robust datasets didn't previously exist.
Technology that understands sign language fits squarely within Google's mission to organize the world's information and make it universally accessible and useful. Google’s AI principles also support this idea and encourage Google to make products that empower people, widely benefit current and future generations, and work for the common good. This collaboration between Google and the Deaf Professional Arts Network will explore AI solutions that can be scaled globally (such as other sign languages), and support individual user experience needs while interacting with products.
Your participation in this competition could help provide Deaf and Hard of Hearing users the option to fingerspell words instead of using a keyboard. Besides convenient text entry for web search, map directions, and texting, there is potential for an app that can then translate this input using sign language-to-speech technology to speak the words. Such an app would enable the Deaf and Hard of Hearing community to communicate with hearing non-signers more quickly and smoothly.
This is a Code Competition. Refer to Code Requirements for details.","The evaluation metric for this contest is the normalized total levenshtein distance. Let the total number of characters in all of the labels be N and the total levenshtein distance be D. The metric equals (N - D) / N.
Submission Process
In this competition you will be submitting a TensorFlow Lite model file. The model must take one or more landmark frames as an input and return a float vector (the predicted probabilities of each sign class) as the output. Your model must be packaged into a submission.zip file and compatible with the TensorFlow Lite Runtime v2.14.0, which we are running using Python 3.10. You are welcome to train your model using the framework of your choice, as long as you convert the model checkpoint into the tflite format prior to submission.
Your model must also perform inference in less than 5 hours and use less than 40 MB of storage space. Expect to see approximately 35 hours of video in the test set.
Each video is loaded with the following function:
def load_relevant_data_subset(pq_path):
    return pd.read_parquet(pq_path, columns=selected_columns)
If you want to load only a subset of the landmarks, include a file named inference_args.json in your submission.zip with the field selected_columns containing a list of the landmark columns you want to use. If that is not included we will load all columns.
Inference is performed (roughly) as follows, ignoring details like how we manage multiple videos:
import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path)

REQUIRED_SIGNATURE = ""serving_default""
REQUIRED_OUTPUT = ""outputs""

with open (""/kaggle/input/fingerspelling-character-map/character_to_prediction_index.json"", ""r"") as f:
    character_map = json.load(f)
rev_character_map = {j:i for i,j in character_map.items()}

found_signatures = list(interpreter.get_signature_list().keys())

if REQUIRED_SIGNATURE not in found_signatures:
    raise KernelEvalException('Required input signature not found.')

prediction_fn = interpreter.get_signature_runner(""serving_default"")
output = prediction_fn(inputs=frames)
prediction_str = """".join([rev_character_map.get(s, """") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])","The goal of this competition is to detect and translate American Sign Language (ASL) fingerspelling into text.
This competition requires submissions to be made in the form of TensorFlow Lite models. You are welcome to train your model using the framework of your choice as long as you convert the model checkpoint into the tflite format prior to submission. Please see the evaluation page for details.
Files
[train/supplemental_metadata].csv
path - The path to the landmark file.
file_id - A unique identifier for the data file.
participant_id - A unique identifier for the data contributor.
sequence_id - A unique identifier for the landmark sequence. Each data file may contain many sequences.
phrase - The labels for the landmark sequence. The train and test datasets contain randomly generated addresses, phone numbers, and urls derived from components of real addresses/phone numbers/urls. Any overlap with real addresses, phone numbers, or urls is purely accidental. The supplemental dataset consists of fingerspelled sentences. Note that some of the urls include adult content. The intent of this competition is to support the Deaf and Hard of Hearing community in engaging with technology on an equal footing with other adults.",kaggle competitions download -c asl-fingerspelling,"['https://www.kaggle.com/code/gusthema/asl-fingerspelling-recognition-w-tensorflow', 'https://www.kaggle.com/code/leonidkulyk/eda-aslfr-animated-visualization', 'https://www.kaggle.com/code/markwijkhuizen/aslfr-transformer-training-inference', 'https://www.kaggle.com/code/irohith/aslfr-transformer', 'https://www.kaggle.com/code/irohith/aslfr-ctc-based-on-prev-comp-1st-place']"
8,"Welcome to Kaggle's annual Machine Learning and Data Science Survey competition! You can read our executive summary here.
This year, as in 2017, 2018, 2019, 2020, and 2021, we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live from 09/16/2022 to 10/16/2022, and after cleaning the data we finished with 23,997 responses!
There's a lot to explore here. The results include raw numbers about who is working with data, what’s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published all of the data rather than only the aggregated survey results, which makes it an unusual example of a survey dataset, as it allows analysts to investigate the data on their own.
This year Kaggle is once again launching an annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community.
In our sixth year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey, and how they have changed from year over year. For that reason, we’re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world.
The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A “story” could be defined any number of ways, and that’s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!
Submissions will be evaluated on the following:
Composition - Is there a clear narrative thread to the story that’s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.
Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.
Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible
To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid. You can make your submission by filling out the submission form.","How to Participate
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will review the last (most recent) entry. Please try to avoid submitting multiple times.
Submissions will be evaluated on the following:
Composition - Is there a clear narrative thread to the story that’s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.
Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.
Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible
To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid.
To make a submission, complete the submission form.","Main Data:
kaggle_survey_2022_responses.csv: 43 questions and 23,997 responses
Responses to multiple choice questions (only a single choice can be selected)
were recorded in individual columns. Responses to multiple selection questions
(multiple choices can be selected) were split into multiple columns (with one
column per answer choice).
Supplementary Data:
kaggle_survey_2022_answer_choices.pdf: list of answer choices for every question
With footnotes describing which questions were asked to which respondents.
kaggle_survey_2022_methodology.pdf: a description of how the survey was conducted
You can ask additional questions by posting in the pinned Q&A thread.",kaggle competitions download -c kaggle-survey-2022,"['https://www.kaggle.com/code/michau96/15-factors-for-data-science-in-your-country', 'https://www.kaggle.com/code/eraikako/data-science-and-mlops-landscape-in-industry', 'https://www.kaggle.com/code/narsil/data-science-salary-benchmarks', 'https://www.kaggle.com/code/spitfire2nd/the-state-of-low-no-code-in-data', 'https://www.kaggle.com/code/imuhammad/the-fellowship-of-kaggle']"
9,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.
First, the series is getting upgraded branding. We've dropped ""Tabular"" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.
Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.
Regardless of these changes, the goals of the Playground Series remain the same—to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!
With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Root Mean Squared Error (RMSE)
Submissions are scored on the root mean squared error. RMSE is defined as:
$$
\textrm{RMSE} = \sqrt{ \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 }
$$
where \( \hat{y}_i \) is the predicted value and \( y_i \) is the original value for each instance \(i\).
Submission File
For each id in the test set, you must predict the value for the target price. The file should contain a header and have the following format:
id,price
22709,200689.01
22710,398870.92
22711,1111145.11
etc.","The dataset for this competition (both train and test) was generated from a deep learning model trained on the Paris Housing Price Prediction. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Files
train.csv - the training dataset; price is the target
test.csv - the test dataset; your objective is to predict price
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e6,"['https://www.kaggle.com/code/abhi011097/ps-s3-e6-complete-eda-model-evaluation', 'https://www.kaggle.com/code/manthanx/ps3e6-xgboost-feature-selection-ensemble', 'https://www.kaggle.com/code/mattop/analysis-of-every-playground-series-episode', 'https://www.kaggle.com/code/jonbown/cluster-feature-engineering-tps3e6', 'https://www.kaggle.com/code/faisaljanjua0555/playground-series-s3-e6-eda-modelling']"
10,"NOTE:
You can now create your own synthetic versions of this dataset by forking and running this notebook.
Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!
With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in May every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Evaluation
Submissions will be evaluated using Mean Absolute Error (MAE),
where each x_i represents the predicted target, y_i represents the ground truth, and n is the number of rows in the test set.
Submission File
For each id in the test set, you must predict the target yield. The file should contain a header and have the following format:
id,yield
15289,6025.194
15290,1256.223
15291,357.44
etc.","NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook.
The dataset for this competition (both train and test) was generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. (Since this is Playground 3.14, it seems like we need a Blueberry Pie joke here?) Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Files
train.csv - the training dataset; yield is the target
test.csv - the test dataset; your objective is to predict the yield given the other features
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e14,"['https://www.kaggle.com/code/inversion/make-synthetic-blueberry-yield-data', 'https://www.kaggle.com/code/tetsutani/ps3e14-eda-various-models-ensemble-baseline', 'https://www.kaggle.com/code/kimtaehun/simple-eda-and-baseline-in-2mintues', 'https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e14-2023-first-place-winning-solution', 'https://www.kaggle.com/code/ravi20076/playgrounds3e14-eda-baseline']"
11,"Goal of the Competition
The goal of this competition is to find continuous gravitational-wave signals. You will develop a model sensitive enough to detect weak yet long-lasting signals emitted by rapidly-spinning neutron stars within noisy data.
Your work will help scientists detect something new: a second class of gravitational waves! The first gravitational wave discoveries earned a Nobel Prize. Further study of these waves may enable scientists to learn about the structure of the most extreme stars in our universe.
Context
When scientists detected the first class of gravitational waves in 2015, they expected the discoveries to continue. There are four classes, yet at present only signals from merging black holes and neutron stars have been detected. Among those remaining are continuous gravitational-wave signals. These are weak yet long-lasting signals emitted by rapidly-spinning neutron stars. Imagine the mass of our Sun but condensed into a ball the size of a city and spinning over 1,000 times a second. The extreme compactness of these stars, composed of the densest material in the universe, could allow continuous waves to be emitted and then detected on Earth. There are potentially many continuous signals from neutron stars in our own galaxy and the current challenge for scientists is to make the first detection, and hopefully data science can help with this mission.
This image, taken from a 2021 paper by the LIGO-Virgo-KAGRA collaboration, shows the maximum amplitude of a continuous wave any of these neutron stars could emit without being found by the search analyses. Circled stars show results constraining the physical properties of specific neutron stars. Traditional approaches to detecting these weak and hard-to-find continuous signals are based on matched-filtering variants. Scientists create a bank of possible signal waveform templates and ask how correlated each waveform is with the measured noisy data. High correlation is consistent with the presence of a signal similar to that waveform. Due to the long duration of these signals, banks could easily contain hundreds of quintillions of templates; yet, with so many possible waveforms, scientists don’t have the computational power to use the approach without making approximations that weaken the sensitivity to the signals.
G2Net is a network of Gravitational Wave, Geophysics and Machine Learning. Via an Action from COST (European Cooperation in Science and Technology), a funding agency for research and innovation networks, G2Net aims to create a broad network of scientists. From four different areas of expertise, namely GW physics, Geophysics, Computing Science and Robotics, these scientists have agreed on a common goal of tackling challenges in data analysis and noise characterization for GW detectors.
By helping G2Net in this challenge you'll enable scientists to improve their sensitivity, leading to new discoveries in the field. As a result, scientists could learn more about the structure of the most extreme stars in our universe.
Resources
Resources for the generation of background noise and continuous gravitational-wave signals can be found in this pinned discussion. A brief notebook summarizing the very basics of generating data using PyFstat is also provided.
Acknowledgments
We acknowledge support from the LIGO-Virgo-Kagra Collaboration of which the hosts are members. Specifically we acknowledge the use of the Gravitational Wave Open Science Centre (GWOSC) and the software resources lalsuite and PyFstat.
This challenge can be cited using the BibTeX entry attached below, should any of the results
here generated be useful for any specific research results:
@techreport{G2NetCWKaggleChallenge,
    author = ""Tenorio, Rodrigo and Williams, Michael J. and Messenger, Chris"",
    title = ""Learning to detect continuous gravitational waves"",
    number = ""LIGO-P2200295"",
    url = {https://dcc.ligo.org/P2200295},
    year = ""2022""
}
   ","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
id,target
00054c878,0.5
0007285a3,0.5
00076c5a6,0.5
etc.","In this competition, you are provided with a training set containing time-frequency data from two gravitational-wave interferometers (LIGO Hanford & LIGO Livingston). Each data sample contains either real or simulated noise and possibly a simulated continuous gravitational-wave signal (CW). The task is to identify when a signal is present in the data (target=1).
Each sample is comprised of a set of Short-time Fourier Transforms (SFTs) and corresponding GPS time stamps for each interferometer. The SFTs are not always contiguous in time, since the interferometers are not continuously online.
The simulated signals are present throughout the entire duration of the set of SFTs in both detectors. The signals are characterised by the location and orientation of the hypothetical astrophysical source as well as two intrinsic parameters: frequency and spin-down. In total there are eight parameters which have all been randomised. These are not provided as part of the data. The typical amplitudes of the resulting signals are one or two orders of magnitude lower than the amplitude of the detector noise.
Further resources regarding the specifics of the data format can be found in this pinned discussion.
Files
[train/|test/] - folders containing the training and test files; Note: a small example set of training data has been provided, although it is expected that competitors will need to generate additional data for training as described here.
train_labels.csv - a file containing the target labels; 1 if the data contains the presence of a gravitational wave, 0 otherwise. (Please note the presence of a small number of files labeled -1. Physicists are currently unable to determine the status of these files.)",kaggle competitions download -c g2net-detecting-continuous-gravitational-waves,"['https://www.kaggle.com/code/rodrigotenorio/generating-continuous-gravitational-wave-signals', 'https://www.kaggle.com/code/essammohamed4320/g2net-time-lstm-vs-frequency-vit-cnn-domain', 'https://www.kaggle.com/code/junkoda/basic-spectrogram-image-classification', 'https://www.kaggle.com/code/edwardcrookenden/g2net-getting-started-eda', 'https://www.kaggle.com/code/vslaykovsky/g2net-pytorch-generated-realistic-noise']"
12,"Goal of the Competition
The goal of this competition is to predict student performance during game-based learning in real-time. You'll develop a model trained on one of the largest open datasets of game logs.

Your work will help advance research into knowledge-tracing methods for game-based learning. You'll be supporting developers of educational games to create more effective learning experiences for students.
Context
Learning is meant to be fun, which is where game-based learning comes in. This educational approach allows students to engage with educational content inside a game framework, making it enjoyable and dynamic. Although game-based learning is being used in a growing number of educational settings, there are still a limited number of open datasets available to apply data science and learning analytic principles to improve game-based learning.
Most game-based learning platforms do not sufficiently make use of knowledge tracing to support individual students. Knowledge tracing methods have been developed and studied in the context of online learning environments and intelligent tutoring systems. But there has been less focus on knowledge tracing in educational games.
Competition host Field Day Lab is a publicly-funded research lab at the Wisconsin Center for Educational Research. They design games for many subjects and age groups that bring contemporary research to the public, making use of the game data to understand how people learn. Field Day Lab's commitment to accessibility ensures all of its games are free and available to anyone. The lab also partners with nonprofits like The Learning Agency Lab, which is focused on developing science of learning-based tools and programs for the social good.
If successful, you'll enable game developers to improve educational games and further support the educators who use these games with dashboards and analytic tools. In turn, we might see broader support for game-based learning platforms.
Acknowledgments
Field Day Lab and the Learning Agency Lab would like to thank the Walton Family Foundation and Schmidt Futures for making this work possible.
                

This is a Code Competition. Refer to Code Requirements for details.","Submissions will be evaluated based on their F1 score.
Submission File
For each session_id / question number pair in the test set, you must predict a binary label for the correct variable as described in the data page.
Note that the sample_submission.csv provided for your usage also includes a grouping variable, session_level, that groups the questions by session and level. This is handled automatically by the timeseries API, so when making predictions you will not have access to this column.
The timeseries API presents the questions and data to you in order of levels - level segments 0-4, 5-12, and 13-22 are each provided in sequence, and you will be predicting the correctness of each segment's questions as they are presented.
The file should contain a header and have the following format:
session_id,correct
20090109393214576_q1,0
20090312143683264_q1,0
20090312331414616_q1,0
20090109393214576_q2,0
20090312143683264_q2,0
20090312331414616_q2,0
20090109393214576_q3,0
20090312143683264_q3,0
20090312331414616_q3,0
...","This competition uses the Kaggle's time series API. Test data will be delivered in groupings that do not allow access to future data. The objective of this competition is to use time series data generated by an online educational game to determine whether players will answer questions correctly. There are three question checkpoints (level 4, level 12, and level 22), each with a number of questions. At each checkpoint, you will have access to all previous test data for that section.
What files do I need?
You have access to the training data and labels. There are 18 questions for each sessions - you are not given the answers, but are simply told whether the user for a particular session answered each question correctly.
When you are ready to predict, use the sample notebook to iterate over the test data, which is split as described above and served up as Pandas dataframes. Make your predictions for each group of questions - at the end of this process a submission.csv file will have been created for you. Simply submit your notebook.
What should I expect the data format to be?
The training columns are as listed below. The label rows are identified with <session_id>_<question #>. Each session will have 18 rows, representing 18 questions.
What am I predicting?
For each <session_id>_<question #>, you are predicting the correct column, identifying whether you believe the user for this particular session will answer this question correctly, using only the previous information for the session.",kaggle competitions download -c predict-student-performance-from-game-play,"['https://www.kaggle.com/code/gusthema/student-performance-w-tensorflow-decision-forests', 'https://www.kaggle.com/code/cdeotte/xgboost-baseline-0-680', 'https://www.kaggle.com/code/cdeotte/game-room-click-eda', 'https://www.kaggle.com/code/kimtaehun/lightgbm-baseline-with-aggregated-log-data', 'https://www.kaggle.com/code/leehomhuang/2023-psp-catboost-baseline-inference']"
13,"Goal of the Competition
The goal of this competition is to predict the function of a set of proteins. You will develop a model trained on the amino-acid sequences of the proteins and on other data. Your work will help researchers better understand the function of proteins, which is important for discovering how cells, tissues, and organs work. This may also aid in the development of new drugs and therapies for various diseases.
Context
Proteins are responsible for many activities in our tissues, organs, and bodies and they also play a central role in the structure and function of cells. Proteins are large molecules composed of 20 types of building-blocks known as amino acids. The human body makes tens of thousands of different proteins, and each protein is composed of dozens or hundreds of amino acids that are linked sequentially. This amino-acid sequence determines the 3D structure and conformational dynamics of the protein, and that, in turn, determines its biological function. Due to ongoing genome sequencing projects, we are inundated with large amounts of genomic sequence data from thousands of species, which informs us of the amino-acid sequence data of proteins for which these genes code. The accurate assignment of biological function to the protein is key to understanding life at the molecular level. However, assigning function to any specific protein can be made difficult due to the multiple functions many proteins have, along with their ability to interact with multiple partners. More knowledge of the functions assigned to proteins—potentially aided by data science—could lead to curing diseases and improving human and animal health and wellness in areas as varied as medicine and agriculture.
Research groups have developed many ways to determine the function of proteins, including numerous methods based on comparing unsolved sequences with databases of proteins whose functions are known. Other efforts aim to mine the scientific literature associated with some of these proteins, while even more methods combine sophisticated machine-learning algorithms with an understanding of biological processes to decipher what these proteins do. However, there are still many challenges in this field, which are driven by ambiguity, complexity, and data integration.
Competition Host
The Function Community of Special Interest (Function-COSI) brings together computational biologists, experimental biologists, and biocurators who are dealing with the important problem of gene and gene product function prediction, to share ideas and create collaborations. The Function-COSI holds annual meetings at the Intelligent Systems for Molecular Biology (ISMB) conference and conducts the multi-year Critical Assessment of protein Function Annotation (CAFA) experiment, an ongoing, global, community-driven effort to evaluate and improve the computational annotation of protein function.
CAFA is co-chaired by Iddo Friedberg (Iowa State University) and Predrag Radivojac (Northeastern University). Additional academic co-organizers of this Kaggle competition include M. Clara De Paolis Kaluza (Northeastern University), Parnal Joshi (Iowa State University), UniProt (European Bioinformatics Institute), and Damiano Piovesan (University of Padova).
Acknowledgments
We gratefully acknowledge the support of Iowa State University who is hosting this competition. We also acknowledge the support of Northeastern University, University of Padova, UniProt, and the International Society for Computational Biology


This is a Code Competition. Refer to Code Requirements for details.","Important Note
This is a prospective (i.e., future) data competition. Many proteins in the Test data do not currently have any assigned functions. Proteins having one or more of their functions published by researchers during the curation phase of the competition will comprise the future test set. Final leaderboard scores will be calculated after the curation phase of the competition.
Background
The organizers provide a set of protein sequences on which the participants are asked to predict Gene Ontology (GO) terms in each of the three subontologies: Molecular Function (MF), Biological Process (BP), and Cellular Component (CC). This set of sequences is referred to as test superset.
The proteins from the test superset that originally had no experimentally assigned functions in a particular subontology and accumulate experimental annotations between the submission deadline and the time of evaluation in that subontology, are referred as the test set for that subontology. There will be three different test sets, one for each subontology, and the participants will be scored on each. The final performance accuracy will be computed by combining the three scores, as described below under Evaluation Metrics.
The organizers also provide the training set containing protein sequences that have at least one experimentally determined GO term in at least one subontology, together with those experimental annotations. These proteins may also appear in the test superset. Of course, they will not be used for evaluation in a particular subontology unless they had 0 experimentally determined terms in that subontology before the submission deadline.
Evaluation Metrics
Submissions will be evaluated on proteins (test set) that did not have experimentally determined functional annotations in at least one subontology of the Gene Ontology (GO) before the submission deadline and have accumulated experimentally-validated functional annotations in that subontology between the submission deadline and the time of evaluation. For example, a protein that had no experimental terms in say Molecular Function (MF) subontology of GO and has accumulated experimental annotations in MF after the submission deadline will be included in the test set for evaluating the MF term predictions. The same holds for the Biological Process (BP) or Cellular Component (CC) subontologies of GO. The proteins that qualify will create three different test sets, one for each subontology of GO. The same protein can appear in more than one test set if it accumulates experimentally-validated annotations in more than a single subontology.
The maximum F-measure based on the weighted precision and recall will be calculated on each of the three test sets and the final performance measure will be an arithmetic mean of the three maximum F-measures (for MF, BP, and CC). The formulas for computing weighted F-measures are provided in the supplement (page 31) of the following paper
Jiang Y, et al. An expanded evaluation of protein function prediction methods shows an improvement in accuracy. Genome Biol. (2016) 17(1): 184.
in the full evaluation mode. The weights (i.e., information content ic(f), where f is a term in any subontology) for each term f of each subontology are provided by the challenge organizers. Note that we equivalently refer to those weights as ia(f), called information accretion for the functional term f. The rationale for using weighted precision and recall is that GO is hierarchical and thus, the terms on top of the hierarchy are implied by their descendants. The weight for a term is determined by the logarithm of the frequency of occurrence of that term in a large pool of proteins. The root terms appear in every protein's annotation and thus, their weights are 0. Terms deep in the ontology tend to appear less frequently, be harder to predict, and thus their weights are larger (Clark & Radivojac, 2013). This does not always hold true however, as highlighted in the following discussion.
Using the terminology from Jiang et al. (2016), the evaluation will be carried out for no-knowledge and limited-knowledge protein targets combined, in the full evaluation mode, using maximum F-measures of information-accretion weighted precision and recall, one for each subontology. The three maximum F-measures will be combined as an arithmetic mean to compute the final performance. The code used for evaluation is publicly available here.
Clark WT, Radivojac P. Information-theoretic evaluation of predicted ontological annotations. Bioinformatics (2013) 29(13): i53-i61.
Leaderboard
The participants are cautioned that the leaderboard was designed to display method performance on a relatively small selection of proteins from the test superset (see Data), provided to us by the UniProtKB team but not available in UniProtKB or other public databases. These proteins will not be included in the test set for the subontologies used for the leaderboard evaluation. The final test set will consist of proteins that will have accumulated functional terms after the submission deadline and therefore, some distribution shift between the sample of proteins used for the leaderboard and the final evaluation sample is to be expected. Overall, the participants are encouraged to maximize the generalization performance and use the leaderboard only as a rough indicator of their model's performance.
Submission File
The list of predictions contains a list of pairs between protein targets and GO terms, followed by the probabilistic estimate of the relationship (one association per line). The target name must correspond to the target ID listed in the test set (in the FASTA header for each sequence). The GO ID must correspond to valid terms in GO's version listed in the Data section---invalid terms are automatically excluded from evaluation. Molecular Function (MF), Biological Process (BP), and Cellular Component (CC) subontologies of GO are to be combined in the prediction files, but they will be evaluated independently and combined at the end as described above. The score must be in the interval (0, 1.000] and contain up to 3 (three) significant figures. A score of 0 is not allowed; that is, the team should simply not list such pairs. In case the predictions in the submitted files are not propagated to the root of ontology, the predictions will be recursively propagated by assigning each parent term a score that is the maximum score among its children's scores. Finally, to limit prediction file sizes, one target cannot be associated with more than 1500 terms for MF, BP, and CC subontologies combined.
For any protein ID in the test superset, you must list a set of GO terms and assign your estimated probability. If a protein ID is not listed in your submitted file, the organizers will assume that all predictions are 0. The file should not contain a header, columns must be tab or space separated. An example submission file may look as follows:
P9WHI7   GO:0009274   0.931   
P9WHI7   GO:0071944   0.540
P9WHI7   GO:0005575   0.324
P04637   GO:1990837   0.23
P04637   GO:0031625   0.989
P04637   GO:0043565   .64
P04637   GO:0001091   0.49
etc.
The participants can manually investigate the UniProtKB entries for P9WHI7 and P04637 to familiarize themselves with biological databases.","Background
The Gene Ontology (GO) is a concept hierarchy that describes the biological function of genes and gene products at different levels of abstraction (Ashburner et al., 2000). It is a good model to describe the multi-faceted nature of protein function.
GO is a directed acyclic graph. The nodes in this graph are functional descriptors (terms or classes) connected by relational ties between them (is_a, part_of, etc.). For example, terms 'protein binding activity' and 'binding activity' are related by an is_a relationship; however, the edge in the graph is often reversed to point from binding towards protein binding. This graph contains three subgraphs (subontologies): Molecular Function (MF), Biological Process (BP), and Cellular Component (CC), defined by their root nodes. Biologically, each subgraph represent a different aspect of the protein's function: what it does on a molecular level (MF), which biological processes it participates in (BP) and where in the cell it is located (CC). See the Gene Ontology Overview for more details.
The protein's function is therefore represented by a subset of one or more of the subontologies.
These annotations are supported by evidence codes, which can be broadly divided into experimental (e.g., as documented in a paper published by a research team of biologists) and non-experimental. Non-experimental terms are usually inferred by computational means. We recommend you read more about the different types of GO evidence codes.
We will use experimentally determined term-protein assignments as class labels for each protein. That is, if a protein is labeled with a term, it means that this protein has this function validated by experimental evidence. By processing these annotated terms, we can generate a dataset of proteins and their ground truth labels for each term. The absence of a term annotation does not necessarily mean a protein does not have this function, only that this annotation does not exist (yet) in the GO. A protein may be annotated by one or more terms from the same subontology, and by terms from more than one subontology.",kaggle competitions download -c cafa-5-protein-function-prediction,"['https://www.kaggle.com/code/inversion/submitting-to-the-cafa-competition', 'https://www.kaggle.com/code/gusthema/cafa-5-protein-function-with-tensorflow', 'https://www.kaggle.com/code/leonidkulyk/eda-cafa5-pfp-interactive-dags-plotly', 'https://www.kaggle.com/code/henriupton/proteinet-pytorch-ems2-t5-protbert-embeddings', 'https://www.kaggle.com/code/alexandervc/baseline-multilabel-to-multitarget-binary']"
14,"Please note - this is a preview/beta launch of an upcoming competition. This competition is intended to help with rule balancing and establishing a fair and fun competition, soon to be launched. As such, this competition does not have cash prizes, points, or medals - but we hope to gain your feedback to help make the upcoming competition as fun as possible!
Introduction
As the sun set on the world an array of lights dotted the once dark horizon. With the help of a brigade of toads, Lux had made it past the terrors in the night to see the dawn of a new age. Seeking new challenges, plans were made to send a forward force with one mission: terraform Mars!
Welcome to the Lux AI Challenge Season 2!
The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand.
All code can be found at our Github, make sure to give it a star while you are there!
Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play Episodes (games) against other bots on the ladder that have a similar skill rating. Over time skill ratings will go up with wins or down with losses and evened out with ties. Every bot submitted will continue to play episodes until the end of the competition, with newer bots playing a much more frequent number of episodes. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page. Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents the uncertainty of that estimate which will decrease over time. When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error and you can download the agent logs to help figure out why. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation. We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. Newly submitted agents will be given an increased rate in the number of episodes run to give you faster feedback. After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates. Ranking System After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates. Final Evaluation At the submission deadline on December 22, 2022, additional submissions will be locked. From December 22, 2022 to December 22th, we will continue to run games. At the conclusion of this period, the leaderboard is final.",,,"['https://www.kaggle.com/code/stonet2000/lux-ai-season-2-jupyter-notebook-tutorial', 'https://www.kaggle.com/code/jmerle/lux-eye-2022-integration', 'https://www.kaggle.com/code/mpwolke/far-below-beta', 'https://www.kaggle.com/code/agenlu/test-luxai-season-2-enviroment', 'https://www.kaggle.com/code/agenlu/lux-ai-season-2-notebook-tutorial-fix-videos']"
15,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.
First, the series is getting upgraded branding. We've dropped ""Tabular"" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.
Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.
Regardless of these changes, the goals of the Playground Series remain the same—to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!
With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.
The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that Oi,j corresponds to the number of Ids i (actual) that received a predicted value j. An N-by-N matrix of weights, w, is calculated based on the difference between actual and predicted values:
$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$
An N-by-N histogram matrix of expected outcomes, E, is calculated assuming that there is no correlation between values.  This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that E and O have the same sum.
From these three matrices, the quadratic weighted kappa is calculated as: 
$$\kappa=1-\frac{\sum_{i,j}w_{i,j}O_{i,j}}{\sum_{i,j}w_{i,j}E_{i,j}}.$$
Submission File
For each Id in the test set, you must predict the value for the target quality. The file should contain a header and have the following format:
Id,quality
2056,5
2057,7
2058,3
etc.","The dataset for this competition (both train and test) was generated from a deep learning model trained on the Wine Quality dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Files
train.csv - the training dataset; quality is the target (ordinal, integer)
test.csv - the test dataset; your objective is to predict quality
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e5,"['https://www.kaggle.com/code/jcaliz/ps-s03e05-a-complete-eda', 'https://www.kaggle.com/code/abhi011097/s3-e5-eda-multi-approach-models-w-o-smote', 'https://www.kaggle.com/code/slythe/ultimate-eda-model-selection-pse3e5', 'https://www.kaggle.com/code/soupmonster/s03e05-eda-modeling', 'https://www.kaggle.com/code/oscarm524/ps-s3-ep5-eda-modeling']"
16,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!
With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in April every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc..
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions will be evaluated based on MPA@3. Each submission can contain up to 3 predictions (all separated by spaces), and the earlier a correct prediction occurs, the higher score it will receive.
Submission File
For each id in the test set, you must predict the target prognosis. The file should contain a header and have the following format:
id,prognosis
707,Dengue West_Nile_fever Malaria
708,Lyme_disease West_Nile_fever Dengue
709,Dengue West_Nile_fever Lyme_disease
etc.","The dataset for this competition (both train and test) was generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note that in the original dataset some prognoses contain spaces, but in the competition dataset spaces have been replaced with underscores to work with the MPA@K metric.
Files
train.csv - the training dataset; prognosis is the target
test.csv - the test dataset; your objective is to predict prognosis
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e13,"['https://www.kaggle.com/code/wlifferth/generating-map-k-predictions', 'https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e13-2023-eda-and-submission', 'https://www.kaggle.com/code/kimtaehun/simple-eda-and-multi-class-classification', 'https://www.kaggle.com/code/tetsutani/ps3e13-eda-decomposition-ensemble-rankpredict', 'https://www.kaggle.com/code/zhukovoleksiy/5-solution-ps3e13-ensemble']"
17,"This may be one of the most challenging Tabular Playground competitions to date! It just so happens that one of Kaggle's software engineers is an avid Rocket League player and he's assembled a dataset of Rocket League gameplay for this month's TPS.
This month's challenge is to predict the probability of each team scoring within the next 10 seconds of the game given a snapshot from a Rocket League match. Sounds awesome, right?
Well, it's not that simple. The training data is fairly large; trying to read and model it in a single go might pose some challenges. The purpose of this month's competition is for you to explore ways you can take a big dataset and make it manageable within the time and resources you have. For most people, typical brute force approaches aren't going to work well.
Can you scale down the dataset?
Can you use, e.g., online learning methods that allow you to train from the data one row at a time? (FTLR is a great place to start if you're not familiar with online learning! e.g., this notebook)
Can you figure out a nice set of features to reduce the dataset down to?
In addition to that challenge, while your predictions must be made pointwise, the training data is made up of timeseries—maybe you can use that temporal information to improve your model? This competition also has plenty of opportunity for data visualizations. Let's see some pretty graphs!
So, share your ideas about tackling this beast of a dataset and have a great time!
Acknowledgments
This competition includes Rocket League data and images from the Rocket League Community Tournament Assets.","For every id in the test data, you will be predicting the probability for whether each of the two teams will score a goal within the next 10 seconds of gameplay. Submissions are scored by the log loss:
$$ \text{score} = - \frac{1}{2}\sum_{m=1}^{M} \frac{1}{N} \sum_{i=1}^{N} \left[ y_{i,m} \log(\hat{y}_{i,m}) + (1 - y_{i,m}) \log(1 - \hat{y}_{i,m})\right] $$
where:
\(N\) is the number of id observations in the test data
\(M\) is the number of scored targets (here \(M=2\), one for each team)
\( \hat{y}_{i,m} \) is the predicted scoring probability of team \(m\) (Team A or Team B in the dataset)
\( y_{i,m} \) is the ground truth for team \(m\), 1 for a goal within 10 seconds, 0 otherwise
\( log() \) is the natural (base e) logarithm
Note: the actual submitted predicted probabilities are replaced with \(max(min(p,1-10^{-15}),10^{-15})\). A smaller log loss is better.
Submission File
You must predict a probability of whether each of the teams will score within the next 10 seconds of gameplay. The file should have a header and be in the following format:
id,team_A_scoring_within_10sec,team_B_scoring_within_10sec
0,0.31,0.99
1,0.02,0.55
etc...","The dataset consists of sequences of snapshots of the state of a Rocket League match, including position and velocity of all players and the ball, as well as extra information. The goal of the competition is to predict -- from a given snapshot in the game -- for each team, the probability that they will score within the next 10 seconds of game time.
The data was taken from professional Rocket League matches. Each event consists of a chronological series of frames recorded at 10 frames per second. All events begin with a kickoff, and most end in one team scoring a goal, but some are truncated and end with no goal scored due to circumstances which can cause gameplay strategies to shift, for example 1) nearing end of regulation (where the game continues until the ball touches the ground) or 2) becoming non-competitive, eg one team winning by 3+ goals with little time remaining.
Files:
train_[0-9].csv: Train set split into 10 files. Rows are sorted by game_num, event_id, and event_time, and each event is entirely contained in one file.
test.csv: Test set. Unlike the train set, the rows are scrambled.
[train|test]_dtypes.csv: pandas dtypes for the columns in the train / test set, which can be pulled and passed to pd.read_csv() on the full set to read it with correct types since by default, pd.read_csv() will use 64-bit types which will waste memory. See below for example code.
sample_submission.csv: A sample submission in the correct format.
Columns:",kaggle competitions download -c tabular-playground-series-oct-2022,"['https://www.kaggle.com/code/chazzer/rocket-league-xgboost-feat-engineering-cv', 'https://www.kaggle.com/code/paddykb/tps-2022-10-fastai', 'https://www.kaggle.com/code/sergiosaharovskiy/tps-oct-2022-viz-players-positions-animated', 'https://www.kaggle.com/code/infrarosso/tps-oct-2022-eda-hybrid-model-ensemble', 'https://www.kaggle.com/code/mattop/rocket-league-tps-eda']"
18,"Goal of the Competition
The goal of this competition is to reconstruct accurate 3D maps. Last year's Image Matching Challenge focused on two-view matching. This year you will take one step further: your task will be to reconstruct the 3D scene from many different views.
Your work could be the key to unlocking mapping the world from assorted and noisy data sources, such as images uploaded by users to services like Google Maps.
Context
Your best camera may just be the phone in your pocket. You might take a snap of a landmark, then share it with friends. By itself, that photo is two-dimensional and only includes the perspective of your shooting location. Of course, many people may have taken photos of that same landmark. If we were able to combine all of our photos, we may be able to create a more complete, three-dimensional view of any given thing. Perhaps machine learning could help better capture the richness of the world using the vast amounts of unstructured image collections freely available on the internet.
The process of reconstructing a 3D model of an environment from a collection of images is called Structure from Motion (SfM). These images are often captured by trained operators or with additional sensor data, such as the cars used by Google Maps. This ensures homogeneous, high-quality data. It is much more difficult to build 3D models from assorted images, given a wide variety of viewpoints, along with lighting, weather, and other changes.
Competition host Google employs SfM techniques across many Google Maps services, such as the 3D models created from StreetView and aerial imagery. In order to accelerate research into this topic and better leverage the volume of data already publicly available, Google presents this competition in collaboration with Haiper and Kaggle.
Your work in helping to build accurate 3D models may have applications to photography, cultural heritage preservation, and many services across Google.
Banner photo by Salvador Altamirano on Unsplash. Other photos courtesy of the Archaelogical Park ""Valle dei Templi"" of Agrigento.
Organization
Eduard Trulls (Google), Dmytro Mishkin (Czech Technical University in Prague/HOVER Inc), Jiri Matas (Czech Technical University in Prague), Fabio Bellavia (University of Palermo), Luca Morelli (University of Trento/Bruno Kessler Foundation), Fabio Remondino (Bruno Kessler Foundation), Weiwei Sun (University of British Columbia), Kwang Moo Yi (University of British Columbia/Haiper).
This is a Code Competition. Refer to Code Requirements for details.
Sponsors
                 ","Evaluation metric
Participants are asked to estimate the pose for each image in a set with \( N \) images. Each camera pose is parameterized with a rotation matrix \( \mathbf{R} \) and a translation vector \( \mathbf{T} \), from an arbitrary frame of reference.
Submissions are evaluated on the mean Average Accuracy (mAA) of the estimated poses. Given a set of cameras, parameterized by their rotation matrices and translation vectors, and the hidden ground truth, we compute the relative error in terms of rotation (\( \epsilon_R \), in degrees) and translation (\( \epsilon_T \), in meters) for every possible pair of images in \( N \), that is, \( {N \choose 2} \) pairs.
We then threshold each of this poses by its accuracy in terms of both rotation, and translation. We do this over ten pairs of thresholds: e.g. at 1 degree and 20 cm at the finest level, and 10 degrees and 5 m at the coarsest level. The actual thresholds vary for each dataset, but they look like this:
thresholds_r = np.linspace(1, 10, 10)  # In degrees.
thresholds_t = np.geomspace(0.2, 5, 10)  # In meters.
We then calculate the percentage of accurate samples (pairs of poses) at every thresholding level, and average the results over all thresholds. This rewards more accurate poses. Note that while you submit \( N \), the metric will process all samples in \( {N \choose 2} \).
Finally, we compute this metric separately for each scene and then average it to compute its mAA. These values are then averaged over datasets, which contain a variable number of scenes, to obtain the final mAA metric.
Submission File
For each image ID in the test set, you must predict its pose. The file should contain a header and have the following format:
image_path,dataset,scene,rotation_matrix,translation_vector
da1/sc1/images/im1.png,da1,sc1,0.1;0.2;0.3;0.4;0.5;0.6;0.7;0.8;0.9,0.1;0.2;0.3
da1/sc2/images/im2.png,da1,sc1,0.1;0.2;0.3;0.4;0.5;0.6;0.7;0.8;0.9,0.1;0.2;0.3
etc
The rotation_matrix (a \( 3 \times 3 \) matrix) and translation_vector (a 3-D vector) are written as ;-separated vectors. Matrices are flattened into vectors in row-major order. Note that this metric does not require the intrinsics (the calibration matrix \( \mathbf{K} \)), usually estimated along with \( \mathbf{R} \) and \( \mathbf{T} \) during the 3D reconstruction process.","Building a 3D model of a scene given an unstructured collection of images taken around it is a longstanding problem in computer vision research. Your challenge in this competition is to generate 3D reconstructions from image sets showing different types of scenes and accurately pose those images.
This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook. Expect to find roughly 1,100 images in the hidden test set. The number of images in a scene may vary from <10 to ~250.
Parts of the dataset (the Haiper subset) were created with the Captur3 app and the Haiper Research team from Haiper AI.
Files
sample_submission.csv A valid, randomly-generated sample submission with the following fields:
image_path: The image filename, including the path.
dataset: The unique identifier for the dataset.
scene: The unique identifier for the scene.
rotation_matrix: The first target column. A \( 3 \times 3 \) matrix, flattened into a vector in row-major convection, with values separated by ;.
translation_vector: The second target column. A 3-D dimensional vector, with values separated by ;.",kaggle competitions download -c image-matching-challenge-2023,"['https://www.kaggle.com/code/leonidkulyk/eda-imc-3d-plots-interactive-vis', 'https://www.kaggle.com/code/eduardtrulls/imc-2023-submission-example', 'https://www.kaggle.com/code/qi7axu/imc-2023-submission-example', 'https://www.kaggle.com/code/eduardtrulls/imc2023-evaluation', 'https://www.kaggle.com/code/jessevanderlinden/image-matching-data-discovery-and-sift-features']"
19,"Introduction
As the sun set on the world an array of lights dotted the once dark horizon. With the help of a brigade of toads, Lux had made it past the terrors in the night to see the dawn of a new age. Seeking new challenges, plans were made to send a forward force with one mission: terraform Mars!
Welcome to the Lux AI Challenge Season 2!
The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand.
All code can be found at our Github, make sure to give it a star while you are there!
Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play Episodes (games) against other bots on the ladder that have a similar skill rating. Over time skill ratings will go up with wins or down with losses and evened out with ties.
Every bot submitted will continue to play episodes until the end of the competition, with newer bots playing a much more frequent number of episodes. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents the uncertainty of that estimate which will decrease over time.
When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error and you can download the agent logs to help figure out why. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. Newly submitted agents will be given an increased rate in the number of episodes run to give you faster feedback.
Ranking System
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
Final Evaluation
At the submission deadline on May 8, 2023, additional submissions will be locked. From May 8, 2023 to May 4th, we will continue to run games. At the conclusion of this period, the leaderboard is final.",,,"['https://www.kaggle.com/code/stonet2000/lux-ai-challenge-season-2-tutorial-python', 'https://www.kaggle.com/code/stonet2000/rl-with-lux-2-rl-problem-solving', 'https://www.kaggle.com/code/stonet2000/rl-with-lux-1-intro-to-rl', 'https://www.kaggle.com/code/scakcotf/building-a-basic-rule-based-agent-in-python', 'https://www.kaggle.com/code/istinetz/picking-a-good-starting-location']"
20,"Goal of the Competition
Enzymes are proteins that act as catalysts in the chemical reactions of living organisms. The goal of this competition is to predict the thermostability of enzyme variants. The experimentally measured thermostability (melting temperature) data includes natural sequences, as well as engineered sequences with single or multiple mutations upon the natural sequences.
Understanding and accurately predict protein stability is a fundamental problem in biotechnology. Its applications include enzyme engineering for addressing the world’s challenges in sustainability, carbon neutrality and more. Improvements to enzyme stability could lower costs and increase the speed scientists can iterate on concepts.
Context
Novozymes finds enzymes in nature and optimizes them for use in industry. In industry, enzymes replace chemicals and accelerate production processes. They help our customers make more from less, while saving energy and generating less waste. Enzymes are widely used in laundry and dishwashing detergents where they remove stains and enable low-temperature washing and concentrated detergents. Other enzymes improve the quality of bread, beer and wine, or increase the nutritional value of animal feed. Enzymes are also used in the production of biofuels where they turn starch or cellulose from biomass into sugars which can be fermented to ethanol. These are just a few examples as we sell enzymes to more than 40 different industries. Like enzymes, microorganisms have natural properties that can be put to use in a variety of processes. Novozymes supplies a range of microorganisms for use in agriculture, animal health and nutrition, industrial cleaning and wastewater treatment.
However, many enzymes are only marginally stable, which limits their performance under harsh application conditions. Instability also decreases the amount of protein that can be produced by the cell. Therefore, the development of efficient computational approaches to predict protein stability carries enormous technical and scientific interest.
Computational protein stability prediction based on physics principles have made remarkable progress thanks to advanced physics-based methods such as FoldX, Rosetta, and others. Recently, many machine learning methods were proposed to predict the stability impact of mutations on protein based on the pattern of variation in natural sequences and their three dimensional structures. More and more protein structures are being solved thanks to the recent breakthrough of AlphaFold2. However, accurate prediction of protein thermal stability remains a great challenge.
In this competition, Novozymes invites you to develop a model to predict/rank the thermostability of enzyme variants based on experimental melting temperature data, which is obtained from Novozymes’s high throughput screening lab. You’ll have access to data from previous scientific publications. The available thermostability data spans from natural sequences to engineered sequences with single or multiple mutations upon the natural sequences. If successful, you'll help tackle the fundamental problem of improving protein stability, making the approach to design novel and useful proteins, like enzymes and therapeutics, more rapidly and at lower cost.
Novozymes is the world’s leading biotech powerhouse. Our growing world is faced with pressing needs, emphasizing the necessity for solutions that can ensure the health of the planet and its population. At Novozymes, we believe biotech is at the core of connecting those societal needs with the challenges and opportunities our customers face. Novozymes is the global market leader in biological solutions, producing a wide range of enzymes, microorganisms, technical and digital solutions which help our customers, amongst other things, add new features to their products and produce more from less.
Together, we find biological answers for better lives in a growing world. Let’s Rethink Tomorrow. This is Novozymes’ purpose statement. Novozymes strives to have great impact by balancing good business for our customers and our company, while spearheading environmental and social change. In 2021, Novozymes enabled savings of 60 million tons of CO2 in global transport.","Submissions are evaluated on the Spearman's correlation coefficient between the ground truth and the predictions.
Submission File
Each seq_id represents a single-mutation variant of an enzyme. Your task is to rank the stability of these variants, assigning greater ranks to more stable variants. For each seq_id in the test set, you must predict the value for for the target tm. The file should contain a header and have the following format:
seq_id,tm
31394,9.7
31395,56.3
31396,112.4
etc.",,,
21,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.
First, the series is getting upgraded branding. We've dropped ""Tabular"" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.
Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.
Regardless of these changes, the goals of the Playground Series remain the same—to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!
To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict the value for the target Class. The file should contain a header and have the following format:
id,Class
341588,0.23
341589,0.92
341590,0.02
etc.",,,
22,"The competing Kaggle merchandise stores we saw in January's Tabular Playground are at it again. This time, they're selling books!
The task for this month's competitions is a bit more complicated. Not only are there six countries and four books to forecast, but you're being asked to forecast sales during the tumultuous year 2021. Can you use your data science skills to predict book sales when conditions are far from the ordinary?
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
Good luck and have fun!
Photo above by Aron Visuals on Unsplash","Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.
Submission File
For each row_id in the test set, you must predict the corresponding num_sold. The file should contain a header and have the following format:
row_id,num_sold
70128,100
70129,100
70130,100
etc.",,,
23,"NOTE:
You can now create your own synthetic versions of this dataset by forking and running this notebook.
Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!
With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in April every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc..
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict the probability of target (likelihood of the presence of a kidney stone). The file should contain a header and have the following format:
id,target
414,0.5
415,0.1
416,0.9
etc.",,,
24,"Goal of the Competition
The goal of this competition is to predict a neutrino particle’s direction. You will develop a model based on data from the ""IceCube"" detector, which observes the cosmos from deep within the South Pole ice.
Your work could help scientists better understand exploding stars, gamma-ray bursts, and cataclysmic phenomena involving black holes, neutron stars and the fundamental properties of the neutrino itself.
Context
One of the most abundant particles in the universe is the neutrino. While similar to an electron, the nearly massless and electrically neutral neutrinos have fundamental properties that make them difficult to detect. Yet, to gather enough information to probe the most violent astrophysical sources, scientists must estimate the direction of neutrino events. If algorithms could be made considerably faster and more accurate, it would allow for more neutrino events to be analyzed, possibly even in real-time and dramatically increase the chance to identify cosmic neutrino sources. Rapid detection could enable networks of telescopes worldwide to search for more transient phenomena.
Researchers have developed multiple approaches over the past ten years to reconstruct neutrino events. However, problems arise as existing solutions are far from perfect. They're either fast but inaccurate or more accurate at the price of huge computational costs.
The IceCube Neutrino Observatory is the first detector of its kind, encompassing a cubic kilometer of ice and designed to search for the nearly massless neutrinos. An international group of scientists is responsible for the scientific research that makes up the IceCube Collaboration.
By making the process faster and more precise, you'll help improve the reconstruction of neutrinos. As a result, we could gain a clearer image of our universe.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated using the mean angular error between the predicted and true event origins. See this notebook for a python copy of the metric.
Submission File
For each event_id in the test set, you must predict the azimuth and zenith. The file should contain a header and have the following format:
event_id,azimuth,zenith
730,1,1
769,1,1
774,1,1
etc.","The goal of this competition is to identify which direction neutrinos detected by the IceCube neutrino observatory came from. When detection events can be localized quickly enough, traditional telescopes are recruited to investigate short-lived neutrino sources such as supernovae or gamma ray bursts. Because the sky is huge better localization will not only associate neutrinos with sources but also to help partner observatories limit their search space. With an average of three thousand events per second to process, it's difficult to keep up with the stream of data using traditional methods. Your challenge in this competition is to quickly and accurately process a large number of events.
This competition uses a hidden test set. When your submitted notebook is scored the actual test data (including a full length sample submission) will be made available to your notebook. Expect to see roughly one million events in the hidden test set, split between multiple batches.
Files
[train/test]_meta.parquet
batch_id (int): the ID of the batch the event was placed into.
event_id (int): the event ID.
[first/last]_pulse_index (int): index of the first/last row in the features dataframe belonging to this event.
[azimuth/zenith] (float32): the [azimuth/zenith] angle in radians of the neutrino. A value between 0 and 2*pi for the azimuth and 0 and pi for zenith. The target columns. Not provided for the test set. The direction vector represented by zenith and azimuth points to where the neutrino came from.",kaggle competitions download -c icecube-neutrinos-in-deep-ice,"['https://www.kaggle.com/code/dschettler8845/ndi-let-s-learn-together-eli5-and-eda', 'https://www.kaggle.com/code/rasmusrse/graphnet-baseline-submission', 'https://www.kaggle.com/code/roberthatch/lb-1-183-lightning-fast-baseline-with-polars', 'https://www.kaggle.com/code/rsmits/tensorflow-lstm-model-inference', 'https://www.kaggle.com/code/shlomoron/icecube-eda-pca-baseline-cv-1-23-lb-1-218']"
25,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.
First, the series is getting upgraded branding. We've dropped ""Tabular"" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.
Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.
Regardless of these changes, the goals of the Playground Series remain the same—to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!
To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each EmployeeNumber in the test set, you must predict the probability for the target variable Attrition. The file should contain a header and have the following format:
EmployeeNumber,Attrition
1677,0.78
1678,0.34
1679,0.55
etc.","The dataset for this competition (both train and test) was generated from a deep learning model trained on a Employee Attrition. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Files
train.csv - the training dataset; Attrition is the binary target
test.csv - the test dataset; your objective is to predict the probability of positive Attrition
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e3,"['https://www.kaggle.com/code/radek1/eda-training-a-1st-model-submission', 'https://www.kaggle.com/code/phongnguyen1/s3e3-from-eda-to-final-submission', 'https://www.kaggle.com/code/jacoporepossi/how-to-use-chatgpt-in-a-competition-eda-part-1', 'https://www.kaggle.com/code/samuelcortinhas/ps-s3e3-hill-climbing-like-a-gm', 'https://www.kaggle.com/code/odins0n/playground-s-3-e-3-eda-modelling']"
26,,,,,
27,,,,,
28,"Goal of the Competition
Millions of students have a learning, physical, or visual disability that prevents a person from reading conventional print. The majority of educational materials for science, technology, engineering, and math (STEM) fields are inaccessible to these students. Technology that makes the written word accessible exists. However, doing so for educational visuals like graphs remains complex and resource intensive. As a result, only a small fraction of educational materials are available for learners with this learning difference —unless machine learning could help bridge that gap.
The goal of this competition is to extract data represented by four types of charts commonly found in STEM textbooks. You will develop an automatic solution trained on a graph dataset.
Your work will help make reading graphs accessible to millions of students with a learning difference or disability.
Context
Current efforts to make existing educational materials accessible are done through a manual process. Though this method produces accessible materials, it is expensive and time-consuming and has to go through several rounds of quality checking. Furthermore, the amount of effort involved in making these materials accessible often means these documents aren’t available for schools or teachers without the funding to license them.
Competition host Benetech is a nonprofit dedicated to reducing social and economic inequity in partnership with the communities that it serves through software for social good. Benetech’s initiatives are transforming how students, job seekers, and older adults across the globe read, learn, and work. Benetech believes that access to information is a human right, and no person should encounter barriers to education, literacy, or employment due to differences or disabilities.
The best submissions will be adopted into Benetech’s PageAI product, which converts books and other educational materials into accessible formats. You could have a hand in serving accessible STEM materials to millions of students with a learning difference or disability.
Acknowledgments
Benetech would like to thank Schmidt Futures for their support in making this work possible.
        
This is a Code Competition. Refer to Code Requirements for details.","The objective of the competition is to predict the data series represented by four kinds of scientific figures (or charts): bar graphs, dot plots, line graphs, and scatter plots.
Please see this page for details on what kind of predictions to make for each chart type: Graph Conventions.
You may find a Python implementation of the metric in this notebook: Benetech Competition Metric.
The Evaluation Metric
The data series for a single figure comprises two instances for evaluation: the series of values along the x-axis and the corresponding series of values along the y-axis. Each data series will be either of numerical type or of categorical type as determined by the type of chart. (See the Graph Conventions.)
Predicted data series are evaluated by a combination of two metrics, Levenshtein distance for categorical (that is, string) data types and RMSE for numerical data types, with an initial exact-match criterion for the chart type and the number of values in the series. Each of these distances is rescaled and mapped to a common similarity scale by a sigmoid-type transform with an optimum value of 1:
$$
\sigma(x) = 2 - \frac{2}{1 + e^{-x}}
$$
Evaluation of a single instance proceeds as follows:
We first check the number of values and type of chart predicted for a series. If either of these is different than that in the ground-truth, the score for that series is 0. Otherwise, we evaluate the predicted series by the data type of that series as given in the field axes/{x|y}-axis/values-type.
a) For series of numerical type, we evaluate predictions by a normalized RMSE:
$$
\textrm{NRMSE} = \sigma\left(\frac{\textrm{RMSE}(y, \widehat{y})}{\textrm{RMSE}(y, \bar{y})}\right)
$$
Note that the argument to the sigmoid transform is equal to the square root of one minus the R2 score.
b) For series of categorical type, we evaluate predictions by a normalized Levenshtein distance:
$$
\textrm{NLev} = \sigma\left(\frac{\sum_i \textrm{Lev}(y_i, \widehat{y}_i)}{\sum_i \textrm{length}(y_i)}\right)
$$
In other words, NLev is the sum of the Levenshtein distances for each category string divided by the total length of all strings for that instance.
The overall score is the mean of the similarity scores over all instances.
Submission File
Each row in the submission file should contain predicted series for one axis of a figure in the test set, where, for instance, abc123_x would give predictions for axis x in figure abc123. The values of the series should be within a single string and delimited by ;. You must also give the appropriate type for the chart the axis belongs to.
The file should contain a header and have the following format:
id,data_series,chart_type
abc123_x,2;3;4;5,horizontal_bar
abc123_y,a;b;c;d,horizontal_bar","This competition's dataset comprises about 65,000 comprehensively-annotated scientific figures of four kinds: bar graphs (both horizontal and vertical), dot plots, line graphs, and scatter plots. While the majority of the figures are synthetic, we also include several thousand figures extracted from professionally-produced sources. Your task is to predict the data series depicted in the test set figures.
This is a Code Competition, in which the actual test set is hidden. In this public version, we give some sample data drawn from the training set to help you author your solutions. When your submission is scored, this example test data will be replaced with the actual test set.
File and Field Descriptions
train/annotations/ Collection of JSON image annotations describing the figures.
source Whether generated or extracted.
chart-type One of dot, horizontal_bar, vertical_bar, line, scatter.
plot-bb Bounding box of the plot within the figure, given by height, width, x0, and y0.
text/id Identifier for a text item within the figure.
text/polygon Region bounding the text item in the image.
text/text The text itself.
text/role The function of the text in the image, whether chart_title, axis_title, tick_label, etc.",kaggle competitions download -c benetech-making-graphs-accessible,"['https://www.kaggle.com/code/ryanholbrook/competition-metric-benetech-mixed-match', 'https://www.kaggle.com/code/nbroad/donut-infer-lb-0-44-benetech', 'https://www.kaggle.com/code/nbroad/donut-train-benetech', 'https://www.kaggle.com/code/leonidkulyk/eda-mga-graphs-with-interactive-annotations', 'https://www.kaggle.com/code/heyytanay/training-pix2struct-pytorch-amp-w-b']"
29,"Goal of the Competition
The goal of this competition is to predict how DNA, RNA, and protein measurements co-vary in single cells as bone marrow stem cells develop into more mature blood cells. You will develop a model trained on a subset of 300,000-cell time course dataset of CD34+ hematopoietic stem and progenitor cells (HSPC) from four human donors at five time points generated for this competition by Cellarity, a cell-centric drug creation company.
In the test set, taken from an unseen later time point in the dataset, competitors will be provided with one modality and be tasked with predicting a paired modality measured in the same cell. The added challenge of this competition is that the test data will be from a later time point than any time point in the training data.
Your work will help accelerate innovation in methods of mapping genetic information across layers of cellular state. If we can predict one modality from another, we may expand our understanding of the rules governing these complex regulatory processes.
Context
In the past decade, the advent of single-cell genomics has enabled the measurement of DNA, RNA, and proteins in single cells. These technologies allow the study of biology at an unprecedented scale and resolution. Among the outcomes have been detailed maps of early human embryonic development, the discovery of new disease-associated cell types, and cell-targeted therapeutic interventions. Moreover, with recent advances in experimental techniques it is now possible to measure multiple genomic modalities in the same cell.
While multimodal single-cell data is increasingly available, data analysis methods are still scarce. Due to the small volume of a single cell, measurements are sparse and noisy. Differences in molecular sampling depths between cells (sequencing depth) and technical effects from handling cells in batches (batch effects) can often overwhelm biological differences. When analyzing multimodal data, one must account for different feature spaces, as well as shared and unique variation between modalities and between batches. Furthermore, current pipelines for single-cell data analysis treat cells as static snapshots, even when there is an underlying dynamical biological process. Accounting for temporal dynamics alongside state changes over time is an open challenge in single-cell data science.
Generally, genetic information flows from DNA to RNA to proteins. DNA must be accessible (ATAC data) to produce RNA (GEX data), and RNA in turn is used as a template to produce protein (ADT data). These processes are regulated by feedback: for example, a protein may bind DNA to prevent the production of more RNA. This genetic regulation is the foundation for dynamic cellular processes that allow organisms to develop and adapt to changing environments. In single-cell data science, dynamic processes have been modeled by so-called pseudotime algorithms that capture the progression of the biological process. Yet, generalizing these algorithms to account for both pseudotime and real time is still an open problem.
Competition host Open Problems in Single-Cell Analysis is an open-source, community-driven effort to standardize benchmarking of single-cell methods. The core efforts of Open Problems include the formalization of existing challenges into measurable tasks, a collection of high-quality datasets, centralized benchmarking of community-contributed methods, and community-focused events that bring together diverse method developers to improve single-cell algorithms. They're excited to be partnering with Cellarity, Chan Zuckerbeg Biohub, the Chan Zuckerberg Initiative, Helmholtz Munich, and Yale to see what progress can be made in predicting changes in genetic dynamics over time through interdisciplinary collaboration.
There are approximately 37 trillion cells in the human body, all with different behaviors and functions. Understanding how a single genome gives rise to a diversity of cellular states is the key to gaining mechanistic insight into how tissues function or malfunction in health and disease. You can help solve this fundamental challenge for single-cell biology. Being able to solve the prediction problems over time may yield new insights into how gene regulation influences differentiation as blood and immune cells mature.
Competition header image by Pawel Czerwinski on Unsplash","We use the Pearson correlation coefficient to rank submissions. For each observation in the Multiome data set, we compute the correlation between the ground-truth gene expressions and the predicted gene expressions. For each observation in the CITEseq data set, we compute the correlation between ground-truth surface protein levels and predicted surface protein levels. The overall score is the average of each sample's correlation score. If a sample's predictions are all the same, the correlation for that sample is scored as -1.0.
Submission File
For each id in the evaluation set, you should predict a target value for that row_id. Your submission should contain a header and have the following format:
row_id,target
0,0.0
1,0.0
2,0.0
3,0.0
...
Your submission file should contain only a subset of the test set observations. See the Data Description for the specific ids that should be included.","The dataset for this competition comprises single-cell multiomics data collected from mobilized peripheral CD34+ hematopoietic stem and progenitor cells (HSPCs) isolated from four healthy human donors. More information about the cells can be found on the vendor website.
Measurements were taken at five time points over a ten-day period. During this time, cells were cultured with StemSpan SFEM media supplemented with CC100 and thrombopoietin (TPO) and incubated at 37ºC. Media was changed every 2-3 days. No additional media supplements were added to the cell culture conditions.
From each culture plate at each sampling time point, cells were collected for measurement with two single-cell assays. The first is the 10x Chromium Single Cell Multiome ATAC + Gene Expression technology (Multiome) and the second is the 10x Genomics Single Cell Gene Expression with Feature Barcoding technology technology using the TotalSeq™-B Human Universal Cocktail, V1.0 (CITEseq).
If you've never worked with this data type before, we've included some links at the bottom of this description.
Each assay technology measures two modalities. The Multiome kit measures chromatin accessibility (DNA) and gene expression (RNA), while the CITEseq kit measures gene expression (RNA) and surface protein levels.
Following the central dogma of molecular biology: DNA --> RNA-->Protein, your task is as follows:
For the Multiome samples: given chromatin accessibility, predict gene expression.
For the CITEseq samples: given gene expression, predict protein levels.",kaggle competitions download -c open-problems-multimodal,"['https://www.kaggle.com/code/ambrosm/msci-eda-which-makes-sense', 'https://www.kaggle.com/code/ambrosm/msci-citeseq-quickstart', 'https://www.kaggle.com/code/pourchot/all-in-one-citeseq-multiome-with-keras', 'https://www.kaggle.com/code/ambrosm/msci-citeseq-keras-quickstart', 'https://www.kaggle.com/code/vslaykovsky/lb-0-811-normalized-ensembles-for-pearson-s-r']"
30,"Help Henry Hacker get to Homecoming during DEFCON30 -- Brought to you by the AI Village! In this series of challenges, you'll be interacting with various machine learning security challenges.
The competition will be live from August 11th to September 12th @ 12:00. If you're in Vegas, stop by the village to chat about the competition. There's also the Kaggle Discussion Board and Discord.
Process
This capture-the-flag (CTF) follows a different flow than most Kaggle competitions. Competitors will be interacting with API endpoints or code/objects stored in the input directory during each of the challenges. Upon successful completion of a challenge, that challenge will return a flag (unique-to-you strings with a length of 128 characters). To update the scoreboard, competitors will submit a .csv containing all of their flags -- see the kaggle documentation or contact the competition organizers for help. Cumulative scores will be weighted based on the difficulty of the challenge. All competitors start at 0 and work their way towards a perfect score of 1.0. There are 22 challenges, ensure your submission.csv has exactly those 22 challenge rows.
NOTE: The template notebook is just a convenience function and method for submitting flags to the scoreboard. Don't feel constrained to that single operating environment. Interact with the challenges from your local host or any other machine that can access the internet. Afterwards, you can transport your flags into the notebook to update the scoreboard. The template is available here.
NOTE: If you want to interact with online challenges through Kaggle (using the template notebook, for instance), you may need to verify your Kaggle account using a phone number.
Here is a handy link to Kaggle's competition documentation, which includes, among other things, instructions on submitting predictions.
Paranoid? If you don't have a kaggle account and don't want to make one, let us know and we can give you instructions for playing the challenges from your own machine. You won't be able to contribute to the scoreboard, but you'll know when you get the right flag.
Please do not try and hack any infrastructure or share flags. Any teams found sharing flags will be disqualified.
Challenges
CTF's are inherently puzzles that are intended to challenge you and help you learn new things. Sometimes they may be a little ambiguous or misleading. That's part of the challenge!
Math Challenges: Four challenges to explore the concept of dimensionality.
Hotdog and Hotterdog: Dogs, wieners, and classifiers. What more could you want?
bad2good: Can you poison a dataset to change how something is classified?
baseball: Can you impersonate someone else by throwing the correct distribution of pitches?
crop: Two challenges to test your ability to manipulate an image cropping model.
deepfake: There's a nasty deepfake getting detected out there, can you help it?
honorstudent: Can you change an image of an F to look like an A? Why would someone want to do such a thing?
salt: This model has some pretty advanced defenses. Can you evade it anyway?
theft: Can you steal this model to get a sneaky owl past it?
token: Sentiment Analysis. Who needs?
waf: A web-app-firewall blocks malicious requests. Can you discover and by-pass the 0-day?
inference: I think something's backwards here. Can you, like, back something out?
forensics: Nice artifact you got there, shame if there was a flag in it.
leakage: Get a password out of a model, is that even possible?
murderbot: Save the humans, escape the bots!
secret_sloth: That sloth has a message. Why? I don't know, but it does.
wifi: Can you pull your wifi password out of the embedding?
Are you in? Of course you are. Come check it out by making a copy of this notebook: https://www.kaggle.com/lucasjt/getting-started
Help
For help, contact us on Discord, use the Kaggle discussion board, or if you're attending DEFCON 30 in-person, come find us at the AI Village.","The evaluation metric for this competition is weighted classification accuracy. You can think of the entire challenge as a multiclass classification problem where each challenge has a weight (difficulty) and unique label (flag). Instead of building a classifier to get the right accuracy, you're going to capture the flags. **Flags are unique to each person solving a challenge.** Please respect the spirit of the competition and do not share flags. Teams found sharing flags or using shared flags will be disqualified! ## Submission Format Submission files should contain two columns: `challenge_id` and `flag`. The file should contain a header and have the following format: There should be 22 rows (not counting the header). ``` challenge_id,flag math_1,WTHGOC3ZQBAZ5VT9IZP5W8YG1EWDAM5QVN86Q3UGYX812658WHZ8A8OHVPTSM5NL83CF93SGI7IU8DAHSFA81HJY68XC8BAP9XLXDN3KGR2Z8T9U18K1DTW1DZB8DNFN ... ``````","This CTF follows a different flow than Most Kaggle Competitions. Competitors will be interacting with API endpoints during each of the challenges. Upon successful completion of a challenge, that challenge will return a flag (a 128 character string). To update the scoreboard, competitors will submit a .csv containing all of their flags. You can do this by uploading a file with all of your flags. We've provided a template. Cumulative scores will be weighted based on the difficulty of the challenge. All competitors start at 0 and work their way towards a perfect score of 1.
Files
sample_submission.csv - a sample submission file in the correct format
Other files are used for specific challenges and referenced in the template notebook.
Columns
challenge_id - The challenge id (found in the markdown header)
flag - The case-sensitive flag",kaggle competitions download -c ai-village-ctf,"['https://www.kaggle.com/code/lucasjt/getting-started', 'https://www.kaggle.com/code/cdeotte/solutions-d3fc0n-ctf-lb-0-894', 'https://www.kaggle.com/code/gladwell/one-crop2-solution', 'https://www.kaggle.com/code/gazu468/classification-with-pytorch', 'https://www.kaggle.com/code/jonbown/ai-ctf-submissions']"
31,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!
With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in March every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc..
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Root Mean Squared Log Error (RMLSE)
Submissions are scored on the root mean squared log error (RMSLE) (the sklearn mean_squared_log_error with squared=False).
Submission File
For each id in the test set, you must predict the value for the target cost. The file should contain a header and have the following format:
id,cost
360336,99.615
360337,87.203
360338,101.111
etc.","The dataset for this competition (both train and test) was generated from a deep learning model trained on the Media Campaign Cost Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Files
train.csv - the training dataset; cost is the target
test.csv - the test dataset; your objective is to predict cost
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e11,"['https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e11-2023-eda-and-submission', 'https://www.kaggle.com/code/ambrosm/pss3e11-zoo-of-models', 'https://www.kaggle.com/code/kimtaehun/quick-eda-and-simple-baseline-with-xgb', 'https://www.kaggle.com/code/janmpia/feature-eng-xgb-cat-ensemble-0-29265', 'https://www.kaggle.com/code/anushreek15/playground-series']"
32,"Goal of the Competition
The goal of this competition is to analyze horse racing tactics, drafting strategies, and path efficiency. You will develop a model using never-before-released coordinate data along with basic race information.
Your work will help racing horse owners, trainers, and veterinarians better understand how equine performance and welfare fit together. With better data analysis, equine welfare could significantly improve.
Context
Injury prevention is a critical component in modern athletics. Sports that involve animals, such as horse racing, are no different than human sport. Typically, efficiency in movement correlates to both improvements in performance and injury prevention.
A wealth of data is now collected, including measures for heart rate, EKG, longitudinal movement, dorsal/ventral movement, medial/lateral deviation, total power and total landing vibration. Your data science skills and analysis are needed to decipher what makes the most positive impact.
In this competition, you will create a model to interpret one aspect of this new data. You’ll be among the first to access X/Y coordinate mapping of horses during races. Using the data, you might analyze jockey decision making, compare race surfaces, or measure the relative importance of drafting. With considerable data, contestants can flex their creativity problem solving skills.
The New York Racing Association (NYRA) and the New York Thoroughbred Horsemen's Association (NYTHA) conduct world class thoroughbred racing at Aqueduct Racetrack, Belmont Park and Saratoga Race Course.
With your help, NYRA and NYTHA will better understand their vast data set, which could lead to new ways of racing and training in a highly traditional industry. With improved use of horse tracking data, you could help improve equine welfare, performance and rider decision making.","Our sport is currently investing significant money in collecting far more precise tracking data in the hopes of improving equine welfare. Along with stride data, we can now collect measures for heart rate, EKG, longitudinal movement, dorsal/ventral movement, medial/lateral deviation, total power and total landing vibration. However, we do not have analysts with the appropriate expertise to help decipher these data sets.
We hope this competition allows us to interact with data scientists to help find solutions to equine safety issues as well as develop a roster of academics and motivated hobbyists who lead us in analyzing the coming generations of data.
Overview
Your challenge is to generate actionable, practical, and novel insights from horse tracking data that devises innovative and data-driven approaches to analyzing racing tactics, drafting strategies and path efficiency. There are several potential topics for participants to analyze.
These include, but are not limited to:
Create a horse rating measuring expected finish position versus actual finish position. How does a horse’s expected finish position change through the running of a race? Does this metric rely solely on a horse’s own position or is it influenced by the position of competitors?
What are optimal racing strategies? Considering different venues, surfaces and race distances. Create a jockey rating based upon path efficiency?
Create a surface measure model which would rate the fairness of different paths on a racecourse that may be beneficial or harmful to finish position based. This may be a result of unknown barometric, weather or maintenance factors.
Create a model measuring the existence (or not) and relevance of a drafting benefit.
Create a model reveal optimal gait patterns. Does the model differ for such factors as age, distance, race section or surface?
Contestants should not feel limited to these suggestions.
The above list is not comprehensive, nor is it meant to be a guide for participants to cover.
Submissions that examine one idea more thoroughly are preferred versus those that examine several ideas somewhat thoroughly.
Scoring
An entry to the competition consists of a Notebook submission that is evaluated on the following five components, where 0 is the low score and 100 is the high score. Submissions will be judged based on how well they address:
Innovation (25 Points Total)
Is this a novel way of looking at tracking data? (10 Pts)
Are the statistical/machine learning approaches using the most up-to-date standards? (5 Pts)
Will the conclusions challenge the status quo of horse racing methods? (10 Pts)
Relevance (30 Points Total)
Can the conclusions influence equine welfare, equine performance or rider decision making? (10 Points)
Can the conclusions be the basis of future research on future (larger, more granular) data sets? (10 Points)
Are the conclusions something that horse racing participants (e.g. owners, trainers, veterinarians) can understand, digest and debate? (10 Points)
Competence (25 Points Total)
Given the data, are the statistical models appropriate? (5 Points)
Are the conclusions supported by the data? (10 Points)
Is the analysis accurate? (10 Points)
Presentation (20 Points Total)
Is the writing clear and free of nomenclature? (5 Points)
Are the charts and tables provided interesting, visually appealing, and accurate? (5 Points)
Can the analysis thread be followed throughout the presentation? (10 Points)
Notebook requirements
All notebooks submitted must be made public on or before the submission deadline to be eligible. If submitting as a team, all team members must be listed as collaborators on all notebooks submitted.","File descriptions
nyra_start_table.csv - horse/jockey race data
nyra_race_table.csv - racetrack race data
nyra_tracking_table.csv - tracking data
nyra_2019_complete.csv - combined table of three above files
Columns
nyra_start_table.csv
track_id - 3 character id for the track the race took place at. AQU -Aqueduct, BEL - Belmont, SAR - Saratoga.
race_date - date the race took place. YYYY-MM-DD.
race_number - Number of the race. Passed as 3 characters but can be cast or converted to int for this data set.
program_number - Program number of the horse in the race passed as 3 characters. Should remain 3 characters as it isn't limited to just numbers. Is essentially the unique identifier of the horse in the race.
weight_carried - An integer of the weight carried by the horse in the race.
jockey - Name of the jockey on the horse in the race. 50 character max.",kaggle competitions download -c big-data-derby-2022,"['https://www.kaggle.com/code/mattop/big-data-derby-2022-eda', 'https://www.kaggle.com/code/tanmay111999/big-data-derby-2022-eda-insights', 'https://www.kaggle.com/code/iamleonie/big-data-derby-step-by-step-analysis', 'https://www.kaggle.com/code/kushtrivedi14728/advance-eda-with-ml-big-data-derby', 'https://www.kaggle.com/code/bkumagai/bayesian-velocity-models-for-horse-race-simulation']"
33,"Goal of the Competition
Join the $1,000,000+ Vesuvius Challenge to resurrect an ancient library from the ashes of a volcano. In this competition you are tasked with detecting ink from 3D X-ray scans and reading the contents. Thousands of scrolls were part of a library located in a Roman villa in Herculaneum, a town next to Pompeii. This villa was buried by the Vesuvius eruption nearly 2000 years ago. Due to the heat of the volcano, the scrolls were carbonized, and are now impossible to open without breaking them. These scrolls were discovered a few hundred years ago and have been waiting to be read using modern techniques. There is a $700,000 grand prize available to the first team that can read these scrolls from a 3D X-ray scan. This Kaggle competition hosts the Ink Detection Progress Prize.
Prizes breakdown:
Grand Prize - $700,000
Ink Detection Progress Prize on Kaggle - $100,000 in prizes
Segmentation Tooling Prize - $45,000 in prizes
First Letters Prize - $50,000 in prizes
To be announced - $200,000+
One of the scrolls, which cannot be physically opened (source)
This Kaggle competition
This Kaggle competition hosts the Ink Detection Progress Prize ($100,000 in prizes), which is about the sub-problem of detecting ink from 3d x-ray scans of fragments of papyrus which became detached from some of the excavated scrolls. This subcontest is run on Kaggle since it's a more traditional data science / machine learning problem of building a model that can be verified against known ground truth data.
The ink used in the Herculaneum scrolls does not show up readily in X-ray scans. But we have found that machine learning models can detect it. Luckily, we have ground truth data. Since the discovery of the Herculaneum Papyri almost 300 years ago, people have tried opening them, often with disastrous results. Many scrolls were destroyed in this process, but ink can be seen on some broken-off fragments, especially under infrared light.
The dataset contains 3d x-ray scans of four such fragments at 4µm resolution, made using a particle accelerator, as well as infrared photographs of the surface of the fragments showing visible ink. These photographs have been aligned with the x-ray scans. We also provide hand-labeled binary masks indicating the presence of ink in the photographs.
Get started exploring this problem:
Tutorial
Notebook
About the competition
Vesuvius Challenge is created by Nat Friedman and Daniel Gross, and is hosted in collaboration with EduceLab. We appreciate Kaggle’s help with this Ink Detection competition. For more information on the organizing team, partner organizations, and other background, check out scrollprize.org.

This is a Code Competition. Refer to Code Requirements for details.","We evaluate how well your output image matches our reference image using a modified version of the Sørensen–Dice coefficient, where instead of using the F1 score, we are using the F0.5 score. The F0.5 score is given by:
$$\frac{(1 + \beta^2) pr}{\beta^2 p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn},\ \beta = 0.5$$
The F0.5 score weights precision higher than recall, which improves the ability to form coherent characters out of detected ink areas.
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
Note that, at the time of encoding, the output should be binary, with 0 indicating ""no ink"" and 1 indicating ""ink"".
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from left to right, then top to bottom: 1 is pixel (1,1), 2 is pixel (1,2), etc.
Your output should be a single file, submission.csv, with this run-length encoded information. This should have a header with two columns, Id and Predicted, and with one row for every directory under test/. For example:
Id,Predicted
a,1 1 5 1 etc.
b,10 20 etc.
For a real-world example of what these files look like, see inklabels_rce.csv in the data directories, which have been generated with this script. We also show how to output a file of this format in the Ink Detection tutorial.","Your challenge is to recover where ink is present from 3d x-ray scans of detached fragments of ancient papyrus scrolls. This is an important subproblem in the overall task of solving the Vesuvius Challenge.
This is a Code Competition. When your submitted notebook is scored, the actual test data will be made available to your notebook. Before that, the test/ directory will contain dummy data. This is done to keep the actual test data secret.
Files
[train/test]/[fragment_id]/surface_volume/[image_id].tif slices from the 3d x-ray surface volume. Each file contains a greyscale slice in the z-direction. Each fragment contains 65 slices. Combined this image stack gives us width * height * 65 number of voxels per fragment. You can expect two fragments in the hidden test set, which together are roughly the same size as a single training fragment. The sample slices available to download in the test folders are simply copied from training fragment one, but when you submit your notebook they will be substituted with the real test data.
[train/test]/[fragment_id]/mask.png — a binary mask of which pixels contain data.
train/[fragment_id]/inklabels.png — a binary mask of the ink vs no-ink labels.
train/[fragment_id]/inklabels_rle.csv — a run-length-encoded version of the labels, generated using this script. This is the same format as you should make your submission in.
train/[fragment_id]/ir.png — the infrared photo on which the binary mask is based.",kaggle competitions download -c vesuvius-challenge-ink-detection,"['https://www.kaggle.com/code/danielhavir/vesuvius-challenge-example-submission', 'https://www.kaggle.com/code/jpposma/vesuvius-challenge-ink-detection-tutorial', 'https://www.kaggle.com/code/fchollet/keras-starter-kit-unet-train-on-full-dataset', 'https://www.kaggle.com/code/tanakar/2-5d-segmentaion-baseline-training', 'https://www.kaggle.com/code/tanakar/2-5d-segmentaion-baseline-inference']"
34,"Goal of the Competition
The goal of this competition is to detect freezing of gait (FOG), a debilitating symptom that afflicts many people with Parkinson’s disease. You will develop a machine learning model trained on data collected from a wearable 3D lower back sensor.
Your work will help researchers better understand when and why FOG episodes occur. This will improve the ability of medical professionals to optimally evaluate, monitor, and ultimately, prevent FOG events.
Context
An estimated 7 to 10 million people around the world have Parkinson’s disease, many of whom suffer from freezing of gait (FOG). During a FOG episode, a patient's feet are “glued” to the ground, preventing them from moving forward despite their attempts. FOG has a profound negative impact on health-related quality of life—people who suffer from FOG are often depressed, have an increased risk of falling, are likelier to be confined to wheelchair use, and have restricted independence.
While researchers have multiple theories to explain when, why, and in whom FOG occurs, there is still no clear understanding of its causes. The ability to objectively and accurately quantify FOG is one of the keys to advancing its understanding and treatment. Collection and analysis of FOG events, such as with your data science skills, could lead to potential treatments.
There are many methods of evaluating FOG, though most involve FOG-provoking protocols. People with FOG are filmed while performing certain tasks that are likely to increase its occurrence. Experts then review the video to score each frame, indicating when FOG occurred. While scoring in this manner is relatively reliable and sensitive, it is extremely time-consuming and requires specific expertise. Another method involves augmenting FOG-provoking testing with wearable devices. With more sensors, the detection of FOG becomes easier, however, compliance and usability may be reduced. Therefore, a combination of these two methods may be the best approach. When combined with machine learning methods, the accuracy of detecting FOG from a lower back accelerometer is relatively high. However, the datasets used to train and test these algorithms have been relatively small and generalizability is limited to date. Furthermore, the emphasis has been on achieving high levels of accuracy, while precision, for example, has largely been ignored.
Competition host, the Center for the Study of Movement, Cognition, and Mobility (CMCM), Neurological Institute, Tel Aviv Sourasky Medical Center, aims to improve the personalized treatment of age-related movement, cognition, and mobility disorders and to alleviate the associated burden. They leverage a combination of clinical, engineering, and neuroscience expertise to: 1) Gain new understandings into the physiologic and pathophysiologic mechanisms that contribute to cognitive and motor function, the factors that influence these functions, and their changes with aging and disease (e.g., Parkinson’s disease, Alzheimer’s). 2) Develop new methods and tools for the early detection and tracking of cognitive and motor decline. A major focus is on using leveraging wearable devices and digital technologies; and 3) Develop and evaluate novel methods for the prevention and treatment of gait, falls, and cognitive function.
Your work will help advance the evaluation, understanding and treatment of FOG, improving the lives of the many people who suffer from this debilitating Parkinson’s disease symptom.
Acknowledgments
The competition data was collected by three research groups:
The Center for the Study of Movement, Cognition and Mobility, as indicated above,
The Neurorehabilitation Research Group at Katholieke Universiteit Leuven in Belgium, and the
Mobility and Falls Translational Research Center at the Hinda and Arthur Marcus Institute for Aging, affiliated with Harvard Medical School in Boston.
The Michael J. Fox Foundation for Parkinson’s Research generously supported the data collection and this data competition.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated by the Mean Average Precision of predictions for each event class. We compute the average precision on predicted confidence scores separately for each of the three event classes (see the Data Description for more details) and take the average of these three scores to get the overall score.
Note that data series in the DeFOG dataset are annotated with Valid and Task labels (in addition to the event labels). Only the portions of the series where both are true should you consider to be annotated. Though not included in the test set series, the metric is aware of these labels and will ignore predictions on the unannotated portions of these series (where either label is false).
Submission File
For each Id in the test set, you must predict a confidence score for each of the three event types. In the ground truth, at most one event class has a non-zero value for each Id, but there is no restriction on the values of predicted scores. The predicted scores may, but are not required, to take the form of a probability (to be in the range 0.0 to 1.0, that is).
The Id values in the submission file should have the form {SeriesId}_{Time} where SeriesId is the identifier of the data series and Time is the time-step in that series of the predictions.
The file should contain a header and have the following format:
Id,StartHesitation,Turn,Walking
003f117e14_0,0,0,0
003f117e14_1,0,0,0
003f117e14_2,0,0,0
003f117e14_3,0,0,0","This competition dataset comprises lower-back 3D accelerometer data from subjects exhibiting freezing of gait episodes, a disabling symptom that is common among people with Parkinson's disease. Freezing of gait (FOG) negatively impacts walking abilities and impinges locomotion and independence.
Your objective is to detect the start and stop of each freezing episode and the occurrence in these series of three types of freezing of gait events: Start Hesitation, Turn, and Walking.
The Datasets
The data series include three datasets, collected under distinct circumstances:
The tDCS FOG (tdcsfog) dataset, comprising data series collected in the lab, as subjects completed a FOG-provoking protocol.
The DeFOG (defog) dataset, comprising data series collected in the subject's home, as subjects completed a FOG-provoking protocol
The Daily Living (daily) dataset, comprising one week of continuous 24/7 recordings from sixty-five subjects. Forty-five subjects exhibit FOG symptoms and also have series in the defog dataset, while the other twenty subjects do not exhibit FOG symptoms and do not have series elsewhere in the data.
Trials from the tdcsfog and defog datasets were videotaped and annotated by expert reviewers documented the freezing of gait episodes. That is, the start, end and type of each episode were marked by the experts. Series in the daily dataset are . You will be detecting FOG episodes for the and series. You may wish to apply unsupervised or semi-supervised methods to the series in the dataset to support your detection modelling.",kaggle competitions download -c tlvmc-parkinsons-freezing-gait-prediction,"['https://www.kaggle.com/code/leonidkulyk/eda-pfogp-interactive-visualisations', 'https://www.kaggle.com/code/kimtaehun/simple-lgbm-multi-class-classification-baseline', 'https://www.kaggle.com/code/mayukh18/pytorch-fog-end-to-end-baseline-lb-0-254', 'https://www.kaggle.com/code/xzj19013742/simple-eda-on-time-for-targets', 'https://www.kaggle.com/code/andradaolteanu/freezingofgate-xgboost-multiclass-with-rapids']"
35,"The August 2022 edition of the Tabular Playground Series is an opportunity to help the fictional company Keep It Dry improve its main product Super Soaker. The product is used in factories to absorb spills and leaks.
The company has just completed a large testing study for different product prototypes. Can you use this data to build a model that predicts product failures?
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
Good luck and have fun!
Photo above by freestocks on Unsplash","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability a failure. The file should contain a header and have the following format:
id,failure
26570,0.2
26571,0.1
26572,0.9
etc.","This data represents the results of a large product testing study. For each product_code you are given a number of product attributes (fixed for the code) as well as a number of measurement values for each individual product, representing various lab testing methods. Each product is used in a simulated real-world environment experiment, and and absorbs a certain amount of fluid (loading) to see whether or not it fails.
Your task is to use the data to predict individual product failures of new codes with their individual lab test results.
Files
train.csv - the training data, which includes the target failure
test.csv - the test set; your task is to predict the likelihood each id will experience a failure
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-aug-2022,"['https://www.kaggle.com/code/ambrosm/tpsaug22-eda-which-makes-sense', 'https://www.kaggle.com/code/devsubhash/tps-august-eda-failure-prediction', 'https://www.kaggle.com/code/usamabalochhh/data-is-imbalanced-use-smote', 'https://www.kaggle.com/code/azminetoushikwasi/classification-comparing-different-algorithms', 'https://www.kaggle.com/code/desalegngeb/tps08-logisticregression-qlattice']"
36,"NOTE:
You can now create your own synthetic versions of this dataset by forking and running this notebook.
Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.
First, the series is getting upgraded branding. We've dropped ""Tabular"" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.
Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.
Regardless of these changes, the goals of the Playground Series remain the same—to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!
To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Root Mean Squared Error (RMSE)
Submissions are scored on the root mean squared error. RMSE is defined as:
$$
\textrm{RMSE} = \sqrt{ \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 }
$$
where \( \hat{y}_i \) is the predicted value and \( y_i \) is the original value for each instance \(i\).
Submission File
For each id in the test set, you must predict the value for the target MedHouseVal. The file should contain a header and have the following format:
id,MedHouseVal
37137,2.01
37138,0.92
37139,1.11
etc.","NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook.
The dataset for this competition (both train and test) was generated from a deep learning model trained on the California Housing Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Files
train.csv - the training dataset; MedHouseVal is the target
test.csv - the test dataset; your objective is to predict MedHouseVal
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e1,"['https://www.kaggle.com/code/inversion/make-synthetic-california-housing-data', 'https://www.kaggle.com/code/radek1/eda-training-a-first-model-submission', 'https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory', 'https://www.kaggle.com/code/phongnguyen1/feature-engineering-with-coordinates', 'https://www.kaggle.com/code/ravi20076/playgrounds3e1-eda-model-pipeline']"
37,"Goal of the Competition
Birds are excellent indicators of biodiversity change since they are highly mobile and have diverse habitat requirements. Changes in species assemblage and the number of birds can thus indicate the success or failure of a restoration project. However, frequently conducting traditional observer-based bird biodiversity surveys over large areas is expensive and logistically challenging. In comparison, passive acoustic monitoring (PAM) combined with new analytical tools based on machine learning allows conservationists to sample much greater spatial scales with higher temporal resolution and explore the relationship between restoration interventions and biodiversity in depth.
For this competition, you'll use your machine-learning skills to identify Eastern African bird species by sound. Specifically, you'll develop computational solutions to process continuous audio data and recognize the species by their calls. The best entries will be able to train reliable classifiers with limited training data. If successful, you'll help advance ongoing efforts to protect avian biodiversity in Africa, including those led by the Kenyan conservation organization NATURAL STATE.
💡Getting Started Notebook
To get started quickly, we made this starter notebook that generates a submission using the new Bird Vocalization Classifier model. It was recently open-sourced by the Google Research team on Kaggle Models.
Context
NATURAL STATE is working in pilot areas around Northern Mount Kenya to test the effect of various management regimes and states of degradation on bird biodiversity in rangeland systems. By using the machine learning algorithms developed within the scope of this competition, NATURAL STATE will be able to demonstrate the efficacy of this approach in measuring the success of restoration projects and the cost-effectiveness of the method. In addition, the ability to cost-effectively monitor the impact of restoration efforts on biodiversity will allow NATURAL STATE to test and build some of the first biodiversity-focused financial mechanisms to channel much-needed investment into the restoration and protection of this landscape upon which so many people depend. These tools are necessary to scale this cost-effectively beyond the project area and achieve our vision of restoring and protecting the planet at scale.
Thanks to your innovations, it will be easier for researchers and conservation practitioners to survey avian population trends accurately. As a result, they'll be able to evaluate threats and adjust their conservation actions regularly and more effectively.
This competition is collaboratively organized by (alphabetic order) the Chemnitz University of Technology, Google Research, K. Lisa Yang Center for Conservation Bioacoustics at the Cornell Lab of Ornithology, LifeCLEF, NATURAL STATE, OekoFor GbR, and Xeno-canto.

This is a Code Competition. Refer to Code Requirements for details.","The evaluation metric for this contest is padded cmAP, a derivative of the macro-averaged average precision score as implemented by scikit-learn. In order to support accepting predictions for species with zero true positive labels and to reduce the impact of species with very few positive labels, prior to scoring we pad each submission and the solution with five rows of true positives. This means that even a baseline submission will get a relatively strong score.
Submission Format
For each row_id, you should predict the probability that a given bird species was present. There is one column per bird species so you will need to provide 264 predictions per row. Each row covers a five second window of audio.
Example metric code
import pandas as pd
import sklearn.metrics

def padded_cmap(solution, submission, padding_factor=5):
    solution = solution.drop(['row_id'], axis=1, errors='ignore')
    submission = submission.drop(['row_id'], axis=1, errors='ignore')
    new_rows = []
    for i in range(padding_factor):
        new_rows.append([1 for i in range(len(solution.columns))])
    new_rows = pd.DataFrame(new_rows)
    new_rows.columns = solution.columns
    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()
    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()
    score = sklearn.metrics.average_precision_score(
        padded_solution.values,
        padded_submission.values,
        average='macro',
    )
    return score
The deployed implementation is here.
Working Note Award Criteria (optional)
Criteria for the BirdCLEF best working note award:
Originality. The value of a paper is a function of the degree to which it presents new or novel technical material. Does the paper present results previously unknown? Does it push forward the frontiers of knowledge? Does it present new methods for solving old problems or new viewpoints on old problems? Or, on the other hand, is it a re-hash of information already known?
Quality. A paper's value is a function of the innate character or degree of excellence of the work described. Was the work performed, or the study made with a high degree of thoroughness? Was high engineering skill demonstrated? Is an experiment described which has a high degree of elegance? Or, on the other hand, is the work described pretty much of a run-of-the-mill nature?
Contribution. The value of a paper is a function of the degree to which it represents an overall contribution to the advancement of the art. This is different from originality. A paper may be highly original but may be concerned with a very minor, or even insignificant, matter or problem. On the other hand, a paper may make a great contribution by collecting and analyzing known data and facts and pointing out their significance. Or, a fine exposition of a known but obscure or complex phenomenon or theory or system or operating technique may be a very real contribution to the art. Obviously, a paper may well score highly on both originality and contribution. Perhaps a significant question is, will the engineer who reads the paper be able to practice his profession more effectively because of having read it?
Presentation. The value of the paper is a function of the ease with which the reader can determine what the author is trying to present. Regardless of the other criteria, a paper is not good unless the material is presented clearly and effectively. Is the paper well written? Is the meaning of the author clear? Are the tables, charts, and figures clear? Is their meaning readily apparent? Is the information presented in the paper complete? At the same time, is the paper concise?
Evaluation of the submitted BirdCLEF working notes:
Each working note will be reviewed by two reviewers and scores averaged. Maximum score: 15.
a) Evaluation of work and contribution
5 points: Excellent work and a major contribution
4 points: Good solid work of some importance
3 points: Solid work but a marginal contribution
2 points: Marginal work and minor contribution
1 point: Work doesn't meet scientific standards
b) Originality and novelty
5 points Trailblazing
4 points: A pioneering piece of work
3 points: One step ahead of the pack
2 points: Yet another paper about…
1 point: It's been said many times before
c) Readability and organization
5 points: Excellent
4 points: Well written
3 points: Readable
2 points: Needs considerable work
1 point: Work doesn't meet scientific standards","Your challenge in this competition is to identify which birds are calling in long recordings made in Kenya. This is an important task for scientists who monitor bird populations for conservation purposes. More accurate solutions could enable more comprehensive monitoring. This year, your notebook must also complete inference in a more constrained time frame. This will make it easier to deploy winning models for on the ground conservation efforts where efficiency is at a premium.
This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook.
Files
train_audio/ The training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. The training data should have nearly all relevant files; we expect there is no benefit to looking for more on xenocanto.org.
test_soundscapes/ When you submit a notebook, the test_soundscapes directory will be populated with approximately 200 recordings to be used for scoring. They are 10 minutes long and in ogg audio format. The file names are randomized. It should take your submission notebook approximately five minutes to load all of the test soundscapes.
train_metadata.csv A wide range of metadata is provided for the training data. The most directly relevant fields are:
primary_label - a code for the bird species. You can review detailed information about the bird codes by appending the code to , such as for the American Crow.",kaggle competitions download -c birdclef-2023,"['https://www.kaggle.com/code/philculliton/inferring-birds-with-kaggle-models', 'https://www.kaggle.com/code/awsaf49/birdclef23-pretraining-is-all-you-need-train', 'https://www.kaggle.com/code/awsaf49/birdclef23-pretraining-is-all-you-need-infer', 'https://www.kaggle.com/code/awsaf49/birdclef23-effnet-fsr-cutmixup-train', 'https://www.kaggle.com/code/burhanuddinlatsaheb/eda-visualizations-audio-exploration']"
38,"Goal of the Competition
Goal! In this competition, you'll detect football (soccer) passes—including throw-ins and crosses—and challenges in original Bundesliga matches. You'll develop a computer vision model that can automatically classify these events in long video recordings.
Your work will help scale the data collection process. Automatic event annotation could enable event data from currently unexplored competitions, like youth or semi-professional leagues or even training sessions.
Context
What does it take to go pro in football (soccer)? From a young age, hopeful talents devote time, money, and training to the sport. Yet, while the next superstar is guaranteed to start off in youth or semi-professional leagues, these leagues often have the fewest resources to invest. This includes resources for the collection of event data which helps generate insights into the performance of the teams and players.
Currently, event data is mostly collected manually by human operators, who gather data in several steps and through numerous personnel involved. This manual process has room for innovation as in its current shape and form it involves a lot of resources and multiple iterations/quality checks. As a result, event data collection is usually reserved for professional competitions only.
Based in Frankfurt, the Deutsche Fußball Liga (DFL) manages Germany's professional football (soccer) leagues: Bundesliga and Bundesliga 2. DFL partners with the operator of one of the largest sports databases in the world, Sportec Solutions. They're responsible for the leagues' sports data and sports technology activities. In addition, Sportec Solutions provides services to global sports entities and media companies.
Automatic event detection could provide event data faster and with greater depth. Having access to a broader range of competitions, match conditions and data scouts would be able to ensure no talented player is overlooked.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on the average precision of detected events, averaged over timestamp error thresholds, averaged over event classes.
Detections are matched to ground-truth events by class-specific error tolerances, with ambiguities resolved in order of decreasing confidence. The timestamp error tolerances, in seconds, for each class are:
Challenge: [ 0.30, 0.40, 0.50, 0.60, 0.70 ]
Play: [ 0.15, 0.20, 0.25, 0.30, 0.35 ]
Throw-In: [ 0.15, 0.20, 0.25, 0.30, 0.35 ]
You may find a Python implementation of the metric in this notebook: Competition Metric - DFL Event Detection AP.
Detailed Description
Evaluation proceeds in four steps:
Selection - Predictions not within a video's scoring intervals are dropped.
Assignment - Predicted events are matched with ground-truth events.
Scoring - Each group of predictions is scored against its corresponding group of ground-truth events via Average Precision.
Reduction - The multiple AP scores are averaged to produce a single overall score.
Selection
With each video there is a defined set of scoring intervals giving the intervals of time over which zero or more ground-truth events might be annotated in that video. A prediction is only evaluated if it falls within a scoring interval. These scoring intervals were chosen to improve the fairness of evaluation by, for instance, ignoring edge-cases or ambiguous events.
For reference, we provide the scoring intervals for the training data. The scoring intervals for the test data will not be available to your model during evaluation, however.
Assignment
For each set of predictions and ground-truths within the same event x tolerance x video_id group, we match each ground-truth to the highest-confidence unmatched prediction occurring within the allowed tolerance.
Some ground-truths may not be matched to a prediction and some predictions may not be matched to a ground-truth. They will still be accounted for in the scoring, however.
Scoring
Collecting the events within each video_id, we compute an Average Precision score for each event x tolerance group. The average precision score is the area under the precision-recall curve generated by decreasing confidence score thresholds over the predictions. In this calculation, matched predictions over the threshold are scored as TP and unmatched predictions as FP. Unmatched ground-truths are scored as FN.
Reduction
The final score is the average of the above AP scores, first averaged over tolerance, then over event.
Submission File
For each video indicated by video_id, predict each event occurring in that video by giving the event type and the time of occurrence (in seconds) as well as a confidence score for that event.
The file should contain a header and have the following format:
video_id,time,event,score
13bfe65e_0,100.0,play,0.0
13bfe65e_0,110.5,challenge,0.07
13bfe65e_1,90.25,throwin,0.13
13bfe65e_1,105.0,play,0.2
40f283bc_0,100.0,challenge,0.27
40f283bc_0,110.5,throwin,0.33
...
Only predictions occurring within a video's defined scoring intervals will be scored; the metric ignores any predictions occurring outside of these scoring intervals. You will not be given the scoring intervals for videos in the test set.","The competition dataset comprises video recordings of nine football games divided into halves. You're challenged to detect three kinds of player events, both the time of occurrence and the type, within these videos. See the Event Descriptions page for a full description of each event type.
This is a Code Competition that will run in two stages. During the training stage, your submission will only be run against test data for the public leaderboard. The test data for the private leaderboard, however, will comprise games occuring after the training period closes, the forecasting stage.
Training Data
train/ - Folder containing videos to be used as training data, comprising video recordings from eight games. Both halves are included for four of the games, while only one half is included for the other four games.
test/ - Folder containing videos to be used as test data. The test data for the public leaderboard comprises video recordings from one full game and four half-games, the other half of each game being in the training set.
clips/ - Short clips from ten additional games, provided without event annotations. You may wish to use these clips to help your model generalize to environments not represented in the training data.
train.csv - Event annotations for videos in the train/ folder.
video_id - Identifies which video the event occurred in.
event - The type of event occurrence, one of challenge, play, or throwin. Also present are labels start and end indicating the scoring intervals of the video. See the Evaluation page for information about scoring intervals. Scoring intervals are not provided for the test set videos.",kaggle competitions download -c dfl-bundesliga-data-shootout,"['https://www.kaggle.com/code/ihelon/bundesliga-data-shootout-eda', 'https://www.kaggle.com/code/its7171/dfl-benchmark-inference', 'https://www.kaggle.com/code/its7171/dfl-benchmark-training', 'https://www.kaggle.com/code/jobayerhossain/yolov7-explanation-and-implementation-from-scratch', 'https://www.kaggle.com/code/shinmurashinmura/dfl-yolov5-ball-detection']"
39,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!
With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in March every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc..
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right],
$$
where
\( n \) is the number of rows in the test set
\( \hat{y}_i \) is the predicted probability the Class is a pulsar
\( y_i \) is 1 if Class is pulsar, otherwise 0
\( log \) is the natural logarithm
The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
For each id in the test set, you must predict the value for the target Class. The file should contain a header and have the following format:
id,Strength
117564,0.11
117565,0.32
117566,0.95
etc.","The dataset for this competition (both train and test) was generated from a deep learning model trained on the Pulsar Classification. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Files
train.csv - the training dataset; Class is the (binary) target
test.csv - the test dataset; your objective is to predict the probability of Class (whether the observation is a pulsar)
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e10,"['https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e10-2023-eda-and-submission', 'https://www.kaggle.com/code/kimtaehun/simple-eda-and-baseline-with-lgbm-in-5minutes', 'https://www.kaggle.com/code/javohirtoshqorgonov/pulsar-stars-binary-classification-research', 'https://www.kaggle.com/code/jcaliz/ps-s03e10-a-complete-eda', 'https://www.kaggle.com/code/paddykb/ps-s3e10-gam-finger-on-the-pulsarrrrr']"
40,"Goal of the Competition
The goal of this competition is to predict monthly microbusiness density in a given area. You will develop an accurate model trained on U.S. county-level data.
Your work will help policymakers gain visibility into microbusinesses, a growing trend of very small entities. Additional information will enable new policies and programs to improve the success and impact of these smallest of businesses.
Context
American policy leaders strive to develop economies that are more inclusive and resilient to downturns. They're also aware that with advances in technology, entrepreneurship has never been more accessible than it is today. Whether to create a more appropriate work/life balance, to follow a passion, or due to loss of employment, studies have demonstrated that Americans increasingly choose to create businesses of their own to meet their financial goals. The challenge is that these ""microbusinesses"" are often too small or too new to show up in traditional economic data sources, making it nearly impossible for policymakers to study them. But data science could help fill in the gaps and provide insights into the factors associated these businesses.
Over the past few years the Venture Forward team at GoDaddy has worked hard to produce data assets about the tens of millions of microbusinesses in the United States. Microbusinesses are generally defined as businesses with an online presence and ten or fewer employees. GoDaddy has visibility into more than 20 million of them, owned by more than 10 million entrepreneurs. We've surveyed this universe of microbusiness owners for several years and have collected a great deal of information on them that you can access via our survey data here.
Current models leverage available internal and census data, use econometric approaches, and focus on understanding primary determinants. While these methods are adequate, there's potential to include additional data and using more advanced approaches to improve predictions and to better inform decision-making.
Competition host GoDaddy is the world’s largest services platform for entrepreneurs around the globe. They're on a mission to empower their worldwide community of 20+ million customers—and entrepreneurs everywhere—by giving them all the help and tools they need to grow online.
Your work will help better inform policymakers as they strive to make the world a better place for microbusiness entrepreneurs. This will have a real and substantial impact on communities across the country and will help our broader economy adapt to a constantly evolving world.","Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.
Submission File
For each row_id you must predict the microbusiness_density. The file should contain a header and have the following format:
row_id,microbusiness_density
1001_2022-11-01,1.2
1002_2022-11-01,2.3
1003_2022-11-01,3.4
etc.
The submission file will remain unchanged throughout the competition. However, the actively scored dates will be updated as new data becomes available. During the active phase of the competition only the most recent month of data will be used for the public leaderboard.","Your challenge in this competition is to forecast microbusiness activity across the United States, as measured by the density of microbusinesses in US counties. Microbusinesses are often too small or too new to show up in traditional economic data sources, but microbusiness activity may be correlated with other economic indicators of general interest.
As historic economic data are widely available, this is a forecasting competition. The forecasting phase public leaderboard and final private leaderboard will be determined using data gathered after the submission period closes. You will make static forecasts that can only incorporate information available before the end of the submission period. This means that while we will rescore submissions during the forecasting period we will not rerun any notebooks.
Files
A great deal of data is publicly available about counties and we have not attempted to gather it all here. You are strongly encouraged to use external data sources for features.
train.csv
row_id - An ID code for the row.
cfips - A unique identifier for each county using the Federal Information Processing System. The first two digits correspond to the state FIPS code, while the following 3 represent the county.
county_name - The written name of the county.
state_name - The name of the state.",kaggle competitions download -c godaddy-microbusiness-density-forecasting,"['https://www.kaggle.com/code/titericz/better-xgb-baseline', 'https://www.kaggle.com/code/cdeotte/linear-regression-baseline-lb-1-092', 'https://www.kaggle.com/code/cdeotte/seasonal-model-with-validation-lb-1-091', 'https://www.kaggle.com/code/tanmay111999/gdmbf-ar-ma-arma-arima-sarima-auto-arima', 'https://www.kaggle.com/code/kimtaehun/complete-baseline-code-with-various-ml-model']"
41,"Goal of the Competition
Over 1.5 million spine fractures occur annually in the United States alone resulting in over 17,730 spinal cord injuries annually. The most common site of spine fracture is the cervical spine. There has been a rise in the incidence of spinal fractures in the elderly and in this population, fractures can be more difficult to detect on imaging due to superimposed degenerative disease and osteoporosis. Imaging diagnosis of adult spine fractures is now almost exclusively performed with computed tomography (CT) instead of radiographs (x-rays). Quickly detecting and determining the location of any vertebral fractures is essential to prevent neurologic deterioration and paralysis after trauma.
Context
RSNA has teamed with the American Society of Neuroradiology (ASNR) and the American Society of Spine Radiology (ASSR) to conduct an AI challenge competition exploring whether artificial intelligence can be used to aid in the detection and localization of cervical spine fractures.
To create the ground truth dataset, the challenge planning task force collected imaging data sourced from twelve sites on six continents, including approximately 3,000 CT studies. Spine radiology specialists from the ASNR and ASSR provided expert image level annotations these studies to indicate the presence, vertebral level and location of any cervical spine fractures.
In this challenge competition, you will try to develop machine learning models that match the radiologists' performance in detecting and localizing fractures to the seven vertebrae that comprise the cervical spine. Winners will be recognized at an event during the RSNA 2022 annual meeting.
For more information on the challenge, contact RSNA Informatics staff at informatics@rsna.org.
A full set of acknowledgments can be found on this page.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated using a weighted multi-label logarithmic loss. Each fracture sub-type is its own row for every exam, and you are expected to predict a probability for a fracture at each of the seven cervical vertebrae designated as C1, C2, C3, C4, C5, C6 and C7. There is also an any label, patient_overall, which indicates that a fracture of ANY kind described before exists in the examination. Fractures in the skull base, thoracic spine, ribs, and clavicles are ignored. The any label is weighted more highly than specific fracture level sub-types.
For each exam Id, you must submit a set of predicted probabilities (a separate row for each cervical level subtype). We then take the log loss for each predicted probability versus its true label.
The binary weighted log loss function for label j on exam i is specified as:
$$
L_{ij} = - w_j * [ y_{ij}*log(p_{ij}) + (1-y_{ij})*log(1-p_{ij}) ]
$$
Finally, loss is averaged across all rows.
There will be 8 rows per image Id. The label indicated by a particular row will look like [image Id]_[Sub-type Name], as follows. There is also a target column, fractured, indicating the probability of whether a fracture exists at the specified level. For each image ID in the test set, you must predict a probability for each of the different possible sub-types and the patient overall. The file should contain a header and have the following format:
row_id,fractured
1_C1,0
1_C2,0
1_C3,0
1_C4,0.6
1_C5,0
1_C6,0.9
1_C7,0.01
1_patient_overall,0.99
2_C1,0
etc.","The goal of this competition is to identify fractures in CT scans of the cervical spine (neck) at both the level of a single vertebrae and the entire patient. Quickly detecting and determining the location of any vertebral fractures is essential to prevent neurologic deterioration and paralysis after trauma.
This competition uses a hidden test. When your submitted notebook is scored the actual test data (including a full length sample submission) will be made available to your notebook.
Files
train.csv Metadata for the train test set.
StudyInstanceUID - The study ID. There is one unique study ID for each patient scan.
patient_overall - One of the target columns. The patient level outcome, i.e. if any of the vertebrae are fractured.
C[1-7] - The other target columns. Whether the given vertebrae is fractured. See this diagram for the real location of each vertbrae in the spine.
test.csv Metadata for the test set prediction structure. Only the first few rows of the test set are available for download.
row_id - The row ID. This will match the same column in the sample submission file.
StudyInstanceUID - The study ID.",kaggle competitions download -c rsna-2022-cervical-spine-fracture-detection,"['https://www.kaggle.com/code/andradaolteanu/rsna-fracture-detection-dicom-images-explore', 'https://www.kaggle.com/code/samuelcortinhas/rsna-fracture-detection-in-depth-eda', 'https://www.kaggle.com/code/vslaykovsky/infer-pytorch-effnetv2-single-model-lb-0-49', 'https://www.kaggle.com/code/vslaykovsky/train-pytorch-effnetv2-baseline-cv-0-49', 'https://www.kaggle.com/code/vslaykovsky/pytorch-effnetv2-vertebrae-detection-acc-0-95']"
42,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.
First, the series is getting upgraded branding. We've dropped ""Tabular"" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.
Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.
Regardless of these changes, the goals of the Playground Series remain the same—to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!
With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Root Mean Squared Error (RMSE)
Submissions are scored on the root mean squared error. RMSE is defined as:
$$
\textrm{RMSE} = \sqrt{ \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 }
$$
where \( \hat{y}_i \) is the predicted value and \( y_i \) is the original value for each instance \(i\).
Submission File
For each id in the test set, you must predict the value for the target Strength. The file should contain a header and have the following format:
id,Strength
5439,55.2
5440,12.3
5441,83.4
etc.","The dataset for this competition (both train and test) was generated from a deep learning model trained on the Concrete Strength Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Files
train.csv - the training dataset; Strength is the target
test.csv - the test dataset; your objective is to predict Strength
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e9,"['https://www.kaggle.com/code/alexandershumilin/ps-s3-e9-ensemble-model', 'https://www.kaggle.com/code/ambrosm/pss3e9-eda-which-makes-sense', 'https://www.kaggle.com/code/ambrosm/pss3e9-winning-model', 'https://www.kaggle.com/code/phongnguyen1/a-framework-for-tabular-regression-e9-8-6', 'https://www.kaggle.com/code/kimtaehun/breif-eda-and-xgb-baseline-with-full-dataset']"
43,"Welcome to the Universal Image Embedding competition! After hosting challenges in the domain of landmarks for the past four years, this year we introduce the first competition in image representations that should work across many object types.
Image representations are a critical building block of computer vision applications. Traditionally, research on image embedding learning has been conducted with a focus on per-domain models. Generally, papers propose generic embedding learning techniques which are applied to different domains separately, rather than developing generic embedding models which could be applied to all domains combined.
In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same object as the query). The images in our dataset comprise a variety of object types, such as apparel, artwork, landmarks, furniture, packaged goods, among others.
This year's competition is structured in a representation learning format: you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality. Both Tensorflow and PyTorch models are supported.
This is a Code Competition. Refer to Code Requirements for details.
Cover image credits: Chris Schrier, CC-BY; Petri Krohn, GNU Free Documentation License; Drazen Nesic, CC0; Marco Verch Professional Photographer, CCBY; Grendelkhan, CCBY; Bobby Mikul, CC0; Vincent Van Gogh, CC0; pxhere.com, CC0; Smart Home Perfected, CC-BY.","Metric
Submissions are evaluated according to the mean Precision @ 5 metric, where we introduce a small modification to avoid penalizing queries with fewer than 5 expected index images. In detail, the metric is computed as follows:
$$mP@5 = \frac{1}{Q} \sum_{q=1}^{Q} \frac{1}{min(n_q, 5)} \sum_{j=1}^{min(n_q, 5)} rel_q(j)$$
where:
\(Q\) is the number of query images
\(n_q\) is the number of index images containing an object in common with the query image \(q\). Note that \(n_q \gt 0\).
\(rel_q(j)\) denotes the relevance of prediciton \(j\) for the \(q\)-th query: it’s 1 if the \(j\)-th prediction is correct, and 0 otherwise
Submission File
Unlike a traditional Kaggle code competition where notebooks are rerun top-to-bottom on a private test set, in this competition you will be submitting a model file. The model must take an image as an input, and return a float vector (i.e., the image embedding) at the output. The embedding dimensionality should be no greater than 64. Your model must be packaged into a submission.zip file, and compatible with either TensorFlow 2.6.4 or Pytorch 1.11.0. In most cases, scoring is expected to take a few hours to complete.
The submission.zip should contain one (and only one) of the following:
files and directories in the Tensorflow's SavedModel format.
file in PyTorch’s TorchScript format, named “saved_model.pt”.
To emphasize once again: the embedding produced by the model must be at most 64D (if not, the submission will error out).
Submission process
Kaggle will use the submitted model to:
Extract embeddings for the private test and index image sets.
Create a kNN (k = 5) lookup for each test sample, using the Euclidean distance between test and index embeddings.
Score the quality of the lookups using the competition metric.
Baseline kernels
To submit your model, all you need is a kernel that produces a submission.zip file containing the model. Here are examples:
Tensorflow. See this kernel for the minimal baseline model and this kernel for the more complicated baseline model.
PyTorch. See this kernel.
Code example to produce model
For Tensorflow, we provide a minimal Tensorflow baseline model based on pretrained Inception-v3 model with a linear layer to project the output feature to 64D. In the end, the model is saved in the SavedModel format:
import tensorflow as tf

image = tf.keras.layers.Input([None, None, 3], dtype=tf.uint8)
output = tf.cast(image, tf.float32)
output = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [224, 224]), name='resize')(output)
output = tf.keras.applications.inception_v3.preprocess_input(output)
output = tf.keras.applications.inception_v3.InceptionV3(weights='imagenet', input_shape=[None, None, 3],
                                                        include_top=False, pooling='avg')(output)
output = tf.keras.layers.Dense(64, name='embedding')(output)
output = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1), name='embedding_norm')(output)
model = tf.keras.Model(inputs=[image], outputs=[output])
model.summary()

tf.saved_model.save(model, 'inceptionv3/')
For more complicated examples, please check the example given in this repository. Specifically, this script shows an example of model exporting, for a model that is trained with this script.
For PyTorch, see a similar example below that uses pretrained Inception-v3 model and a linear projection layer, and exports the model in the TorchScript format:
import torch
import torch.nn as nn
from torchvision import models
from torchvision import transforms

class MyModel(nn.Module):
  def __init__(self):
    super().__init__()
    inception_model = models.inception_v3(pretrained=True)
    inception_model.fc = nn.Linear(2048, 64)
    self.feature_extractor = inception_model

  def forward(self, x):
    x = transforms.functional.resize(x,size=[224, 224])
    x = x/255.0
    x = transforms.functional.normalize(x, 
                                            mean=[0.485, 0.456, 0.406], 
                                            std=[0.229, 0.224, 0.225])
    return self.feature_extractor(x).logits

model = MyModel()
model.eval()
saved_model = torch.jit.script(model)
saved_model.save('saved_model.pt')
Feature extraction code
When your model is submitted, we run it in the competition images in order to extract embeddings. Please find below code snippets that show how the model is used in detail. These could be helpful for participants to make sure that their models are extracting embeddings correctly.
Tensorflow:
import numpy as np
from PIL import Image
import tensorflow as tf

# Model loading.
model = tf.saved_model.load(saved_model_path)
embedding_fn = model.signatures[""serving_default""]

# Load image and extract its embedding.
image_tensor = tf.convert_to_tensor(
        np.array(Image.open(image_path).convert(""RGB""))
    )
expanded_tensor = tf.expand_dims(image_tensor, axis=0)
embedding = embedding_fn(expanded_tensor)[""embedding_norm""]
PyTorch:
from PIL import Image
import torch
from torchvision import transforms

# Model loading.
model = torch.jit.load(saved_model_path)
model.eval()
embedding_fn = model

# Load image and extract its embedding.
input_image = Image.open(image_path).convert(""RGB"")
convert_to_tensor = transforms.Compose([transforms.PILToTensor()])
input_tensor = convert_to_tensor(input_image)
input_batch = input_tensor.unsqueeze(0)
with torch.no_grad():
  embedding = torch.flatten(embedding_fn(input_batch)[0]).cpu().data.numpy()","In this competition, you are asked to develop models that can efficiently retrieve images from a large database. For each query image, the model is expected to retrieve the most similar images from an index set.
We do not provide a training set. In accordance with the rules, any training data may be used, as long as it is disclosed in the forum by the relevant deadline. There exist many public datasets for different object types (e.g., artworks, landmarks, products, etc), and we encourage participants to experiment with them as needed.
Our evaluation dataset, which is kept private, contains images of the following types of object: apparel & accessories, packaged goods, landmarks, furniture & home decor, storefronts, dishes, artwork, toys, memes, illustrations and cars. The header image above shows examples of images in these domains, which are similar to the ones in the evaluation set. Labels are defined generally at the instance level, for example: the same T-shirt, the same building, the same painting, the same dish.
The following plot shows the distribution of object types in the dataset:
Please see the Evaluation page for details on how the model should be formatted and submitted.",kaggle competitions download -c google-universal-image-embedding,"['https://www.kaggle.com/code/francischen1991/tf-baseline-v2-submission', 'https://www.kaggle.com/code/vinitkp/image-retrieval', 'https://www.kaggle.com/code/ehyeok9/urop-1', 'https://www.kaggle.com/code/phmbchinh/9th-place-guie-fintune-tf-clip-w-training-code', 'https://www.kaggle.com/code/dhanrajbhosale/team-9-anc-project-universal-image-embedding']"
44,"Goal of the Competition
The goal of this competition is to streamline the process of matching educational content to specific topics in a curriculum. You will develop an accurate and efficient model trained on a library of K-12 educational materials that have been organized into a variety of topic taxonomies. These materials are in diverse languages, and cover a wide range of topics, particularly in STEM (Science, Technology, Engineering, and Mathematics).
Your work will enable students and educators to more readily access relevant educational content to support and supplement learning.
Context
Every country in the world has its own educational structure and learning objectives. Most materials are categorized against a single national system or are not organized in a way that facilitates discovery. The process of curriculum alignment, the organization of educational resources to fit standards, is challenging as it varies between country contexts.
Current efforts to align digital materials to national curricula are manual and require time, resources, and curricular expertise, and the process needs to be made more efficient in order to be scalable and sustainable. As new materials become available, they require additional efforts to be realigned, resulting in a never-ending process. There are no current algorithms or other AI interventions that address the resource constraints associated with improving the process of curriculum alignment.
Competition host Learning Equality is committed to enabling every person in the world to realize their right to a quality education, by supporting the creation, adaptation, and distribution of open educational resources, and creating supportive tools for innovative pedagogy. Their core product is Kolibri, an adaptable set of open solutions and tools specially designed to support offline-first teaching and learning for the 37% of the world without Internet access. Their close partner UNHCR has consistently highlighted the strong need and innovation required to create automated alignment tools to ensure refugee learners and teachers are provided with relevant digital learning resources. They have been jointly exploring this challenge in depth for the past few years, engaging with curriculum designers, teachers, and machine learning experts. In addition, Learning Equality is partnering with The Learning Agency Lab, an independent nonprofit focused on developing science of learning-based tools and programs for social good, along with UNHCR to engage you in this important process.
You have the opportunity to use your skills in machine learning to support educators and students around the world in accessing aligned learning materials that are relevant for their particular context. Better curriculum alignment processes are especially impactful during the onset of new emergencies or crises, where rapid support is needed, such as for refugee learners, and during school closures as took place during COVID-19.
Acknowledgments
Learning Equality and the Learning Agency Lab would like to thank Schmidt Futures and UNHCR for making this work possible. They also extend their gratitude to UNHCR for the ongoing collaboration in this effort to automate the process of curriculum alignment.
                         
This is a Code Competition. Refer to Code Requirements for details.","Submissions will be evaluated based on their mean F2 score. The mean is calculated in a sample-wise fashion, meaning that an F2 score is calculated for every predicted row, then averaged.
Submission File
For each topic_id in the test set, you must predict a space-delimited list of recommended content_ids for that topic. The file should contain a header and have the following format:
topic_id,content_ids
t_00004da3a1b2,c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c_76231f9d0b5e
t_00068291e9a4,c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c_ebb7fdf10a7e
t_00069b63a70a,c_11a1dc0bfb99
...","The dataset presented here is drawn from the Kolibri Studio curricular alignment tool, in which users can create their own channel, then build out a topic tree that represents a curriculum taxonomy or other hierarchical structure, and finally organize content items into these topics, by uploading their own content and/or importing existing materials from the Kolibri Content Library of Open Educational Resources.
An example of a branch of a topic tree is: Secondary Education >> Ordinary Level >> Mathematics >> Further Learning >> Activities >> Trigonometry. The leaf topic in this branch might then contain (be correlated with) a content item such as a video entitled Polar Coordinates.
You are challenged to predict which content items are best aligned to a given topic in a topic tree, with the goal of matching the selections made by curricular experts and other users of the Kolibri Studio platform. In other words, your goal is to recommend content items to curators for potential inclusion in a topic, to reduce the time they spend searching for and discovering relevant content to consider including in each topic.
Please note that this is a Code Competition, in which the actual test set is hidden. In this public version, we give some sample data drawn from the training set to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set.
The full test set includes an additional 10,000 topics (none present in the training set) and a large number of additional content items. The additional content items are only correlated to test set topics.
Files and Fields",kaggle competitions download -c learning-equality-curriculum-recommendations,"['https://www.kaggle.com/code/ryanholbrook/curriculum-recommendation-efficiency-leaderboard', 'https://www.kaggle.com/code/jamiealexandre/sample-notebook-data-exploration', 'https://www.kaggle.com/code/jamiealexandre/sample-notebook-inference', 'https://www.kaggle.com/code/jamiealexandre/tips-and-recommendations-from-hosts', 'https://www.kaggle.com/code/hasanbasriakcay/learning-equality-eda-fe-modeling']"
45,"Goal of the Competition
The goal of this competition is to classify the blood clot origins in ischemic stroke. Using whole slide digital pathology images, you'll build a model that differentiates between the two major acute ischemic stroke (AIS) etiology subtypes: cardiac and large artery atherosclerosis.
Your work will enable healthcare providers to better identify the origins of blood clots in deadly strokes, making it easier for physicians to prescribe the best post-stroke therapeutic management and reducing the likelihood of a second stroke.
Context
Stroke remains the second-leading cause of death worldwide. Each year in the United States, over 700,000 individuals experience an ischemic stroke caused by a blood clot blocking an artery to the brain. A second stroke (23% of total events are recurrent) worsens the chances of the patient’s survival. However, subsequent strokes may be mitigated if physicians can determine stroke etiology, which influences the therapeutic management following stroke events.
During the last decade, mechanical thrombectomy has become the standard of care treatment for acute ischemic stroke from large vessel occlusion. As a result, retrieved clots became amenable to analysis. Healthcare professionals are currently attempting to apply deep learning-based methods to predict ischemic stroke etiology and clot origin. However, unique data formats, image file sizes, as well as the number of available pathology slides create challenges you could lend a hand in solving.
The Mayo Clinic is a nonprofit American academic medical center focused on integrated health care, education, and research. Stroke Thromboembolism Registry of Imaging and Pathology (STRIP) is a uniquely large multicenter project led by Mayo Clinic Neurovascular Lab with the aim of histopathologic characterization of thromboemboli of various etiologies and examining clot composition and its relation to mechanical thrombectomy revascularization.
To decrease the chances of subsequent strokes, the Mayo Clinic Neurovascular Research Laboratory encourages data scientists to improve artificial intelligence-based etiology classification so that physicians are better equipped to prescribe the correct treatment. New computational and artificial intelligence approaches could help save the lives of stroke survivors and help us better understand the world's second-leading cause of death.","Submissions are evaluated using a weighted multi-class logarithmic loss. The overall effect is such that each class is roughly equally important for the final score.
Each image has been labeled with an etiology class, either CE or LAA. For each image, you must submit a probability for each class. The formula is then:
$$\text{Log Loss} = - \left( \frac{\sum^{M}_{i=1} w_{i} \cdot \sum_{j=1}^{N_{i}} \frac{y_{ij}}{N_{i}} \cdot \ln p_{ij} }{\sum^{M}_{i=1} w_{i}} \right)$$
where \(N\) is the number of images in the class set, \(M\) is the number of classes,  \(\ln\) is the natural logarithm, \(y_{ij}\) is 1 if observation \(i\) belongs to class \(j\) and 0 otherwise, \(p_{ij}\) is the predicted probability that image \(i\) belongs to class \(j\).
The submitted probabilities for a given image are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, each predicted probability \(p\) is replaced with \(\max(\min(p,1-10^{-15}),10^{-15})\).
Submission File
For each patient_id in the test set, you must predict a probability for each of the two etiology classes. The file should contain a header and have the following format:
patient_id,CE,LAA
01f2b3,0.5,0.5
04de22,0.5,0.5
0a47c9,0.5,0.5
0af8b6,0.5,0.5
...","The dataset for this competition comprises over a thousand high-resolution whole-slide digital pathology images. Each slide depicts a blood clot from a patient that had experienced an acute ischemic stroke.
The slides comprising the training and test sets depict clots with an etiology (that is, origin) known to be either CE (Cardioembolic) or LAA (Large Artery Atherosclerosis). We include a set of supplemental slides with a either an unknown etiology or an etiology other than CE or LAA.
Your task is to classify the etiology (CE or LAA) of the slides in the test set for each patient.
File and Data Field Descriptions
train/ - A folder containing images in the TIFF format to be used as training data.
test/ - A folder containing images to be used as test data. The actual test data comprises about 280 images.
other/ - A supplemental set of images with a either an unknown etiology or an etiology other than CE or LAA.
train.csv Contains annotations for images in the train/ folder.
image_id - A unique identifier for this instance having the form {patient_id}_{image_num}. Corresponds to the image {image_id}.tif.
center_id - Identifies the medical center where the slide was obtained.
patient_id - Identifies the patient from whom the slide was obtained.",kaggle competitions download -c mayo-clinic-strip-ai,
46,"Goal of the Competition
The goal of this competition is to classify isolated American Sign Language (ASL) signs. You will create a TensorFlow Lite model trained on labeled landmark data extracted using the MediaPipe Holistic Solution.
Your work may improve the ability of PopSign* to help relatives of deaf children learn basic signs and communicate better with their loved ones.^
Context
Every day, 33 babies are born with permanent hearing loss in the U.S.
Around 90% of which are born to hearing parents many of which may not know American Sign Language. (kdhe.ks.gov, deafchildren.org) Without sign language, deaf babies are at risk of Language Deprivation Syndrome. This syndrome is characterized by a lack of access to naturally occurring language acquisition during their critical language-learning years. It can cause serious impacts on different aspects of their lives, such as relationships, education, and employment.
Learning sign language is challenging.
Learning American Sign Language is as difficult for English speakers as learning Japanese. (jstor.org) It takes time and resources, which many parents don't have. They want to learn sign language, but it's hard when they are working long hours just to make ends meet. And even if they find the time and money for classes, the classes are often far away.
Games can help.
PopSign is a smartphone game app that makes learning American Sign Language fun, interactive, and accessible. Players match videos of ASL signs with bubbles containing written English words to pop them.
PopSign is designed to help parents with deaf children learn ASL, but it's open to anyone who wants to learn sign language vocabulary. By adding a sign language recognizer from this competition, PopSign players will be able to sign the type of bubble they want to shoot, providing the player with the opportunity to practice the sign themselves instead of just watching videos of other people signing.
You can help connect deaf children and their parents.
By training a sign language recognizer for PopSign, you can help make the game more interactive and improve the learning and confidence of players who want to learn sign language to communicate with their loved ones.
Why TensorFlow Lite
To allow the ML model to run on device in an attempt to limit latency inside the game, PopSign doesn’t send user videos to the cloud. Therefore, all inference must be done on the phone itself. PopSign is building its recognition pipeline on top of TensorFlow Lite, which runs on both Android and iOS. In order for the competition models to integrate seamlessly with PopSign, we are asking our competitors to submit their entries in the form of TensorFlow Lite models.
Special thanks to our partners
We’d like to thank the Georgia Institute of Technology, the National Technical Institute for the Deaf at Rochester Institute of Technology, and Deaf Professional Arts Network for their work to create the dataset, the PopSign game, and overall competition preparation.
This is a Code Competition. Refer to Code Requirements for details.
*PopSign is an app developed by the Georgia Institute of Technology and the National Technical Institute for the Deaf at Rochester Institute of Technology. The app is available in beta on Android and iOS.
^We cannot guarantee the competition will benefit the competitors or the disabled community directly.","The evaluation metric for this contest is simple classification accuracy.
Submission Process
In this competition you will be submitting a TensorFlow Lite model file. The model must take one or more landmark frames as an input and return a float vector (the predicted probabilities of each sign class) as the output. Your model must be packaged into a submission.zip file and compatible with the TensorFlow Lite Runtime v2.9.1. You are welcome to train your model using the framework of your choice, as long as you convert the model checkpoint into the tflite format prior to submission.
Your model must also perform inference with less than 100 milliseconds of latency per video on average and use less than 40 MB of storage space. Expect to see approximately 40,000 videos in the test set. We allow an additional 10 minute buffer for loading the data and miscellaneous overhead.
Each video is loaded with the following function:
ROWS_PER_FRAME = 543  # number of landmarks per frame

def load_relevant_data_subset(pq_path):
    data_columns = ['x', 'y', 'z']
    data = pd.read_parquet(pq_path, columns=data_columns)
    n_frames = int(len(data) / ROWS_PER_FRAME)
    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))
    return data.astype(np.float32)
Inference is performed (roughly) as follows, ignoring details like how we manage multiple videos:
import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path)

found_signatures = list(interpreter.get_signature_list().keys())

if REQUIRED_SIGNATURE not in found_signatures:
    raise KernelEvalException('Required input signature not found.')

prediction_fn = interpreter.get_signature_runner(""serving_default"")
output = prediction_fn(inputs=frames)
sign = np.argmax(output[""outputs""])",,,
47,"Goal of the Competition
The goal of this competition is to detect external contact experienced by players during an NFL football game. You will use video and player tracking data to identify moments with contact to help improve player safety.
Context
The National Football League (NFL) has teamed up with Amazon Web Services (AWS) to strengthen its commitment to predict player injuries. The NFL aspires to have the best injury surveillance and mitigation program in any sport. With your machine learning and computer vision skills, you can help the NFL accurately identify when players experience contact throughout a football play.
In prior years, the NFL challenged the Kaggle community to create helmet impact detection and identification algorithms. This year the NFL looks to automatically identify all moments when players experience contact. This competition will be successful if we can reliably detect moments when players are in contact with one another and when a player’s body is in contact with the ground.
Currently, the NFL uses its tracking system to monitor a large number of statistics about players’ load during the season. The league has a solution that predicts contact between players, but it only leverages the player tracking data. This competition hopes to improve the predictive power by including video in addition to tracking data. Categorizing ground contact will also provide a more comprehensive view of impacts, improving analysis for player health and safety.
More accurate data is an important step toward the NFL’s injury surveillance and mitigation goals. With complete contact detection, the league can identify correlations between certain types of contact and injury, a contributor to future prevention. Your efforts could help mitigate unsafe situations to reduce injury to all players.
The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. This competition is part of the Digital Athlete, a joint effort between the NFL and AWS to build a virtual, 360-degree representation of an NFL player’s experience. The Digital Athlete hopes to generate a precise picture of what they need when it comes to preventing and recovering from injuries while performing at their best. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit the NFL Player Health and Safety website.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on Matthews Correlation Coefficient between the predicted and actual contact events.
Submission File
For every allowable contact_id (formed by concatenating the game_play_step_player1_player2), you must predict whether the involved players are in contact at that moment in time. sample_submission.csv provides the exhaustive list of contact_ids. Note that the ground, denoted as player G, is included as a possible contact in place of player2. The player with the lower id is always listed first in the contact_id.
The file should contain a header and have the following format:
contact_id,contact
58168_003392_0_38590_43854,0
58168_003392_0_38590_41257,1
58168_003392_0_38590_41944,0
etc.",,,
48,,,,,
49,,,,,
50,"This Christmas season, we’re printing in house.
Our vendor’s supply eaten by a field mouse!
The paper was laid on the printer with care
In hopes that the arms soon would reach there;
The elves are nestled all snug in their pods;
In hopes that the picture would earn some applause.
Can you help move the arms and put them in place?
Please optimize this configuration space!
Santa’s elves relied on the same vendor every year to print the annual Christmas card. But a mouse ate through the printer cables and the elves are forced to print at the North Pole Workshop this year! They managed to create their own giant printer with an 8-axis robotic arm that can print one pixel of the card at a time. But moving the arm and changing the color is not only expensive, the elves need to spend the least amount of time possible making the card so they can get back to making toys!
Your job is to determine the most optimal way to craft this year’s Christmas card, by selecting the most efficient path of both moving the robotic arm and changing the print color to craft this year’s image. Each link of the printer arm can be moved independently each step, but you'll also need to account for the time needed to change the printing color.
Can you save Christmas by finding the most efficient way to print Santa’s Christmas cards?
Photos by Kelly Sikkema and Eric Prouzet on Unsplash","A robotic arm with eight links with lengths \([64, 32, 16, 8, 4, 2, 1, 1]\) must ""print"" each point of the following \(257 \times 257\) image:
The configuration of the arm is described by a list of displacement vectors, like \([(64, -44), (32, 10), (-2, 16), …, (0, 1), (-1, 0)]\), where a vector \((x, y)\) of length \(l\) must satisfy \(\max(\mid x \mid, \mid y \mid) = l\). This condition means that at least one of the components of the vector must be equal to plus-or-minus the length of the vector.
The position of the arm is the sum of these displacement vectors and indicates the location of the tip of the arm. The base of the arm (the origin of the first vector) is at \((0, 0)\), which is the midpoint of the image.
The arm can be reconfigured step-by-step by rotating any or all of the links by 1 unit, incurring a total reconfiguration cost equal to the square root of the number of links changed. Additionally, it incurs a color cost equal to the sum of the absolute differences in the color components from one step to the next and multiplied by a scaling factor of 3.0.
The task is to find a sequence of configurations with positions at every point in the solution image having a minimal cost.
See the Getting Started notebook for more details.
Submission File
The submission file should contain a sequence of configurations with the components each displacement vector delimited by a semicolon, like so: x0 y0;x1 y1;..., and subject to the conditions given above.
The file should contain a header and have the following format:
configuration
64 0;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0
64 -1;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0
64 -2;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0
...
64 0;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0
In order for the submission to be valid:
The first and last configurations must be 64 0;-32 0;-16 0;-8 0;-4 0;-2 0;-1 0;-1 0.
The set of positions indicated by the configurations must be equal to the set of points in the image. (The tip of the arm must move across the entire image, in other words.)
All numbers must be integers.","Your task in this competition is to create a sequence of arm configurations covering every point on the image that minimizes the total movement of the arm and also the change in color from point to point. See the Evaluation page and Getting Started notebook for more details.
Files
image.csv - The RGB values for each point in the target image. Points are given in Cartesian coordinates x, y each with an associated triple r, g, b of color components with values in the range [0, 1]. This file should be taken as the canonical source of image values for optimization purposes.
image.png - The image in PNG format, only for illustration.
sample_submission.csv - A submission file in the correct format.",kaggle competitions download -c santa-2022,"['https://www.kaggle.com/code/ryanholbrook/getting-started-with-santa-2022', 'https://www.kaggle.com/code/crodoc/82409-improved-baseline-santa-2022', 'https://www.kaggle.com/code/oxzplvifi/pixel-travel-map', 'https://www.kaggle.com/code/hocop1/baseline-but-faster', 'https://www.kaggle.com/code/cnumber/lower-bound-using-minimum-spanning-tree']"
51,"When you think of “life hacks,” normally you’d imagine productivity techniques. But how about the kind that helps you understand your body at a molecular level? It may be possible! Researchers must first determine the function and relationships among the 37 trillion cells that make up the human body. A better understanding of our cellular composition could help people live healthier, longer lives.
A previous Kaggle competition aimed to annotate cell population neighborhoods that perform an organ’s main physiologic function, also called functional tissue units (FTUs). Manually annotating FTUs (e.g., glomeruli in kidney or alveoli in the lung) is a time-consuming process. In the average kidney, there are over 1 million glomeruli FTUs. While there are existing cell and FTU segmentation methods, we want to push the boundaries by building algorithms that generalize across different organs and are robust across different dataset differences.
The Human BioMolecular Atlas Program (HuBMAP) is working to create a Human Reference Atlas at the cellular level. Sponsored by the National Institutes of Health (NIH), HuBMAP and Indiana University’s Cyberinfrastructure for Network Science Center (CNS) have partnered with institutions across the globe for this endeavor. A major partner is the Human Protein Atlas (HPA), a Swedish research program aiming to map the protein expression in human cells, tissues, and organs, funded by the Knut and Alice Wallenberg Foundation.
In this competition, you’ll identify and segment functional tissue units (FTUs) across five human organs. You'll build your model using a dataset of tissue section images, with the best submissions segmenting FTUs as accurately as possible.
If successful, you'll help accelerate the world’s understanding of the relationships between cell and tissue organization. With a better idea of the relationship of cells, researchers will have more insight into the function of cells that impact human health. Further, the Human Reference Atlas constructed by HuBMAP will be freely available for use by researchers and pharmaceutical companies alike, potentially improving and prolonging human life.
This is a Code Competition. Refer to Code Requirements for details.",,"The goal of this competition is to identify the locations of each functional tissue unit (FTU) in biopsy slides from several different organs. The underlying data includes imagery from different sources prepared with different protocols at a variety of resolutions, reflecting typical challenges for working with medical data.
This competition uses data from two different consortia, the Human Protein Atlas (HPA) and Human BioMolecular Atlas Program (HuBMAP). The training dataset consists of data from public HPA data, the public test set is a combination of private HPA data and HuBMAP data, and the private test set contains only HuBMAP data. Adapting models to function properly when presented with data that was prepared using a different protocol will be one of the core challenges of this competition. While this is expected to make the problem more difficult, developing models that generalize is a key goal of this endeavor.
This competition uses a hidden test. When your submitted notebook is scored the actual test data (including a full length sample submission) will be made available to your notebook.
Files
[train/test].csv Metadata for the train/test set. Only the first few rows of the test set are available for download.
id - The image ID.
organ - The organ that the biopsy sample was taken from.
data_source - Whether the image was provided by HuBMAP or HPA.",kaggle competitions download -c hubmap-organ-segmentation,"['https://www.kaggle.com/code/ishandutta/hubmap-complete-understanding-and-eda-w-b', 'https://www.kaggle.com/code/thedevastator/training-fastai-baseline', 'https://www.kaggle.com/code/dschettler8845/eda-hubmap-hpa-organ-segmentation', 'https://www.kaggle.com/code/alincijov/training-hubmap-lb-0-75-swin-transformer-v1', 'https://www.kaggle.com/code/thedevastator/inference-fastai-baseline']"
52,"Goal of the Competition
The goal of this competition is to identify breast cancer. You'll train your model with screening mammograms obtained from regular screening.
Your work improving the automation of detection in screening mammography may enable radiologists to be more accurate and efficient, improving the quality and safety of patient care. It could also help reduce costs and unnecessary medical procedures.
Context
According to the WHO, breast cancer is the most commonly occurring cancer worldwide. In 2020 alone, there were 2.3 million new breast cancer diagnoses and 685,000 deaths. Yet breast cancer mortality in high-income countries has dropped by 40% since the 1980s when health authorities implemented regular mammography screening in age groups considered at risk. Early detection and treatment are critical to reducing cancer fatalities, and your machine learning skills could help streamline the process radiologists use to evaluate screening mammograms.
Currently, early detection of breast cancer requires the expertise of highly-trained human observers, making screening mammography programs expensive to conduct. A looming shortage of radiologists in several countries will likely worsen this problem. Mammography screening also leads to a high incidence of false positive results. This can result in unnecessary anxiety, inconvenient follow-up care, extra imaging tests, and sometimes a need for tissue sampling (often a needle biopsy).
The competition host, the Radiological Society of North America (RSNA) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research, and technological innovation.
Your efforts in this competition could help extend the benefits of early detection to a broader population. Greater access could further reduce breast cancer mortality worldwide.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated using the probabilistic F1 score (pF1). This extension of the traditional F score accepts probabilities instead of binary classifications. You can find a Python implementation here.
With pX as the probabilistic version of X:
$$
pF_1 = 2\frac{pPrecision \cdot pRecall}{pPrecision+pRecall}
$$
where:
$$
pPrecision = \frac{pTP}{pTP+pFP}
$$
$$
pRecall = \frac{pTP}{TP+FN}
$$
Submission Format
For each prediction_id, you should predict the likelihood of cancer in the corresponding cancer column. The submission file should have the following format:
prediction_id,cancer
0-L,0
0-R,0.5
1-L,1
...","Note: The dataset for this challenge contains radiographic breast images of female subjects.
The goal of this competition is to identify cases of breast cancer in mammograms from screening exams. It is important to identify cases of cancer for obvious reasons, but false positives also have downsides for patients. As millions of women get mammograms each year, a useful machine learning tool could help a great many people.

This competition uses a hidden test. When your submitted notebook is scored the actual test data (including a full length sample submission) will be made available to your notebook.

Files
[train/test]_images/[patient_id]/[image_id].dcm The mammograms, in dicom format. You can expect roughly 8,000 patients in the hidden test set. There are usually but not always 4 images per patient. Note that many of the images use the jpeg 2000 format which may you may need special libraries to load.

sample_submission.csv A valid sample submission. Only the first few rows are available for download.

[train/test].csv Metadata for each patient and image. Only the first few rows of the test set are available for download.",kaggle competitions download -c rsna-breast-cancer-detection,"['https://www.kaggle.com/code/radek1/eda-training-a-fast-ai-model-submission', 'https://www.kaggle.com/code/radek1/fast-ai-starter-pack-train-inference', 'https://www.kaggle.com/code/andradaolteanu/rsna-breast-cancer-eda-pytorch-baseline', 'https://www.kaggle.com/code/theoviel/dicom-resized-png-jpg', 'https://www.kaggle.com/code/radek1/how-to-process-dicom-images-to-pngs']"
53,"The June edition of the 2022 Tabular Playground series is all about data imputation. The dataset has similarities to the May 2022 Tabular Playground, except that there are no targets. Rather, there are missing data values in the dataset, and your task is to predict what these values should be.
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. We've also provided a notebook to get people started
Good luck and have fun!
Acknowledgments
Photo by Mika Baumeister on Unsplash.","Submissions are scored on the root mean squared error. RMSE is defined as:
$$
\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$
where \( \hat{y}_i \) is the predicted value and \(y_i\) is the original value for each instance \(i\).
Submission File
For each row-col pair in the in the sample_submission.csv file (corresponding to all of the missing values found in data.csv), you must predict the missing value of that data point. The file should contain a header and have the following format:
row-col, value
0-F_1_14, 1.54
0-F_3_23, -0.56
1-F_3_24, 0.01
etc.","For this challenge, you are given (simulated) manufacturing control data that contains missing values due to electronic errors. Your task is to predict the values of all missing data in this dataset. (Note, while there are continuous and categorical features, only the continuous features have missing values.)
Here's a notebook that you can use to get started.
Good luck!
Files
data.csv - the file includes normalized continuous data and categorical data; your task is to predict the values of the missing data.
sample_submission.csv - a sample submission file in the correct format; the row-col indicator corresponds to the row and column of each missing value in data.csv",kaggle competitions download -c tabular-playground-series-jun-2022,"['https://www.kaggle.com/code/azminetoushikwasi/all-imputation-techniques-with-pros-and-cons', 'https://www.kaggle.com/code/ravi20076/tpsjun22-extensiveeda-groups13', 'https://www.kaggle.com/code/alexryzhkov/tps-jun-22-lightautoml-imputer', 'https://www.kaggle.com/code/abdulravoofshaik/top-3-solution-lgbm-mean', 'https://www.kaggle.com/code/abdulravoofshaik/quick-eda-and-missing-values-tutorial']"
54,"Goal of the Competition
The goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session.
Your work will help improve the shopping experience for everyone involved. Customers will receive more tailored recommendations while online retailers may increase their sales.
Context
Online shoppers have their pick of millions of products from large retailers. While such variety may be impressive, having so many options to explore can be overwhelming, resulting in shoppers leaving with empty carts. This neither benefits shoppers seeking to make a purchase nor retailers that missed out on sales. This is one reason online retailers rely on recommender systems to guide shoppers to products that best match their interests and motivations. Using data science to enhance retailers' ability to predict which products each customer actually wants to see, add to their cart, and order at any given moment of their visit in real-time could improve your customer experience the next time you shop online with your favorite retailer.
Current recommender systems consist of various models with different approaches, ranging from simple matrix factorization to a transformer-type deep neural network. However, no single model exists that can simultaneously optimize multiple objectives. In this competition, you’ll build a single entry to predict click-through, add-to-cart, and conversion rates based on previous same-session events.
With more than 10 million products from over 19,000 brands, OTTO is the largest German online shop. OTTO is a member of the Hamburg-based, multi-national Otto Group, which also subsidizes Crate & Barrel (USA) and 3 Suisses (France).
Your work will help online retailers select more relevant items from a vast range to recommend to their customers based on their real-time behavior. Improving recommendations will ensure navigating through seemingly endless options is more effortless and engaging for shoppers.","Submissions are evaluated on Recall@20 for each action type, and the three recall values are weight-averaged:
$$
score = 0.10 \cdot R_{clicks} + 0.30 \cdot R_{carts} + 0.60 \cdot R_{orders}
$$
where \( R \) is defined as
$$
R_{type} = \frac{\sum_{i}^{N} | \{ \text{predicted aids} \}_{i, type} \cap \{ \text{ground truth aids} \}_{i, type} | }{\sum_{i}^{N} \min{( 20, | \{ \text{ground truth aids} \}_{i, type} | )}}
$$
and \( N \) is the total number of sessions in the test set, and \( \text{predicted aids} \) are the predictions for each session-type (e.g., each row in the submission file) truncated after the first 20 predictions.
For each session in the test data, your task it to predict the aid values for each type that occur after the last timestamp ts the test session. In other words, the test data contains sessions truncated by timestamp, and you are to predict what occurs after the point of truncation.
For clicks there is only a single ground truth value for each session, which is the next aid clicked during the session (although you can still predict up to 20 aid values). The ground truth for carts and orders contains all aid values that were added to a cart and ordered respectively during the session.
Each session and type combination should appear on its own session_type row in the submission, and predictions should be space delimited.
Submission File
For each session id and type combination in the test set, you must predict the aid values in the label column, which is space delimited. You can predict up to 20 aid values per row. The file should contain a header and have the following format:
session_type,labels
12906577_clicks,135193 129431 119318 ...
12906577_carts,135193 129431 119318 ...
12906577_orders,135193 129431 119318 ...
12906578_clicks, 135193 129431 119318 ...
etc. ","The goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session.
The training data contains full e-commerce session information. For each session in the test data, your task it to predict the aid values for each session type thats occur after the last timestamp ts in the test session. In other words, the test data contains sessions truncated by timestamp, and you are to predict what occurs after the point of truncation.
For additional background, please see the published OTTO Recommender Systems Dataset GitHub.
Files
train.jsonl - the training data, which contains full session data
session - the unique session id
events - the time ordered sequence of events in the session
aid - the article id (product code) of the associated event
ts - the Unix timestamp of the event
type - the event type, i.e., whether a product was clicked, added to the user's cart, or ordered during the session
test.jsonl - the test data, which contains truncated session data",kaggle competitions download -c otto-recommender-system,"['https://www.kaggle.com/code/inversion/read-a-chunk-of-jsonl', 'https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575', 'https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix', 'https://www.kaggle.com/code/edwardcrookenden/otto-getting-started-eda-baseline', 'https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565']"
55,"You may have heard that blending predictions from model predictions can give better results than using the output of a single model. There are many different strategies that can be employed for this, and they are great to learn if you're looking for an effectively free boost in model scores. The November Tabular Playground is the chance to practice this skill!
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
Good luck and have fun!
Photo by RhondaK Native Florida Folk Artist on Unsplash","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right],
$$
where
\( n \) is the number of scored observations
\( \hat{y}_i \) is the predicted probability of each observation
\( y_i \) binary ground truth label
\( log \) is the natural logarithm
The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
For each id in the sample_submission, you must predict a probability for the pred variable. The file should contain a header and have the following format:
id,pred
20000,0.640707
20001,0.636904
20002,0.392496
etc.","In this competition, you are given a folder of submissions that contain predictions for a binary classification task. The ground truth for these rows are provided in the file train_labels.csv. Each file name in the submissions folder corresponds to the logloss score of the the first half of the prediction rows against the ground truth labels in that file. This is effectively the ""training"" set.
Your task is to use blend (ensemble) the various submission files to produce better model predictions. You can tell if the blend produces better results because you have the ground truth labels for the first half of the rows. In other words, you can blend two files together, check the score on the first half of the rows, and the score improves, you can submit the results from the rest of the rows to the leaderboard.
A simple example is shown in this notebook.
Files
submission_files/ - a folder containing binary model predictions
train_labels.csv - the ground truth labels for the first half of the rows in the submission files
sample_submission.csv - a sample submission file in the correct format, only containing the row ids for the second half of each file in the submissions folder; your task is to blend together submissions that achieve the improvements in the score.",kaggle competitions download -c tabular-playground-series-nov-2022,"['https://www.kaggle.com/code/inversion/simple-blending-example', 'https://www.kaggle.com/code/ambrosm/tpsnov22-eda-which-makes-sense', 'https://www.kaggle.com/code/hasanbasriakcay/tpsnov22-pseudo-labels-lgbm-xgb-lb-0-514', 'https://www.kaggle.com/code/jcaliz/tps-nov22-quickstart-eda-baseline', 'https://www.kaggle.com/code/infrarosso/tps-nov-2022-eda-hybrid-stacking']"
56,"Stock exchanges are fast-paced, high-stakes environments where every second counts. The intensity escalates as the trading day approaches its end, peaking in the critical final ten minutes. These moments, often characterised by heightened volatility and rapid price fluctuations, play a pivotal role in shaping the global economic narrative for the day.
Each trading day on the Nasdaq Stock Exchange concludes with the Nasdaq Closing Cross auction. This process establishes the official closing prices for securities listed on the exchange. These closing prices serve as key indicators for investors, analysts and other market participants in evaluating the performance of individual securities and the market as a whole.
Within this complex financial landscape operates Optiver, a leading global electronic market maker. Fueled by technological innovation, Optiver trades a vast array of financial instruments, such as derivatives, cash equities, ETFs, bonds, and foreign currencies, offering competitive, two-sided prices for thousands of these instruments on major exchanges worldwide.
In the last ten minutes of the Nasdaq exchange trading session, market makers like Optiver merge traditional order book data with auction book data. This ability to consolidate information from both sources is critical for providing the best prices to all market participants.
In this competition, you are challenged to develop a model capable of predicting the closing price movements for hundreds of Nasdaq listed stocks using data from the order book and the closing auction of the stock. Information from the auction can be used to adjust prices, assess supply and demand dynamics, and identify trading opportunities.
Your model can contribute to the consolidation of signals from the auction and order book, leading to improved market efficiency and accessibility, particularly during the intense final ten minutes of trading. You'll also get firsthand experience in handling real-world data science problems, similar to those faced by traders, quantitative researchers and engineers at Optiver.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on the Mean Absolute Error (MAE) between the predicted return and the observed target. The formula is given by:
$$MAE = \frac{1}{n} \sum\limits_{i=1}^{n} {|y_i - x_i|} $$
Where:
n is the total number of data points.
y_i is the predicted value for data point i.
x_i is the observed value for data point i.
Submitting
You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow the template in this notebook.","This dataset contains historic data for the daily ten minute closing auction on the NASDAQ stock exchange. Your challenge is to predict the future price movements of stocks relative to the price future price movement of a synthetic index composed of NASDAQ-listed stocks.
This is a forecasting competition using the time series API. The private leaderboard will be determined using real market data gathered after the submission period closes.
Files
[train/test].csv The auction data. The test data will be delivered by the API.
stock_id - A unique identifier for the stock. Not all stock IDs exist in every time bucket.
date_id - A unique identifier for the date. Date IDs are sequential & consistent across all stocks.
imbalance_size - The amount unmatched at the current reference price (in USD).
imbalance_buy_sell_flag - An indicator reflecting the direction of auction imbalance.
buy-side imbalance; 1
sell-side imbalance; -1
no imbalance; 0
reference_price - The price at which paired shares are maximized, the imbalance is minimized and the distance from the bid-ask midpoint is minimized, in that order. Can also be thought of as being equal to the near price bounded between the best bid and ask price.",kaggle competitions download -c optiver-trading-at-the-close,"['https://www.kaggle.com/code/sohier/optiver-2023-basic-submission-demo', 'https://www.kaggle.com/code/tomforbes/optiver-trading-at-the-close-introduction', 'https://www.kaggle.com/code/ravi20076/optiver-memoryreduction', 'https://www.kaggle.com/code/yuanzhezhou/baseline-lgb-xgb-and-catboost', 'https://www.kaggle.com/code/mpwolke/volatility-matters-buckets-bid-prices']"
57,"Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions are evaluated on micro-averaged F1-Score between pricted and actual values.
Submission File
For each id in the test set, you must predict the corresponding outcome. The file should contain a header and have the following format:
id,outcome
1235,lived
1236,lived
1237,died
etc.","The dataset for this competition (both train and test) was generated from a deep learning model trained on a portion of the Horse Survival Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.
Good luck!
Files
train.csv - the training dataset; outcome is the (categorical) target
test.csv - the test dataset; your objective is to predict outcome
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e22,"['https://www.kaggle.com/code/kimtaehun/eda-and-baseline-with-multiple-models', 'https://www.kaggle.com/code/yaaangzhou/playground-s3-e22-eda-modeling', 'https://www.kaggle.com/code/arunklenin/advanced-feature-engg-techniques-beyond-basics', 'https://www.kaggle.com/code/ravi20076/playgrounds3e22-eda-baseline', 'https://www.kaggle.com/code/oscarm524/ps-s3-ep22-eda-modeling-submission']"
58,"Human biology can be complex, in part due to the function and interplay of the body's approximately 37 trillion cells, which are organized into tissues, organs, and systems. However, recent advances in single-cell technologies have provided unparalleled insight into the function of cells and tissues at the level of DNA, RNA, and proteins. Yet leveraging single-cell methods to develop medicines requires mapping causal links between chemical perturbations and the downstream impact on cell state. These experiments are costly and labor intensive, and not all cells and tissues are amenable to high-throughput transcriptomic screening. If data science could help accurately predict chemical perturbations in new cell types, it could accelerate and expand the development of new medicines.
Several methods have been developed for drug perturbation prediction, most of which are variations on the autoencoder architecture (Dr.VAE, scGEN, and ChemCPA). However, these methods lack proper benchmarking datasets with diverse cell types to determine how well they generalize. The largest available training dataset is the NIH-funded Connectivity Map (CMap), which comprises over 1.3M small molecule perturbation measurements. However, the CMap includes observations of only 978 genes, less than 5% of all genes. Furthermore, the CMap data is comprised almost entirely of measurements in cancer cell lines, which may not accurately represent human biology.
Competition host Open Problems in Single-Cell Analysis is a non-profit scientific collaboration aiming to drive innovation in single-cell data science. They are partnering to host this competition with Cellarity, a first-of-its-kind therapeutics company that develops medicines by studying and altering the cellular signatures of disease.
Although it is impossible to measure all perturbations in all cells, we hypothesize that it is possible to measure a subset of combinations and infer the rest. Today, we are far from this goal, but we hope that this competition will serve as an important proof of concept.
Your work in helping to accurately predict chemical perturbations in new cell types could accelerate the discovery and enable the creation of new medicines to treat or cure disease.","We use the Mean Rowwise Root Mean Squared Error to score submissions, computed as follows:
$$
\textrm{MRRMSE} = \frac{1}{R}\sum_{i=1}^R\left(\frac{1}{n} \sum_{j=1}^{n} (y_{ij} - \widehat{y}_{ij})^2\right)^{1/2}
$$
where \(R\) is the number of scored rows, and \(y_{ij}\) and \(\widehat{y}_{ij}\) are the actual and predicted values, respectively, for row \(i\) and column \(j\), and \(n\) is the number of columns.
Submission File
For each id in the evaluation set, you should predict a value for each of the 18,211 genes named in the remaining columns. Each id corresponds to a cell_type / sm_name pair, which you may identify from the id_map.csv file.
Your submission should contain a header and have the following format:
id,A1BG,A1BG-AS1,...,ZZEF1
0,0.0,0.0,...,0.0
1,0.0,0.0,...,0.0
2,0.0,0.0,...,0.0
3,0.0,0.0,...,0.0
...","For this competition, we designed and generated a novel single-cell perturbational dataset in human peripheral blood mononuclear cells (PBMCs). We selected 144 compounds from the Library of Integrated Network-Based Cellular Signatures (LINCS) Connectivity Map dataset (PMID: 29195078) and measured single-cell gene expression profiles after 24 hours of treatment. The experiment was repeated in three healthy human donors, and the compounds were selected based on diverse transcriptional signatures observed in CD34+ hematopoietic stem cells (data not released). We performed this experiment in human PBMCs because the cells are commercially available with pre-obtained consent for public release and PBMCs are a primary, disease-relevant tissue that contains multiple mature cell types (including T-cells, B-cells, myeloid cells, and NK cells) with established markers for annotation of cell types. To supplement this dataset, we also measured cells from each donor at baseline with joint scRNA and single-cell chromatin accessibility measurements using the 10x Multiome assay. We hope that the addition of rich multi-omic data for each donor and cell type at baseline will help establish biological priors that explain the susceptibility of particular genes to exhibit perturbation responses in difference biological contexts.
Technical details about the experiment
To understand the dataset, it is important to know the design of the plates used to measure the treamtment effect. PBMCs from donors were thawed and plated on 96-well plates. Two columns of the plates were dedicated to positive controls (dabrfenib and belinostat) and one column was dedicated to a negative control (DMSO). The positive controls were selected because they tend to have a large impact on transcription, and the negative control is used as a solvent for the compounds used in this study. The remaining wells on the plate are allocated to each of 72 compounds. The full dataset comprises 2 different compound plates per donor for a total of 6 plates.",kaggle competitions download -c open-problems-single-cell-perturbations,"['https://www.kaggle.com/code/alexandervc/op2-eda-baseline-s', 'https://www.kaggle.com/code/ayushs9020/understanding-the-competition-open-problems', 'https://www.kaggle.com/code/meeratif/smiles-open-problems', 'https://www.kaggle.com/code/meeratif/open-problems-a-visual-story', 'https://www.kaggle.com/code/zmcxjt/streamlined-baseline-approach']"
59,"Introduction
As the sun set on the world an array of lights dotted the once dark horizon. With the help of a brigade of toads, Lux had made it past the terrors in the night to see the dawn of a new age. Seeking new challenges, plans were made to send a forward force with one mission: terraform Mars!
Welcome to the Lux AI Challenge Season 2 - NeurIPS Edition (Stage 2)!
The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand. This competition is an official NeurIPS 2023 competition and its results will be presented at the NeurIPS 2023 conference and additionally be submitted as a benchmark paper to NeurIPS 2024.
In Season 2 NeurIPS Edition, we are providing a platform to research large scale decision-making, benchmarking reinforcement learning, imitation learning etc. at scale. There are 2 modes of scale through which our competition is actively supporting.
The first is the environment itself, it is a 64x64 map with hundreds to thousands of controllable units per team with complex interactions and rules, meaning any successful agent must be capable of processing and making optimal decisions for a large number of units. The map is procedurally generated meaning a strong agent must be able to adapt their strategy according to the map, in addition to the opponent.
Moreover, we open source a GPU parallelized version of the environment powered by Jax, allowing you to easily parallel run thousands of environments on a single GPU for an over 100x speedup compared to running on a CPU. Massive thanks to our team and collaborators at Parametrix.ai for their contributions!
Stage 2 of the competition is a scaled up version of the Season 2 environment with now larger 64x64 sized maps, which previously ran as a competition on Kaggle where 600+ teams competed, resulting in extremely diverse rule-based strategies at the top along with RL and IL solutions in the top 10.
All code can be found at our Github, make sure to give it a star while you are there! To get started go to the getting started section on our GitHub, which will show you how to install the environment and run baselines.
Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play Episodes (games) against other bots on the ladder that have a similar skill rating. Over time skill ratings will go up with wins or down with losses and evened out with ties.
Every bot submitted will continue to play episodes until the end of the competition, with newer bots playing a much more frequent number of episodes. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents the uncertainty of that estimate which will decrease over time.
When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error and you can download the agent logs to help figure out why. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. Newly submitted agents will be given an increased rate in the number of episodes run to give you faster feedback.
Ranking System
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
Final Evaluation
At the submission deadline on November 17, 2023, additional submissions will be locked. From November 17, 2023 to November 28th, we will continue to run games. At the conclusion of this period, the leaderboard is final.",,,"['https://www.kaggle.com/code/stonet2000/jax-environment-extended-tutorial', 'https://www.kaggle.com/code/stonet2000/lux-ai-challenge-neurips-23-tutorial-python']"
60,"Motivation
Fully erasing the influence of the data requested to be deleted is challenging since, aside from simply deleting it from databases where it’s stored, it also requires erasing the influence of that data on other artifacts such as trained machine learning models. Moreover, recent research [1, 2] has shown that in some cases it may be possible to infer with high accuracy whether an example was used to train a machine learning model using membership inference attacks (MIAs). This can raise privacy concerns, as it implies that even if an individual's data is deleted from a database, it may still be possible to infer whether that individual's data was used to train a model.
Given the above, machine unlearning is an emergent subfield of machine learning that aims to remove the influence of a specific subset of training examples — the ""forget set"" — from a trained model. Furthermore, an ideal unlearning algorithm would remove the influence of certain examples while maintaining other beneficial properties, such as the accuracy on the rest of the train set and generalization to held-out examples. A straightforward way to produce this unlearned model is to retrain the model on an adjusted training set that excludes the samples from the forget set. However, this is not always a viable option, as retraining deep models can be computationally expensive. An ideal unlearning algorithm would instead use the already-trained model as a starting point and efficiently make adjustments to remove the influence of the requested data.
Competition
The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned.
Your goal is to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms.
The starting kit provides an example of unlearning algorithms for participants to build their unlearning models upon. Note that the starting kit uses the CIFAR10 dataset, while submissions will be scored using a different (secret) dataset. You may also want to review this example submission notebook.
This is a Code Competition. Refer to Code Requirements for details.","Metric
Submissions are evaluated using a metric outlined in this paper that takes into account both forgetting quality as well as model utility. For forgetting quality, our metric is an estimate of the the indistinguishability of the distribution of unlearned models and the distribution of retrained-from-scratch models. We then penalize unlearning algorithms that reduce the accuracy of the model. Full details of the metric will be released once the competition is over.
Important note: To measure forgetting quality we require an estimation of two distributions: the one obtained from unlearning and that obtained from retraining-from-scratch. In the case of retraining-from-scratch, two important sources of randomness are the initialization and the order with which data appears in mini-batches. In the case of unlearning, we expect that the provided original model is used as the initialization, but it is up to the participants to ensure different ordering of data in each epoch (e.g. by shuffling the dataset). If no shuffling is done, for example, it might be that running unlearning many times may not accurately capture the desired distribution and may consequently lead to a lower score. You are responsible for introducing sufficient randomness into your forgetting runs.
The metric's estimation of forgetting quality is unavoidably not perfect. The evaluation protocol of the challenge has been implemented with a high level of care, given our available human and computational resources and using state-of-the-art knowledge. However, no guarantees are provided by the organizers that our metric is flawless, nor that the results of the challenge will be indicative of the fitness of the algorithms to perform unlearning in other settings. Competitors should understand these factors as among the risks of the competition.
Submission Mechanics
You will need to submit a notebook that generates a file named submission.zip containing 512 Pytorch model checkpoints derived by causing original_model.pth to unlearn the image IDs in forget.csv. Failing to introduce random variation will most likely degrade your score. We recommend, at a minimum, shuffling the data each iteration. At the moment you must create your checkpoint files outside of the working directory to avoid exceeding each notebook's exported file disk size cap. We expect zipping to take approximately one hour.
For most of the competition your submission notebooks will only have access to the public set. At the end of the competition we will rescore your selected submissions on the private set. Unlike most competitions, it's possible to get a public leaderboard score from a submission that will not ultimately generate a private score. You are strongly encouraged to add error handling to your code and to test it against a variety of inputs. Competitors should understand this as among the risks of the competition.","Removing examples from a trained machine learning model is a major unsolved problem of ML privacy research. This competition challenges you to cause a simple model to forget images from several individuals.
Hidden Dataset Files
This is a code competition with a hidden dataset which is completely unavailable for download. We highly recommend reviewing this example notebook for a demonstration of how to generate a valid submission.
[retain/forget/validation].csv The data used to train and validate the original model checkpoint, with the original train set split into ids that need to be retained and images from 15 subjects that must be unlearned. Unlike most competitions, the public and private scores are generated using the same dataset with different splits and evaluated across separate notebook runs.
person_id - A unique ID code for each subject.
age_group - Binned ages. The target label for the models.
age - The age of the subject.
image_id - A unique ID for the image. Includes the person_id to ensure each value is unique, like 'abc-1'.
images/[person_id]/[image_id].png Images of people's faces. If the image ID is 'abc-1' the path is images/abc/1.png. All of the images have been resized to 32x32 pixels. Expect approximately 30,000 images in the hidden dataset. Note that roughly 2% of the images in the dataset have identical perceptual hashes; this is a known limitation of the problem setup.",kaggle competitions download -c neurips-2023-machine-unlearning,"['https://www.kaggle.com/code/eleni30fillou/run-unlearn-finetune', 'https://www.kaggle.com/code/scottclowe/run-unlearn-finetune-with-classweights', 'https://www.kaggle.com/code/ipythonx/keras-core-machine-unlearning-challenge', 'https://www.kaggle.com/code/izammohammed/run-unlearn-finetune-with-classweights', 'https://www.kaggle.com/code/ipythonx/keras-machine-unlearning-challenge-starting-kit']"
61,"Ribonucleic acid (RNA) is essential for most biological functions. A better understanding of how to manipulate RNA could help usher in an age of programmable medicine, including first cures for pancreatic cancer and Alzheimer’s disease as well as much-needed antibiotics and new biotechnology approaches for climate change. But first, researchers must better understand each RNA molecule's structure, an ideal problem for data science.
Recent efforts to predict RNA structure have run into a number of challenges: (1) a paucity of training data, (2) lack of intellectual and computational power, and (3) difficulties in rigorously splitting training and test data. Can a Kaggle competition close these gaps?
Towards sourcing a large and diverse collection of RNA molecules for data acquisition, the host Das laboratory brings together scientists and gamers to solve puzzles and invent medicine in the Eterna project. The Eterna community has previously unlocked new scientific principles, designed thermodynamically-optimized riboswitches, and revealed RNA degradation patterns for improving the shelf life of mRNA vaccines, which formed the basis of the Kaggle OpenVaccine challenge. Now the Eterna project is creating diverse sequences that are predicted to form complex structures.
The data for this new Ribonanza competition are experimental measurements of the chemical reactivity at each position of an RNA molecule. These data are exquisitely sensitive to the structure – or multiple structures – that each RNA forms in the test tube. An algorithm that could perfectly predict these chemical reactivities would need to have an implicit ‘understanding’ of RNA structure to do so. Such an oracle could be then utilized to predictively model structures of novel RNA molecules. As in the OpenVaccine competition, the majority of the private leaderboard data will be collected in parallel with your prediction efforts -- no one will know the answers until the competition closes!
To reiterate the potential impact of your participation, an accurate model that solves the RNA structure prediction problem could be a game changer for medical researchers who are trying to identify unique RNA-based drug targets in the many bacterial, viral, neurological, and cancer genes that remain undruggable at the protein level. In addition, accurate RNA structure prediction is needed to predictively design RNA-based medicines such as mRNA vaccines and CRISPR gene therapeutics that promise to treat nearly all human disease. More than its medical implications, RNA molecules underlie and can even dominate core biological processes for all of life, including the very first forms of life on Earth and the plants and marine organisms that fix most of the carbon on our planet now. A full understanding of life requires a full, predictive understanding of RNA.
In addition to being made available as open source code for the entire science and medicine communities, winners’ innovations will be featured in a keynote talk by Rhiju Das at the flagship conference in machine learning in structure biology at NeurIPS in December 2023.
         ","Submissions are scored using MAE, mean absolute error:
$$
\textrm{MAE} = \frac{1}{N}\sum_{i=1}^{N} \left| y_{i} - \widehat{y}_{i} \right|
$$
where \(N\) is the number of scored ground truth values, and \(y\) and \(\widehat{y}\) are the actual and predicted values, respectively. The \(y\) values will be clipped between 0 and 1 before calculating MAE, that is:
$$
y_{i} = \max(\min( y^\textrm{RAW}_{i},1.0),0.0)
$$
where \(y^\textrm{RAW}\) are the raw data values.
From the Data page: At each position of each RNA sequence, there will be two ground truth values, corresponding to reactivity determined from two kinds of chemical mapping experiments, DMS_MaP and 2A3_MaP.
Submission File
For each sequence in the test set, you must predict target reactivities for each sequence position id, two per row. If the sum of the lengths of all 1,343,824 sequences in the test set is 269,796,671, then you should make 2 \(\times\) 269,796,670 = 539,593,340 predictions.
id,reactivity_DMS_MaP,reactivity_2A3_MaP
0,0.1,0.3
1,0.3,0.2
2,0.5,0.4
etc.","In this competition, you will be predicting the reactivity of an RNA sequence to two chemical modifiers DMS and 2A3. These data can be measured efficiently through a mutational profiling (MaP) experiment read out by high-throughput sequencing and positions that are protected from chemical modification are likely to be forming base pairs or other kinds of RNA structure.
Files
train_data.csv - the training data
test_sequences.csv - the test set sequences, without any columns associated with the ground truth.
sample_submission.csv - a sample submission file in the correct format
Columns
id - integer (0,1,…) that identifies each sequence position in the sample submission.
id_min, id_max - (integer) minimum and maximum id values for each test sequence.
sequence_id - (string) An arbitrary identifier like 8cdfeef00 for each sequence.
sequence - (string) Describes the RNA sequence, a combination of A, G, U, and C for each sample. Should be 115 to 457 characters long).
experiment_type - (string) Either DMS_MaP or 2A3_MaP to describe the type of chemical mapping experiment that was used to generate each profile. References: , .",kaggle competitions download -c stanford-ribonanza-rna-folding,"['https://www.kaggle.com/code/brainbowrna/rna-science-computational-environment', 'https://www.kaggle.com/code/ayushs9020/understanding-the-competition-standford-ribonaza', 'https://www.kaggle.com/code/iafoss/rna-starter-0-186-lb', 'https://www.kaggle.com/code/iafoss/rna-starter-submission-0-186-lb', 'https://www.kaggle.com/code/dschettler8845/sr-rna-reactivity-learn-eda-baseline']"
62,"A bit of technical background on an AI compiler will help you get started! An AI model can be represented as a graph, where a node is a tensor operation (e.g. matrix multiplication, convolution, etc), and an edge represents a tensor. A compilation configuration controls how the compiler transforms the graph for a specific optimization pass. In particular, Alice can control two types of configurations/optimizations:
A layout configuration control how tensors in the graph are laid out in the physical memory, by specifying the dimension order of each input and output of an operation node.
A tile configuration controls the tile size of each fused subgraph.

Being able to predict an optimal configuration for a given graph will not only help Alice's team but also improve the compiler's heuristic to select the best configuration without human's intervention. This will make AI models run more efficiently, consuming less time and resources overall!
In this competition, your aim is to train a machine learning model based on the runtime data provided to you in the training dataset and further predict the runtime of graphs and configurations in the test dataset.
Here is a handy link to Kaggle's competition documentation, which includes, among other things, instructions on submitting predictions.
About the dataset
Our dataset, called TpuGraphs, is the performance prediction dataset on XLA HLO graphs running on Tensor Processing Units (TPUs) v3. There are 5 data collections in total:layout:xla:random, layout:xla:default, layout:nlp:random, layout:nlp:default, and tile:xla. The final score will be the average across all collections. To download the entire dataset and view more information, you may navigate to the Data tab.
Baseline models
We provide baseline models along with a training set up readily for you to get started at https://github.com/google-research-datasets/tpu_graphs. Please refer to our dataset paper on the details of the baseline models.","Evaluation Metric
As driven by realistic requirements, we use two evaluation metrics, and average them.
Specifically, for the collection tile:xla, we use the (1-slowdown) incurred of the top-K predictions to reflect how much slower the top-K configurations predicted by the model is from the actual fastest configuration as follows:
$$
1 - \left(\frac{\text{The best runtime of the top-k predictions}}{\text{The best runtime of all configurations}} - 1\right) = 2 - \frac{\min_{i \in K} y_i}{\min_{i \in A} y_i}
$$
where K is the top-K predictions, A is all configurations of the given graph from the dataset collection, and y is the measured execution time.
For the collections layout:*, we use the Kendal Tau Correlation (a ranking metric: how well does your model-predicted ranking, correspond to the real ranking of runtimes).
The choice of metrics is justified as follows. For the tile size search space, since the number of possibilities is relatively small, one can enumerate all possibilities and invoke a model on each, then choose the best few (=5, here) configurations as suggested by the model, compile with each of them, then measure the runtime of each and commit to the best. On the other hand, for the layout:* collections, the search space is quite large. Therefore, common search strategies, such as Genetic Algorithm, Simulated Annealing, and Langevin Dynamics, need access to a fitness/utility function (which can be your model). Therefore, it is important that the model can well-preserve the order of the configurations (from fastest to slowest).
Submission File
Your submission must be a csv file with header ID,TopConfigs. Each npz/**/test/*.npz file (see Data) must have one row in the csv file.
The ID is {collection}:{test_filename_without_extension}, where collection is one of tile:xla, layout:xla:random, layout:xla:default, layout:nlp:random, and layout:nlp:default.
The TopConfigs should list the indices of configurations from fastest (smallest runtime) to slowest (largest runtime) according to your model prediction separated by "";"".
For the tile:xla collection, only the first 5 entries will be considered and the rest will be ignored.
For the layout:* collections, all entries will be considered (you should output a permutation of the number of configurations!).
For a sample submission file, please download sample_submission.csv from the Data tab.
You can also generate the .csv using our starter code:
github.com/google-research-datasets/tpu_graphs -- search for combine_csvs.py","Download
You have two options to download the dataset:
(Recommended) From this page. Please download the npz_all zip file (scroll towards bottom of this page, on the right, look for ""Data Explorer"", click the npz_all directory, then click the download icon left of the ""Data Explorer"" pane. Then, unzip this file to the path ~/data/tpugraphs.
You may download the dataset from tensorflow.org. You may follow instructions on our GitHub repo or by running curl https://raw.githubusercontent.com/google-research-datasets/tpu_graphs/main/echo_download_commands.py | python | bash -- this command automatically downloads onto ~/data/tpugraphs
After either above options, your final data path should have ~/data/tpugraphs/npz/layout and ~/data/tpugraphs/npz/tile. By default, the trainer scripts (see GitHub repo) read from ~/data/tpugraphs, but you may override flag --data_root in the trainer of tile or layout collections.
In addition, you may download sample_submission.csv which contains the example submission file in the expected format.
Dataset Description
Once you have downloaded the dataset (e.g., to ~/data/tpugraphs, which is the default path for ) and unzip it, then you will get the following directory structure.",kaggle competitions download -c predict-ai-model-runtime,"['https://www.kaggle.com/code/ayushs9020/understanding-the-competition-google-slow-vs-fast', 'https://www.kaggle.com/code/werus23/tile-xla-end-to-end-train-infer', 'https://www.kaggle.com/code/abaojiang/google-fast-or-slow-detailed-eda', 'https://www.kaggle.com/code/robikscube/thinking-fast-slower-runtime-predictions', 'https://www.kaggle.com/code/carlosborrajo/google-fast-or-slow-eda-and-data-load']"
63,"Goal of the Competition
The goal of this competition is to detect sleep onset and wake. You will develop a model trained on wrist-worn accelerometer data in order to determine a person's sleep state.
Your work could make it possible for researchers to conduct more reliable, larger-scale sleep studies across a range of populations and contexts. The results of such studies could provide even more information about sleep.
The successful outcome of this competition can also have significant implications for children and youth, especially those with mood and behavior difficulties. Sleep is crucial in regulating mood, emotions, and behavior in individuals of all ages, particularly children. By accurately detecting periods of sleep and wakefulness from wrist-worn accelerometer data, researchers can gain a deeper understanding of sleep patterns and better understand disturbances in children.
Context
The “Zzzs” you catch each night are crucial for your overall health. Sleep affects everything from your development to cognitive functioning. Even so, research into sleep has proved challenging, due to the lack of naturalistic data capture alongside accurate annotation. If data science could help researchers better analyze wrist-worn accelerometer data for sleep monitoring, sleep experts could more easily conduct large-scale studies of sleep, thus improving the understanding of sleep's importance and function.
Current approaches for annotating sleep data include sleep logs, which are the gold standard for detecting the onset of sleep. However, they are impractical for many participants to use reliably, and fail to capture the nuanced difference between heading to bed and falling asleep (or, conversely, waking up and getting out of bed). Heuristic-based software is another solution that attempts to identify sleep windows, though these rely on human-engineered features of sleep (i.e. arm angle) that vary across individuals and don’t accurately summarize the sleep windows that experts can visually detect from their data. With improved tools to analyze sleep data on a large scale, researchers can explore the relationship between sleep and mood/behavioral difficulties. This knowledge can lead to more targeted interventions and treatment strategies.
Competition host Child Mind Institute (CMI) transforms the lives of children and families struggling with mental health and learning disorders by giving them the help they need. CMI has become the leading independent nonprofit in children’s mental health by providing gold-standard evidence-based care, delivering educational resources to millions of families each year, training educators in underserved communities, and developing tomorrow’s breakthrough treatments.
Established with a foundational grant from the Stavros Niarchos Foundation (SNF), the SNF Global Center for Child and Adolescent Mental Health at the Child Mind Institute works to accelerate global collaboration on under-researched areas of children’s mental health and expand worldwide access to culturally appropriate trainings, resources, and treatment. A major goal of the SNF Global Center is to expand innovations in clinical assessment and intervention, to include building, testing, and deploying new technologies to augment mental health care and research, including mobile apps, sensors, and analytical tools.
Your work will improve researchers' ability to analyze accelerometer data for sleep monitoring and enable them to conduct large-scale studies of sleep. Ultimately, the work of this competition could improve awareness and guidance surrounding the importance of sleep. The valuable insights into how environmental factors impact sleep, mood, and behavior can inform the development of personalized interventions and support systems tailored to the unique needs of each child.
Acknowledgments
The data used for this competition was provided by the Healthy Brain Network, a landmark mental health study based in New York City that will help children around the world. In the Healthy Brain Network, families, community leaders, and supporters are partnering with the Child Mind Institute to unlock the secrets of the developing brain. In addition to generous support provided by the Kaggle team, financial support has been provided by the Stavros Niarchos Foundation (SNF) as part of its Global Health Initiative (GHI) through the SNF Global Center for Child and Adolescent Mental Health at the Child Mind Institute.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on the average precision of detected events, averaged over timestamp error tolerance thresholds, averaged over event classes.
Detections are matched to ground-truth events within error tolerances, with ambiguities resolved in order of decreasing confidence. For both event classes, we use error tolerance thresholds of 1, 3, 5, 7.5, 10, 12.5, 15, 20, 25, 30 in minutes, or 12, 36, 60, 90, 120, 150, 180, 240, 300, 360 in steps.
You may find Python code for this metric here: Event Detection AP.
Detailed Description
Evaluation proceeds in three steps:
Assignment - Predicted events are matched with ground-truth events.
Scoring - Each group of predictions is scored against its corresponding group of ground-truth events via Average Precision.
Reduction - The multiple AP scores are averaged to produce a single overall score.
Assignment
For each set of predictions and ground-truths within the same event x tolerance x series_id group, we match each ground-truth to the highest-confidence unmatched prediction occurring within the allowed tolerance.
Some ground-truths may not be matched to a prediction and some predictions may not be matched to a ground-truth. They will still be accounted for in the scoring, however.
Scoring
Collecting the events within each series_id, we compute an Average Precision score for each event x tolerance group. The average precision score is the area under the precision-recall curve generated by decreasing confidence score thresholds over the predictions. In this calculation, matched predictions over the threshold are scored as TP and unmatched predictions as FP. Unmatched ground-truths are scored as FN.
Reduction
The final score is the average of the above AP scores, first averaged over tolerance, then over event.
Submission File
For each series indicated by series_id, predict each event occurring in that series by giving the event type and the step where the event occurred as well as a confidence score for that event. The evaluation metric additionally requires a row_id with an enumeration of events.
The file should contain a header and have the following format:
row_id,series_id,step,event,score
0,038441c925bb,100,onset,0.0
1,038441c925bb,105,wakeup,0.0
2,03d92c9f6f8a,80,onset,0.5
3,03d92c9f6f8a,110,wakeup,0.5
4,0402a003dae9,90,onset,1.0
5,0402a003dae9,120,wakeup,1.0
...
Note that while the ground-truth annotations were made following certain conventions (as described on the Data page), there are no such restrictions on your submission file.","The dataset comprises about 500 multi-day recordings of wrist-worn accelerometer data annotated with two event types: onset, the beginning of sleep, and wakeup, the end of sleep. Your task is to detect the occurrence of these two events in the accelerometer series.
While sleep logbooks remain the gold-standard, when working with accelerometer data we refer to sleep as the longest single period of inactivity while the watch is being worn. For this data, we have guided raters with several concrete instructions:
A single sleep period must be at least 30 minutes in length
A single sleep period can be interrupted by bouts of activity that do not exceed 30 consecutive minutes
No sleep windows can be detected unless the watch is deemed to be worn for the duration (elaborated upon, below)
The longest sleep window during the night is the only one which is recorded
If no valid sleep window is identifiable, neither an onset nor a wakeup event is recorded for that night.
Sleep events do not need to straddle the day-line, and therefore there is no hard rule defining how many may occur within a given period. However, no more than one window should be assigned per night. For example, it is valid for an individual to have a sleep window from 01h00–06h00 and 19h00–23h30 in the same calendar day, though assigned to consecutive nights
There are roughly as many nights recorded for a series as there are 24-hour periods in that series.",kaggle competitions download -c child-mind-institute-detect-sleep-states,"['https://www.kaggle.com/code/carlmcbrideellis/zzzs-random-forest-model-starter', 'https://www.kaggle.com/code/patrick0302/viz-of-sleeping-time-series', 'https://www.kaggle.com/code/carlmcbrideellis/zzzs-make-small-starter-datasets-target', 'https://www.kaggle.com/code/andradaolteanu/zzz-good-night-sleep-with-80-memory-reduction', 'https://www.kaggle.com/code/renatoreggiani/feature-eng-ideas-and-lightgbm-cmi']"
64,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!
With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in August every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","This is a different type of competition. Instead of submitting predictions, your task is to submit a dataset that will be used to train a random forest regressor model. This model will then be used to make predictions against a hidden test dataset. Your score will be the Root Mean Square Error (RMSE) between the model predictions and ground truth of the test set.
Model
Your submission will be used as a training dataset to train the following model and make predictions against a hidden test dataset.
from sklearn.ensemble import RandomForestRegressor

y_train = train.pop('target') # train is your submission!
rf = RandomForestRegressor(
       n_estimators=1000,
       max_depth=7,
       n_jobs=-1,
       random_state=42)
rf.fit(train, y_train)
y_hat = rf.predict(test) # test set is hidden from you
Submission File
You are submitting a dataset that will be used train a random forest model. Your submission must have all of the columns contained in the sample_submission.csv on the Data tab. Your submission must not contain any NaN values (it will error if it does). In addition, your submission may have fewer rows than the provided sample_submission.csv, but may not have a greater number of rows.
id,target,O2_1,O2_2,O2_3,...
0,8.59,7.5,9,9.545,...
1,9.1,13.533,40.9,8.77,...
2,8.21,3.71,5.42,8.77,...
etc.","This is a very different type of challenge! For this challenge, your task is to improve a dataset that is being used to train a random forest model; in other words, your submission will be training data, not predictions. A random forest model will be trained on your submission, used to make predictions, and then those predictions will be used to generate your score.
The dataset for this competition is a synthetic dataset based off of the Dissolved oxygen prediction in river water dataset. You are free to use the original in any way that you find useful.
Please see important information on the Evaluation tab about the model that will be trained on your submitted data.
Good luck!
Files
sample_submission.csv - this is an example of the training data you will be submitting. Your submission must have all of the columns contained in this sample file. Your submission must not contain any NaN values (it will error if it does). In addition, your submission may have fewer rows than the provided sample_submission.csv, but may not have a greater number of rows.",kaggle competitions download -c playground-series-s3e21,"['https://www.kaggle.com/code/serhiikharchuk/new-way-test', 'https://www.kaggle.com/code/thomaswrightanderson/ps3e21-1st-place-solution', 'https://www.kaggle.com/code/maximeperez/4th-place-solution', 'https://www.kaggle.com/code/asif00/clean-and-simple-beginner-feature-selection', 'https://www.kaggle.com/code/jokerinthapack/bring-in-the-noise-bring-in-the-funk']"
65,"Goal of the Competition
Traumatic injury is the most common cause of death in the first four decades of life and a major public health problem around the world. There are estimated to be more than 5 million annual deaths worldwide from traumatic injury. Prompt and accurate diagnosis of traumatic injuries is crucial for initiating appropriate and timely interventions, which can significantly improve patient outcomes and survival rates. Computed tomography (CT) has become an indispensable tool in evaluating patients with suspected abdominal injuries due to its ability to provide detailed cross-sectional images of the abdomen.
Interpreting CT scans for abdominal trauma, however, can be a complex and time-consuming task, especially when multiple injuries or areas of subtle active bleeding are present. This challenge seeks to harness the power of artificial intelligence and machine learning to assist medical professionals in rapidly and precisely detecting injuries and grading their severity. The development of advanced algorithms for this purpose has the potential to improve trauma care and patient outcomes worldwide.
Context
Blunt force abdominal trauma is among the most common types of traumatic injury, with the most frequent cause being motor vehicle accidents. Abdominal trauma may result in damage and internal bleeding of the internal organs, including the liver, spleen, kidneys, and bowel. Detection and classification of injuries are key to effective treatment and favorable outcomes. A large proportion of patients with abdominal trauma require urgent surgery. Abdominal trauma often cannot be diagnosed clinically by physical exam, patient symptoms, or laboratory tests.
Prompt diagnosis of abdominal trauma using medical imaging is thus critical to patient care. AI tools that assist and expedite diagnosis of abdominal trauma have the potential to substantially improve patient care and health outcomes in the emergency setting.
The RSNA Abdominal Trauma Detection AI Challenge, organized by the RSNA in collaboration with the American Society of Emergency Radiology (ASER) and the Society for Abdominal Radiology (SAR), gives researchers the task of building models that detect severe injury to the internal abdominal organs, including the liver, kidneys, spleen, and bowel, as well as any active internal bleeding.
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated using the average of the sample weighted log losses from each injury type and an any_injury prediction generated by the metric. The metric implementation notebook can be found here.
The sample weights are as follows:
1 for all healthy labels.
2 for low grade solid organ injuries (liver, spleen, kidney).
4 for high grade solid organ injuries.
2 for bowel injuries.
6 for extravasation.
6 for the auto-generated any_injury label.
For each patient ID in the test set, you must predict a probability for each of the different possible injury types and degrees. The file should contain a header and have the following format:
patient_id,bowel_healthy,bowel_injury,extravasation_healthy,extravasation_injury,kidney_healthy,kidney_low,kidney_high,liver_healthy,liver_low,liver_high,spleen_healthy,spleen_low,spleen_high
10102,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5
10107,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5
etc.","The goal of this competition is to identify several potential injuries in CT scans of trauma patients. Any of these injuries can be fatal on a short time frame if untreated so there is great value in rapid diagnosis.
This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a full length sample submission) will be made available to your notebook.
Files
train.csv Target labels for the train set. Note that patients labeled healthy may still have other medical issues, such as cancer or broken bones, that don't happen to be covered by the competition labels.
patient_id - A unique ID code for each patient.
[bowel/extravasation]_[healthy/injury] - The two injury types with binary targets.
[kidney/liver/spleen]_[healthy/low/high] - The three injury types with three target levels.
any_injury - Whether the patient had any injury at all.
[train/test]_images/[patient_id]/[series_id]/[image_instance_number].dcm The CT scan data, in DICOM format. Scans from dozens of different CT machines have been reprocessed to use the run length encoded lossless compression format but retain other differences such as the number of bits per pixel, pixel range, and pixel representation. Expect to see roughly 1,100 patients in the test set.",kaggle competitions download -c rsna-2023-abdominal-trauma-detection,"['https://www.kaggle.com/code/aritrag/kerascv-starter-notebook-train', 'https://www.kaggle.com/code/aritrag/kerascv-starter-notebook-infer', 'https://www.kaggle.com/code/ayushs9020/understanding-the-competition-rsna', 'https://www.kaggle.com/code/awsaf49/rsna-atd-cnn-tpu-train', 'https://www.kaggle.com/code/prashantverma13/rsna-2023-atd-in-depth-analysis-and-eda']"
66,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!
With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in July every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
This Episode is a similar to the Kaggle/Zindi Hackathon that was held at the Kaggle@ICLR 2023: ML Solutions in Africa workshop in Rwanda, and builds on an ongoing partnership between Kaggle and Zindi to build community-driven impact across Africa. Zindi is a professional network for data scientists to learn, grow their careers, and get jobs. If you haven't done so recently, stop by Zindi and see what they're up to!
Predicting CO2 Emissions
The ability to accurately monitor carbon emissions is a critical step in the fight against climate change. Precise carbon readings allow researchers and governments to understand the sources and patterns of carbon mass output. While Europe and North America have extensive systems in place to monitor carbon emissions on the ground, there are few available in Africa.
The objective of this challenge is to create a machine learning models using open-source CO2 emissions data from Sentinel-5P satellite observations to predict future carbon emissions.
These solutions may help enable governments, and other actors to estimate carbon emission levels across Africa, even in places where on-the-ground monitoring is not possible.
Acknowledgements
We acknowledge Carbon Monitor for the use of the GRACED dataset, and special thanks Darius Moruri from Zindi for his work in preparing the dataset and starter notebooks.","Root Mean Squared Error (RMSE)
Submissions are scored on the root mean squared error. RMSE is defined as:
$$
\textrm{RMSE} = \sqrt{ \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 }
$$
where \( \hat{y}_i \) is the predicted value and \( y_i \) is the original value for each instance \(i\).
Submission File
For each ID_LAT_LON_YEAR_WEEK row in the test set, you must predict the value for the target emission. The file should contain a header and have the following format:
ID_LAT_LON_YEAR_WEEK,emission
ID_-0.510_29.290_2022_00,81.94
ID_-0.510_29.290_2022_01,81.94
ID_-0.510_29.290_2022_02,81.94
etc.","The objective of this challenge is to create machine learning models that use open-source emissions data (from Sentinel-5P satellite observations) to predict carbon emissions.
Approximately 497 unique locations were selected from multiple areas in Rwanda, with a distribution around farm lands, cities and power plants. The data for this competition is split by time; the years 2019 - 2021 are included in the training data, and your task is to predict the CO2 emissions data for 2022 through November.
Seven main features were extracted weekly from Sentinel-5P from January 2019 to November 2022. Each feature (Sulphur Dioxide, Carbon Monoxide, etc) contain sub features such as column_number_density which is the vertical column density at ground level, calculated using the DOAS technique. You can read more about each feature in the below links, including how they are measured and variable definitions. You are given the values of these features in the test set and your goal to predict CO2 emissions using time information as well as these features.
Sulphur Dioxide - COPERNICUS/S5P/NRTI/L3_SO2
Carbon Monoxide - COPERNICUS/S5P/NRTI/L3_CO
Nitrogen Dioxide - COPERNICUS/S5P/NRTI/L3_NO2
Formaldehyde - COPERNICUS/S5P/NRTI/L3_HCHO
UV Aerosol Index - COPERNICUS/S5P/NRTI/L3_AER_AI
Ozone - COPERNICUS/S5P/NRTI/L3_O3
Cloud - COPERNICUS/S5P/OFFL/L3_CLOUD",kaggle competitions download -c playground-series-s3e20,"['https://www.kaggle.com/code/inversion/getting-started-modeling', 'https://www.kaggle.com/code/inversion/getting-started-eda', 'https://www.kaggle.com/code/ambrosm/pss3e20-eda-which-makes-sense', 'https://www.kaggle.com/code/kacperrabczewski/rwanda-co2-step-by-step-guide', 'https://www.kaggle.com/code/yaaangzhou/en-playground-s3-e20-eda-modeling']"
67,"Goal of the Competition
The goal of this competition is to recognize Bengali speech from out-of-distribution audio recordings. You will build a model trained on the first Massively Crowdsourced (MaCro) Bengali speech dataset with 1,200 hours of data from ~24,000 people from India and Bangladesh. The test set contains samples from 17 different domains that are not present in training.
Your efforts could improve Bengali speech recognition using the first Bengali out-of-distribution speech recognition dataset. In addition, your submission will be among the first open-source speech recognition methods for Bengali.
Context
Bengali is one of the most spoken languages in the world, with approximately 340 million native and second-language speakers globally. With that comes diversity in dialects and prosodic features (combinations of sounds). For example, Muslim religious sermons in Bengali are often delivered with a pace and tonality that is significantly different from regular speech. Such ‘shifts’ can be challenging even for commercially available speech recognition methods (the Google Speech API for Bengali has a Word Error Rate of 74% for Bengali religious sermons).
There are no robust open-source speech recognition models for Bengali currently, though your data science skills could certainly help change that. In particular, out-of-distribution generalization is a common machine learning problem. When test and training data are similar, they’re in-distribution. To account for Bengali’s diversity, this competition’s data is intentionally out-of-distribution, with the challenge to improve results..
Competition host Bengali.AI is a non-profit community initiative working to accelerate language technology research for Bengali (known locally as Bangla). Bengali.AI crowdsources large-scale datasets through community-driven collection campaigns and crowdsource solutions for their datasets through research competitions. All the outcomes from Bengali.AI's two-pronged approach, including datasets and trained models, are open-sourced for public use.
Your work in this competition could have an impact beyond speech recognition improvements for one of the world's most popular, yet low-resource languages. You could also provide a much-needed push towards solving one of speech recognition's major challenges, out-of-distribution generalization.
Acknowledgments
We specially thank our collaborators from Aspire to Innovate (a2i) program by the Govt. Bangladesh, Bangladesh University of Engineering and Technology (BUET), and Shahjalal University of Science and Technology (SUST).
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated by a mean Word Error Rate, proceeding as follows:
The WER is computed for each instance in the test set.
The WERs are averaged within domains, weighted by the number of words in the sentence.
The (unweighted) mean of the domain averages is the final score.
This Python code computes the metric:
import jiwer  # you may need to install this library

def mean_wer(solution, submission):
    joined = solution.merge(submission.rename(columns={'sentence': 'predicted'}))
    domain_scores = joined.groupby('domain').apply(
        # note that jiwer.wer computes a weighted average wer by default when given lists of strings
        lambda df: jiwer.wer(df['sentence'].to_list(), df['predicted'].to_list()),
    )
    return domain_scores.mean()

assert (solution.columns == ['id', 'domain', 'sentence']).all()
assert (submission.columns == ['id',' sentence']).all()
Submission Format
The submission files should contain two columns: id and sentence. You will need to predict the sentence for each recording in the test/ folder.
The submission file should contain a header and have the following format:
id,sentence
0f3dac00655e,এছাড়াও নিউজিল্যান্ড এ ক্রিকেট দলের হয়েও খেলছেন তিনি।
a9395e01ad21,এছাড়াও নিউজিল্যান্ড এ ক্রিকেট দলের হয়েও খেলছেন তিনি।
bf36ea8b718d,এছাড়াও নিউজিল্যান্ড এ ক্রিকেট দলের হয়েও খেলছেন তিনি।
...","The competition dataset comprises about 1200 hours of recordings of Bengali speech. Your goal is to transcribe recordings of speech that is out-of-distribution with respect to the training set.
Note that this is a Code Competition, in which the actual test set is hidden. In this public version, we give some sample data in the correct format to help you author your solutions. The full test set contains about 20 hours of speech in almost 8000 MP3 audio files. All of the files in the test set are encoded at a sample rate of 32k, a bit rate of 48k, in one channel. The test set annotations were normalized with the bnUnicodeNormalizer.
Details on the dataset are available in the dataset paper: https://arxiv.org/abs/2305.09688
Citation:
@article{rakib2023oodspeech,
  title={{OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking},
  author={Rakib, Fazle Rabbi and Dip, Souhardya Saha and Alam, Samiul and Tasnim, Nazia and Shihab, Md Istiak Hossain and Ansary, Md Nazmuddoha and Hossen, Syed Mobassir and Meghla, Marsia Haque and Mamun, Mamunur and Sadeque, Farig and others},
  journal={Proc. Interspeech 2023},
  year={2023}
}",kaggle competitions download -c bengaliai-speech,"['https://www.kaggle.com/code/reasat/yellowking-dlsprint-inference', 'https://www.kaggle.com/code/sujaykapadnis/bengali-speech-recognition-for-everyone', 'https://www.kaggle.com/code/nischaydnk/bengali-finetuning-baseline-wav2vec2-inference', 'https://www.kaggle.com/code/ttahara/bengali-sr-public-wav2vec2-0-w-lm-baseline', 'https://www.kaggle.com/code/mbmmurad/detailed-eda-normalizer-and-wer']"
68,"Goal of the Competition
The goal of this competition is to assess the quality of summaries written by students in grades 3-12. You'll build a model that evaluates how well a student represents the main idea and details of a source text, as well as the clarity, precision, and fluency of the language used in the summary. You'll have access to a collection of real student summaries to train your model.
Your work will assist teachers in evaluating the quality of student work and also help learning platforms provide immediate feedback to students.
Context
Summary writing is an important skill for learners of all ages. Summarization enhances reading comprehension, particularly among second language learners and students with learning disabilities. Summary writing also promotes critical thinking, and it’s one of the most effective ways to improve writing abilities. However, students rarely have enough opportunities to practice this skill, as evaluating and providing feedback on summaries can be a time-intensive process for teachers. Innovative technology like large language models (LLMs) could help change this, as teachers could employ these solutions to assess summaries quickly.
There have been advancements in the automated evaluation of student writing, including automated scoring for argumentative or narrative writing. However, these existing techniques don't translate well to summary writing. Evaluating summaries introduces an added layer of complexity, where models must consider both the student writing and a single, longer source text. Although there are a handful of current techniques for summary evaluation, these models have often focused on assessing automatically-generated summaries rather than real student writing, as there has historically been a lack of these types of datasets.
Competition host CommonLit is a nonprofit education technology organization. CommonLit is dedicated to ensuring that all students, especially students in Title I schools, graduate with the reading, writing, communication, and problem-solving skills they need to be successful in college and beyond. The Learning Agency Lab, Vanderbilt University, and Georgia State University join CommonLit in this mission.
As a result of your help to develop summary scoring algorithms, teachers and students alike will gain a valuable tool that promotes this fundamental skill. Students will have more opportunities to practice summarization, while simultaneously improving their reading comprehension, critical thinking, and writing abilities.
Acknowledgments
CommonLit, the Learning Agency Lab, Vanderbilt University, and Georgia State University would like to thank the Walton Family Foundation and Schmidt Futures for their support in making this work possible.
                         
This is a Code Competition. Refer to Code Requirements for details.","Submissions are scored using MCRMSE, mean columnwise root mean squared error:
$$
\textrm{MCRMSE} = \frac{1}{N_{t}}\sum_{j=1}^{N_{t}}\left(\frac{1}{n} \sum_{i=1}^{n} (y_{ij} - \hat{y}_{ij})^2\right)^{1/2}
$$
where \(N_t\) is the number of scored ground truth target columns, and \(y\) and \(\hat{y}\) are the actual and predicted values, respectively.
Submission File
For each student_id in the test set, you must predict a value for each of the two analytic measures (described on the Data page). The file should contain a header and have the following format:
student_id,content,wording
000000ffffff,0.0,0.0
111111eeeeee,0.0,0.0
222222cccccc,0.0,0.0
333333dddddd,0.0,0.0
...","The dataset comprises about 24,000 summaries written by students in grades 3-12 of passages on a variety of topics and genres. These summaries have been assigned scores for both content and wording. The goal of the competition is to predict content and wording scores for summaries on unseen topics.
File and Field Information
summaries_train.csv - Summaries in the training set.
student_id - The ID of the student writer.
prompt_id - The ID of the prompt which links to the prompt file.
text - The full text of the student's summary.
content - The content score for the summary. The first target.
wording - The wording score for the summary. The second target.
summaries_test.csv - Summaries in the test set. Contains all fields above except content and wording.
prompts_train.csv - The four training set prompts. Each prompt comprises the complete summarization assignment given to students.
prompt_id - The ID of the prompt which links to the summaries file.
prompt_question - The specific question the students are asked to respond to.
prompt_title - A short-hand title for the prompt.",kaggle competitions download -c commonlit-evaluate-student-summaries,"['https://www.kaggle.com/code/ryanholbrook/evaluate-student-summaries-efficiency-lb', 'https://www.kaggle.com/code/gusthema/commonlit-evaluate-student-summaries-w-tfdf', 'https://www.kaggle.com/code/ayushs9020/understanding-the-competition-commonlit', 'https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect', 'https://www.kaggle.com/code/cody11null/tuned-debertav3-lgbm-autocorrect']"
69,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!
With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in July every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.
Submission File
For each id in the test set, you must predict the corresponding num_sold. The file should contain a header and have the following format:
id,num_sold
136950,100
136950,100
136950,100
etc.","For this challenge, you will be predicting a full year worth of sales for various fictitious learning modules from different fictitious Kaggle-branded stores in different (real!) countries. This dataset is completely synthetic, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. You are given the task of predicting sales during for year 2022.
Good luck!
Files
train.csv - the training set, which includes the sales data for each date-country-store-item combination.
test.csv - the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c playground-series-s3e19,
70,"Goal of the Competition
Inspired by the OpenBookQA dataset, this competition challenges participants to answer difficult science-based questions written by a Large Language Model.
Your work will help researchers better understand the ability of LLMs to test themselves, and the potential of LLMs that can be run in resource-constrained environments.
Context
As the scope of large language model capabilities expands, a growing area of research is using LLMs to characterize themselves. Because many preexisting NLP benchmarks have been shown to be trivial for state-of-the-art models, there has also been interesting work showing that LLMs can be used to create more challenging tasks to test ever more powerful models.
At the same time methods like quantization and knowledge distillation are being used to effectively shrink language models and run them on more modest hardware. The Kaggle environment provides a unique lens to study this as submissions are subject to both GPU and time limits.
The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.
Right now we estimate that the largest models run on Kaggle are around 10 billion parameters, whereas gpt3.5 clocks in at 175 billion parameters. If a question-answering model can ace a test written by a question-writing model more than 10 times its size, this would be a genuinely interesting result; on the other hand if a larger model can effectively stump a smaller one, this has compelling implications on the ability of LLMs to benchmark and test themselves.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated according to the Mean Average Precision @ 3 (MAP@3):
$$MAP@3 = \frac{1}{U} \sum_{u=1}^{U} \sum_{k=1}^{min(n,3)} P(k) \times rel(k)$$
where \( U \) is the number of questions in the test set, \( P(k) \) is the precision at cutoff \( k \), \( n \) is the number predictions per question, and \( rel(k) \) is an indicator function equaling 1 if the item at rank \( k \) is a relevant (correct) label, zero otherwise.
Once a correct label has been scored for an individual question in the test set, that label is no longer considered relevant for that question, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0.
[A, B, C, D, E]
[A, A, A, A, A]
[A, B, A, C, A]
Submission File
For each id in the test set, you may predict up to 3 labels for your prediction. The file should contain a header and have the following format:
id,prediction
0,A B C
1,B C A
2,C A B
etc.","Your challenge in this competition is to answer multiple-choice questions written by an LLM. While the specifics of the process used to generate these questions aren't public, we've included 200 sample questions with answers to show the format, and to give a general sense of the kind of questions in the test set. However, there may be a distributional shift between the sample questions and the test set, so solutions that generalize to a broad set of questions are likely to perform better. Each question consists of a prompt (the question), 5 options labeled A, B, C, D, and E, and the correct answer labeled answer (this holds the label of the most correct answer, as defined by the generating LLM).
This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook. The test set has the same format as the provided test.csv but has ~4000 questions that may be different is subject matter.
Files
train.csv - a set of 200 questions with the answer column
test.csv - the test set; your task it to predict the top three most probable answers given the prompt. NOTE: the test data you see here just a copy of the training data without the answers. The unseen re-run test set is comprised of ~4,000 different prompts.
sample_submission.csv - a sample submission file in the correct format
Columns",kaggle competitions download -c kaggle-llm-science-exam,
71,"Welcome to the 2023 edition of Kaggle's Playground Series!
Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!
With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in June every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.
💡Getting Started Notebook
To get started quickly, feel free to take advantage of this starter notebook.
Synthetically-Generated Datasets
Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!","Submissions are evaluated on area under the ROC curve between the predicted probability and the ground truth for each target, and the final score is the average of the individual AUCs of each predicted column.
Submission File
For each id in the test set, you must predict the value for the targets EC1 and EC2. The file should contain a header and have the following format:
id,EC1,EC2
14838,0.22,0.71
14839,0.78,0.43
14840,0.53,0.11
etc.",,,
72,,,,,
73,,,,,
74,,,,,
75,,,,,
76,"The goal of this competition is to understand the relationship between code and comments in Python notebooks. You are challenged to reconstruct the order of markdown cells in a given notebook based on the order of the code cells, demonstrating comprehension of which natural language references which code.
Context
Research teams across Google and Alphabet are exploring new ways that machine learning can assist software developers, and want to rally more members of the developer community to help explore this area too. Python notebooks provide a unique learning opportunity, because unlike a lot of standard source code, notebooks often follow narrative format, with comment cells implemented in markdown that explain a programmer's intentions for corresponding code cells. An understanding of the relationships between code and markdown could lend to fresh improvements across many aspects of AI-assisted development, such as the construction of better data filtering and preprocessing pipelines for model training, or automatic assessments of a notebook's readability.
We have assembled a dataset of approximately 160,000 public Python notebooks from Kaggle and have teamed up with X, the moonshot factory to design a competition that challenges participants to use this dataset of published notebooks to build creative techniques aimed at better understanding the relationship between comment cells and code cells.
After the submission deadline, Kaggle and X will evaluate the performance of submitted techniques on new, previously unseen notebooks. We're excited to see how the insights learned from this competition affect the future of notebook authorship.","Predictions are evaluated by the Kendall tau correlation between predicted cell orders and ground truth cell orders accumulated across the entire collection of test set notebooks.
Let \(S\) be the number of swaps of adjacent entries needed to sort the predicted cell order into the ground truth cell order. In the worst case, a predicted order for a notebook with \(n\) cells will need \(\frac{1}{2}n (n - 1)\) swaps to sort.
We sum the number of swaps from your predicted cell order across the entire collection of test set notebooks, and similarly with the worst-case number of swaps. We then compute the Kendall tau correlation as:
\[K = 1 - 4 \frac{\sum_i S_{i}}{\sum_i n_i(n_i - 1)}\]
You may find a Python implementation in this notebook: Competition Metric - Kendall Tau Correlation.
Submission File
For each id in the test set (representing a notebook), you must predict cell_order, the correct ordering of its cells in terms of the cell ids. The file should contain a header and have the following format:
id,cell_order
0009d135ece78d,ddfd239c c6cd22db 1372ae9b ...
0010483c12ba9b,54c7cab3 fe66203e 7844d5f8 ...
0010a919d60e4f,aafc3d23 80e077ec b190ebb4 ...
0028856e09c5b7,012c9d02 d22526d1 3ae7ece3 ...
etc.","The dataset for this competition comprises about 160,000 Jupyter notebooks published by the Kaggle community. Jupyter notebooks are the tool of choice for many data scientists for their ability to tell a narrative with both code and natural language. These two types of discourse are contained within cells of the notebook, and we refer to these cells as either code cells or markdown cells (markdown being the text formatting language used by Jupyter).
Your task is to predict the correct ordering of the cells in a given notebook whose markdown cells have been shuffled.
The notebooks in this dataset have been selected and processed to ensure their suitability for the competition task. All notebooks:
Have been published publicly on Kaggle under the Apache 2.0 open source license.
Represent the most recently published version of the notebook.
Contain at least one code cell and at least one markdown cell.
Have code written in the Python language.
Have had empty cells removed.
This is a code competition, in which you will submit a model or code that will be run against a future test set:
The first-stage test set contains notebooks from an approximately 90-day historical window of time.
The second-stage test set will contain a similar number of notebooks, collected from a future 90-day window of time. This is necessary to prevent models from looking up the order of existing public notebooks. The selection criteria for second-stage notebooks will be monitored for competition fairness purposes. For example, it will exclude competition participants' own notebooks.",kaggle competitions download -c AI4Code,"['https://www.kaggle.com/code/vad13irt/optimization-approaches-for-transformers', 'https://www.kaggle.com/code/ryanholbrook/getting-started-with-ai4code', 'https://www.kaggle.com/code/aerdem4/ai4code-pytorch-distilbert-baseline', 'https://www.kaggle.com/code/odins0n/ai4code-detailed-eda', 'https://www.kaggle.com/code/andradaolteanu/ai4code-language-detection-and-model-tuning']"
77,"The May edition of the 2022 Tabular Playground series binary classification problem that includes a number of different feature interactions. This competition is an opportunity to explore various methods for identifying and exploiting these feature interactions.
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
We've also built a starter notebook for you that uses TensorFlow Decision Forests, a TensorFlow library that matches the power of XGBoost with a friendly, straightforward user interface.
Good luck and have fun!
Acknowledgments
Photo by Clarisse Croset on Unsplash.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
id,target
900000,0.65
900001,0.97
900002,0.02
etc.","For this challenge, you are given (simulated) manufacturing control data and are tasked to predict whether the machine is in state 0 or state 1. The data has various feature interactions that may be important in determining the machine state.
Good luck!
Files
train.csv - the training data, which includes normalized continuous data and categorical data
test.csv - the test set; your task is to predict binary target variable which represents the state of a manufacturing process
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-may-2022,"['https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense', 'https://www.kaggle.com/code/kellibelcher/tps-may-2022-eda-lgbm-neural-networks', 'https://www.kaggle.com/code/hasanbasriakcay/tpsmay22-insightful-eda-fe-baseline', 'https://www.kaggle.com/code/wti200/analysing-interactions-with-shap', 'https://www.kaggle.com/code/ambrosm/tpsmay22-advanced-keras']"
78,"Goal of the Competition
The goal of this competition is to compute smartphones location down to the decimeter or even centimeter resolution which could enable services that require lane-level accuracy such as HOV lane ETA estimation. You'll develop a model based on raw location measurements from Android smartphones collected in opensky and light urban roads using datasets collected by the host.
Your work will help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with improved granularity. As a result, new navigation methods could be built upon the more precise data.
Context
Have you ever missed the lane change before a highway exit? Do you want to know the estimated time of arrival (ETA) of a carpool lane rather than other lanes? These and other useful features require precise smartphone positioning services. Machine learning models can improve the accuracy of Global Navigation Satellite System (GNSS) data. With more refined data, billions of Android phone users could have a more fine-tuned positioning experience.
GNSS chipsets provide raw measurements, which can be used to compute the smartphone’s position. Current mobile phones only offer 3-5 meters of positioning accuracy. For advanced use cases, the results are not fine enough nor reliable. Urban obstructions create the largest barriers to GPS accuracy. The data in this challenge includes only traces collected on opensky and light urban roads. These highways and main streets are the most widely used roads and will test the limits of smartphone positioning.
The Android GPS team in Google hosted the Smartphone Decimeter Challenge in 2021. Works by the three winners were presented at the ION GNSS+ 2021 Conference. This year, co-sponsored by the Institute of Navigation, this competition continues to seek advanced research in smartphone GNSS positioning accuracy and help people better navigate the world around them. In order to build upon last year’s progress, the data also includes traces from the 2021 competition.
Future competitions could include traces collected in harsher environments, such as deep urban areas with obstacles to satellite signals. Your efforts in this competition could impact how this more difficult data is interpreted. With decimeter level position accuracy, mobile users could gain better lane-level navigation, AR walk/drive, precise agriculture via phones, and greater specificity in the location of road safety issues. It will also enable a more personalized fine tuned navigation experience.
Photos by Jared Murray, Thaddaeus Lim and Tobias Rademacher on Unsplash.","Submissions are scored on the mean of the 50th and 95th percentile distance errors. For every phone and once per second, the horizontal distance (in meters) is computed between the predicted latitude/longitude and the ground truth latitude/longitude. These distance errors form a distribution from which the 50th and 95th percentile errors are calculated (i.e. the 95th percentile error is the value, in meters, for which 95% of the distance errors are smaller). The 50th and 95th percentile errors are then averaged for each phone. Lastly, the mean of these averaged values is calculated across all phones in the test set.
Submission File
For each phone and UnixTimeMillis in the sample submission, you must predict the latitude and longitude. The sample submission typically requires a prediction once per second but may include larger gaps if there were too few valid GNSS signals. The submission file should contain a header and have the following format:
phone,UnixTimeMillis,LatitudeDegrees,LongitudeDegrees
2020-05-15-US-MTV-1_Pixel4,1273608785432,37.904611315634504,-86.48107806249548
2020-05-15-US-MTV-1_Pixel4,1273608786432,37.904611315634504,-86.48107806249548
2020-05-15-US-MTV-1_Pixel4,1273608787432,37.904611315634504,-86.48107806249548","This challenge provides data from a variety of instruments useful for determining a phone's position: signals from GPS satellites, accelerometer readings, gyroscope readings, and more. Compared to last year's competition you will see more data overall, a wider variety of routes, and only a single phone per drive in the test set.
As this challenge’s design is focused on post-processing applications such as lane-level mapping, future data along a route will be available to generate positions as precisely as possible. In order to encourage the development of a general GNSS positioning algorithm, in-phone GPS chipset locations will not be provided, as they are derived from a manufacturer proprietary algorithm that varies by phone model and other factors.
The data collection process used the same core approach described in this paper. If publishing work based on this dataset/challenge, please ensure proper citation per the Competition Rules.
Files
[train/test]/[drive_id]/[phone_name]/supplemental/[phone_name][.20o/.21o/.22o/.nmea] - Equivalent data to the gnss logs in other formats used by the GPS community.


train/[drive_id]/[phone_name]/ground_truth.csv - Reference locations at expected timestamps.
MessageType - ""Fix"", the prefix of sentence.",kaggle competitions download -c smartphone-decimeter-2022,"['https://www.kaggle.com/code/saitodevel01/gsdc2-baseline-submission', 'https://www.kaggle.com/code/taroz1461/carrier-smoothing-robust-wls-kalman-smoother', 'https://www.kaggle.com/code/robikscube/smartphone-competition-2022-twitch-stream', 'https://www.kaggle.com/code/timeverett/getting-started-with-rtklib', 'https://www.kaggle.com/code/mehrankazeminia/gsdc22-coordinate-with-nearestneighbors']"
79,"In 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, oncologists are able to visualize the daily position of the tumor and intestines, which can vary day to day. In these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate—unless deep learning could help automate the segmentation process. A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment.
The UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.
In this competition, you’ll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.
In this figure, the tumor (pink thick line) is close to the stomach (red thick line). High doses of radiation are directed to the tumor while avoiding the stomach. The dose levels are represented by the rainbow of outlines, with higher doses represented by red and lower doses represented by green.
Cancer takes enough of a toll. If successful, you'll enable radiation oncologists to safely deliver higher doses of radiation to tumors while avoiding the stomach and intestines. This will make cancer patients' daily treatments faster and allow them to get more effective treatment with less side effects and better long-term cancer control.
Acknowledgments:
Sangjune Laurence Lee MSE MD FRCPC DABR
Poonam Yadav Ph.D., DABR
Yin Li PhD
Jason J. Meudt BS, RTT
Jessica Strang
Dustin Hebel
Alyx Alfson MS CMD, R.T.(T)
Stephanie J. Olson RTT (BS), CMD (MS)
Tera R. Kruser MS, RTT, CMD
Jennifer B Smilowitz, Ph.D., DABR, FAAPM
Kailee Borchert
Brianne Loritz
John Bayouth PhD
Michael Bassetti MD PhD
Work funded by the University of Wisconsin Carbone Cancer Center Pancreas Pilot Research Grant.

This is a Code Competition. Refer to Code Requirements for details.","This competition is evaluated on the mean Dice coefficient and 3D Hausdorff distance. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:
$$ \frac{2 * |X \cap Y|}{|X| + |Y|},$$
where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 0 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.
Hausdorff distance is a method for calculating the distance between segmentation objects A and B, by calculating the furthest point on object A from the nearest point on object B. For 3D Hausdorff, we construct 3D volumes by combining each 2D segmentation with slice depth as the Z coordinate and then find the Hausdorff distance between them. (In this competition, the slice depth for all scans is set to 1.) The scipy code for Hausdorff is linked. The expected / predicted pixel locations are normalized by image size to create a bounded 0-1 score.
The two metrics are combined, with a weight of 0.4 for the Dice metric and 0.6 for the Hausdorff distance.
Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.  Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
Note that, at the time of encoding, the mask should be binary, meaning the masks for all objects in an image are joined into a single large mask. A value of 0 should indicate pixels that are not masked, and a value of 1 will indicate pixels that are masked.
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
The file should contain a header and have the following format:
id,class,predicted
1,large_bowel,1 1 5 1
1,small_bowel,1 1
1,stomach,1 1
2,large_bowel,1 5 2 17
etc.","In this competition we are segmenting organs cells in images. The training annotations are provided as RLE-encoded masks, and the images are in 16-bit grayscale PNG format.
Each case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test. The goal of this competition is to be able to generalize to both partially and wholly unseen cases.
Note that, in this case, the test set is entirely unseen. It is roughly 50 cases, with a varying number of days and slices, as seen in the training set.
How does an entirely hidden test set work?
The test set in this competition is only available when your code is submitted. The sample_submission.csv provided in the public set is an empty placeholder that shows the required submission format; you should perform your modeling, cross-validation, etc., using the training set, and write code to process a non-empty sample submission. It will contain rows with id, class and predicted columns as described in the Evaluation page.
When you submit your notebook, your code will be run against the non-hidden test set, which has the same folder format (<case>/<case_day>/<scans>) as the training data.
Files",kaggle competitions download -c uw-madison-gi-tract-image-segmentation,"['https://www.kaggle.com/code/dschettler8845/uwm-gi-tract-image-segmentation-eda', 'https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch', 'https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-infer-pytorch', 'https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-train-pytorch', 'https://www.kaggle.com/code/awsaf49/uwmgi-unet-infer-pytorch']"
80,"When you look for nearby restaurants or plan an errand in an unknown area, you expect relevant, accurate information. To maintain quality data worldwide is a challenge, and one with implications beyond navigation. Businesses make decisions on new sites for market expansion, analyze the competitive landscape, and show relevant ads informed by location data. For these, and many other uses, reliable data is critical.
Large-scale datasets on commercial points-of-interest (POI) can be rich with real-world information. To maintain the highest level of accuracy, the data must be matched and de-duplicated with timely updates from multiple sources. De-duplication involves many challenges, as the raw data can contain noise, unstructured information, and incomplete or inaccurate attributes. A combination of machine-learning algorithms and rigorous human validation methods are optimal to de-dupe datasets.
With 12+ years of experience perfecting such methods, Foursquare is the #1 independent provider of global POI data. The leading independent location technology and data cloud platform, Foursquare is dedicated to building meaningful bridges between digital spaces and physical places. Trusted by leading enterprises like Apple, Microsoft, Samsung, and Uber, Foursquare’s tech stack harnesses the power of places and movement to improve customer experiences and drive better business outcomes.
In this competition, you’ll match POIs together. Using a dataset of over one-and-a-half million Places entries heavily altered to include noise, duplications, extraneous, or incorrect information, you'll produce an algorithm that predicts which Place entries represent the same point-of-interest. Each Place entry includes attributes like the name, street address, and coordinates. Successful submissions will identify matches with the greatest accuracy.
By efficiently and successfully matching POIs, you'll make it easier to identify where new stores or businesses would benefit people the most.
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated by the mean Intersection over Union (IoU, aka the Jaccard index) of the ground-truth entry matches and the predicted entry matches. The mean is taken sample-wise, meaning that an IoU score is calculated for each row in the submission file, and the final score is their average.
Submission File
For each place entry id in the test set, you should submit a space-delimited list of matching place ids. Places always self-match, so the list of matches for an id should always contain that id.
The file should contain a header, be named submission.csv, and have the following format:
id,matches
E_00001118ad0191,E_00001118ad0191
E_000020eb6fed40,E_000020eb6fed40
E_00002f98667edf,E_00002f98667edf
E_001b6bad66eb98,E_001b6bad66eb98 E_0283d9f61e569d
E_0283d9f61e569d,E_0283d9f61e569d E_001b6bad66eb98
You should predict matches for every id. For example, if you believe A matches B and C, your submission file should include rows A,A B C, but also B,B A C and C,C A B.","The data presented here comprises over one-and-a-half million place entries for hundreds of thousands of commercial Points-of-Interest (POIs) around the globe. Your task is to determine which place entries describe the same point-of-interest. Though the data entries may represent or resemble entries for real places, they may also contain artificial information or additional noise.
Training Data
train.csv - The training set, comprising eleven attribute fields for over one million place entries, together with:
id - A unique identifier for each entry.
point_of_interest - An identifier for the POI the entry represents. There may be one or many entries describing the same POI. Two entries ""match"" when they describe a common POI.
pairs.csv - A pregenerated set of pairs of place entries from train.csv designed to improve detection of matches. You may wish to generate additional pairs to improve your model's ability to discriminate POIs.
match - Whether (True or False) the pair of entries describes a common POI.
Example Test Data
To help you author submission code, we include a few example instances selected from the test set. When you submit your notebook for scoring, this example data will be replaced by the actual test data. The actual test set has approximately 600,000 place entries with POIs that are distinct from the POIs in the training set.",kaggle competitions download -c foursquare-location-matching,"['https://www.kaggle.com/code/ryotayoshinobu/foursquare-lightgbm-baseline', 'https://www.kaggle.com/code/ragnar123/flm-xlmroberta-inference-baseline', 'https://www.kaggle.com/code/guoyonfan/binary-lgb-baseline-0-834', 'https://www.kaggle.com/code/edwintyh/foursquare-starter-eda', 'https://www.kaggle.com/code/nlztrk/public-0-861-pykakasi-radian-coordinates']"
81,"Success in any financial market requires one to identify solid investments. When a stock or derivative is undervalued, it makes sense to buy. If it's overvalued, perhaps it's time to sell. While these finance decisions were historically made manually by professionals, technology has ushered in new opportunities for retail investors. Data scientists, specifically, may be interested to explore quantitative trading, where decisions are executed programmatically based on predictions from trained models.
There are plenty of existing quantitative trading efforts used to analyze financial markets and formulate investment strategies. To create and execute such a strategy requires both historical and real-time data, which is difficult to obtain especially for retail investors. This competition will provide financial data for the Japanese market, allowing retail investors to analyze the market to the fullest extent.
Japan Exchange Group, Inc. (JPX) is a holding company operating one of the largest stock exchanges in the world, Tokyo Stock Exchange (TSE), and derivatives exchanges Osaka Exchange (OSE) and Tokyo Commodity Exchange (TOCOM). JPX is hosting this competition and is supported by AI technology company AlpacaJapan Co.,Ltd.
This competition will compare your models against real future returns after the training phase is complete. The competition will involve building portfolios from the stocks eligible for predictions (around 2,000 stocks). Specifically, each participant ranks the stocks from highest to lowest expected returns and is evaluated on the difference in returns between the top and bottom 200 stocks. You'll have access to financial data from the Japanese market, such as stock information and historical stock prices to train and test your model.
All winning models will be made public so that other participants can learn from the outstanding models. Excellent models also may increase the interest in the market among retail investors, including those who want to practice quantitative trading. At the same time, you'll gain your own insights into programmatic investment methods and portfolio analysis―and you may even discover you have an affinity for the Japanese market.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on the Sharpe Ratio of the daily spread returns. You will need to rank each stock active on a given day. The returns for a single day treat the 200 highest (e.g. 0 to 199) ranked stocks as purchased and the lowest (e.g. 1999 to 1800) ranked 200 stocks as shorted. The stocks are then weighted based on their ranks and the total returns for the portfolio are calculated assuming the stocks were purchased the next day and sold the day after that. You can find a python implementation of the metric here.
You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks:
import jpx_tokyo_market_prediction
env = jpx_tokyo_market_prediction.make_env()   # initialize the environment
iter_test = env.iter_test()    # an iterator which loops over the test files
for (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:
    sample_prediction_df['Rank'] = np.arange(len(sample_prediction))  # make your predictions here
    env.predict(sample_prediction_df)   # register your predictions
You will get an error if you:
Use ranks that are below zero or greater than or equal to the number of stocks for a given date.
Submit any duplicated ranks.
Change the order of the rows.","This dataset contains historic data for a variety of Japanese stocks and options. Your challenge is to predict the future returns of the stocks.
As historic stock prices are not confidential this will be a forecasting competition using the time series API. The data for the public leaderboard period is included as part of the competition dataset. Expect to see many people submitting perfect submissions for fun. Accordingly, the active phase public leaderboard for this competition is intended as a convenience for anyone who wants to test their code. The forecasting phase leaderboard will be determined using real market data gathered after the submission period closes.
Files
stock_prices.csv The core file of interest. Includes the daily closing price for each stock and the target column.
options.csv Data on the status of a variety of options based on the broader market. Many options include implicit predictions of the future price of the stock market and so may be of interest even though the options are not scored directly.
secondary_stock_prices.csv The core dataset contains on the 2,000 most commonly traded equities but many less liquid securities are also traded on the Tokyo market. This file contains data for those securities, which aren't scored but may be of interest for assessing the market as a whole.
trades.csv Aggregated summary of trading volumes from the previous business week.",kaggle competitions download -c jpx-tokyo-stock-exchange-prediction,"['https://www.kaggle.com/code/chumajin/easy-to-understand-the-competition', 'https://www.kaggle.com/code/kellibelcher/jpx-stock-market-analysis-prediction-with-lgbm', 'https://www.kaggle.com/code/abaojiang/jpx-detailed-eda', 'https://www.kaggle.com/code/smeitoma/jpx-competition-metric-definition', 'https://www.kaggle.com/code/paulorzp/jpx-simple-overfitting-model-lb-3']"
82,"In this turn-based simulation game you control a small armada of spaceships. As you mine the rare mineral “kore” from the depths of space, you teleport it back to your homeworld. But it turns out you aren’t the only civilization with this goal. In each game two players will compete to collect the most kore from the board. Whoever has the largest kore cache by the end of 400 turns—or eliminates all of their opponents from the board before that—will be the winner!
Your algorithms determine the movements of your fleets to collect kore, but it's up to you to figure out how to make effective and efficient moves. You control your ships, build new ships, create shipyards, eliminate opponents, and mine the kore on the game board.
May your fleet live long and prosper!","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play Episodes (games) against other agents on the ladder that have a similar skill rating. Over time skill ratings will go up with wins, down with losses, or evened out with ties.
Every agent submitted will continue to play episodes until the end of the competition, with newer agents playing a much more frequent number of episodes. On the leaderboard only your best scoring agents will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents the uncertainty of that estimate which will decrease over time.
When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error and you can download the agent logs to help figure out why. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. Newly submitted agents will be given an increased rate in the number of episodes run to give you faster feedback.
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your agent wins or loses an Episode does not affect the skill rating updates.
Ranking System
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your agent wins or loses an Episode does not affect the skill rating updates.
Final Evaluation
At the submission deadline on July 12th, additional submissions will be locked. From July 13th to July 27th, we will continue to run games. At the conclusion of this period, the leaderboard is final and is used to determine who gets various ranking based prizes.",,,"['https://www.kaggle.com/code/bovard/kore-intro-i-the-basics', 'https://www.kaggle.com/code/lesamu/reinforcement-learning-baseline-in-python', 'https://www.kaggle.com/code/bovard/kore-intro-ii-mining-kore', 'https://www.kaggle.com/code/huikang/kore-2022-match-analysis', 'https://www.kaggle.com/code/edwardcrookenden/kore-starter-6th-in-beta-rule-based-agent']"
83,"2023 Update: you may want to check Image Matching Challenge 2023


For most of us, our best camera is part of the phone in our pocket. We may take a snap of a landmark, like the Trevi Fountain in Rome, and share it with friends. By itself, that photo is two-dimensional and only includes the perspective of our shooting location. Of course, a lot of people have taken photos of that fountain. Together, we may be able to create a more complete, three-dimensional view. What if machine learning could help better capture the richness of the world using the vast amounts of unstructured image collections freely available on the internet?
The process to reconstruct 3D objects and buildings from images is called Structure-from-Motion (SfM). Typically, these images are captured by skilled operators under controlled conditions, ensuring homogeneous, high-quality data. It is much more difficult to build 3D models from assorted images, given a wide variety of viewpoints, lighting and weather conditions, occlusions from people and vehicles, and even user-applied filters.
The first part of the problem is to identify which parts of two images capture the same physical points of a scene, such as the corners of a window. This is typically achieved with local features (key locations in an image that can be reliably identified across different views). Local features contain short description vectors that capture the appearance around the point of interest. By comparing these descriptors, likely correspondences can be established between the pixel coordinates of image locations across two or more images. This “image registration” makes it possible to recover the 3D location of the point by triangulation.
Google employs Structure-from-Motion techniques across many Google Maps services, such as the 3D models created from StreetView and aerial imagery. In order to accelerate research into this topic, and better leverage the volume of data already publicly available, Google presents this competition in collaboration with the University of British Columbia and Czech Technical University.
In this code competition, you’ll create a machine learning algorithm that registers two images from different viewpoints. With access to a dataset of thousands of images to train and test your model, top-scoring notebooks will do so with the most accuracy.
If successful, you'll help solve this well-known problem in computer vision, making it possible to map the world with unstructured image collections. Your solutions will have applications in photography and cultural heritage preservation, along with Google Maps. Winners will also be invited to give a presentation as part of the Image Matching: Local Features and Beyond workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) in June.
Resources
Image Matching Challenge 2021: Last year's competition (outside Kaggle).
Organization
Eduard Trulls (Google), Yuhe Jin & Kwang Moo Yi (University of British Columbia, Vancouver, Canada), Dmytro Mishkin & Jiri Matas (Czech Technical University, Prague, Czech Republic)
Acknowledgments
The organizers would like to thank the Machine Learning Lab at the Faculty of Applied Sciences, Ukrainian Catholic University (Lviv, Ukraine) for their help with dataset creation.

Banner photo by Taneli Lahtinen on Unsplash. Trevi Fountain photos, left to right, then top to bottom: sarah|rose, kmaschke, jamingray, deglispiriti, Lucas Uyezu, justinknabb, Bogdan Migulski, S outH CheN, Melirius, 2bethere, Steve AM, L'amande.
This is a Code Competition. Refer to Code Requirements for details.","Evaluation metric
Participants are asked to estimate the relative pose of one image with respect to another. Submissions are evaluated on the mean Average Accuracy (mAA) of the estimated poses. Given a fundamental matrix and the hidden ground truth, we compute the error in terms of rotation ( \( \epsilon_R \), in degrees) and translation (\( \epsilon_T \), in meters). Given one threshold over each, we classify a pose as accurate if it meets both thresholds. We do this over ten pairs of thresholds, one pair at a time (e.g. at 1\(^o\) and 20 cm at the finest level, and 10\(^o\) and 5 m at the coarsest level):
thresholds_r = np.linspace(1, 10, 10)  # In degrees.
thresholds_t = np.geomspace(0.2, 5, 10)  # In meters.
We then calculate the percentage of image pairs that meet every pair of thresholds, and average the results over all thresholds, which rewards more accurate poses. As the dataset contains multiple scenes, which may have a different number of pairs, we compute this metric separately for each scene and average it afterwards. A python implementation of this metric is available on this notebook.
Submission File
For each ID in the test set, you must predict the fundamental matrix between the two views. The file should contain a header and have the following format:
sample_id,fundamental_matrix
a;b;c-d,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
a;b;e-f,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
a;b;g-h,0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
etc
Note that fundamental_matrix is a \( 3 \times 3 \) matrix, flattened into a vector in row-major order.","Aligning photographs of the same scene is a problem of longstanding interest to computer vision researchers. Your challenge in this competition is to generate mappings between pairs of photos from various cities.
This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook.
Files
train/*/calibration.csv
image_id: The image filename.
camera_intrinsics: The \( 3 \times 3 \) calibration matrix \( \mathbf{K} \) for this image, flattened into a vector by row-major indexing.
rotation_matrix: The \( 3 \times 3 \) rotation matrix \( \mathbf{R} \) for this image, flattened into a vector by row-major indexing.
translation_vector: The translation vector \( \mathbf{T} \).
train/*/pair_covisibility.csv
pair: A string identifying a pair of images, encoded as two image filenames (without the extension) separated by a hyphen, as , where > .",kaggle competitions download -c image-matching-challenge-2022,"['https://www.kaggle.com/code/cbeaud/imc-2022-kornia-score-0-725', 'https://www.kaggle.com/code/remekkinas/detector-free-local-feature-matching-w-transformer', 'https://www.kaggle.com/code/dschettler8845/image-matching-challenge-2022-eda', 'https://www.kaggle.com/code/ammarali32/imc-2022-kornia-loftr-from-0-533-to-0-721', 'https://www.kaggle.com/code/mcwema/imc-2022-kornia-loftr-score-plateau-0-726']"
84,"Welcome to the April edition of the 2022 Tabular Playground Series! This month's challenge is a time series classification problem.
You've been provided with thousands of sixty-second sequences of biological sensor data recorded from several hundred participants who could have been in either of two possible activity states. Can you determine what state a participant was in from the sensor data?
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each sequence in the test set, you must predict a probability for the state variable. The file should contain a header and have the following format:
sequence,state
25968,0
25969,0
25970,0
...","In this competition, you'll classify 60-second sequences of sensor data, indicating whether a subject was in either of two activity states for the duration of the sequence.
Files and Field Descriptions
train.csv - the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants
sequence - a unique id for each sequence
subject - a unique id for the subject in the experiment
step - time step of the recording, in one second intervals
sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step
train_labels.csv - the class label for each sequence.
sequence - the unique id for each sequence.
state - the state associated to each sequence. This is the target which you are trying to predict.
test.csv - the test set. For each of the ~12,000 sequences, you should predict a value for that sequence's state.
sample_submission.csv - a sample submission file in the correct format.",kaggle competitions download -c tabular-playground-series-apr-2022,"['https://www.kaggle.com/code/vigneshr2036/vr2036-pca-gmm', 'https://www.kaggle.com/code/yashgyy/stacking-classifier-and-eda', 'https://www.kaggle.com/code/rdurango92/tps-apr-2022-sensors-ml', 'https://www.kaggle.com/code/sarthak872/eda-feature-engineering', 'https://www.kaggle.com/code/akshatpattiwar/eda-feature-engineering-tabular-playground']"
85,"Can you extract meaning from a large, text-based dataset derived from inventions? Here's your chance to do so.
The U.S. Patent and Trademark Office (USPTO) offers one of the largest repositories of scientific, technical, and commercial information in the world through its Open Data Portal. Patents are a form of intellectual property granted in exchange for the public disclosure of new and useful inventions. Because patents undergo an intensive vetting process prior to grant, and because the history of U.S. innovation spans over two centuries and 11 million patents, the U.S. patent archives stand as a rare combination of data volume, quality, and diversity.
“The USPTO serves an American innovation machine that never sleeps by granting patents, registering trademarks, and promoting intellectual property around the globe. The USPTO shares over 200 years' worth of human ingenuity with the world, from lightbulbs to quantum computers. Combined with creativity from the data science community, USPTO datasets carry unbounded potential to empower AI and ML models that will benefit the progress of science and society at large.”
— USPTO Chief Information Officer Jamie Holcombe
In this competition, you will train your models on a novel semantic similarity dataset to extract relevant information by matching key phrases in patent documents. Determining the semantic similarity between phrases is critically important during the patent search and examination process to determine if an invention has been described before. For example, if one invention claims ""television set"" and a prior publication describes ""TV set"", a model would ideally recognize these are the same and assist a patent attorney or examiner in retrieving relevant documents. This extends beyond paraphrase identification; if one invention claims a ""strong material"" and another uses ""steel"", that may also be a match. What counts as a ""strong material"" varies per domain (it may be steel in one domain and ripstop fabric in another, but you wouldn't want your parachute made of steel). We have included the Cooperative Patent Classification as the technical domain context as an additional feature to help you disambiguate these situations.
Can you build a model to match phrases in order to extract contextual information, thereby helping the patent community connect the dots between millions of patent documents?

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on the Pearson correlation coefficient between the predicted and actual similarity scores.
Submission File
For each id (representing a pair of phrases) in the test set, you must predict the similarity score. The file should contain a header and have the following format:
id,score
4112d61851461f60,0
09e418c93a776564,0.25
36baf228038e314b,1
etc.","In this dataset, you are presented pairs of phrases (an anchor and a target phrase) and asked to rate how similar they are on a scale from 0 (not at all similar) to 1 (identical in meaning). This challenge differs from a standard semantic similarity task in that similarity has been scored here within a patent's context, specifically its CPC classification (version 2021.05), which indicates the subject to which the patent relates. For example, while the phrases ""bird"" and ""Cape Cod"" may have low semantic similarity in normal language, the likeness of their meaning is much closer if considered in the context of ""house"".
This is a code competition, in which you will submit code that will be run against an unseen test set. The unseen test set contains approximately 12k pairs of phrases. A small public test set has been provided for testing purposes, but is not used in scoring.
Information on the meaning of CPC codes may be found on the USPTO website. The CPC version 2021.05 can be found on the CPC archive website.
Score meanings
The scores are in the 0-1 range with increments of 0.25 with the following meanings:
1.0 - Very close match. This is typically an exact match except possibly for differences in conjugation, quantity (e.g. singular vs. plural), and addition or removal of stopwords (e.g. “the”, “and”, “or”).
0.75 - Close synonym, e.g. “mobile phone” vs. “cellphone”. This also includes abbreviations, e.g. ""TCP"" -> ""transmission control protocol"".",kaggle competitions download -c us-patent-phrase-to-phrase-matching,"['https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners', 'https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster', 'https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-w-w-b-train', 'https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-inference', 'https://www.kaggle.com/code/remekkinas/eda-and-feature-engineering']"
86,"Description
Camera Traps enable the automatic collection of large quantities of image data. Ecologists all over the world use camera traps to monitor biodiversity and population density of animal species. In order to estimate the abundance (how many there are) and population density of species in camera trap data, ecologists need to know not just which species were seen, but also how many of each species were seen. However, because images are taken in motion-triggered bursts to increase the likelihood of capturing the animal(s) of interest, object detection alone is not sufficient as it could lead to over- or under-counting. For example, if you get 3 images taken at one frame per second, and in the first you see 3 gazelles, in the second you see 5 gazelles, and in the last you see 4 gazelles, how many total gazelles have you seen? This is more challenging than strictly detecting and categorizing species, as it requires reasoning and tracking of individuals across sparse temporal samples.
This year our iWildCam competition will focus entirely on counting animals. We have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to count individual animals across sequences in the test cameras. To explore multimodal solutions, we allow competitors to train on the following data:
Our camera trap training set — data provided by the Wildlife Conservation Society (WCS).
iNaturalist 2017-2021 data.
Multispectral imagery from Landsat-8 for each of the camera trap locations.
Check the Data section for a more comprehensive description of all these resources and for accessing the train set, test set and metadata. These are mirrored on the competition's GitHub page as well, where we also provide the multispectral data, a taxonomy file mapping our classes into the iNaturalist taxonomy, a subset of the iNaturalist data mapped into our class set, a camera trap detection model (the MegaDetector) along with the corresponding detections, and a class-agnostic instance segmentation model (DeepMAC) along with the segmentation masks for the MegaDetector's bounding boxes.
Acknowledgements
This competition is part of the FGVC9 workshop at CVPR 2022 and is sponsored by Wildlife Insights. Data is primarily provided by the Wildlife Conservation Society (WCS) and iNaturalist, and is hosted on Azure by Microsoft AI for Earth. Count annotations were generously provided by Centaur Labs.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","Evaluation
Submissions will be evaluated using Mean Absolute Error (MAE),
where each x_i represents the predicted count of animals in sequence i, y_i represents the ground truth count for that sequence, and n is the number of sequences in the test set.
We selected this simple metric for this year because it's easy to interpret and because count errors on large groups of animals (which will inevitably happen!) are not as hardly penalized as in the case of Root Mean Square Error (RMSE).
Submission Format
Solutions should be submited as a CSV file with the following format:
Id,Predicted
58857ccf-23d2-11e8-a6a3-ec086b02610b,0
591e4006-23d2-11e8-a6a3-ec086b02610b,1
...
The Id column corresponds to the test sequence id, while Predicted holds an integer value that indicates the number of individual animals predicted for that test sequence.","Data Overview
The iWildCam 2022 WCS training set contains 201,399 images from 323 locations, and the WCS test set contains 60,029 images from 91 locations. These 414 locations are spread across the globe. A location ID (location) is given for each image, and in some special cases where two cameras were set up by ecologists at the same location, we have provided a sub_location identifier. Camera traps operate with a motion trigger and, after motion is detected, the camera will take a sequence of photos (from 1 to 10 images depending on the camera). We provide a seq_id for each sequence, and your task is to count the number of individuals across each test sequence.
This year we are also providing count annotations on 1780 of the 36,292 train sequences (check the metadata/train_sequence_counts.csv file). We hope you will find them useful in building better models. We do not provide any count annotations for the test set.
We provide GPS locations for the majority of the camera traps, obfuscated within 1km for security and privacy reasons. Some of the obfuscated GPS locations (all from one country) were not released at the request of WCS, but knowing that the locations not listed in the metadata/gps_locations.json file are all from the same country should help competitors narrow down the set of possible species for those locations based on what is seen in the training data.
You may also choose to use supplemental training data from the iNaturalist 2017, iNaturalist 2018, iNaturalist 2019, and iNaturalist 2021 competition datasets. As a courtesy, we have curated all the images from iNaturalist 2017-2018 datasets containing classes that might be in the test set, and mapped them into the iWildCam categories.
We provide Landsat-8 multispectral imagery for each camera location as supplementary data. In particular, each site is associated with a series of patches collected between 2013 and 2019. The patches are extracted from a ""Tier 1"" Landsat product, which consists only of data that meets certain geometric and radiometric quality standards. Consequently, the number of patches per site varies from 39 to 406 (median: 147). Each patch is 200x200x9 pixels, covering an area of 6km^2 at a resolution of 30 meters / pixel across 9 spectral bands. Note that all patches for a given site are registered, but are not centered exactly at the camera location to protect the integrity of the site.",kaggle competitions download -c iwildcam2022-fgvc9,"['https://www.kaggle.com/code/mpwolke/iwildcam-2022-cnn', 'https://www.kaggle.com/code/kalilurrahman/iwildcam-2022-analysis-notebook', 'https://www.kaggle.com/code/stefanistrate/iwildcam-2022-visualize-deepmac-instance-masks', 'https://www.kaggle.com/code/stpeteishii/iwildcam2022-show-and-extract', 'https://www.kaggle.com/code/vickyskarthik/gps-data-visualization']"
87,"Please note - this is a preview/beta launch of an upcoming competition. This competition is intended to help with rule balancing and establishing a fair and fun competition, soon to be launched. As such, this competition does not have cash prizes, points, or medals - but we hope to gain your feedback for when the featured competition goes live!
When you want to mine kore quickly: go alone. But when you want to mine the most kore: build a fleet.
In this turn-based simulation game you control a small armada of spaceships. As you mine the rare mineral “kore” from the depths of space, you teleport it back to your homeworld. But it turns out you aren’t the only civilization with this goal. In each game four players will compete to collect the most kore from the board. Whoever has the largest kore cache by the end of 400 turns—or eliminates all of their opponents from the board before that—will be the winner!
Your algorithms determine the movements of your fleets to collect kore, but it's up to you to figure out how to make effective and efficient moves. You control your ships, build new ships, create shipyards, eliminate opponents, and mine the kore on the game board.
May your fleet live long and prosper!","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play Episodes (games) against other bots on the ladder that have a similar skill rating. Over time skill ratings will go up with wins or down with losses and evened out with ties.
Every bot submitted will continue to play episodes until the end of the competition, with newer bots playing a much more frequent number of episodes. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents the uncertainty of that estimate which will decrease over time.
When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error and you can download the agent logs to help figure out why. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. Newly submitted agents will be given an increased rate in the number of episodes run to give you faster feedback.
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
Ranking System
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
Final Evaluation
At the submission deadline on December 6th, additional submissions will be locked. From December 7th to December 20th, we will continue to run games. At the conclusion of this period, the leaderboard is final and is used to determine who gets various ranking based prizes.",,,"['https://www.kaggle.com/code/bovard/basic-python-starter-notebook', 'https://www.kaggle.com/code/bovard/kore-fleets-intro-part-1-the-basics', 'https://www.kaggle.com/code/masaishi/kore2022-q-learning', 'https://www.kaggle.com/code/masaishi/plot-of-mining-rate-and-distance', 'https://www.kaggle.com/code/bovard/kore-intro-part-4-combat']"
88,"Hotel Recognition to Combat Human Trafficking
Victims of human trafficking are often photographed in hotel rooms as in the below examples. Identifying these hotels is vital to these trafficking investigations but poses particular challenges due to low quality of images and uncommon camera angles.
Even without victims in the images, hotel identification in general is a challenging fine-grained visual recognition task with a huge number of classes and potentially high intraclass and low interclass variation. In order to support research into this challenging task and create image search tools for human trafficking investigators, we created the TraffickCam mobile application, which allows every day travelers to submit photos of their hotel room. Read more about TraffickCam on TechCrunch.
Example images from one hotel in the TraffickCam dataset are shown below:
In this contest, competitors are tasked with identifying the hotel seen in test images from the TraffickCam dataset, which are based on a large gallery of training images with known hotel IDs.
Our team currently supports an image search system used at the National Center for Missing and Exploited Children in human trafficking investigations. Novel and interesting approaches have the potential to be incorporated in this search system.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):
$$MAP@5 = \frac{1}{U} \sum_{u=1}^{U} \sum_{k=1}^{min(n,5)} P(k) \times rel(k)$$
where \( U \) is the number of images, \( P(k) \) is the precision at cutoff \( k \), \( n \) is the number of predictions per image, and \( rel(k) \) is an indicator function equaling 1 if the item at rank \( k \) is a relevant correct label, zero otherwise.
Once a correct label has been scored for an observation, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0.
A B C D E
A A A A A
A B A C A
Submission File
For each image in the test set, you must predict a space-delimited list of hotel IDs that could match that image. The list should be sorted such that the first ID is considered the most relevant one and the last the least relevant one. The file should contain a header and have the following format:
image,hotel_id 
99e91ad5f2870678.jpg,36363 53586 18807 64314 60181
b5cc62ab665591a9.jpg,36363 53586 18807 64314 60181
d5664a972d5a644b.jpg,36363 53586 18807 64314 60181","Identifying the location of a hotel room is a challenging problem of great interest for combating human trafficking. This competition provides a rich dataset of photos of hotel room interiors for this purpose. Unlike last year's competition the test set images in this competition are all partially masked to emulate the added challenge of having a person in the blocking part of the view of the room. Models that can overcome this extra layer of difficulty should be even more useful in practice.
Files
sample_submission.csv - A sample submission file in the correct format.
image_id The image ID
hotel_id The hotel ID. The target class.
train_images - The training set contains many images from hotels from across the globe. All of the images for each hotel are in a dedicated subfolder for that chain. These images do not have any occlusions by default.
train_masks - Occlusions like the ones that will be present in the images in the test set.
test_images - The test set images. This competition has a hidden test set: only one image is provided here as a sample while the remaining 5,000 images will be available to your notebook once it is submitted.",kaggle competitions download -c hotel-id-to-combat-human-trafficking-2022-fgvc9,"['https://www.kaggle.com/code/mpwolke/images-operations-roi-bitwise-masking', 'https://www.kaggle.com/code/mpwolke/hotel-recognition-gan', 'https://www.kaggle.com/code/michaln/hotel-id-starter-classification-inference', 'https://www.kaggle.com/code/mpwolke/it-s-u-u-net', 'https://www.kaggle.com/code/mpwolke/hotel-identification-transfer-learning-attempt']"
89,"Description Task
The aim of this competition is to predict the localization of plant and animal species.
To do so, 1.6M geo-localized observations from France and the US of 17K species are provided (9K plant species and 8K animal species).
These observations are paired with aerial images and environmental features around them (as illustrated above).
The goal is, for each GPS position in the test set (for which we provide the associated aerial images and environmental features), to return a set of candidate species that should contain the true observed species.
Motivation
Automatic prediction of the list of species most likely to be observed at a given location is useful for many scenarios related to biodiversity management and conservation.
First, this would allow to improve species identification tools - automatic, semi-automatic, or based on traditional field guides - by reducing the list of candidate species observable at a given site.
More generally, it could facilitate biodiversity inventories through the development of location-based recommendation services (e.g. on mobile phones), encourage the involvement of citizen scientist observers, and accelerate the annotation and validation of species observations to produce large, high-quality data sets.
Finally, this could be used for educational purposes through biodiversity discovery applications with features such as contextualized educational pathways.
Context
This competition is held jointly as part of:
the LifeCLEF 2022 lab of the CLEF 2022 conference, and of
the FGVC9 workshop organized in conjunction with CVPR 2022 conference.
Being part of scientific research, the participants are encouraged to participate to both event.
In particular, only participants who submitted a working note paper to LifeCLEF (see below) will be part of the officially published ranking used for scientific communication.
FGVC9 at CVPR 2022
This competition is part of the Fine-Grained Visual Categorization FGVC9 workshop at the Computer Vision and Pattern Recognition Conference CVPR 2022.
A panel will review the top submissions for the competition based on the description of the methods provided.
From this, a subset may be invited to present their results at the workshop.
Attending the workshop is not required to participate in the competition; however, only teams that are attending the workshop will be considered to present their work.
CVPR 2022 will take place in New Orleans, USA, 19-24 June 2022.
PLEASE NOTE: CVPR frequently sells out early, we cannot guarantee CVPR registration after the competition's end.
If you are interested in attending, please plan ahead.
You can see a list of all of the FGVC9 competitions here.
LifeCLEF 2022
LifeCLEF lab is part of the Conference and Labs of the Evaluation Forum (CLEF).
CLEF consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems.
CLEF 2022 will be hosted by the Università di Bologna, Italy, 5-8 September 2022.
More details can be found on the CLEF 2022 website.
To participate to the LifeCLEF lab, participants must register using this form (and checking ""Task 3 - GeoLifeCLEF"" of ""LifeCLEF"" section).
This registration is free of charge and will close on 22 April 2022, however free of charge late registration will still be possible at the end of the competition.
This will allow those participants to submit, at the end of the competition, a working note paper to LifeCLEF which will be peer-reviewed and published in CEUR-WS proceedings.
This paper should provide sufficient information to reproduce the final submitted runs.
Submitting a working note with the full description of the methods used in each run is mandatory.
Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results.
Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP.
According to the CEUR-WS policies, a light review of the working notes will be conducted by LifeCLEF organizing committee to ensure quality.
As an illustration, LifeCLEF 2021 working notes (task overviews and participant working notes) can be found within CLEF 2021 CEUR-WS proceedings.
Credits
This project has received funding from the French National Research Agency under the Investments for the Future Program, referred to as ANR-16-CONV-0004, and from the European Union’s Horizon 2020 research and innovation program under grant agreement No 863463 (Cos4Cloud project).","Evaluation Metric
The evaluation metric for this competition is the top-30 error rate.
Each observation \( i \) is associated with a single ground-truth label \( y_i \) corresponding to the observed species.
For each observation, the submissions will provide 30 candidate labels \( \hat{y}_{i,1}, \hat{y}_{i,2}, \dots, {\hat{y}}_{i,30} \).
The top-30 error rate is then computed using
\[ \text{Top-30 error rate} = \frac{1}{N} \sum_{i=1}^N e_i \quad \text{where} \quad e_i = \begin{cases} 1 & \text{if } \, \forall k \in \{1,\dots,30\}, \, \hat{y}_{i,k} \neq y_i \\ 0 & \text{otherwise} \end{cases} \]
Train/test Split
In order to limit the spatial bias during evaluation, the observations were split into training and test sets using a spatial block holdout procedure.
This procedure is illustrated in the previous figure: the test data - in red - is drawn from different spatial blocks than the training data - in blue.
In more detail, all the observations are assigned into a grid of 5km×5km quadrats.
2.5% of these quadrats are randomly sampled for the test set while the remaining ones are used for the training set.
A validation set built using the same splitting procedure is provided (column subset of observations_*_train.csv).
The code for this splitting procedure is also available on the GitHub repository.
Leaderboard Baselines
We provide 4 baselines in the leaderboard:
top-30 most present species: baseline always predicting the most present species;
random forest on environmental vectors: a classical method used in ecology using only the environmental vectors,
scikit-learn random forest of 100 trees with max depth 16 and minimum samples split of 30;
CNN on RGB patches: CNN trained on the RGB patches without any environmental vectors,
standard ResNet-50 pretrained on ImageNet and finetuned using a learning rate of 0.01, a batch size of 32 and an early stopping strategy on the validation top-30 error rate;
CNN on Red, Green and Near-IR patches: same as above but the blue channel was replaced by near-infrared highlighting the importance of this wavelength for the task.
We also provide a notebook explaining how to compute some simpler baselines.
Submission Format
The submission format is a CSV file containing two columns for each observation:
Id column containing integers corresponding to the observations ids
Predicted column containing space-delimited lists of the 30 predicted labels made for each observation
The file should contain a header and have the following format:
Id, Predicted
1, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
2, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
...
See also the following notebook showing how submission files can be generated.","This year's challenge, GeoLifeCLEF 2022, uses a cleaned-up version of the data used for last year's challenge, GeoLifeCLEF 2021.
A changelog is available at the end of this page.
The challenge relies on a collection of 1.6 million of geo-localized observations of plants and animals in the US and France, coming from the iNaturalist and Pl@ntNet citizen science platforms.
There are 17K species in the dataset, 9K plant species and 8K animal species.
Each observation consists of a species name with the GPS coordinates where it was observed.
In addition, observations are paired with a set of covariates characterizing the landscape and environment around them.
Covariates include high-resolution remote sensing imagery, land cover data, and altitude, as well as traditional low-resolution climate and soil variables.
A walkthrough notebook describing the data structure and how to load it and visualize it is available in the Code tab.
A complete description of how the original GeoLifeCLEF dataset was built can be found in the associated paper.
Data loading and visualization utilities are provided on the GitHub of the competition.",kaggle competitions download -c geolifeclef-2022-lifeclef-2022-fgvc9,"['https://www.kaggle.com/code/tlorieul/geolifeclef2022-data-loading-and-visualization', 'https://www.kaggle.com/code/tlorieul/geolifeclef2022-baselines-and-submission', 'https://www.kaggle.com/code/mpwolke/geolifeclef-rasters', 'https://www.kaggle.com/code/mpwolke/geolifeclef-2022-species-observations', 'https://www.kaggle.com/code/lucasmorin/geolifeclef2022-eda']"
90,"Overview
The Sorghum-100 dataset is a curated subset of the RGB imagery captured during the TERRA-REF experiments, labeled by cultivar. This data could be used to develop and assess a variety of plant phenotyping models which seek to answer questions relating to the presence or absence of desirable traits (e.g., ""does this plant exhibit signs of water stress?''). In this contest, we focus on the question: ""What cultivar is shown in this image?''
Predicting the cultivar in an image is an especially good challenge problem for familiarizing the machine learning community with the TERRA-REF data. At first blush, the task of predicting the cultivar from an image of a plant may not seem to be the most biologically compelling question to answer -- in the context of plant breeding, the cultivar, or parental lines are typically known. A high accuracy machine learning predictor of the species captured by the sensor data, however, can be used to determine where errors in the planting process may have occurred. For example, seed may be mislabeled prior to planting, or planters may get jammed, depositing seeds non-uniformly in a field. Both types of errors are surprisingly common and can cause major problems when processing data from large-scale field experiments with hundreds of cultivars and complex field planting layouts.
Data Description
The Sorghum-100 dataset consists of 48,106 images and 100 different sorghum cultivars grown in June of 2017 (the images come from the middle of the growing season when the plants were quite large but not yet lodging -- or falling over).
Each image is taken using an RGB spectral camera taken from a vertical view of the sorghum plants in the TERRA-REF field in Arizona.","The evaluation metric for this contest is simple mean classification accuracy -- how often do contestants predict that an image came from the correct cultivar?
Submission Format
Contestants should upload a csv that includes the headers ""filename"" and ""cultivar"", and should include one row per image in the test set:
filename cultivar
000VZKNBNWEROVY7AFUX8IH8.png PI_153877
00367TN3ZQFJQXCDDJULMF5P.png PI_153877
00AFRVXB8FPIQRF29DSO5W9G.png PI_153877
… …
ZZZYE3LDORWJLF100IS3QKP3.png PI_153877
This file should have 25,472 rows (one row per file in the test set, plus the header row).","The Sorghum-100 dataset consists of 48,106 images and 100 different sorghum cultivars grown in June of 2017 (the images come from the middle of the growing season when the plants were quite large but not yet lodging -- or falling over). In the above image, we show a sample of images from four different cultivars. Each row includes six images from different dates in June. This figure highlights the high inter-class visual similarity between the different classes, as well as the high variability in the imaging conditions from one day to the next, or even over the course of a day.
The dataset is divided into a training dataset and a testing dataset. Each cultivar was grown in two separate plots in the TERRA-REF field to account for extremely local field or soil conditions that might impact the growth of plants in one particular plot. We leverage this natural split in the data when dividing our dataset between train and test -- images for a given cultivar in the training dataset come from one plot, while the test images from that same cultivar come from the other plot. This means that a model cannot achieve high performance by memorizing features that aren't meaningful phenotypes (e.g., by memorizing patterns observed in the dirt).",kaggle competitions download -c sorghum-id-fgvc-9,"['https://www.kaggle.com/code/pegasos/sorghum-pytorch-lightning-starter-training', 'https://www.kaggle.com/code/jhoward/resize-images', 'https://www.kaggle.com/code/leoooo333/lb-0-885-sorghum-higer-accuracy', 'https://www.kaggle.com/code/leoooo333/sorghum-speed-up', 'https://www.kaggle.com/code/meetnagadia/sorghum-100-cultivar-baseline']"
91,"WiDS Datathon 2022: Phase II Excellence in Research
We invite you to build a team, hone your data science skills, and join us for Phase II of the 5th Annual WiDS Datathon focused on social impact!
This year’s WiDS Datathon, organized by the WiDS Worldwide team, Stanford University, Harvard University IACS, and the WiDS Datathon Committee, will address the multi-faceted impacts of climate change. The WiDS Datathon Committee is partnering with experts from many disciplines at Climate Change AI (CCAI), Lawrence Berkeley National Laboratory (Berkeley Lab), US Environmental Protection Agency (EPA), and MIT Critical Data. Phase I of this year's datathon focused on an important way to mitigate the effects of climate change - improving building energy efficiency through forecasting usage. In the WiDS Datathon Excellence in Research Award (Phase II), we will broaden our focus to examine the impacts of climate change across multiple domains.
To qualify for Phase II, participants must:
have participated in the first phase of the WiDS Datathon 2022 on Kaggle
REGISTER for the Excellence in Research Award (Phase II)
Background
Climate change is a globally relevant, urgent, and multi-faceted issue heavily impacting many industries and aspects of public life. Participants in Phase II will have the opportunity to examine the climate change from different perspectives. Participants will choose to explore one dataset among several, spanning sectors including healthcare, energy and environmental protection. Participants will also have opportunities to take deeper dives into their dataset and tackle a range of impactful real-world tasks. Teams will submit a research report at the end of Phase II.
New this year, participants in Phase II can receive mentorship from experts in the domain related to their choice of dataset and task. Domain expert mentorship in Phase II will allow participants to both strengthen their foundational data science skills as well as develop skills needed to conduct research in data science. Teams with outstanding paper submissions from Phase II will be invited to submit their work for publication.
The Excellence in Research Award (Phase II) is open from March 8 - June 30, 2022. Submissions are due on Submittable.
After the June 30, 2022 research paper deadline, submissions will be reviewed for their potential for real-world impact, rigor in scientific methodology, and clarity of communication, by subject matter experts from the WiDS Datathon Committee, the National Science Foundation Big Data Innovation Hubs, and Datathon partners.
Phase II Datathon Partners and Research Tracks
Phase II participants will be able to choose one of three research tracks to explore:
US Environmental Protection Agency (EPA): weather, air pollutant, and census data
MIT Critical Data: CDC county level COVID data
Climate Change AI: Fine grained building energy usage data
Eligibility and Teams
To be eligible for the award, all entrants must have participated in the first phase of the WiDS Datathon 2022 on Kaggle.
We encourage Phase II individuals and author teams of up to 8 people to work together. You may collaborate with individuals in a “new” team for the second phase of the Datathon, as long as (1) each person participated in the first phase of the datathon on Kaggle and (2) your new team still includes 50% or more individuals identifying as women. Each team member must complete Phase II registration.
Note that you do NOT need to merge teams within the Kaggle platform (it's actually disabled), but all team members must be listed as collaborators on your submitted research paper, and all team members must accept the competition rules before the submission deadline of June 30th.
Acknowledgements
The WiDS Datathon Excellence in Research Award 2022 is a collaboration led by the WiDS Worldwide team at Stanford University, the Institute for Applied Computational Sciences at Harvard University and the WiDS Datathon Committee. WiDS Datathon 2022 cash prizes are provided by Kaggle. The Excellence in Research Award is supported by the National Science Foundation under Grants 1916573, 1916481, and 1915774, as part of a network of Big Data Innovation Hubs. Special thanks to our datathon partners Climate Change AI, US Environmental Protection Agency (EPA), and MIT Critical Data.",,"Research Tracks and Data
Participants must choose one of following three tracks to explore:
US Environmental Protection Agency (EPA): weather, air pollutant, and census data
MIT Critical Data: CDC county level COVID data
Climate Change AI: Fine grained building energy usage data
Each track comes with a starter dataset as well as a list of suggested research questions. Some tracks require participants to download additional data in order to answer research questions. For example, the MIT Critical Data track requires participants to download COVID outcomes in order to build predictive models. The necessary links to required additional datasets will be provided as part of the starter files (data_links.txt).
Participants are welcome to explore their own research questions as well as augment the starter dataset with additional data from any public source. For example, participants are encouraged to explore the relationship between factors involved in climate change and the outcomes in each track. To do this, participants are welcomed to enrich the dataset from their track with climate change related datasets (e.g. the US Climate Extremes Index).
Throughout the competition, participants will have opportunities to interact with domain experts in their chosen track and receive feedback on their work. Sign up to attend the Phase II mentorship office hours.",kaggle competitions download -c phase-ii-widsdatathon2022,"['https://www.kaggle.com/code/mpwolke/learning-with-our-vulnerability-covid-19', 'https://www.kaggle.com/code/iamleonie/wids-datathon-2022-phase-ii-climate-change-ai', 'https://www.kaggle.com/code/mpwolke/the-air-we-breathe-particulate-matter-pm-2-5-pcm', 'https://www.kaggle.com/code/mpwolke/climate-change-ai-ccai-geopandas', 'https://www.kaggle.com/code/mpwolke/how-well-or-not-we-live-health-rankings-wids']"
92,"For the March edition of the 2022 Tabular Playground Series you're challenged to forecast twelve-hours of traffic flow in a U.S. metropolis. The time series in this dataset are labelled with both location coordinates and a direction of travel -- a combination of features that will test your skill at spatio-temporal forecasting within a highly dynamic traffic network.
Which model will prevail? The venerable linear regression? The deservedly-popular ensemble of decision trees? Or maybe a cutting-edge graph neural-network? We can't wait to see!
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.","Submissions are evaluated on the mean absolute error between predicted and actual congestion values for each time period in the test set.
Submission File
For each row_id in the test set, you should predict a congestion measurement. The file should contain a header and have the following format:
row_id,congestion
140140,0.0
140141,0.0
140142,0.0
...
The congestion target has integer values from 0 to 100, but your predictions may be any floating-point number.","In this competition, you'll forecast twelve-hours of traffic flow in a major U.S. metropolitan area. Time, space, and directional features give you the chance to model interactions across a network of roadways.
Files and Field Descriptions
train.csv - the training set, comprising measurements of traffic congestion across 65 roadways from April through September of 1991.
row_id - a unique identifier for this instance
time - the 20-minute period in which each measurement was taken
x - the east-west midpoint coordinate of the roadway
y - the north-south midpoint coordinate of the roadway
direction - the direction of travel of the roadway. EB indicates ""eastbound"" travel, for example, while SW indicates a ""southwest"" direction of travel.
congestion - congestion levels for the roadway during each hour; the target. The congestion measurements have been normalized to the range 0 to 100.
test.csv - the test set; you will make hourly predictions for roadways identified by a coordinate location and a direction of travel on the day of 1991-09-30.
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-mar-2022,"['https://www.kaggle.com/code/ambrosm/tpsmar22-eda-which-makes-sense', 'https://www.kaggle.com/code/rohan1506/optimizing-pandas-like-a-pro', 'https://www.kaggle.com/code/kellibelcher/time-series-forecasting-arima-ets-boosting', 'https://www.kaggle.com/code/sytuannguyen/tps-mar-2022-eda-model', 'https://www.kaggle.com/code/lordozvlad/tps-mar-fast-workflow-using-scikit-learn-intelex']"
93,"📣  Recommended Competition
We highly recommend Titanic - Machine Learning from Disaster to get familiar with the basics of machine learning and Kaggle competitions.
Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.
The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.
While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!
To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.
Help save them and change history!
💡  Getting Started Notebook
To get started quickly, feel free to take advantage of this starter notebook.
If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle
Acknowledgments
Photos by Joel Filipe, Richard Gatley and ActionVance on Unsplash.","Metric
Submissions are evaluated based on their classification accuracy, the percentage of predicted labels that are correct.
Submission Format
The submission format for the competition is a csv file with the following format:
PassengerId,Transported
0013_01,False
0018_01,False
0019_01,False
0021_01,False
etc.","In this competition your task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.
File and Data Field Descriptions
train.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.
PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.
HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.
CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.
Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.
Destination - The planet the passenger will be debarking to.
Age - The age of the passenger.
VIP - Whether the passenger has paid for special VIP service during the voyage.",kaggle competitions download -c spaceship-titanic,"['https://www.kaggle.com/code/gusthema/spaceship-titanic-with-tfdf', 'https://www.kaggle.com/code/areebsyed237/ensemble-exponential-weighted', 'https://www.kaggle.com/code/areebsyed237/catboost-thresholding', 'https://www.kaggle.com/code/areebsyed237/svm-explained', 'https://www.kaggle.com/code/sh2orc/spaceship-titanic-with-lightgbm']"
94,"Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our eighth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US women's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.
You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.
In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2022 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2022 results.
And don't forget to take a look at our companion competition that looks to predict the outcome of the US men's college basketball tournament!
Acknowledgments
Banner image by Ben Hershey on Unsplash","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right],
$$
where
\( n \) is the number of games played
\( \hat{y}_i \) is the predicted probability of team 1 beating team 2
\( y_i \) is 1 if team 1 wins, 0 if team 2 wins
\( log \) is the natural logarithm
The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1--historical model building--or stage 2--the 2022 tournament. Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict \( (64*63)/2 = 2,016\) matchups.
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2016_3106_3107"" indicates team 3106 played team 3107 in the year 2016. You must predict the probability that the team with the lower id beats the team with the higher id.
The resulting submission format looks like the following, where Pred represents the predicted probability that the first team will win:
ID,Pred
2016_3106_3107,0.5
2016_3106_3110,0.5
2016_3106_3113,0.5
...","Each season there are thousands of NCAA® basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness® game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.
If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.
As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Remember as well that you are required to disclose your external sources of data prior to the start of the tournament.
We extend our gratitude to Kenneth Massey for providing much of the historical data.
Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.",kaggle competitions download -c womens-march-mania-2022,"['https://www.kaggle.com/code/theoviel/using-last-year-s-2nd-place', 'https://www.kaggle.com/code/kalilurrahman/2022-march-mania-quick-eda-fe', 'https://www.kaggle.com/code/ninjaac/2022-march-mania-eda-first-look', 'https://www.kaggle.com/code/verracodeguacas/xgboost-with-raddar-s-methodology-and-overrides', 'https://www.kaggle.com/code/rishirajacharya/mmlm22-women-tuned']"
95,"As the “extinction capital of the world,” Hawai'i has lost 68% of its bird species, the consequences of which can harm entire food chains. Researchers use population monitoring to understand how native birds react to changes in the environment and conservation efforts. But many of the remaining birds across the islands are isolated in difficult-to-access, high-elevation habitats. With physical monitoring difficult, scientists have turned to sound recordings. Known as bioacoustic monitoring, this approach could provide a passive, low labor, and cost-effective strategy for studying endangered bird populations.
Current methods for processing large bioacoustic datasets involve manual annotation of each recording. This requires specialized training and prohibitively large amounts of time. Thankfully, recent advances in machine learning have made it possible to automatically identify bird songs for common species with ample training data. However, it remains challenging to develop such tools for rare and endangered species, such as those in Hawai'i.
The Cornell Lab of Ornithology's K. Lisa Yang Center for Conservation Bioacoustics (KLY-CCB) develops and applies innovative conservation technologies across multiple ecological scales to inspire and inform the conservation of wildlife and habitats. KLY-CCB does this by collecting and interpreting sounds in nature and they've joined forces with Google Bioacoustics Group, LifeCLEF, Listening Observatory for Hawaiian Ecosystems (LOHE) Bioacoustics Lab at the University of Hawai'i at Hilo, and Xeno-Canto for this competition.
In this competition, you’ll use your machine learning skills to identify bird species by sound. Specifically, you'll develop a model that can process continuous audio data and then acoustically recognize the species. The best entries will be able to train reliable classifiers with limited training data.
If successful, you'll help advance the science of bioacoustics and support ongoing research to protect endangered Hawaiian birds. Thanks to your innovations, it will be easier for researchers and conservation practitioners to accurately survey population trends. They'll be able to regularly and more effectively evaluate threats and adjust their conservation actions.
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on a metric that is most similar to the macro F1 score. Given the amount of audio data used in this competition it wasn't feasible to label every single species found in every soundscape. Instead only a subset of species are actually scored for any given audio file. After dropping all of the un-scored rows we technically run a weighted classification accuracy with the weights set such that all of the species are assigned the same total weight and the true negatives and true positives for each species have the same weight. The extra complexity exists purely to allow us to have a great deal of control over which birds are scored for a given soundscape. For offline cross validation purposes, the macro F1 is the closest analogue to the actual metric.
Submission File
For each row_id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
row_id,target
soundscape_1000170626_akiapo_5,False
soundscape_1000170626_akiapo_10,False
soundscape_1000170626_akiapo_15,False
etc.
Working Note Award Criteria (optional)
Criteria for the BirdCLEF best working note award:
Originality. The value of a paper is a function of the degree to which it presents new or novel technical material. Does the paper present results previously unknown? Does it push forward the frontiers of knowledge? Does it present new methods for solving old problems or new viewpoints on old problems? Or, on the other hand, is it a re-hash of information already known?
Quality. A paper's value is a function of the innate character or degree of excellence of the work described. Was the work performed, or the study made with a high degree of thoroughness? Was high engineering skill demonstrated? Is an experiment described which has a high degree of elegance? Or, on the other hand, is the work described pretty much of a run-of-the-mill nature?
Contribution. The value of a paper is a function of the degree to which it represents an overall contribution to the advancement of the art. This is different from originality. A paper may be highly original but may be concerned with a very minor, or even insignificant, matter or problem. On the other hand, a paper may make a great contribution by collecting and analyzing known data and facts and pointing out their significance. Or, a fine exposition of a known but obscure or complex phenomenon or theory or system or operating technique may be a very real contribution to the art. Obviously, a paper may well score highly on both originality and contribution. Perhaps a significant question is, will the engineer who reads the paper be able to practice his profession more effectively because of having read it?
Presentation. The value of the paper is a function of the ease with which the reader can determine what the author is trying to present. Regardless of the other criteria, a paper is not good unless the material is presented clearly and effectively. Is the paper well written? Is the meaning of the author clear? Are the tables, charts, and figures clear? Is their meaning readily apparent? Is the information presented in the paper complete? At the same time, is the paper concise?
Evaluation of the submitted BirdCLEF working notes:
Each working note will be reviewed by two reviewers and scores averaged. Maximum score: 15.
a) Evaluation of work and contribution
5 points: Excellent work and a major contribution
4 points: Good solid work of some importance
3 points: Solid work but a marginal contribution
2 points: Marginal work and minor contribution
1 point: Work doesn't meet scientific standards
b) Originality and novelty
5 points Trailblazing
4 points: A pioneering piece of work
3 points: One step ahead of the pack
2 points: Yet another paper about…
1 point: It's been said many times before
c) Readability and organization
5 points: Excellent
4 points: Well written
3 points: Readable
2 points: Needs considerable work
1 point: Work doesn't meet scientific standards","Your challenge in this competition is to identify which birds are calling in long recordings given quite limited training data. This is the exact challenge faced by scientists trying to monitor rare birds in Hawaii. For example, there are only a few thousand individual Nene geese left in the world, which makes it difficult to acquire recordings of their calls.
This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook.
Files
train_metadata.csv - A wide range of metadata is provided for the training data. The most directly relevant fields are:
primary_label - a code for the bird species. You can review detailed information about the bird codes by appending the code to https://ebird.org/species/, such as https://ebird.org/species/amecro for the American Crow.
secondary_labels: Background species as annotated by the recordist. An empty list does not mean that no background birds are audible.
author - the eBird user who provided the recording.
filename: the associated audio file.
rating: Float value between 0.0 and 5.0 as an indicator of the quality rating on Xeno-canto and the number of background species, where 5.0 is the highest and 1.0 is the lowest. 0.0 means that this recording has no user rating yet.",kaggle competitions download -c birdclef-2022,"['https://www.kaggle.com/code/shreyasajal/birdclef-librosa-audio-feature-extraction', 'https://www.kaggle.com/code/awsaf49/birdclef23-pretraining-is-all-you-need-train', 'https://www.kaggle.com/code/hasanbasriakcay/birdclef22-eda-noise-reduction', 'https://www.kaggle.com/code/kaerunantoka/birdclef2022-ex005-f0-infer', 'https://www.kaggle.com/code/julian3833/birdclef-21-2nd-place-model-submit-0-66']"
96,"The Herbarium 2022: Flora of North America is a part of a project of the New York Botanical Garden funded by the National Science Foundation to build tools to identify novel plant species around the world. The dataset strives to represent all known vascular plant taxa in North America, using images gathered from 60 different botanical institutions around the world.
In botany, a ‘flora’ is a complete account of the plants found in a geographic region. The dichotomous keys and detailed descriptions of diagnostic morphological features contained within a flora are used by botanists to determine which names to apply to plant specimens. This year's competition dataset aims to encapsulate the flora of North America so that we can test the capability of artificial intelligence to replicate this traditional tool —a crucial first step to harnessing AI’s potential botanical applications.
The Herbarium 2022: Flora of North America dataset comprises 1.05 M images of 15,501 vascular plants, which constitute more than 90% of the taxa documented in North America. Our dataset is constrained to include only vascular land plants (lycophytes, ferns, gymnosperms, and flowering plants).
Our dataset has a long-tail distribution. The number of images per taxon is as few as seven and as many as 100 images. Although more images are available, we capped the maximum number in an attempt to ensure sufficient but manageable training data size for competition participants.
About
This is an FGVC competition hosted as part of the FGVC9 workshop at CVPR 2022 and sponsored by NYBG.
Details of this competition are mirrored on the github page. Please post in the forum or open an issue if you have any questions or problems with the dataset.
Acknowledgements
The images are provided by the New York Botanical Garden and 59 other institutions around the world.","Submissions are evaluated using the [macro F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). The F1 score is given by $$ F_1 = 2\frac{precision \cdot recall}{precision+recall} $$ where: $$ precision = \frac{TP}{TP+FP}, $$ $$ recall = \frac{TP}{TP+FN}. $$ In ""macro"" F1 a separate F1 score is calculated for each `species` value and then averaged. #Submission Format For each image `Id`, you should predict the corresponding image label (`category_id`) in the `Predicted` column. The submission file should have the following format:
Id,Predicted
0,1
1,27
2,42
...","Data Overview
The training and test sets contain images of herbarium specimens from 15,501 species of vascular plants. Each image contains exactly one specimen. The text labels on the specimen images have been blurred to remove category information in the image.
The data has been approximately split 80%/20% for training/test. Each category has at least 1 instance in both the training and test datasets. Note that the test set distribution is slightly different from the training set distribution. The training set has a number of examples representing species capped at a maximum of 80.
Dataset Details
Images
Each image has different image dimensions, with a maximum of 1000 pixels in the larger dimension. These have been resized from the original image resolution. All images are in JPEG format.
Hierarchical Structure of Classes category_id
In addition to the images, we also include a hierarchical taxonomic structure of category_id. The categories in the training_metadata.json contain three levels of hierarchical structure, family - genus - species, from the highest rank to the lowest rank. One can think about this as a directed graph, where families are the root nodes and the species are the leaf nodes. Please note that is only unique under its parent node , which means that we can find multiple categories with the same name under different names. This is due to the taxonomic nature that plants are named after, and the - pair is always unique in our data.",kaggle competitions download -c herbarium-2022-fgvc9,"['https://www.kaggle.com/code/kalelpark/herbarium2022-eda-with-pytorch', 'https://www.kaggle.com/code/odins0n/json-pandas-herbarium-2022', 'https://www.kaggle.com/code/ohseokkim/herbarium-flipping-through-the-pages-of-memories', 'https://www.kaggle.com/code/manthanx/herbarium-lightning-resnet50-starter-train', 'https://www.kaggle.com/code/heyytanay/herbarium-jax-flax-training-kfolds-w-b']"
97,"H&M Group is a family of brands and businesses with 53 online markets and approximately 4,850 stores. Our online store offers shoppers an extensive selection of products to browse through. But with too many choices, customers might not quickly find what interests them or what they are looking for, and ultimately, they might not make a purchase. To enhance the shopping experience, product recommendations are key. More importantly, helping customers make the right choices also has a positive implications for sustainability, as it reduces returns, and thereby minimizes emissions from transportation.
In this competition, H&M Group invites you to develop product recommendations based on data from previous transactions, as well as from customer and product meta data. The available meta data spans from simple data, such as garment type and customer age, to text data from product descriptions, to image data from garment images.
There are no preconceptions on what information that may be useful – that is for you to find out. If you want to investigate a categorical data type algorithm, or dive into NLP and image processing deep learning, that is up to you.","Submissions are evaluated according to the Mean Average Precision @ 12 (MAP@12):
$$MAP@12 = \frac{1}{U} \sum_{u=1}^{U} \frac{1}{min(m,12)} \sum_{k=1}^{min(n,12)} P(k) \times rel(k)$$
where \( U \) is the number of customers, \( P(k) \) is the precision at cutoff \( k \), \( n \) is the number predictions per customer, \( m \) is the number of ground truth values per customer, and \( rel(k) \) is an indicator function equaling 1 if the item at rank \( k \) is a relevant (correct) label, zero otherwise.
Notes:
You will be making purchase predictions for all customer_id values provided, regardless of whether these customers made purchases in the training data.
Customer that did not make any purchase during test period are excluded from the scoring.
There is never a penalty for using the full 12 predictions for a customer that ordered fewer than 12 items; thus, it's advantageous to make 12 predictions for each customer.
Submission File
For each customer_id observed in the training data, you may predict up to 12 labels for the article_id, which is the predicted items a customer will buy in the next 7-day period after the training time period. The file should contain a header and have the following format:
customer_id,prediction
00000dba,0706016001 0706016002 0372860001 ...
0000423b,0706016001 0706016002 0372860001 ...
...",,,
98,"When you visit a doctor, how they interpret your symptoms can determine whether your diagnosis is accurate. By the time they’re licensed, physicians have had a lot of practice writing patient notes that document the history of the patient’s complaint, physical exam findings, possible diagnoses, and follow-up care. Learning and assessing the skill of writing patient notes requires feedback from other doctors, a time-intensive process that could be improved with the addition of machine learning.
Until recently, the Step 2 Clinical Skills examination was one component of the United States Medical Licensing Examination® (USMLE®). The exam required test-takers to interact with Standardized Patients (people trained to portray specific clinical cases) and write a patient note. Trained physician raters later scored patient notes with rubrics that outlined each case’s important concepts (referred to as features). The more such features found in a patient note, the higher the score (among other factors that contribute to the final score for the exam).
However, having physicians score patient note exams requires significant time, along with human and financial resources. Approaches using natural language processing have been created to address this problem, but patient notes can still be challenging to score computationally because features may be expressed in many ways. For example, the feature ""loss of interest in activities"" can be expressed as ""no longer plays tennis."" Other challenges include the need to map concepts by combining multiple text segments, or cases of ambiguous negation such as “no cold intolerance, hair loss, palpitations, or tremor” corresponding to the key essential “lack of other thyroid symptoms.”
In this competition, you’ll identify specific clinical concepts in patient notes. Specifically, you'll develop an automated method to map clinical concepts from an exam rubric (e.g., “diminished appetite”) to various ways in which these concepts are expressed in clinical patient notes written by medical students (e.g., “eating less,” “clothes fit looser”). Great solutions will be both accurate and reliable.
If successful, you'll help tackle the biggest practical barriers in patient note scoring, making the approach more transparent, interpretable, and easing the development and administration of such assessments. As a result, medical practitioners will be able to explore the full potential of patient notes to reveal information relevant to clinical skills assessment.
This competition is sponsored by the National Board of Medical Examiners® (NBME®). Through research and innovation, NBME supports medical school and residency program educators in addressing issues around the evolution of teaching, learning, technology, and the need for meaningful feedback. NBME offers high-quality assessments and educational services for students, professionals, educators, regulators, and institutions dedicated to the evolving needs of medical education and health care. To serve these communities, NBME collaborates with a diverse and comprehensive array of practicing health professionals, medical educators, state medical board members, test developers, academic researchers, scoring experts and public representatives.
NBME gratefully acknowledges the valuable input of Dr Le An Ha from the University of Wolverhampton’s Research Group in Computational Linguistics.

This is a Code Competition. Refer to Code Requirements for details.","This competition is evaluated by a micro-averaged F1 score.
For each instance, we predict a set of character spans. A character span is a pair of indexes representing a range of characters within a text. A span i j represents the characters with indices i through j, inclusive of i and exclusive of j. In Python notation, a span i j is equivalent to a slice i:j.
For each instance there is a collection of ground-truth spans and a collection of predicted spans. The spans we delimit with a semicolon, like: 0 3; 5 9.
We score each character index as:
TP if it is within both a ground-truth and a prediction,
FN if it is within a ground-truth but not a prediction, and,
FP if it is within a prediction but not a ground truth.
Finally, we compute an overall F1 score from the TPs, FNs, and FPs aggregated across all instances.
Example
Suppose we have an instance:
| ground-truth | prediction    |
|--------------|---------------|
| 0 3; 3 5     | 2 5; 7 9; 2 3 |
These spans give the sets of indices:
| ground-truth | prediction |
|--------------|------------|
| 0 1 2 3 4    | 2 3 4 7 8  |
We therefore compute:
TP = size of {2, 3, 4} = 3
FN = size of {0, 1} = 2
FP = size of {7, 8} = 2
Repeat for all instances, collect the TPs, FNs, and FPs, and compute the final F1 score.
Sample Submission
For each id in the test set, you must predict zero or more spans delimited by a semicolon. The file should contain a header and have the following format:
id,location
00016_000,0 100
00016_001,
00016_002,200 250;300 500
...
For 00016_000 you should give predictions for feature 000 in patient note 00016.",,,
99,,,,,
100,"We use fingerprints and facial recognition to identify people, but can we use similar approaches with animals? In fact, researchers manually track marine life by the shape and markings on their tails, dorsal fins, heads and other body parts. Identification by natural markings via photographs—known as photo-ID—is a powerful tool for marine mammal science. It allows individual animals to be tracked over time and enables assessments of population status and trends. With your help to automate whale and dolphin photo-ID, researchers can reduce image identification times by over 99%. More efficient identification could enable a scale of study previously unaffordable or impossible.
Currently, most research institutions rely on time-intensive—and sometimes inaccurate—manual matching by the human eye. Thousands of hours go into manual matching, which involves staring at photos to compare one individual to another, finding matches, and identifying new individuals. While researchers enjoy looking at a whale photo or two, manual matching limits the scope and reach.
Algorithms developed in this competition will be implemented in Happywhale, a research collaboration and citizen science web platform. Its mission is to increase global understanding and caring for marine environments through high quality conservation science and education. Happywhale aims to make it easy and rewarding for the public to participate in science by building innovative tools to engage anyone interested in marine mammals. The platform also serves the research community with powerful collaborative tools.
In this competition, you’ll develop a model to match individual whales and dolphins by unique—but often subtle—characteristics of their natural markings. You'll pay particular attention to dorsal fins and lateral body views in image sets from a multi-species dataset built by 28 research institutions. The best submissions will suggest photo-ID solutions that are fast and accurate.
If successful, you'll have a hand in building advanced technology to better understand and manage the impact on the world’s changing oceans. Previous automation attempts resulted in a global database of over 50,000 whales and an agreement with cruise ships to operate at a maximum speed of 11 mph in the most whale-rich region. Your ideas to automate the identification of marine life will help overcome increasing human impacts on oceans, providing a critical tool for conservation science. If there's a whale, there's a way!","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):
$$MAP@5 = \frac{1}{U} \sum_{u=1}^{U} \sum_{k=1}^{min(n,5)} P(k) \times rel(k)$$
where \( U \) is the number of images, \( P(k) \) is the precision at cutoff \( k \), \( n \) is the number predictions per image, and \( rel(k) \) is an indicator function equaling 1 if the item at rank \( k \) is a relevant (correct) label, zero otherwise.
Once a correct label has been scored for an observation, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0.
[A, B, C, D, E]
[A, A, A, A, A]
[A, B, A, C, A]
Submission File
For each image in the test set, you may predict up to 5 individual_id labels. There are individuals in the test set that are not seen in the training data; these should be predicted as new_individual. The file should contain a header and have the following format:
image,predictions 
000188a72f2562.jpg,37c7aba965a5 114207cab555 a6e325d8e924 19fbb960f07d new_individual 
000ba09273d6f3.jpg,37c7aba965a5 114207cab555 a6e325d8e924 19fbb960f07d new_individual 
...","In the previous HappyWhale competition, the task was to predict individual humpback whales from images of their flukes. Whales and dolphins in this dataset can be identified by shapes, features and markings (some natural, some acquired) of dorsal fins, backs, heads and flanks. Some species and some individuals have highly distinct features, others are very much less distinct. Further, individual features may change over time. This competition expands that task significantly: data in this competition contains images of over 15,000 unique individual marine mammals from 30 different species collected from 28 different research organizations. Individuals have been manually identified and given an individual_id by marine researches, and your task is to correctly identify these individuals in images. It's a challenging task that has the potential to drive significant advancements in understanding and protecting marine mammals across the globe.
An important note about data quality: Bringing together this dataset from many different research organization posed a number of practical challenges. Significant effort has been made to minimize data quality issues and as well as to minimize leakage as much as possible. There are undoubtably issues. We encourage the community to report these things so that future versions of the data can be improved, but unless there is a significant issue, we don't expect to make updates to the data during the competition.
Files
train_images/ - a folder containing the training images
train.csv - provides the species and the individual_id for each of the training images
test_images/ - a folder containing the test images; for each image, your task is to predict the individual_id; no species information is given for the test data; there are individuals in the test data that are not observed in the training data, which should be predicted as .",kaggle competitions download -c happy-whale-and-dolphin,"['https://www.kaggle.com/code/debarshichanda/pytorch-arcface-gem-pooling-starter', 'https://www.kaggle.com/code/ks2019/happywhale-arcface-baseline-tpu', 'https://www.kaggle.com/code/remekkinas/remove-background-salient-object-detection', 'https://www.kaggle.com/code/awsaf49/happywhale-data-distribution', 'https://www.kaggle.com/code/andradaolteanu/whales-dolphins-effnet-embedding-cos-distance']"
101,"  A picture is worth a thousand words. But did you know a picture can save a thousand lives? Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster. But what makes a good picture? With the help of data science, you may be able to accurately determine a pet photo’s appeal and even suggest improvements to give these rescue animals a higher chance of loving homes.
PetFinder.my is Malaysia’s leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.
Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.
In this competition, you’ll analyze raw images and metadata to predict the “Pawpularity” of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.
If successful, your solution will be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements. As a result, stray dogs and cats can find their ""furever"" homes much faster. With a little assistance from the Kaggle community, many precious lives could be saved and more happy families created.
Top participants may be invited to collaborate on implementing their solutions and creatively improve global animal welfare with their AI skills.
 
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/petfinder-pawpularity-score/overview/code-requirements) for details.**","Root Mean Squared Error 𝑅𝑀𝑆𝐸
Submissions are scored on the root mean squared error. RMSE is defined as:
[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} ]
where ( \hat{y}_i ) is the predicted value and (y_i) is the original value for each instance (i).
Submission File
For each Id in the test set, you must predict a probability for the target variable, Pawpularity. The file should contain a header and have the following format:
Id, Pawpularity
0008dbfb52aa1dc6ee51ee02adf13537, 99.24
0014a7b528f1682f0cf3b73a991c17a0, 61.71
0019c1388dfcd30ac8b112fb4250c251, 6.23
00307b779c82716b240a24f028b0031b, 9.43
00320c6dd5b4223c62a9670110d47911, 70.89
etc.","In this competition, your task is to predict engagement with a pet's profile based on the photograph for that profile. You are also provided with hand-labelled metadata for each photo. The dataset for this competition therefore comprises both images and tabular data.
How Pawpularity Score Is Derived
The Pawpularity Score is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.
Duplicate clicks, crawler bot accesses and sponsored profiles are excluded from the analysis.
Purpose of Photo Metadata
We have included optional Photo Metadata, manually labeling each photo for key visual quality and composition parameters.
These labels are not used for deriving our Pawpularity score, but it may be beneficial for better understanding the content and co-relating them to a photo's attractiveness. Our end goal is to deploy AI solutions that can generate intelligent recommendations (i.e. show a closer frontal pet face, add accessories, increase subject focus, etc) and automatic enhancements (i.e. brightness, contrast) on the photos, so we are hoping to have predictions that are more easily interpretable.
You may use these labels as you see fit, and optionally build an intermediate / supplementary model to predict the labels from the photos. If your supplementary model is good, we may integrate it into our AI tools as well.",kaggle competitions download -c petfinder-pawpularity-score,"['https://www.kaggle.com/code/phalanx/train-swin-t-pytorch-lightning', 'https://www.kaggle.com/code/cdeotte/rapids-svr-boost-17-8', 'https://www.kaggle.com/code/tanlikesmath/petfinder-pawpularity-eda-fastai-starter', 'https://www.kaggle.com/code/warotjanpinitrat/lovely-doggo-with-bonky-fastai-timm', 'https://www.kaggle.com/code/alexteboul/tutorial-part-1-eda-for-beginners']"
102,"For the February 2022 Tabular Playground Series competition, your task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. In this technique, 10-mer snippets of DNA are sampled and analyzed to give the histogram of base count. In other words, the DNA segment \(\text{ATATGGCCTT}\) becomes \(\text{A}_2\text{T}_4\text{G}_2\text{C}_2\). Can you use this lossy information to accurately predict bacteria species?
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
Good luck and have fun!
Acknowledgements
The idea for this competition came from the following paper:
@ARTICLE{10.3389/fmicb.2020.00257,
AUTHOR={Wood, Ryan L. and Jensen, Tanner and Wadsworth, Cindi and Clement, Mark and Nagpal, Prashant and Pitt, William G.},   
TITLE={Analysis of Identification Method for Bacterial Species and Antibiotic Resistance Genes Using Optical Data From DNA Oligomers},      
JOURNAL={Frontiers in Microbiology},      
VOLUME={11},      
YEAR={2020},      
URL={https://www.frontiersin.org/article/10.3389/fmicb.2020.00257},       
DOI={10.3389/fmicb.2020.00257},      
ISSN={1664-302X}}","Evaluation
Submissions will be evaluated based on their categorization accuracy.
Submission Format
The submission format for the competition is a csv file with the following format:
row_id,target
200000,Streptococcus_pneumoniae
200001,Enterococcus_hirae
etc.","For this challenge, you will be predicting bacteria species based on repeated lossy measurements of DNA snippets. Snippets of length 10 are analyzed using Raman spectroscopy that calculates the histogram of bases in the snippet. In other words, the DNA segment \(\text{ATATGGCCTT}\) becomes \(\text{A}_2\text{T}_4\text{G}_2\text{C}_2\).
Each row of data contains a spectrum of histograms generated by repeated measurements of a sample, each row containing the output of all 286 histogram possibilities (e.g., \(\text{A}_0\text{T}_0\text{G}_0\text{C}_{10}\) to \(\text{A}_{10}\text{T}_0\text{G}_0\text{C}_0\)), which then has a bias spectrum (of totally random ATGC) subtracted from the results.
The data (both train and test) also contains simulated measurement errors (of varying rates) for many of the samples, which makes the problem more challenging.
Files
train.csv - the training set, which contains the spectrum of 10-mer histograms for each sample
test.csv - the test set; your task is to predict the bacteria species (target) for each row_id
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-feb-2022,"['https://www.kaggle.com/code/ambrosm/tpsfeb22-01-eda-which-makes-sense', 'https://www.kaggle.com/code/usharengaraju/tensorflow-tabtransformer', 'https://www.kaggle.com/code/odins0n/tps-feb-22-eda-modelling', 'https://www.kaggle.com/code/lordozvlad/fast-random-forest-using-scikit-learn-intelex', 'https://www.kaggle.com/code/devsubhash/tps-feb-2022-eda-models-extratrees-rf-xgb']"
103,"What do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier.
Current simulators are trained as an ensemble, where each model simulates a single lung setting. However, lungs and their attributes form a continuous space, so a parametric approach must be explored that would consider the differences in patient lungs.
Partnering with Princeton University, the team at Google Brain aims to grow the community around machine learning for mechanical ventilation control. They believe that neural networks and deep learning can better generalize across lungs with varying characteristics than the current industry standard of PID controllers.
In this competition, you’ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account.
If successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. This will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe.
Photo by Nino Liverani on Unsplash","The competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored. The score is given by:
$$|X-Y|$$
where \(X\) is the vector of predicted pressure and \(Y\) is the vector of actual pressures across all breaths in the test set.
Submission File
For each id in the test set, you must predict a value for the pressure variable. The file should contain a header and have the following format:
id,pressure
1,20
2,23
3,24
etc.","The ventilator data used in this competition was produced using a modified open-source ventilator connected to an artificial bellows test lung via a respiratory circuit. The diagram below illustrates the setup, with the two control inputs highlighted in green and the state variable (airway pressure) to predict in blue. The first control input is a continuous variable from 0 to 100 representing the percentage the inspiratory solenoid valve is open to let air into the lung (i.e., 0 is completely closed and no air is let in and 100 is completely open). The second control input is a binary variable representing whether the exploratory valve is open (1) or closed (0) to let air out.
In this competition, participants are given numerous time series of breaths and will learn to predict the airway pressure in the respiratory circuit during the breath, given the time series of control inputs.
Each time series represents an approximately 3-second breath. The files are organized such that each row is a time step in a breath and gives the two control signals, the resulting airway pressure, and relevant attributes of the lung, described below.
Files
train.csv - the training set
test.csv - the test set",kaggle competitions download -c ventilator-pressure-prediction,"['https://www.kaggle.com/code/theoviel/deep-learning-starter-simple-lstm', 'https://www.kaggle.com/code/cdeotte/ensemble-folds-with-median-0-153', 'https://www.kaggle.com/code/tenffe/finetune-of-tensorflow-bidirectional-lstm', 'https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-112', 'https://www.kaggle.com/code/cdeotte/lstm-feature-importance']"
104,"Regardless of your investment strategy, fluctuations are expected in the financial market. Despite this variance, professional investors try to estimate their overall returns. Risks and returns differ based on investment types and other factors, which impact stability and volatility. To attempt to predict returns, there are many computer-based algorithms and models for financial market trading. Yet, with new techniques and approaches, data science could improve quantitative researchers' ability to forecast an investment's return.
Ubiquant Investment (Beijing) Co., Ltd is a leading domestic quantitative hedge fund based in China. Established in 2012, they rely on international talents in math and computer science along with cutting-edge technology to drive quantitative financial market investment. Overall, Ubiquant is committed to creating long-term stable returns for investors.
In this competition, you’ll build a model that forecasts an investment's return rate. Train and test your algorithm on historical prices. Top entries will solve this real-world data science problem with as much accuracy as possible.
If successful, you could improve the ability of quantitative researchers to forecast returns. This will enable investors at any scale to make better decisions. You may even discover you have a knack for financial datasets, opening up a world of new opportunities in many industries.
See more information about Ubiquant below:
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on the mean of the Pearson correlation coefficient for each time ID.
You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks:
import ubiquant
env = ubiquant.make_env()   # initialize the environment
iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission
for (test_df, sample_prediction_df) in iter_test:
    sample_prediction_df['target'] = 0  # make your predictions here
    env.predict(sample_prediction_df)   # register your predictions
You will get an error if you submission includes nulls or infinities and submissions that only include one prediction value will receive a score of -1.","This dataset is no longer available for download.
This dataset contains features derived from real historic data from thousands of investments. Your challenge is to predict the value of an obfuscated metric relevant for making trading decisions.
This is a code competition that relies on a time-series API to ensure models do not peek forward in time. To use the API, follow the instructions on the Evaluation page. When you submit your notebook, it will be rerun on an unseen test. This is also a forecasting competition, where the final private leaderboard will be determined using data gathered after the training period closes.
Files
train.csv
row_id - A unique identifier for the row.
time_id - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.
investment_id - The ID code for an investment. Not all investment have data in all time IDs.
target - The target.
[f_0:f_299] - Anonymized features generated from market data.",kaggle competitions download -c ubiquant-market-prediction,"['https://www.kaggle.com/code/lucamassaron/eda-target-analysis', 'https://www.kaggle.com/code/robikscube/fast-data-loading-and-low-mem-with-parquet-files', 'https://www.kaggle.com/code/lonnieqin/ubiquant-market-prediction-with-dnn', 'https://www.kaggle.com/code/valleyzw/ubiquant-lgbm-baseline', 'https://www.kaggle.com/code/kartushovdanil/the-most-advanced-analytics']"
105,"A picture is worth a thousand words, yet sometimes a few will do. We all rely on online images for knowledge sharing, learning, and understanding. Even the largest websites are missing visual content and metadata to pair with their images. Captions and “alt text” increase accessibility and enable better search. The majority of images on Wikipedia articles, for example, don't have any written context connected to the image. Open models could help anyone improve accessibility and learning for all.
Current solutions rely on simple methods based on translations or page interlinks, which have limited coverage. Even the most advanced computer vision image captioning isn't suitable for images with complex semantics.
In this competition, you’ll build a model that automatically retrieves the text closest to an image. Specifically, you'll train your model to associate given images with article titles or complex captions, in multiple languages. The best models will account for the semantic granularity of Wikipedia images.
If successful, you'll be contributing to the accessibility of the largest online encyclopedia. The millions of Wikipedia readers and editors will be able to more easily understand, search, and describe media at scale. As a result, you’ll contribute to an open model to improve learning for all.
--
This competition is organized by the Research team at the Wikimedia Foundation. This competition is based on the WIT dataset published by Google Research as detailed in this SIGIR paper.","Submissions will be evaluated using NDCG@5 (Normalized Discounted Cumulative Gain).
Submission File
The submission should be a list of id,caption_title_and_reference_description pairs ranked from top to bottom according to their relevance (i.e., the top id is the most relevant caption_title_and_reference_description), with up to 5 predictions per id. Each line should be a single id,caption_title_and_reference_description pair.
The file should contain a header and have the following format:
id,caption_title_and_reference_description
0,kaggle the home of data science
0,lorem ipsum
...
1,the quick brown fox
etc.","The objective of this competition is to predict the target caption_title_and_reference_description given information about an images. The targets for this competition are in multiple languages.
Files
train-{0000x}-of-00005.tsv - the training data (tab delimited)
test.tsv - the test data; the objective is to predict the target caption_title_and_reference_description for each row id
test_captions_list.csv - a list of captions to retrieve for test predictions
sample_submission.csv - a sample submission file in the correct format; note that multiple predictions (up to 5) are allowed for each id in the test data.
image_data_test/
image_pixels/test_image_pixels_part-{0000x}.csv.gz
image_url: url to the original image file, e.g. https://upload.wikimedia.org/wikipedia/commons/e/ec/Hovden.jpg
b64_bytes: base64 encoded bytes of the image file at a 300px resolution
metadata_url: url to the commons page of the image, e.g. https://commons.wikimedia.org/wiki/File:Hovden.jpg
resnet_embeddings/test_resnet_embeddings_part-{0000x}.csv.gz
image_url: url to the original image file, e.g.",kaggle competitions download -c wikipedia-image-caption,"['https://www.kaggle.com/code/mpwolke/wikimedia-urllib', 'https://www.kaggle.com/code/debarshichanda/pytorch-wikipedia-image-caption-starter', 'https://www.kaggle.com/code/thedrcat/wiki-image-caption-eda-and-baseline-new-data', 'https://www.kaggle.com/code/aakashnain/can-we-read-faster', 'https://www.kaggle.com/code/debarshichanda/eda-versioning-easy-to-use-dataset']"
106,"We've heard your feedback from the 2021 Tabular Playground Series, and now Kaggle needs your help going forward in 2022!
There are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. We've decided to see if the Kaggle community could help us figure out which of the store chains would have the best sales going forward. So, we've collected some data and are asking you to build forecasting models to help us decide.
Help us figure out whether KaggleMart or KaggleRama should become the official Kaggle outlet!
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.","Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.
Submission File
For each row_id in the test set, you must predict the corresponding num_sold. The file should contain a header and have the following format:
row_id,num_sold
26298,100
26299,100
26300,100
etc.","For this challenge, you will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow you to try numerous different modeling approaches.
Good luck!
Files
train.csv - the training set, which includes the sales data for each date-country-store-item combination.
test.csv - the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-jan-2022,"['https://www.kaggle.com/code/ambrosm/tpsjan22-03-linear-model', 'https://www.kaggle.com/code/ambrosm/tpsjan22-01-eda-which-makes-sense', 'https://www.kaggle.com/code/usharengaraju/tensorflow-tf-data-keraspreprocessinglayers-w-b', 'https://www.kaggle.com/code/teckmengwong/tps2201-hybrid-time-series', 'https://www.kaggle.com/code/lordozvlad/tps-jan-fast-pycaret-with-scikit-learn-intelex']"
107,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the claim variable. The file should contain a header and have the following format:
id,claim
957919,0.5
957920,0.5
957921,0.5
etc.","For this competition, you will predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.
Files
train.csv - the training data with the target claim column
test.csv - the test set; you will be predicting the claim for each row in this file
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-sep-2021,"['https://www.kaggle.com/code/bextuychiev/lgbm-optuna-hyperparameter-tuning-w-understanding', 'https://www.kaggle.com/code/dwin183287/tps-september-2021-eda', 'https://www.kaggle.com/code/bextuychiev/model-explainability-with-shap-only-guide-u-need', 'https://www.kaggle.com/code/rohanrao/automl-tutorial-tps-september-2021', 'https://www.kaggle.com/code/mlanhenke/tps-09-simple-blend-stacking-xgb-lgbm-catb']"
108,"Writing is a critical skill for success. However, less than a third of high school seniors are proficient writers, according to the National Assessment of Educational Progress. Unfortunately, low-income, Black, and Hispanic students fare even worse, with less than 15 percent demonstrating writing proficiency. One way to help students improve their writing is via automated feedback tools, which evaluate student writing and provide personalized feedback.
There are currently numerous automated writing feedback tools, but they all have limitations. Many often fail to identify writing structures, such as thesis statements and support for claims, in essays or do not do so thoroughly. Additionally, the majority of the available tools are proprietary, with algorithms and feature claims that cannot be independently backed up. More importantly, many of these writing tools are inaccessible to educators because of their cost. This problem is compounded for under-serviced schools which serve a disproportionate number of students of color and from low-income backgrounds. In short, the field of automated writing feedback is ripe for innovation that could help democratize education.
Georgia State University (GSU) is an undergraduate and graduate urban public research institution in Atlanta. U.S. News & World Report ranked GSU as one of the most innovative universities in the nation. GSU awards more bachelor’s degrees to African-Americans than any other non-profit college or university in the country. GSU and The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good.
In this competition, you’ll identify elements in student writing. More specifically, you will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students. You'll have access to the largest dataset of student writing ever released in order to test your skills in natural language processing, a fast-growing area of data science.
If successful, you'll make it easier for students to receive feedback on their writing and increase opportunities to improve writing outcomes. Virtual writing tutors and automated writing systems can leverage these algorithms while teachers may use them to reduce grading time. The open-sourced algorithms you come up with will allow any educational organization to better help young writers develop.
Acknowledgements
Georgia State University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures and Chan Zuckerberg Initiative for their support in making this work possible.
                         
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on the overlap between ground truth and predicted word indices.
For each sample, all ground truths and predictions for a given class are compared.
If the overlap between the ground truth and prediction is >= 0.5, and the overlap between the prediction and the ground truth >= 0.5, the prediction is a match and considered a true positive. If multiple matches exist, the match with the highest pair of overlaps is taken.
Any unmatched ground truths are false negatives and any unmatched predictions are false positives.
Example:
Ground Truth
id,class,predictionstring
1,Claim,1 2 3 4 5
1,Claim,6 7 8
1,Claim,21 22 23 24 25
Prediction
id,class,predictionstring
1,Claim,1 2
1,Claim,6 7 8
The first prediction would not have >= 0.5 overlap with either ground truth and would be a false positive. The second prediction would overlap perfectly with the second ground truth and be a true positive. The third ground truth would be unmatched, and would be a false negative.
The final score is arrived at by calculating TP/FP/FN for each class, then taking the macro F1 score across all classes.
The word indices are calculated by using Python's .split() function and taking the indices in the resulting list. The two overlaps are calculated by taking the set() of each list of indices in a ground truth / prediction pair and calculating the intersection between the two sets divided by the length of each set.
Submission File
For each sample in the test set, you must extract any strings from the document that you feel aligns with a class, then submit the sample id, class and word indices predictionstring of that string. If you have multiple predictions for a class or sample, simply submit multiple rows. The file should contain a header and have the following format:
id,class,predictionstring
2,Claim,300 301 302 303
5,Evidence,56 57 58 59 60 61 62
6,Lead,0 1 2 3 4 5 6
6,Lead,9 10 11 12
etc.","The dataset contains argumentative essays written by U.S students in grades 6-12. The essays were annotated by expert raters for elements commonly found in argumentative writing.
Note that this is a code competition, in which you will submit code that will be run against an unseen test set. The unseen test set is approximately 10k documents. A small public test sample has been provided for testing your notebooks.
Your task is to predict the human annotations. You will first need to segment each essay into discrete rhetorical and argumentative elements (i.e., discourse elements) and then classify each element as one of the following:
Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader’s attention and point toward the thesis
Position - an opinion or conclusion on the main question
Claim - a claim that supports the position
Counterclaim - a claim that refutes another claim or gives an opposing reason to the position
Rebuttal - a claim that refutes a counterclaim
Evidence - ideas or examples that support claims, counterclaims, or rebuttals.
Concluding Statement - a concluding statement that restates the claims
The training set will consist of individual essays in a folder of .txt files, as well as a .csv file containing the annotated version of these essays. It is important to note that some parts of the essays will be unannotated (i.e., they do not fit into one of the classifications above).",kaggle competitions download -c feedback-prize-2021,"['https://www.kaggle.com/code/cdeotte/tensorflow-longformer-ner-cv-0-633', 'https://www.kaggle.com/code/abhishek/two-longformers-are-better-than-1', 'https://www.kaggle.com/code/cdeotte/pytorch-bigbird-ner-cv-0-615', 'https://www.kaggle.com/code/erikbruin/nlp-on-student-writing-eda', 'https://www.kaggle.com/code/abhishek/taking-a-long-break-from-kaggle']"
109,"Introduction
The night is dark and full of terrors. Two teams must fight off the darkness, collect resources, and advance through the ages. Daytime finds a desperate rush to gather the resources that can carry you through the impending night whilst growing your city. Plan and expand carefully -- any city that fails to produce enough light will be consumed by darkness.
Welcome to the Lux AI Challenge Season 1!
The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand.
All code can be found at our Github, make sure to give it a star while you are there!
Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.
Sponsors
We would like to thank our 3 sponsors, QuantCo, J Ventures, and QAImera this year for allowing us to provide a prize pool and exciting opportunities to our competitors! For more information on them, go to the sponsors tab","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play Episodes (games) against other bots on the ladder that have a similar skill rating. Over time skill ratings will go up with wins or down with losses and evened out with ties.
Every bot submitted will continue to play episodes until the end of the competition, with newer bots playing a much more frequent number of episodes. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents the uncertainty of that estimate which will decrease over time.
When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error and you can download the agent logs to help figure out why. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. Newly submitted agents will be given an increased rate in the number of episodes run to give you faster feedback.
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
Ranking System
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
Final Evaluation
At the submission deadline on December 6th, additional submissions will be locked. From December 7th to December 20th, we will continue to run games. At the conclusion of this period, the leaderboard is final and is used to determine who gets various ranking based prizes.",,,"['https://www.kaggle.com/code/huikang/lux-ai-working-title-bot', 'https://www.kaggle.com/code/stonet2000/lux-ai-season-1-jupyter-notebook-tutorial', 'https://www.kaggle.com/code/robga/simulations-episode-scraper-match-downloader', 'https://www.kaggle.com/code/aithammadiabdellatif/lux-ai-reinforcement-learning', 'https://www.kaggle.com/code/ilialar/lux-ai-risk-averse-baseline']"
110,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. This dataset is based off of the original Forest Cover Type Prediction competition.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.","Submissions are evaluated on multi-class classification accuracy.
Submission File
For each Id in the test set, you must predict the Cover_Type class. The file should contain a header and have the following format:
Id,Cover_Type
4000000,2
4000001,1
4000001,3
etc.","For this competition, you will be predicting a categorical target based on a number of feature columns given in the data.
The data is synthetically generated by a GAN that was trained on a the data from the Forest Cover Type Prediction. This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.
Please refer to this data page for a detailed explanation of the features.
Files
train.csv - the training data with the target Cover_Type column
test.csv - the test set; you will be predicting the Cover_Type for each row in this file (the target integer class)
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-dec-2021,"['https://www.kaggle.com/code/odins0n/tps-dec-eda-modelling', 'https://www.kaggle.com/code/lordozvlad/tps-dec-fast-feature-importance-with-sklearnex', 'https://www.kaggle.com/code/sergiosaharovskiy/tps-dec-2021-a-complete-guide-eda-pytorch', 'https://www.kaggle.com/code/gulshanmishra/tps-dec-21-tensorflow-nn-feature-engineering', 'https://www.kaggle.com/code/durgancegaur/autoxgb-xgboost-optuna-score-0-95437-dec']"
111,"With nearly 1.4 billion people, India is the second-most populated country in the world. Yet Indian languages, like Hindi and Tamil, are underrepresented on the web. Popular Natural Language Understanding (NLU) models perform worse with Indian languages compared to English, the effects of which lead to subpar experiences in downstream web applications for Indian users. With more attention from the Kaggle community and your novel machine learning solutions, we can help Indian users make the most of the web.
Predicting answers to questions is a common NLU task, but not for Hindi and Tamil. Current progress on multilingual modeling requires a concentrated effort to generate high-quality datasets and modelling improvements. Additionally, for languages that are typically underrepresented in public datasets, it can be difficult to build trustworthy evaluations. We hope the dataset provided for this competition—and additional datasets generated by participants—will enable future machine learning for Indian languages.
In this competition, your goal is to predict answers to real questions about Wikipedia articles. You will use chaii-1, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. It provides a realistic information-seeking task with questions written by native-speaking expert data annotators. You will be provided with a baseline model and inference code to build upon.
If successful, you'll improve upon the baseline performance of NLU models in Indian languages. The results could improve the web experience for many of the nearly 1.4 billion people of India. Additionally, you’ll contribute to multilingual NLP, which could be applied beyond the languages in this competition.
Acknowledgments
Google Research India contributes fundamental advances in computer science and applies their research to big problems impacting India, Google, and communities around the world. The Natural Language Understanding group at Google Research India works specifically with ML to address the unique challenges in the Indian context (such as code mixing in Search, diversity of languages, dialects and accents in Assistant), learning from limited resources and advancing multilingual models.
chaii (Challenge in AI for India) is a Google Research India initiative created with the purpose of sparking AI applications to address some of the pressing problems in India and to find unique ways to address them. Starting with a focus on NLU, chaii hopes to make progress towards multilingual modelling, as language diversity is significantly underserved on the web. Google Research India is working on transformational approaches to healthcare, agriculture and education, and also improving apps and services such as search, assistant and payments, e.g., to deal with challenges arising out of the diversity of languages in India. We also acknowledge the support from the AI4Bharat Team at the Indian Institute of Technology Madras.","The metric in this competition is the word-level Jaccard score. A good description of Jaccard similarity for strings is here.
A Python implementation based on the links above, and matched with the output of the C# implementation on the back end, is provided below.
def jaccard(str1, str2): 
    a = set(str1.lower().split()) 
    b = set(str2.lower().split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
The formula for the overall metric, then, is:
$$
score = \frac{1}{n} \sum_{i=1}^n jaccard( gt_i, dt_i )
$$
where:
$$
n = \textrm{number of documents}
$$
$$
jaccard = \textrm{the function provided above}
$$
$$
gt_i = \textrm{the ith ground truth}
$$
$$
dt_i = \textrm{the ith prediction}
$$
Submission File
For each ID in the test set, you must predict the string that best answers the provided question based on the context. Note that the selected text needs to be quoted and complete to work correctly. Include punctuation, etc. - the above code splits ONLY on whitespace. The file should contain a header and have the following format:
id,PredictionString
8c8ee6504,""1""
3163c22d0,""2 string""
66aae423b,""4 word 6""
722085a7b,""1""
etc.","In this competition, you will be predicting the answers to questions in Hindi and Tamil. The answers are drawn directly (see the Evaluation page for details) from a limited context. We have provided a small number of samples to check your code with. There is also a hidden test set.
All files should be encoded as UTF-8.
Files
train.csv - the training set, containing context, questions, and answers. Also includes the start character of the answer for disambiguation.
test.csv - the test set, containing context and questions.
sample_submission.csv - a sample submission file in the correct format
Columns
id - a unique identifier
context - the text of the Hindi/Tamil sample from which answers should be derived
question - the question, in Hindi/Tamil
answer_text (train only) - the answer to the question (manual annotation) (note: for test, this is what you are attempting to predict)",kaggle competitions download -c chaii-hindi-and-tamil-question-answering,"['https://www.kaggle.com/code/prosenjitgoswami/question-answering-starter-roberta', 'https://www.kaggle.com/code/pranithchowdary/google-chaii-v2', 'https://www.kaggle.com/code/chetangarg365col/big-bird-our-data', 'https://www.kaggle.com/code/yongsunyoon/01-baseline', 'https://www.kaggle.com/code/smartpy/chaii-tutorial2']"
112,"Goal of the Competition
The goal of this competition is to accurately identify starfish in real-time by building an object detection model trained on underwater videos of coral reefs.
Your work will help researchers identify species that are threatening Australia's Great Barrier Reef and take well-informed action to protect the reef for future generations.
Context
Australia's stunningly beautiful Great Barrier Reef is the world’s largest coral reef and home to 1,500 species of fish, 400 species of corals, 130 species of sharks, rays, and a massive variety of other sea life.
Unfortunately, the reef is under threat, in part because of the overpopulation of one particular starfish – the coral-eating crown-of-thorns starfish (or COTS for short). Scientists, tourism operators and reef managers established a large-scale intervention program to control COTS outbreaks to ecologically sustainable levels.
To know where the COTS are, a traditional reef survey method, called ""Manta Tow"", is performed by a snorkel diver. While towed by a boat, they visually assess the reef, stopping to record variables observed every 200m. While generally effective, this method faces clear limitations, including operational scalability, data resolution, reliability, and traceability.
The Great Barrier Reef Foundation established an innovation program to develop new survey and intervention methods to provide a step change in COTS Control. Underwater cameras will collect thousands of reef images and AI technology could drastically improve the efficiency and scale at which reef managers detect and control COTS outbreaks.
To scale up video-based surveying systems, Australia’s national science agency, CSIRO has teamed up with Google to develop innovative machine learning technology that can analyse large image datasets accurately, efficiently, and in near real-time.

This is a Code Competition. Refer to Code Requirements for details.
Citation
Please cite this short paper if you are using this dataset for research purposes.
@misc{liu2021csiro,
      title={The CSIRO Crown-of-Thorn Starfish Detection Dataset}, 
      author={Jiajun Liu and Brano Kusy and Ross Marchant and Brendan Do and Torsten Merz and Joey Crosswell and Andy Steven and Nic Heaney and Karl von Richter and Lachlan Tychsen-Smith and David Ahmedt-Aristizabal and Mohammad Ali Armin and Geoffrey Carlin and Russ Babcock and Peyman Moghadam and Daniel Smith and Tim Davis and Kemal El Moujahid and Martin Wicke and Megha Malpani},
      year={2021},
      eprint={2111.14311},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}","This competition is evaluated on the F2 Score at different intersection over union (IoU) thresholds. The F2 metric weights recall more heavily than precision, as in this case it makes sense to tolerate some false positives in order to ensure very few starfish are missed.
The metric sweeps over IoU thresholds in the range of 0.3 to 0.8 with a step size of 0.05, calculating an F2 score at each threshold. For example, at a threshold of 0.5, a predicted object is considered a ""hit"" if its IoU with a ground truth object is at least 0.5.
A true positive is the first (in confidence order, see details below) submission box in a sample with an IoU greater than the threshold against an unmatched solution box.
Once all submission boxes have been evaluated, any unmatched submission boxes are false positives; any unmatched solution boxes are false negatives.
The final F2 Score is calculated as the mean of the F2 scores at each IoU threshold. Within each IoU threshold the competition metric uses micro averaging; every true positive, false positive, and false negative has equal weight compared to each other true positive, false positive, and false negative.
In your submission, you are also asked to provide a confidence level for each bounding box. Bounding boxes are evaluated in order of their confidence levels. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.
You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks:
Submission Format
import greatbarrierreef
env = greatbarrierreef.make_env()   # initialize the environment
iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission
for (pixel_array, sample_prediction_df) in iter_test:
    sample_prediction_df['annotations'] = '0.5 0 0 100 100'  # make your predictions here
    env.predict(sample_prediction_df)   # register your predictions
The submission format requires a space delimited set of bounding boxes. For example:
0.5 0 0 100 100
indicates that the image has a bounding box with a confidence of 0.5, at x == 0 and y == 0, with a width and height of 100.
0.3 0 0 50 50 0.5 10 10 30 30
would predict two bounding boxes in the image. Each prediction row needs to include all bounding boxes for the image.","In this competition, you will predict the presence and position of crown-of-thorns starfish in sequences of underwater images taken at various times and locations around the Great Barrier Reef. Predictions take the form of a bounding box together with a confidence score for each identified starfish. An image may contain zero or more starfish.
This competition uses a hidden test set that will be served by an API to ensure you evaluate the images in the same order they were recorded within each video. When your submitted notebook is scored, the actual test data (including a sample submission) will be availabe to your notebook.
Files
train/ - Folder containing training set photos of the form video_{video_id}/{video_frame_number}.jpg.
[train/test].csv - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.
video_id - ID number of the video the image was part of. The video ids are not meaningfully ordered.
video_frame - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.
sequence - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.
sequence_frame - The frame number within a given sequence.
image_id - ID code for the image, in the format '{video_id}-{video_frame}'",kaggle competitions download -c tensorflow-great-barrier-reef,"['https://www.kaggle.com/code/awsaf49/great-barrier-reef-yolov5-train', 'https://www.kaggle.com/code/remekkinas/yolox-training-pipeline-cots-dataset-lb-0-507', 'https://www.kaggle.com/code/awsaf49/great-barrier-reef-yolov5-infer', 'https://www.kaggle.com/code/parapapapam/yolox-inference-tracking-on-cots-lb-0-539', 'https://www.kaggle.com/code/remekkinas/yolox-inference-on-kaggle-for-cots-lb-0-507']"
113,"You’ve gotta watch out
You probably will cry
You’ll smile throughout
I'm telling you why
SantaTV’s coming to town
So give them a list,
They’re watching it thrice
They’re gonna find out what’s sugar, what’s spice.
SantaTV’s is coming to town
The elves have lots of movies
Let’s hope they stay awake
Cause Christmas season’s coming soon
And there’s toys for them to make!


People seem to be getting in the Christmas spirit earlier and earlier each year. Decorations appear for sale in stores in the fall, Christmas songs are on the radio in October…
The Elves at the North Pole are starting to recognize this, and need to work as fast as possible to launch their latest holiday offering: SantaTV+! A 24/7 streaming television channel where it’s “Always Christmas, All the Time.” To debut their new station, they’ve decided to kick things off with a made-for-television Christmas movie marathon! They’re excited for the premiere of such movies as 🎅, 🤶, 🦌, 🧝, 🎄, 🎁, and 🎀!
But elves know that just as important as the movie themselves is the order they’ll be aired. So the elves have decided the best way to figure out which order is best is to watch all the movies in every possible combination to see which feels the most Christmas-y.
Your job is to help the elves by giving them the shortest viewing schedules that shows them every combination of movies so they can get SantaTV+ live as soon as possible! The elves have formed three movie-watching teams to lighten the load, so every combination must be seen by at least one of their groups. But they’re also pretty sure they want to kick off the movie marathon with the 🎅 and 🤶 movies back-to-back, so be sure that each group has all the combinations that start with those. And finally, the elves have agreed to two sugar breaks, so you’re allowed to give each group up to two 🌟 wildcards, which will play all the movies at once while they’re snacking, which will help speed things along.
They can’t launch SantaTV+ until all the groups have finished watching - so help give them the most efficient schedule to see every Christmas movie combination, and help them get back to making toys!
Acknowledgments
Photos by Erwan Hesry and Diljaz TM on Unsplash.","Objective
Your objective is to find a set of three strings containing every permutation of the seven symbols 🎅, 🤶, 🦌, 🧝, 🎄, 🎁, and 🎀 as substrings, subject to the following conditions:
Every permutation must be in at least one string.
Each permutation beginning with 🎅🤶 must be in all three strings.
Each string may have up to two wildcards 🌟, which will match any symbol in a permutation. No string of length seven containing more than one wildcard will count as a permutation.
Your score is the length of the longest of the three strings. This is a minimization problem, so lower scores are better.
Example
Let's consider a simplified problem where we only use three symbols 🎅, 🤶, 🦌and no wildcard, and where our solution consists of only two strings.
There are six permutations of these three symbols: 🎅🤶🦌, 🎅🦌🤶, 🤶🎅🦌, 🤶🦌🎅, 🦌🎅🤶, and 🦌🤶🎅. The permutation 🎅🤶🦌 must be a substring of both solution strings while the other five permutations must be in at least one of the strings.
A valid solution for this problem is:
🤶🎅🦌🤶🎅🤶🦌
🎅🤶🦌🎅🤶
which would have a score of 7, the length of string 1.
If we were allowed the use of one wildcard, we could have the solution:
🎅🤶🌟🦌🤶🎅
🎅🤶🦌🎅🤶
with a score of 6. The wildcard can represent different symbols in different permutations.
Submission File
Your solution should consist of three schedules containing permutations of the seven symbols with optional wildcards as described above. The file should contain a header and have the following format:
schedule
🎅🌟🦌🎁🎀🎅🧝🎄🦌🤶🎅🧝🎄🎁🎀...
🤶🎁🎀🎅🧝🎄🦌🤶🎅🧝🎅🦌🤶🌟🎅...
🦌🎅🤶🦌🤶🎅🦌🌟🤶🎅🧝🎄🎁🎀🎄...","Your task in this competition is to create three schedules that contain all permutations of the seven symbols 🎅, 🤶, 🦌, 🧝, 🎄, 🎁, and 🎀, subject to the conditions described on the Evaluation page.
None of the files included here is necessary to solve the problem, but are provided for your convenience.
Files
sample_submission.csv - A submission file in the correct format.
permutations.csv - All permutations of the symbols 🎅, 🤶, 🦌, 🧝, 🎄, 🎁, and 🎀.
distance_matrix.csv - As discussed in the Getting Started notebook, the problem can be solved as type of Traveling Salesman Problem. This file gives the distance between each permutation in the TSP formulation. The entry at index [i, j] is the distance from permutation i to permutation j. This distance function is not symmetric.
wildcards.csv - This file gives a mapping from each seven-letter substring (Factors) containing a wildcard symbol to the Permutation it can represent.",kaggle competitions download -c santa-2021,"['https://www.kaggle.com/code/cdeotte/santa-2021-tsp-baseline-2500', 'https://www.kaggle.com/code/ryanholbrook/getting-started-with-santa-2021', 'https://www.kaggle.com/code/ilialar/santa-2021-baseline-and-optimization-ideas', 'https://www.kaggle.com/code/yosshi999/wildcard-postprocessing-using-dynamic-programming', 'https://www.kaggle.com/code/kostyaatarik/colored-traveling-salesman-problem']"
114,"Welcome to the fourth Landmark Retrieval competition! This year, we introduce a lot more diversity in the challenge’s test images in order to measure global landmark retrieval performance in a fairer manner. And following last year’s success, we set this up as a code competition.
Image retrieval is a central problem in computer vision, relevant to many applications. The problem is usually posed as follows: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph.
In this competition, the developed models are expected to retrieve relevant database images to a given query image (i.e., the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the Landmark Recognition Challenge 2021. Both challenges will be discussed at the Instance-Level Recognition workshop in ICCV 21.
In contrast to previous editions of this challenge (2018, 2019, and 2020), this year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring.
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated according to mean Average Precision @ 100, commonly referred to as (mAP@100):
$$mAP@100 = \frac{1}{Q} \sum_{q=1}^{Q} \frac{1}{min(m_q, 100)} \sum_{k=1}^{min(n_q,100)} P_q(k) rel_q(k)$$
where:
\(Q\) is the number of query images
\(m_q\) is the number of index images containing a landmark in common with the query image \(q\). Note that (m_q \gt 0).
\(n_q\) is the number of predictions made by the system for query \(q\)
\(P_q(k)\) is the precision at rank \(k\) for the \(q\)-th query
\(rel_q(k)\) denotes the relevance of prediciton \(k\) for the \(q\)-th query: it’s 1 if the \(k\)-th prediction is correct, and 0 otherwise
Submission File
For each query id in the test set, you must predict a space-delimited list of index images that depict the same landmarks as the query. The list should be sorted, such that the first index image is considered the most relevant one, and the last the least relevant one. The file should contain a header and have the following format:
id,images
000088da12d664db,0370c4c856f096e8 766677ab964f4311 e3ae4dcee8133159...
etc.","In this competition, you are asked to develop models that can efficiently retrieve landmark images from a large database. The training set is available in the train/ folder, with corresponding landmark labels in train.csv. The query images are listed in the test/ folder, while the ""index"" images from which you are retrieving are listed in index/. Each image has a unique id. Since there are a large number of images, each image is placed within three subfolders according to the first three characters of the image id (i.e. image abcdef.jpg is placed in a/b/c/abcdef.jpg).
This is a synchronous rerun code competition. The provided test set is a representative set of files to demonstrate the format of the private test set. When you submit your notebook, Kaggle will rerun your code on the private dataset.
Dataset
For this year, we introduce new test and index sets which are sampled from many countries, increasing the diversity in worldwide representation. See our paper for more details.
The training data for this competition comes from a cleaned version of the Google Landmarks Dataset v2 (GLDv2), which is available here. Please refer to the paper for more details on the dataset construction and how to use it. See this code example for an example of a pretrained model.
If you make use of this dataset in your research, please consider citing:
""Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval"", T. Weyand, A. Araujo, B. Cao and J. Sim, Proc. CVPR'20",kaggle competitions download -c landmark-retrieval-2021,"['https://www.kaggle.com/code/hidehisaarai1213/glret21-efficientnetb0-baseline-training', 'https://www.kaggle.com/code/ks2019/stratified-tfrecords-training-pipeline', 'https://www.kaggle.com/code/hidehisaarai1213/glret21-efficientnetb0-baseline-inference', 'https://www.kaggle.com/code/debarshichanda/pytorch-w-b-glret-2021', 'https://www.kaggle.com/code/muhammadimran112233/eda-google-landmark-retrieval-2021']"
115,"In Jigsaw's fourth Kaggle competition, we return to the Wikipedia Talk page comments featured in our first Kaggle competition. When we ask human judges to look at individual comments, without any context, to decide which ones are toxic and which ones are innocuous, it is rarely an easy task. In addition, each individual may have their own bar for toxicity. We've tried to work around this by aggregating the decisions with a majority vote. But many researchers have rightly pointed out that this discards meaningful information.
😄 🙂 😐 😕 😞
A much easier task is to ask individuals which of two comments they find more toxic. But if both comments are non-toxic, people will often select randomly. When one comment is obviously the correct choice, the inter-annotator agreement results are much higher.
In this competition, we will be asking you to score a set of about fourteen thousand comments. Pairs of comments were presented to expert raters, who marked one of two comments more harmful — each according to their own notion of toxicity. In this contest, when you provide scores for comments, they will be compared with several hundred thousand rankings. Your average agreement with the raters will determine your individual score. In this way, we hope to focus on ranking the severity of comment toxicity from innocuous to outrageous, where the middle matters as much as the extremes.
Can you build a model that produces scores that rank each pair of comments the same way as our professional raters?
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.
Related Work
The paper ""Ruddit: Norms of Offensiveness for English Reddit Comments"" by Hada et al. introduced a similar dataset that involved tuples of four sentences that were marked with best-worst scoring, and this data may be directly useful for building models.
We also note ""Constructing Interval Variables via Faceted Rasch Measurement and Multitask Deep Learning: a Hate Speech Application"" by Kennedy et al. which compares a variety of different rating schemes and argues that binary classification as typically done in NLP tasks discards valuable information. Combining data from multiple sources, even with different annotation guidelines, may be essential for success in this competition.
Resources
The English language resources from our first Kaggle competition, and our second Kaggle competition, which are both available in the TensorFlow datasets Wikipedia Toxicity Subtypes and Civil Comments can be used to build models.
One example of a starting point is the open source UnitaryAI model.
Google Jigsaw
Google's Jigsaw team explores threats to open societies and builds technology that inspires scalable solutions. One Jigsaw product is PerspectiveAPI which is used by publishers and platforms worldwide as part of their overall moderation strategy.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on Average Agreement with Annotators. For the ground truth, annotators were shown two comments and asked to identify which of the two was more toxic. Pairs of comments can be, and often are, rated by more than one annotator, and may have been ordered differently by different annotators.
For each of the approximately 200,000 pair ratings in the ground truth test data, we use your predicted toxicity score to rank the comment pair. The pair receives a 1 if this ranking matches the annotator ranking, or 0 if it does not match.
The final score is the average across all the pair evaluations.
Please note the following:
score is not constrained to any numeric range (e.g., you can predict [0, 1] or [-999, 999]).
There is no tie breaking; tied comment scores will always be evaluated as 0. You could consider using something like scipy.stats.rankdata to force unique value.
Submission File
For each comment_id found in the comments_to_score.csv file, you must predict the toxic severity score associated with the comment text. The submission file should contain a header and have the following format:
comment_id,score
114890,0.43
732895,0.98
1139051,0.27
etc.","In this competition you will be ranking comments in order of severity of toxicity. You are given a list of comments, and each comment should be scored according to their relative toxicity. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity.
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.
Note, there is no training data for this competition. You can refer to previous Jigsaw competitions for data that might be useful to train models. But note that the task of previous competitions has been to predict the probability that a comment was toxic, rather than the degree or severity of a comment's toxicity.
Toxic Comment Classification Challenge
Jigsaw Unintended Bias in Toxicity Classification
Jigsaw Multilingual Toxic Comment Classification
While we don't include training data, we do provide a set of paired toxicity rankings that can be used to validate models.
Files
comments_to_score.csv - for each comment text in this file, your task is to predict a score that represents the relative toxic severity of the comment. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity; scores are relative, and not constrained to a certain range of values. the rerun version of this file has ~14k comments that will be scored by your submitted model.",kaggle competitions download -c jigsaw-toxic-severity-rating,"['https://www.kaggle.com/code/debarshichanda/pytorch-w-b-jigsaw-starter', 'https://www.kaggle.com/code/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768', 'https://www.kaggle.com/code/nkitgupta/text-representations', 'https://www.kaggle.com/code/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-train', 'https://www.kaggle.com/code/samarthagarwal23/mega-b-ridge-to-the-top-lb-0-85']"
116,"Welcome to the fourth Landmark Recognition competition! This year, we introduce a lot more diversity in the challenge’s test images in order to measure global landmark recognition performance in a fairer manner. And following last year’s success, we set this up as a code competition.
Have you ever gone through your vacation photos and asked yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.
Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.
This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring. This is similar to the 2020 version of the competition. In older editions (2018 and 2019), submissions had been handled by uploading prediction files to the system.
This challenge is organized in conjunction with the Landmark Retrieval Challenge 2021. Both challenges will be discussed at the Instance-Level Recognition workshop in ICCV 21.
This is a Code Competition. Refer to Code Requirements for details.
Cover image credits: Muhammad Mahdi Karim. The original is available on Wikimedia here. License: GNU Free Documentation License.","Submissions are evaluated using Global Average Precision (GAP) at (k), where (k=1). This metric is also known as micro Average Precision (\mu AP), as per references 1 and 2 below. It works as follows:
For each test image, you will predict one landmark label and a corresponding confidence score. The evaluation treats each prediction as an individual data point in a long list of predictions, sorted in descending order by confidence scores, and computes the Average Precision based on this list.
If a submission has (N) predictions, formatted as label/confidence pairs, sorted in descending order by their confidence scores, then the Global Average Precision is computed as:
$$GAP = \frac{1}{M}\sum_{i=1}^N P(i) rel(i)$$
where:
(N) is the total number of predictions returned by the system, across all queries
(M) is the total number of queries with at least one landmark from the training set visible in it. Note that some queries may not depict landmarks.
(P(i)) is the precision at rank (i)
(rel(i)) denotes the relevance of prediciton (i): it’s 1 if the (i)-th prediction is correct, and 0 otherwise
References:
1) F. Perronnin, Y. Liu, and J.-M. Renders, ""A Family of Contextual Measures of Similarity between Distributions with Application to Image Retrieval,"" Proc. CVPR'09
2) T. Weyand, A. Araujo, B. Cao and J. Sim, ""Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval,"" Proc. CVPR'20
Submission File
For each id in the test set, you can predict at most one landmark and its corresponding confidence score. Some query images may contain no landmarks. You may decide not to predict any result for a given query, by submitting an empty prediction. The submission file should contain a header and have the following format — note that larger scores denote more confident matches:
id,landmarks
000088da12d664db,8815 0.03
0001623c6d808702,
0001bbb682d45002,5328 0.5
etc.","In this competition, you are asked to take test images and recognize which landmarks (if any) are depicted in them. The training set is available in the train/ folder, with corresponding landmark labels in train.csv. The test set images are listed in the test/ folder. Each image has a unique id. Since there are a large number of images, each image is placed within three subfolders according to the first three characters of the image id (i.e. image abcdef.jpg is placed in a/b/c/abcdef.jpg).
This is a synchronous rerun code competition. The provided test set is a representative set of files to demonstrate the format of the private test set. When you submit your notebook, Kaggle will rerun your code on the private dataset. Additionally, this competition also has two unique characteristics:
To facilitate recognition-by-retrieval approaches, the private training set contains only a 100k subset of the total public training set. This 100k subset contains all of the training set images associated with the landmarks in the private test set. You may still attach the full training set as an external data set if you wish.
Submissions are given 12 hours to run, as compared to the site-wide session limit of 9 hours. While your commit must still finish in the 9 hour limit in order to be eligible to submit, the rerun may take the full 12 hours.
Dataset
For this year, we introduce a new test set which is sampled from many countries, increasing the diversity in worldwide representation. See our paper for more details.
The training data for this competition comes from a cleaned version of the Google Landmarks Dataset v2 (GLDv2), which is available . Please refer to the for more details on the dataset construction and how to use it. See for an example of a pretrained model.",kaggle competitions download -c landmark-recognition-2021,"['https://www.kaggle.com/code/ks2019/stratified-tfrecords-training-pipeline', 'https://www.kaggle.com/code/lucamtb/glrecogn21-efficientnetb0-baseline-inference', 'https://www.kaggle.com/code/muhammadimran112233/google-landmark-retrieval-2021', 'https://www.kaggle.com/code/ysthehurricane/landmark-recognition-densenet201-transfer-learning', 'https://www.kaggle.com/code/takedarts/inference-and-submission-pytorch-resnet34']"
117,"Over $40 billion worth of cryptocurrencies are traded every day. They are among the most popular assets for speculation and investment, yet have proven wildly volatile. Fast-fluctuating prices have made millionaires of a lucky few, and delivered crushing losses to others. Could some of these price movements have been predicted in advance?
In this competition, you'll use your machine learning expertise to forecast short term returns in 14 popular cryptocurrencies. We have amassed a dataset of millions of rows of high-frequency market data dating back to 2018 which you can use to build your model. Once the submission deadline has passed, your final score will be calculated over the following 3 months using live crypto data as it is collected.
The simultaneous activity of thousands of traders ensures that most signals will be transitory, persistent alpha will be exceptionally difficult to find, and the danger of overfitting will be considerable. In addition, since 2018, interest in the cryptomarket has exploded, so the volatility and correlation structure in our data are likely to be highly non-stationary. The successful contestant will pay careful attention to these considerations, and in the process gain valuable insight into the art and science of financial forecasting.
G-Research is Europe’s leading quantitative finance research firm. We have long explored the extent of market prediction possibilities, making use of machine learning, big data, and some of the most advanced technology available. Specializing in data science and AI education for workforces, Cambridge Spark is partnering with G-Research for this competition. Watch our introduction to the competition below:

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on a weighted version of the Pearson correlation coefficient. You can find additional details in the 'Prediction Details and Evaluation' section of this tutorial notebook.
You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks:
import gresearch_crypto
env = gresearch_crypto.make_env()   # initialize the environment
iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission
for (test_df, sample_prediction_df) in iter_test:
    sample_prediction_df['Target'] = 0  # make your predictions here
    env.predict(sample_prediction_df)   # register your predictions
A more detailed introduction to the API is available here.
You will get an error if you submission includes nulls or infinities.","This dataset contains information on historic trades for several cryptoassets, such as Bitcoin and Ethereum. Your challenge is to predict their future returns.
As historic cryptocurrency prices are not confidential this will be a forecasting competition using the time series API. Furthermore the public leaderboard targets are publicly available and are provided as part of the competition dataset. Expect to see many people submitting perfect submissions for fun. Accordingly, the active phase public leaderboard for this competition was not meaningful and was only provided as a convenience for anyone who wants to test their code. The forecasting phase public leaderboard and final private leaderboard will be determined using real market data gathered after the submission period closes.
Files
train.csv - The training set
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.",kaggle competitions download -c g-research-crypto-forecasting,"['https://www.kaggle.com/code/cstein06/tutorial-to-the-g-research-crypto-competition', 'https://www.kaggle.com/code/julian3833/g-research-starter-lgbm-pipeline', 'https://www.kaggle.com/code/odins0n/exploring-time-series-plots-beginners-guide', 'https://www.kaggle.com/code/iamleonie/time-series-interpreting-acf-and-pacf', 'https://www.kaggle.com/code/odins0n/g-research-plots-eda']"
118,"The National Football League (NFL) and Amazon Web Services (AWS) are teaming up to develop the best sports injury surveillance and mitigation program. In previous competitions, Kaggle has helped detect helmet impacts. As a next step, the NFL wants to assign specific players to each helmet, which would help accurately identify each player's “exposures” throughout a football play.
Currently, the NFL manually annotates a subset of plays each year to determine a sample of exposures for each player. To expand this program, the current player assignment requires a field map to determine player locations. The NFL is interested in matching this model's accuracy without the need for the mapping step. The league is calling on Kagglers to invent a better way to identify individual players.
The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit www.NFL.com/PlayerHealthandSafety.
In this competition, you’ll identify and assign football players’ helmets from video footage. In particular, you'll create algorithms capable of assigning detected helmet impacts to correct players via tracking information. Successful submissions should aim for 90% accuracy.
If successful, you'll support the NFL in its efforts to efficiently improve player safety. If the league no longer has to manually label each exposure, it would dramatically increase the speed and scale at which they could answer complex research questions related to helmet impact. Automatic player detection would also allow the NFL to back-calculate historic exposure trends, allowing for deeper insights into how to mitigate them in the future.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated using a weighted accuracy metric for all active players’ helmets in the provided videos. Helmet boxes associated with a definitive impact will be weighted 1000x more than helmets not involved in an impact. Ground truth helmet boxes will be paired to the submission box that has the the largest Intersection over Union (IoU) from the submission. The IoU between the ground truth box and submission box must meet a minimum IoU threshold of 0.35. The IoU of a proposed bounding box and a ground truth bounding box is calculated as:
$$ IoU (A,B) = \frac{A \cap B}{ A \cup B} $$
The metric will assign each ground truth helmet to a single submitted box based on IoU.
Accuracy will be calculated for all ground truth helmet boxes excluding sideline players (labels H00 and V00). Helmet boxes are counted once for each frame they appear in a video. Helmet boxes at the moment of a definitive impact are given a weight of 1000. All other helmet boxes are given a weight of 1.
$$ Weighted Accuracy = \frac{TotalCorrect_{nonimp}+(TotalCorrect_{imp} * 1000)}{TotalHelmets_{nonimp}+(TotalHelmets_{imp} * 1000)} $$
Where:
TotalCorrect_nonimp is the number of correctly assigned non-definitive impact helmet boxes.
TotalCorrect_imp is the number of correctly assigned definitive impact helmet boxes.
TotalHelmets_nonimp is the total number of non-definitive helmets boxes.
TotalHelmets_imp is the total number of definitive helmet impact boxes.
Submissions must also meet the following requirements:
No more than 22 helmet predictions per video frame. Predictions should not be submitted for players not actively participating in the play who are on the sideline. Sideline players are labeled “H00” and “V00” in the training dataset.
A players’ helmet label must only be predicted once per video frame, i.e. no duplicated labels per frame.
All submitted helmet boxes must be unique per video frame.
Boxes must be within video area:
- top and left must be >= 0
- The sum of left and width must be <= 1280
- The sum of top and height must be must be <= 720
Submission File
For each video_frame in the test set, you must predict a bounding box left, width, top, height, and predicted label of the assigned player. The file should contain a header and have the following format:
video_frame,left,width,top,height,label
57590_003607_Endzone_1,1,1,1,1,H1
57590_003607_Sideline_1,1,1,1,1,V59
57595_001252_Endzone_1,1,1,1,1,V52","In this competition, you are tasked with assigning the correct player from game footage. Each play has two associated videos, showing a sideline and endzone view, and the videos are aligned so that frames correspond between the videos. The training set videos are in train/ with corresponding labels in train_labels.csv, while the videos for which you must predict are in the test/ folder.
To aid with helmet detection, you are also provided an ancillary dataset of images showing helmets with labeled bounding boxes. These files are located in images and the bounding boxes in image_labels.csv.
This year we are also providing baseline helmet detection boxes for the training and test set. train_baseline_helmets.csv is the output from a baseline helmet detection model which was trained on the images and labels from the images folder.
train_player_tracking.csv provides 10 Hz tracking data for each player on the field during the provided plays.
This is a code competition. When you submit, your model will be rerun on a set of 15 unseen plays located in a holdout test set. The publicly provided test videos are simply a set of mock plays (copied from the training set) which are not used in scoring.
The associated test_baseline_helmets.csv and test_player_tracking.csv are available to your model when submitting.
Note: the dataset provided for this competition has been carefully designed for the purposes of training computer vision models and therefore contains plays that have much higher incidence of helmet impacts than is normal. This dataset should not be used to make inferences about the incidence of helmet impact rates during football games, as it is not a representative sample of those rates.",kaggle competitions download -c nfl-health-and-safety-helmet-assignment,"['https://www.kaggle.com/code/robikscube/nfl-helmet-assignment-getting-started-guide', 'https://www.kaggle.com/code/its7171/nfl-baseline-simple-helmet-mapping', 'https://www.kaggle.com/code/robikscube/helper-code-helmet-mapping-deepsort', 'https://www.kaggle.com/code/kmat2019/nfl-1stplace-inference', 'https://www.kaggle.com/code/firefliesqn/tuning-deepsort-helmet-mapping']"
119,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting identifying spam emails via various extracted features from the email. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
id,target
600000,0.5
600001,0.9
600002,0.1
etc.","For this competition, you will be predicting a binary target based on 100 feature columns given in the data. All columns are continuous.
The data is synthetically generated by a GAN that was trained on a real-world dataset used to identify spam emails via various extracted features from the email.
Files
train.csv - the training data with the target column
test.csv - the test set; you will be predicting the target for each row in this file (the probability of the binary target)
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-nov-2021,"['https://www.kaggle.com/code/lordozvlad/tps-nov-logistic-regression-with-pytorch', 'https://www.kaggle.com/code/hadeux/tps-nov-xgboost-baseline', 'https://www.kaggle.com/code/lordozvlad/fast-feature-importance-using-scikit-learn-intelex', 'https://www.kaggle.com/code/sergiosaharovskiy/tps-nov-2021-a-complete-guide', 'https://www.kaggle.com/code/usharengaraju/differentialprivacy-using-diffprivlib-w-b']"
120,"Nelson Mandela believed education was the most powerful weapon to change the world. But not every student has equal opportunities to learn. Effective policies and plans need to be enacted in order to make education more equitable—and perhaps your innovative data analysis will help reveal the solution.
Current research shows educational outcomes are far from equitable. The imbalance was exacerbated by the COVID-19 pandemic. There's an urgent need to better understand and measure the scope and impact of the pandemic on these inequities.
Education technology company LearnPlatform was founded in 2014 with a mission to expand equitable access to education technology for all students and teachers. LearnPlatform’s comprehensive edtech effectiveness system is used by districts and states to continuously improve the safety, equity, and effectiveness of their educational technology. LearnPlatform does so by generating an evidence basis for what’s working and enacting it to benefit students, teachers, and budgets.
In this analytics competition, you’ll work to uncover trends in digital learning. Accomplish this with data analysis about how engagement with digital learning relates to factors like district demographics, broadband access, and state/national level policies and events. Then, submit a Kaggle Notebook to propose your best solution to these educational inequities.
Your submissions will inform policies and practices that close the digital divide. With a better understanding of digital learning trends, you may help reverse the long-term learning loss among America’s most vulnerable, making education more equitable.
Problem Statement
The COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America’s most vulnerable learners continue to grow.
Challenge
We challenge the Kaggle community to explore (1) the state of digital learning in 2020 and (2) how the engagement of digital learning relates to factors such as district demographics, broadband access, and state/national level policies and events.
We encourage you to guide the analysis with questions that are related to the themes that are described above (in bold font). Below are some examples of questions that relate to our problem statement:
What is the picture of digital connectivity and engagement in 2020?
What is the effect of the COVID-19 pandemic on online and distance learning, and how might this also evolve in the future?
How does student engagement with different types of education technology change over the course of the pandemic?
How does student engagement with online learning platforms relate to different geography? Demographic context (e.g., race/ethnicity, ESL, learning disability)? Learning context? Socioeconomic status?
Do certain state interventions, practices or policies (e.g., stimulus, reopening, eviction moratorium) correlate with the increase or decrease online engagement?","What is the state of digital learning in 2020? And how does the engagement of digital learning relate to factors such as district demographics, broadband access, and state/national level policies and events?
This is an Analytics competition where your task is to create a Notebook that best addresses the Evaluation criteria below. Submissions should be shared directly as specified under Submission Instructions with host and will be judged by the LearnPlatform team based on how well they address:
Clarity (5 pts)
Did the author present a clear thread of questions or themes motivating their analysis?
Did the author document why/what/how a set of methods was chosen and used for their analysis?
Is the notebook documented in a way that is easily reproducible (e.g., code, additional data sources, citations)?
Does the notebook contain clear data visualizations that help effectively communicate the author’s findings to both experts and non-experts?
Accuracy (5 pts)
Did the author process the data (e.g., merging) and/or additional data sources accurately?
Is the methodology used in the analysis appropriate and reasonable?
Are the interpretations based on the analysis and visualization reasonable and convincing?
Creativity (5 pts)
Does the notebook help the reader learn something new or challenge the reader to think in a new way?
Does the notebook leverage novel methods and/or visualizations that help reveal insights from data and/or communicate findings?
Did the author utilize additional public data sources in their analysis?
A more detailed problem statement and challenge description can be found on the Description page.","We have provided a set of daily edtech engagement data from over 200 school districts in 2020, and we encourage you to leverage other publicly available data sources in your analysis. We include three basic sets of files to help you get started:
The engagement_ data folder is based on LearnPlatform’s Student Chrome Extension. The extension collects page load events of over 10K education technology products in our product library, including websites, apps, web apps, software programs, extensions, ebooks, hardwares, and services used in educational institutions. The engagement data have been aggregated at school district level, and each file represents data from one school district.
The products_info.csv file includes information about the characteristics of the top 372 products with most users in 2020.
The districts_info.csv file includes information about the characteristics of school districts, including data from NCES and FCC.
The definitions of each column in the three data sets are detailed in the README file.
In addition to the files provided, we encourage you to use other public data sources such as COVID-19 US State Policy database, KIDS Count, and KFF.",kaggle competitions download -c learnplatform-covid19-impact-on-digital-learning,"['https://www.kaggle.com/code/ruchi798/covid-19-impact-on-digital-learning-eda-w-b', 'https://www.kaggle.com/code/muhammadimran112233/covid-19-impact-on-digital-learning', 'https://www.kaggle.com/code/iamleonie/how-to-approach-analytics-challenges', 'https://www.kaggle.com/code/andradaolteanu/covid-19-can-we-learn-in-a-pandemic', 'https://www.kaggle.com/code/michau96/most-popular-tools-in-2020-digital-learning']"
121,"Neurological disorders, including neurodegenerative diseases such as Alzheimer's and brain tumors, are a leading cause of death and disability across the globe. However, it is hard to quantify how well these deadly disorders respond to treatment. One accepted method is to review neuronal cells via light microscopy, which is both accessible and non-invasive. Unfortunately, segmenting individual neuronal cells in microscopic images can be challenging and time-intensive. Accurate instance segmentation of these cells—with the help of computer vision—could lead to new and effective drug discoveries to treat the millions of people with these disorders.
Current solutions have limited accuracy for neuronal cells in particular. In internal studies to develop cell instance segmentation models, the neuroblastoma cell line SH-SY5Y consistently exhibits the lowest precision scores out of eight different cancer cell types tested. This could be because neuronal cells have a very unique, irregular and concave morphology associated with them, making them challenging to segment with commonly used mask heads.
Sartorius is a partner of the life science research and the biopharmaceutical industry. They empower scientists and engineers to simplify and accelerate progress in life science and bioprocessing, enabling the development of new and better therapies and more affordable medicine. They're a magnet and dynamic platform for pioneers and leading experts in the field. They bring creative minds together for a common goal: technological breakthroughs that lead to better health for more people.
In this competition, you’ll detect and delineate distinct objects of interest in biological images depicting neuronal cell types commonly used in the study of neurological disorders. More specifically, you'll use phase contrast microscopy images to train and test your model for instance segmentation of neuronal cells. Successful models will do this with a high level of accuracy.
If successful, you'll help further research in neurobiology thanks to the collection of robust quantitative data. Researchers may be able to use this to more easily measure the effects of disease and treatment conditions on neuronal cells. As a result, new drugs could be discovered to treat the millions of people with these leading causes of death and disability.

This is a Code Competition. Refer to Code Requirements for details.","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.
At each threshold value \(t\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.
Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed
and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.
The file should contain a header and have the following format. Each row in your submission represents a single predicted nucleus segmentation for the given ImageId.
id,predicted  
0114f484a16c152baa2d82fdd43740880a762c93f436c8988ac461c5c9dbe7d5,1 1  
0999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,1 1  
0999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,2 3 8 9  
etc...
Submission files may take several minutes to process due to the size.",,,
122,,,,,
123,,,,,
124,,,,,
125,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is based on a real dataset, but has synthetic-generated aspects to it. The original dataset deals with predicting air pollution in a city via various input sensor values (e.g., a time series).
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.","Submissions are evaluated using the mean column-wise root mean squared logarithmic error.
The RMSLE for a single column calculated as:
$$ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },$$
where:
\( n \) is the total number of observations
\( p_i \) is your prediction
\( a_i \) is the actual value
\( \log(x) \) is the natural logarithm of \( x \)
The final score is the mean of the RMSLE over all columns, in this case, 3.
Submission File
For each ID in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:
date_time,target_carbon_monoxide,target_benzene,target_nitrogen_oxides
2011-01-01 01:00:00,2.0,10.0,300.0
2011-01-01 02:00:00,2.0,10.0,300.0
2011-01-01 03:00:00,2.0,10.0,300.0
etc.","In this competition you are predicting the values of air pollution measurements over time, based on basic weather information (temperature and humidity) and the input values of 5 sensors.
The three target values to you to predict are: target_carbon_monoxide, target_benzene, and target_nitrogen_oxides
Files
train.csv - the training data, including the weather data, sensor data, and values for the 3 targets
test.csv - the same format as train.csv, but without the target value; your task is to predict the value for each of these targets.
sample_submission.csv - a sample submission file in the correct format.",kaggle competitions download -c tabular-playground-series-jul-2021,"['https://www.kaggle.com/code/bextuychiev/7-coolest-packages-top-kagglers-are-using', 'https://www.kaggle.com/code/alexryzhkov/tps-lightautoml-baseline-with-pseudolabels', 'https://www.kaggle.com/code/bextuychiev/every-pandas-function-to-manipulate-time-series', 'https://www.kaggle.com/code/bextuychiev/25-pandas-functions-you-didn-t-know-existed', 'https://www.kaggle.com/code/junhyeok99/automl-pycaret']"
126,"Goal of the Competition
In this “getting started” competition, you’ll use time-series forecasting to forecast store sales on data from Corporación Favorita, a large Ecuadorian-based grocery retailer.
Specifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores. You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.
Get Started
We highly recommend the Time Series course, which walks you through how to make your first submission. The lessons in this course are inspired by winning solutions from past Kaggle time series forecasting competitions.
Context
Forecasts aren’t just for meteorologists. Governments forecast economic growth. Scientists attempt to predict the future population. And businesses forecast product demand—a common task of professional data scientists. Forecasts are especially relevant to brick-and-mortar grocery stores, which must dance delicately with how much inventory to buy. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leading to lost revenue and upset customers. More accurate forecasting, thanks to machine learning, could help ensure retailers please customers by having just enough of the right products at the right time.
Current subjective forecasting methods for retail have little data to back them up and are unlikely to be automated. The problem becomes even more complex as retailers add new locations with unique needs, new products, ever-transitioning seasonal tastes, and unpredictable product marketing.
Potential Impact
If successful, you'll have flexed some new skills in a real world example. For grocery stores, more accurate forecasting can decrease food waste related to overstocking and improve customer satisfaction. The results of this ongoing competition, over time, might even ensure your local store has exactly what you need the next time you shop.","The evaluation metric for this competition is Root Mean Squared Logarithmic Error.
The RMSLE is calculated as:
[\sqrt{ \frac{1}{n} \sum_{i=1}^n \left(\log (1 + \hat{y}_i) - \log (1 + y_i)\right)^2}]
where:
\(n\) is the total number of instances,
\(\hat{y}_i\) is the predicted value of the target for instance (i),
\(y_i\) is the actual value of the target for instance (i), and,
\(\log\) is the natural logarithm.
Submission File
For each id in the test set, you must predict a value for the sales variable. The file should contain a header and have the following format:
id,sales
3000888,0.0
3000889,0.0
3000890,0.0
3000891,0.0
3000892,0.0
etc.","In this competition, you will predict sales for the thousands of product families sold at Favorita stores located in Ecuador. The training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers. Additional files include supplementary information that may be useful in building your models.
File Descriptions and Data Field Information
train.csv
The training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.
store_nbr identifies the store at which the products are sold.
family identifies the type of product sold.
sales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).
onpromotion gives the total number of items in a product family that were being promoted at a store at a given date.
test.csv
The test data, having the same features as the training data. You will predict the target sales for the dates in this file.
The dates in the test data are for the 15 days after the last date in the training data.",kaggle competitions download -c store-sales-time-series-forecasting,"['https://www.kaggle.com/code/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide', 'https://www.kaggle.com/code/kashishrastogi/store-sales-analysis-time-serie', 'https://www.kaggle.com/code/odins0n/exploring-time-series-plots-beginners-guide', 'https://www.kaggle.com/code/maricinnamon/store-sales-time-series-forecast-visualization', 'https://www.kaggle.com/code/kartushovdanil/econometrics-is-all-you-need']"
127,"It's been said that teamwork makes the dream work. This couldn't be truer for the breakthrough discovery of gravitational waves (GW), signals from colliding binary black holes in 2015. It required the collaboration of experts in physics, mathematics, information science, and computing. GW signals have led researchers to observe a new population of massive, stellar-origin black holes, to unlock the mysteries of neutron star mergers, and to measure the expansion of the Universe. These signals are unimaginably tiny ripples in the fabric of space-time and even though the global network of GW detectors are some of the most sensitive instruments on the planet, the signals are buried in detector noise. Analysis of GW data and the detection of these signals is a crucial mission for the growing global network of increasingly sensitive GW detectors. These challenges in data analysis and noise characterization could be solved with the help of data science.
As with the multi-disciplined approach to the discovery of GWs, additional expertise will be needed to further GW research. In particular, social and natural sciences have taken an interest in machine learning, deep learning, classification problems, data mining, and visualization to develop new techniques and algorithms to efficiently handle complex and massive data sets. The increase in computing power and the development of innovative techniques for the rapid analysis of data will be vital to the exciting new field of GW Astronomy. Potential outcomes may include increased sensitivity to GW signals, application to control and feedback systems for next-generation detectors, noise removal, data conditioning tools, and signal characterization.
G2Net is a network of Gravitational Wave, Geophysics and Machine Learning. Via an Action from COST (European Cooperation in Science and Technology), a funding agency for research and innovation networks, G2Net aims to create a broad network of scientists. From four different areas of expertise, namely GW physics, Geophysics, Computing Science and Robotics, these scientists have agreed on a common goal of tackling challenges in data analysis and noise characterization for GW detectors.
In this competition, you’ll aim to detect GW signals from the mergers of binary black holes. Specifically, you'll build a model to analyze simulated GW time-series data from a network of Earth-based detectors.

The series of images above were taken from the 2015 paper announcing the discovery of gravitational waves from a pair of merging black holes.
If successful, you'll play a part in solving a crucial mission in the exciting new field of GW science. With the development of new algorithms, scientists will have a better handle on the potential power of the data science community and their innovative approaches to data analysis. Moreover, it will enable closer interaction between computer science and physics, which could benefit both disciplines. Your participation can further this collaboration and the help advance this breakthrough discovery.
Acknowledgments
We acknowledge support from the LIGO-Virgo-Kagra Collaboration of which the hosts are members. Specifically we acknowledge the use of the software resource lalsuite.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
id,target
00005bced6,0.5
0000806717,0.5
0000ef4fe1,0.5
etc.","In this competition you are provided with a training set of time series data containing simulated gravitational wave measurements from a network of 3 gravitational wave interferometers (LIGO Hanford, LIGO Livingston, and Virgo). Each time series contains either detector noise or detector noise plus a simulated gravitational wave signal. The task is to identify when a signal is present in the data (target=1).
The parameters that determine the exact form of a binary black hole waveform are the masses, sky location, distance, black hole spins, binary orientation angle, gravitational wave polarisation, time of arrival, and phase at coalescence (merger). These parameters (15 in total) have been randomised according to astrophysically motivated prior distributions and used to generate the simulated signals present in the data, but are not provided as part of the competition data.
Each data sample (npy file) contains 3 time series (1 for each detector) and each spans 2 sec and is sampled at 2,048 Hz.
The integrated signal-to noise ratio (SNR) is classically the most informative measure of how detectable a signal is and a typical level of detectability is when this integrated SNR exceeds ~8. This shouldn't confused with the instantaneous SNR - the factor by which the signal rises above the noise - and in nearly all cases the (unlike the first gravitational wave detection GW150914) these signals are not visible by eye in the time series.
Files
train/ - the training set files, one npy file per observation; labels are provided in a files shown below
test/ - the test set files; you must predict the probability that the observation contains a gravitational wave",kaggle competitions download -c g2net-gravitational-wave-detection,"['https://www.kaggle.com/code/ihelon/g2net-eda-and-modeling', 'https://www.kaggle.com/code/yasufuminakama/g2net-efficientnet-b7-baseline-training', 'https://www.kaggle.com/code/headsortails/when-stars-collide-g2net-eda', 'https://www.kaggle.com/code/allunia/signal-where-are-you', 'https://www.kaggle.com/code/andradaolteanu/g2net-searching-the-sky-pytorch-effnet-w-meta']"
128,"Volatility is one of the most prominent terms you’ll hear on any trading floor – and for good reason. In financial markets, volatility captures the amount of fluctuation in prices. High volatility is associated to periods of market turbulence and to large price swings, while low volatility describes more calm and quiet markets. For trading firms like Optiver, accurately predicting volatility is essential for the trading of options, whose price is directly related to the volatility of the underlying product.
As a leading global electronic market maker, Optiver is dedicated to continuously improving financial markets, creating better access and prices for options, ETFs, cash equities, bonds and foreign currencies on numerous exchanges around the world. Optiver’s teams have spent countless hours building sophisticated models that predict volatility and continuously generate fairer options prices for end investors. However, an industry-leading pricing algorithm can never stop evolving, and there is no better place than Kaggle to help Optiver take its model to the next level.
In the first three months of this competition, you’ll build models that predict short-term volatility for hundreds of stocks across different sectors. You will have hundreds of millions of rows of highly granular financial data at your fingertips, with which you'll design your model forecasting volatility over 10-minute periods. Your models will be evaluated against real market data collected in the three-month evaluation period after training.
Through this competition, you'll gain invaluable insight into volatility and financial market structure. You'll also get a better understanding of the sort of data science problems Optiver has faced for decades. We look forward to seeing the creative approaches the Kaggle community will apply to this ever complex but exciting trading challenge.
Getting started
In order to make Kagglers better prepared for this competition, Optiver's data scientists have created a tutorial notebook debriefing competition data and relevant financial concepts of this trading challenge. Also, Optiver's online course can tell you more about financial market and market making.
For more information about exciting data science opportunities at Optiver, check out their data science landing page here or e-mail their recruiting team directly at datascience@optiver.com.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated using the root mean square percentage error, defined as:
$$ \text{RMSPE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} ((y_i - \hat{y}_i)/y_i)^2} $$
Submission File
For each row_id in the test set, you must predict the target variable. The file should contain a header and have the following format:
row_id,target
0-0,0.003
0-1,0.002
0-2,0.001
...","This dataset contains stock market data relevant to the practical execution of trades in the financial markets. In particular, it includes order book snapshots and executed trades. With one second resolution, it provides a uniquely fine grained look at the micro-structure of modern financial markets.
This is a code competition where only the first few rows of the test set are available for download. The rows that are visible are intended to illustrate the hidden test set format and folder structure. The remainder will only be available to your notebook when it is submitted. The hidden test set contains data that can be used to construct features to predict roughly 150,000 target values. Loading the entire dataset will take slightly more than 3 GB of memory, by our estimation.
This is also a forecasting competition, where the final private leaderboard will be determined using data gathered after the training period closes, which means that the public and private leaderboards will have zero overlap. During the active training stage of the competition a large fraction of the test data will be filler, intended only to ensure the hidden dataset has approximately the same size as the actual test data. The filler data will be removed entirely during the forecasting phase of the competition and replaced with real market data.
Files
book_[train/test].parquet A parquet file partitioned by stock_id. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.
stock_id - ID code for the stock. Not all stock IDs exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; .",kaggle competitions download -c optiver-realized-volatility-prediction,"['https://www.kaggle.com/code/jiashenliu/introduction-to-financial-concepts-and-data', 'https://www.kaggle.com/code/tommy1028/lightgbm-starter-with-feature-engineering-idea', 'https://www.kaggle.com/code/alexioslyon/lgbm-baseline', 'https://www.kaggle.com/code/ragnar123/optiver-realized-volatility-lgbm-baseline', 'https://www.kaggle.com/code/chumajin/optiver-realized-eda-for-starter-english-version']"
129,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.","Submissions are evaluated using multi-class logarithmic loss. Each row in the dataset has been labeled with one true Class. For each row, you must submit the predicted probabilities that the product belongs to each class label. The formula is:
$$ \text{log loss} = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}), $$
where \(N\) is the number of rows in the test set, \(M\) is the number of class labels, \( \text{log}\) is the natural logarithm, \(y_{ij}\) is 1 if observation \(i\) is in class \(j\) and 0 otherwise, and \(p_{ij}\) is the predicted probability that observation \(i\) belongs to class \(j\).
The submitted probabilities for a given product are not required to sum to one; they are rescaled prior to being scored, each row is divided by the row sum. In order to avoid the extremes of the \(\text{log}\) function, predicted probabilities are replaced with \(max(min(p,1-10^{-15}),10^{-15})\).
Submission File
You must submit a csv file with the product id and the predicted probability that the product belongs to each of the classes seen in the dataset. The order of the rows does not matter. The file must have a header and should look like the following:
id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9
200000,0.05,0.14,0.21,0.05,0.20,0.04,0.00,0.20,0.11
200001,0.21,0.06,0.10,0.20,0.13,0.01,0.04,0.10,0.15
200002,0.15,0.12,0.18,0.10,0.16,0.16,0.03,0.01,0.09
etc.","The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.
This competition dataset is similar to the Tabular Playground Series - May 2021 dataset, but with increased observations, increased features, and increased class labels.
Files
train.csv - the training data, one product (id) per row, with the associated features (feature_*) and class label (target)
test.csv - the test data; you must predict the probability the id belongs to each class
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-jun-2021,"['https://www.kaggle.com/code/bextuychiev/lgbm-optuna-hyperparameter-tuning-w-understanding', 'https://www.kaggle.com/code/usharengaraju/tps-june-weights-and-biases', 'https://www.kaggle.com/code/dwin183287/tps-june-2021-eda', 'https://www.kaggle.com/code/bextuychiev/every-pandas-function-to-manipulate-time-series', 'https://www.kaggle.com/code/bextuychiev/how-to-beat-the-heck-out-of-xgboost-with-lightgbm']"
130,"As of August 1st, we are in the evaluation phase of the competition, where the leaderboard will be refreshed periodically based on performance of participant submissions on the future-looking test set. No new submissions will be accepted at this time.
A player hits a walk-off home run. A pitcher throws a no-hitter. A team gets red hot going into the Postseason. We know some of the catalysts that increase baseball fan interest. Now Major League Baseball (MLB) and Google Cloud want the Kaggle community’s help to identify the many other factors which pique supporter engagement and create deeper relationships betweens players and fans.
The sport has a long history of being numbers-driven. Nearly every day from at least April through October, baseball fans watch, read, and search for information about players. Which individuals they seek can depend on player performance, team standings, popularity, among other, currently unknown factors—which could be better understood thanks to data science.
Since at least the early 1990s, MLB has led the sports world in the use of data, showing fans, players, coaches, and media what’s possible when you combine data with human performance. MLB continues its leadership using technology to engage fans and provide new fans innovative ways to experience America’s Favorite Pastime.
MLB has teamed up with Google Cloud to transform the fan experience through data. Google Cloud proudly supports this Kaggle contest to celebrate the launch of Vertex AI: Google Cloud’s new platform to unify your ML workflows.
In this competition, you’ll predict how fans engage with MLB players’ digital content on a daily basis for a future date range. You’ll have access to player performance data, social media data, and team factors like market size. Successful models will provide new insights into what signals most strongly correlate with and influence engagement.
Imagine if you could predict MLB All Stars all season long or when each of a team’s 25 players has his moment in the spotlight. These insights are possible when you dive deeper into the fandom of America’s pastime. Be part of the first method of its kind to try to understand digital engagement at the player level in this granular, day-to-day fashion. Simultaneously help MLB build innovation more easily using Google Cloud’s data analytics, Vertex AI and MLOps tools. You could play a part in shaping the future of MLB fan and player engagement.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated on the mean column-wise mean absolute error (MCMAE). A mean absolute error is calculated for each of the four target variables and the score is the average of those four MAE values.
Submission File
You must submit to this competition using the provided mlb python time-series module, which ensures that models do not peek forward in time. For each date_playerId in the test set, you must predict the value of the four target variables. The prediction dataframe has the following format, where date_playerId contains your predictions for a given player on a given date:
date_playerId,target1,target2,target3,target4
20210501_642727,0,100,70,50
etc.
The mlb module writes the submission.csv file automatically, based on the predictions you make each for each date in the test set. To use the module, follow the this template in Kaggle Notebooks:
import mlb
env = mlb.make_env() # initialize the environment
iter_test = env.iter_test() # iterator which loops over each date in test set

for (test_df, sample_prediction_df) in iter_test:
    sample_prediction_df['target1'] = 100 #make predictions here
    env.predict(sample_prediction_df)","As of August 1st, we are in the evaluation phase of the competition, where the leaderboard will be refreshed periodically based on performance of participant submissions on the future-looking test set. No new submissions will be accepted at this time.
You are tasked with forecasting four different measures of engagement (target1-target4) for a subset of MLB players who are active in the 2021 season. The data contains a set of static files that do not change with time (players.csv, teams.csv, seasons.csv, awards.csv) as well as daily data (train.csv) which is grouped by day. When predicting on a given date, you are forecasting the target variables for the next day (i.e. for date d, you're predicting the engagement for day d+1).
This is a code competition that relies on a time-series module to ensure models do not peek forward in time. The time series module provides you with the test data and writes your submission file automatically. The test data arrives in a data frame identical in format to train.csv, except it does not contain the target values. To submit, follow the instructions on the Evaluation page. When you submit your notebook, it will be rerun on an unseen test set:
During the Training phase of the competition, this unseen test set is comprised of data for the month of May 2021 and the set of active players this year.
During the Evaluation phase, the test set will be a future in-season range of approximately one month.",kaggle competitions download -c mlb-player-digital-engagement-forecasting,"['https://www.kaggle.com/code/chumajin/eda-of-mlb-for-starter-version', 'https://www.kaggle.com/code/ryanholbrook/getting-started-with-mlb-player-digital-engagement', 'https://www.kaggle.com/code/ulrich07/mlb-ann-with-lags-tf-keras', 'https://www.kaggle.com/code/chumajin/eda-of-mlb-for-starter-english-ver', 'https://www.kaggle.com/code/somayyehgholami/fork-of-lightgbm-catboost-ann-2505f2']"
131,"Five times more deadly than the flu, COVID-19 causes significant morbidity and mortality. Like other pneumonias, pulmonary infection with COVID-19 results in inflammation and fluid in the lungs. COVID-19 looks very similar to other viral and bacterial pneumonias on chest radiographs, which makes it difficult to diagnose. Your computer vision model to detect and localize COVID-19 would help doctors provide a quick and confident diagnosis. As a result, patients could get the right treatment before the most severe effects of the virus take hold.
Currently, COVID-19 can be diagnosed via polymerase chain reaction to detect genetic material from the virus or chest radiograph. However, it can take a few hours and sometimes days before the molecular test results are back. By contrast, chest radiographs can be obtained in minutes. While guidelines exist to help radiologists differentiate COVID-19 from other types of infection, their assessments vary. In addition, non-radiologists could be supported with better localization of the disease, such as with a visual bounding box.
As the leading healthcare organization in their field, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation. SIIM has partnered with the Foundation for the Promotion of Health and Biomedical Research of Valencia Region (FISABIO), Medical Imaging Databank of the Valencia Region (BIMCV) and the Radiological Society of North America (RSNA) for this competition.
In this competition, you’ll identify and localize COVID-19 abnormalities on chest radiographs. In particular, you'll categorize the radiographs as negative for pneumonia or typical, indeterminate, or atypical for COVID-19. You and your model will work with imaging data and annotations from a group of radiologists.
If successful, you'll help radiologists diagnose the millions of COVID-19 patients more confidently and quickly. This will also enable doctors to see the extent of the disease and help them make decisions regarding treatment. Depending upon severity, affected patients may need hospitalization, admission into an intensive care unit, or supportive therapies like mechanical ventilation. As a result of better diagnosis, more patients will quickly receive the best care for their condition, which could mitigate the most severe effects of the virus.

This is a Code Competition. Refer to Code Requirements for details.

Host Organizations
FISABIO, The Foundation for the Promotion of Health and Biomedical Research of Valencia Region
The Foundation for the Promotion of Health and Biomedical Research of Valencia Region, FISABIO, is a non-profit scientific and healthcare entity, whose primary purpose is to encourage, to promote and to develop scientific and technical health and biomedical research in Valencia Region. FISABIO integrates and manages the Health Research Map of the Centre for Public Health Research, Dr. Peset University Hospital Foundation, Alicante University General Hospital Foundation, Elche University General Hospital Foundation, and the Mediterranean Ophthalmological Foundation. The BIMCV facility is connected with a multi-level vendor neutral archive (VNA). The imaging population facility is storing data from the Valencia Region, which accounts for more than 5.1 million habitants.
Radiological Society of North America (RSNA)
The Radiological Society of North America (RSNA) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research and technological innovation.
RSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world’s largest radiology conference and is dedicated to building the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its inception. RSNA also supports and facilitates artificial intelligence (AI) research in medical imaging by sponsoring an ongoing series of AI challenge competitions.","The challenge uses the standard PASCAL VOC 2010 mean Average Precision (mAP) at IoU > 0.5. Note that the linked document describes VOC 2012, which differs in some minor ways (e.g. there is no concept of ""difficult"" classes in VOC 2010). The P/R curve and AP calculations remain the same.
In this competition, we are making predictions at both a study (multi-image) and image level.
Study-level labels
Studies in the test set may contain more than one label. They are as follows:
""negative"", ""typical"", ""indeterminate"", ""atypical""
Please see the Data page for further details.
For each study in the test set, you should predict at least one of the above labels. The format for a given label's prediction would be a class ID from the above list, a confidence score, and 0 0 1 1 is a one-pixel bounding box.
Image-level labels
Images in the test set may contain more than one object. For each object in a given test image, you must predict a class ID of ""opacity"", a confidence score, and bounding box in format xmin ymin xmax ymax. If you predict that there are NO objects in a given image, you should predict none 1.0 0 0 1 1, where none is the class ID for ""No finding"", 1.0 is the confidence, and 0 0 1 1 is a one-pixel bounding box.
Submission File
The submission file should contain a header and have the following format:
Id,PredictionString
2b95d54e4be65_study,negative 1 0 0 1 1
2b95d54e4be66_study,typical 1 0 0 1 1
2b95d54e4be67_study,indeterminate 1 0 0 1 1 atypical 1 0 0 1 1
2b95d54e4be68_image,none 1 0 0 1 1
2b95d54e4be69_image,opacity 0.5 100 100 200 200 opacity 0.7 10 10 20 20
etc.","In this competition, we are identifying and localizing COVID-19 abnormalities on chest radiographs. This is an object detection and classification problem.
For each test image, you will be predicting a bounding box and class for all findings. If you predict that there are no findings, you should create a prediction of ""none 1 0 0 1 1"" (""none"" is the class ID for no finding, and this provides a one-pixel bounding box with a confidence of 1.0).
Further, for each test study, you should make a determination within the following labels:
'Negative for Pneumonia'
'Typical Appearance'
'Indeterminate Appearance'
'Atypical Appearance'
To make a prediction of one of the above labels, create a prediction string similar to the ""none"" class above: e.g. atypical 1 0 0 1 1
Please see the Evaluation page for more details about formatting predictions.",kaggle competitions download -c siim-covid19-detection,"['https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing', 'https://www.kaggle.com/code/ayuraj/train-covid-19-detection-using-yolov5', 'https://www.kaggle.com/code/andradaolteanu/siim-covid-19-box-detect-dcm-metadata', 'https://www.kaggle.com/code/ruchi798/siim-covid-19-detection-eda-data-augmentation', 'https://www.kaggle.com/code/h053473666/siim-cov19-efnb7-yolov5-infer']"
132,"“Are we alone in the Universe?”
It’s one of the most profound—and perennial—human questions. As technology improves, we’re finding new and more powerful ways to seek answers. The Breakthrough Listen team at the University of California, Berkeley, employs the world’s most powerful telescopes to scan millions of stars for signs of technology. Now it wants the Kaggle community to help interpret the signals they pick up.
The Listen team is part of the Search for ExtraTerrestrial Intelligence (SETI) and uses the largest steerable dish on the planet, the 100-meter diameter Green Bank Telescope. Like any SETI search, the motivation to communicate is also the major challenge. Humans have built enormous numbers of radio devices. It’s hard to search for a faint needle of alien transmission in the huge haystack of detections from modern technology.
Current methods use two filters to search through the haystack. First, the Listen team intersperses scans of the target stars with scans of other regions of sky. Any signal that appears in both sets of scans probably isn’t coming from the direction of the target star. Second, the pipeline discards signals that don’t change their frequency, because this means that they are probably nearby the telescope. A source in motion should have a signal that suggests movement, similar to the change in pitch of a passing fire truck siren. These two filters are quite effective, but we know they can be improved. The pipeline undoubtedly misses interesting signals, particularly those with complex time or frequency structure, and those in regions of the spectrum with lots of interference.
In this competition, use your data science skills to help identify anomalous signals in scans of Breakthrough Listen targets. Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals (that they call “needles”) in the haystack of data from the telescope. They have identified some of the hidden needles so that you can train your model to find more. The data consist of two-dimensional arrays, so there may be approaches from computer vision that are promising, as well as digital signal processing, anomaly detection, and more. The algorithm that’s successful at identifying the most needles will win a cash prize, but also has the potential to help answer one of the biggest questions in science.
Acknowledgments
The Breakthrough Listen science and engineering effort is headquartered at the University of California, Berkeley SETI Research Center. The Breakthrough Prize Foundation funds the Breakthrough Initiatives which manages Breakthrough Listen. The Green Bank Observatory is supported by the National Science Foundation, and is operated by Associated Universities, Inc. under a cooperative agreement.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
id,target
00034abb3629,0.5
0004be0baf70,0.5
0005be4d0752,0.5
etc.","In this competition you are tasked with looking for technosignature signals in cadence snippets taken from the Green Bank Telescope (GBT). Please read the extended description on the Data Information tab for detailed information about the data (that's too lengthy to include here).
Files
train/ - a training set of cadence snippet files stored in numpy float16 format (v1.20.1), one file per cadence snippet id, with corresponding labels found in the train_labels.csv file. Each file has dimension (6, 273, 256), with the 1st dimension representing the 6 positions of the cadence, and the 2nd and 3rd dimensions representing the 2D spectrogram.
test/ - the test set cadence snippet files; you must predict whether or not the cadence contains a ""needle"", which is the target for this competition
sample_submission.csv - a sample submission file in the correct format
train_labels - targets corresponding (by id) to the cadence snippet files found in the train/ folder
old_leaky_data - full pre-relaunch data, including test labels; you should not assume this data is helpful (it may or may not be).",kaggle competitions download -c seti-breakthrough-listen,"['https://www.kaggle.com/code/yasufuminakama/seti-nfnet-l0-starter-training', 'https://www.kaggle.com/code/ihelon/signal-search-exploratory-data-analysis', 'https://www.kaggle.com/code/usharengaraju/seti-eda-baseline-tensorflow-and-tpu', 'https://www.kaggle.com/code/awsaf49/seti-bl-channelwise-alignment-tf-tpu', 'https://www.kaggle.com/code/ttahara/rerun-seti-e-t-resnet18d-baseline']"
133,"Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.
Currently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.
CommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.
In this competition, you’ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.
If successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.
Acknowledgements
CommonLit would like to extend a special thanks to Professor Scott Crossley's research team at the Georgia State University Departments of Applied Linguistics and Learning Sciences for their partnership on this project.
The organizers would like to thank Schmidt Futures for their advice and support for making this work possible.
        
This is a Code Competition. Refer to Code Requirements for details.","Submissions are scored on the root mean squared error. RMSE is defined as:
$$ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} $$
where \( \hat{y} \) is the predicted value, \( y \) is the original value, and \( n \) is the number of rows in the test data.
Submission File
For each row in the test set, you must predict the value of the target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:
id,target
eaf8e7355,0.0
60ecc9777,0.5
c0f722661,-2.0
etc.","In this competition, we're predicting the reading ease of excerpts from literature. We've provided excerpts from several time periods and a wide range of reading ease scores. Note that the test set includes a slightly larger proportion of modern texts (the type of texts we want to generalize to) than the training set.
Also note that while licensing information is provided for the public test set (because the associated excerpts are available for display / use), the hidden private test set includes only blank license / legal information.
Files
train.csv - the training set
test.csv - the test set
sample_submission.csv - a sample submission file in the correct format
Columns
id - unique ID for excerpt
url_legal - URL of source - this is blank in the test set.
license - license of source material - this is blank in the test set.
excerpt - text to predict reading ease of",kaggle competitions download -c commonlitreadabilityprize,"['https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently', 'https://www.kaggle.com/code/ruchi798/commonlit-readability-prize-eda-baseline', 'https://www.kaggle.com/code/mdfahimreshm/bert-in-depth-understanding', 'https://www.kaggle.com/code/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning', 'https://www.kaggle.com/code/rhtsingh/speeding-up-transformer-w-optimization-strategies']"
134,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams, to inspire broad participation we are limiting winner's of swag to once per person for this series. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.","Submissions are evaluated using multi-class logarithmic loss. Each row in the dataset has been labeled with one true Class. For each row, you must submit the predicted probabilities that the product belongs to each class label. The formula is:
$$ \text{log loss} = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}), $$
where \(N\) is the number of rows in the test set, \(M\) is the number of class labels, \( \text{log}\) is the natural logarithm, \(y_{ij}\) is 1 if observation \(i\) is in class \(j\) and 0 otherwise, and \(p_{ij}\) is the predicted probability that observation \(i\) belongs to class \(j\).
The submitted probabilities for a given product are not required to sum to one; they are rescaled prior to being scored, each row is divided by the row sum. In order to avoid the extremes of the \(\text{log}\) function, predicted probabilities are replaced with \(max(min(p,1-10^{-15}),10^{-15})\).
Submission File
You must submit a csv file with the product id and the predicted probability that the product belongs to each of the classes seen in the dataset. The order of the rows does not matter. The file must have a header and should look like the following:
id,Class_1,Class_2,Class_3,Class_4
100000,0.1,0.3,0.2,0.4
100001,0.5,0.1,0.1,0.3
100002,0.4,0.4,0.1,0.1
etc.","The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.
Files
train.csv - the training data, one product (id) per row, with the associated features (feature_*) and class label (target)
test.csv - the test data; you must predict the probability the id belongs to each class
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-may-2021,"['https://www.kaggle.com/code/bextuychiev/lgbm-optuna-hyperparameter-tuning-w-understanding', 'https://www.kaggle.com/code/subinium/tps-may-categorical-eda', 'https://www.kaggle.com/code/andreshg/automl-libraries-comparison', 'https://www.kaggle.com/code/ruchi798/tps-may-rapids', 'https://www.kaggle.com/code/andreshg/tps-may-a-complete-analysis']"
135,"Birds of a feather flock together. Thankfully, this makes it easier to hear them! There are over 10,000 bird species around the world. Identifying the red-winged blackbirds or Bewick’s wrens in an area, for example, can provide important information about the habitat. As birds are high up in the food chain, they are excellent indicators of deteriorating environmental quality and pollution. Monitoring the status and trends of biodiversity in ecosystems is no small task. With proper sound detection and classification—aided by machine learning—researchers can improve their ability to track the status and trends of biodiversity in important ecosystems, enabling them to better support global conservation efforts.
Recent advances in machine listening have improved acoustic data collection. However, it remains a challenge to generate analysis outputs with high precision and recall. The majority of data is unexamined due to a lack of effective tools for efficient and reliable extraction of the signals of interests (e.g., bird calls).
The Cornell Lab of Ornithology is dedicated to advancing the understanding and protection of birds and the natural world. The Lab joins with people from all walks of life to make new scientific discoveries, share insights, and galvanize conservation action. For this competition, they're collaborating with Google Research, LifeCLEF, and Xeno-canto.
In this competition, you’ll automate the acoustic identification of birds in soundscape recordings. You'll examine an acoustic dataset to build detectors and classifiers to extract the signals of interest (bird calls). Innovative solutions will be able to do so efficiently and reliably.
The ornithology community is collecting many petabytes of acoustic data every year, but the majority of data remains unexamined. If successful, you'll help researchers properly detect and classify bird sounds, significantly improving their ability to monitor the status and trends of biodiversity in important ecosystems. Researchers will better be able to infer factors about an area’s quality of life based on a changing bird population, which allows them to identify how they can best support global conservation efforts.
This is a Code Competition. Refer to Code Requirements for details.
The LifeCLEF Bird Recognition Challenge (BirdCLEF) focuses on developing machine learning algorithms to identify avian vocalizations in continuous soundscape data to aid conservation efforts worldwide. Launched in 2014, it has become one of the largest bird sound recognition competitions in terms of dataset size and species diversity.","Submissions will be evaluated based on their row-wise micro averaged F1 score.
For each row_id/time window, you need to provide a space delimited list of the set of unique birds that made a call beginning or ending in that time window. If there are no bird calls in a time window, use the code nocall.
The submission file must have a header and should look like the following:
Submission File
row_id,birds
3575_COL_5,wewpew batpig1
3575_COL_10,wewpew batpig1
3575_COL_15,wewpew batpig1
...
Working Note Award Criteria (optional)
Criteria for the BirdCLEF best working note award:
Originality. The value of a paper is a function of the degree to which it presents new or novel technical material. Does the paper present results previously unknown? Does it push forward the frontiers of knowledge? Does it present new methods for solving old problems or new viewpoints on old problems? Or, on the other hand, is it a re-hash of information already known?
Quality. A paper's value is a function of the innate character or degree of excellence of the work described. Was the work performed, or the study made with a high degree of thoroughness? Was high engineering skill demonstrated? Is an experiment described which has a high degree of elegance? Or, on the other hand, is the work described pretty much of a run-of-the-mill nature?
Contribution. The value of a paper is a function of the degree to which it represents an overall contribution to the advancement of the art. This is different from originality. A paper may be highly original but may be concerned with a very minor, or even insignificant, matter or problem. On the other hand, a paper may make a great contribution by collecting and analyzing known data and facts and pointing out their significance. Or, a fine exposition of a known but obscure or complex phenomenon or theory or system or operating technique may be a very real contribution to the art. Obviously, a paper may well score highly on both originality and contribution. Perhaps a significant question is, will the engineer who reads the paper be able to practice his profession more effectively because of having read it?
Presentation. The value of the paper is a function of the ease with which the reader can determine what the author is trying to present. Regardless of the other criteria, a paper is not good unless the material is presented clearly and effectively. Is the paper well written? Is the meaning of the author clear? Are the tables, charts, and figures clear? Is their meaning readily apparent? Is the information presented in the paper complete? At the same time, is the paper concise?
Evaluation of the submitted BirdCLEF working notes:
Each working note will be reviewed by two reviewers and scores averaged. Maximum score: 15.
a) Evaluation of work and contribution
5 points: Excellent work and a major contribution
4 points: Good solid work of some importance
3 points: Solid work but a marginal contribution
2 points: Marginal work and minor contribution
1 point: Work doesn't meet scientific standards
b) Originality and novelty
5 points Trailblazing
4 points: A pioneering piece of work
3 points: One step ahead of the pack
2 points: Yet another paper about…
1 point: It's been said many times before
c) Readability and organization
5 points: Excellent
4 points: Well written
3 points: Readable
2 points: Needs considerable work
1 point: Work doesn't meet scientific standards","Your challenge in this competition is to identify which birds are calling in long recordings, given training data generated in meaningfully different contexts. This is the exact problem facing scientists trying to automate the remote monitoring of bird populations. This competition builds on the previous one by adding soundscapes from new locations, more bird species, richer metadata about the test set recordings, and soundscapes to the train set.
Files
train_short_audio -
The bulk of the training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. The training data should have nearly all relevant files; we expect there is no benefit to looking for more on xenocanto.org.
train_soundscapes -
Audio files that are quite comparable to the test set. They are all roughly ten minutes long and in the ogg format. The test set also has soundscapes from the two recording locations represented here.
test_soundscapes -
When you submit a notebook, the test_soundscapes directory will be populated with approximately 80 recordings to be used for scoring. These will be roughly 10 minutes long and in ogg audio format. The file names include the date the recording was taken, which can be especially useful for identifying migratory birds.",kaggle competitions download -c birdclef-2021,"['https://www.kaggle.com/code/kneroma/clean-fast-simple-bird-identifier-inference', 'https://www.kaggle.com/code/shreyasajal/birdclef-librosa-audio-feature-extraction', 'https://www.kaggle.com/code/awsaf49/birdclef23-pretraining-is-all-you-need-train', 'https://www.kaggle.com/code/hidehisaarai1213/pytorch-training-birdclef2021-starter', 'https://www.kaggle.com/code/kneroma/clean-fast-simple-bird-identifier-training-colab']"
136,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to ""cheat"" by using public labels for predictions. How well does your model perform on truly private test labels?
Good luck and have fun!
Getting Started
Check out the original Titanic competition which walks you through how to build various models.
For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.","Goal
Your task is to predict whether or not a passenger survived the sinking of the Synthanic (a synthetic, much larger dataset based on the actual Titanic dataset). For each PasengerId row in the test set, you must predict a 0 or 1 value for the Survived target.
Your score is the percentage of passengers you correctly predict. This is known as accuracy.
Submission File
You should submit a csv file with exactly 100,000 rows plus a header row. Your submission will show an error if you have extra columns or extra rows.
The file should have exactly 2 columns:
PassengerId (sorted in any order)
Survived (contains your binary predictions: 1 for survived, 0 for deceased)
You can download an example submission file (sample_submission.csv) on the Data page
PassengerId,Survived
100000,0
100001,1
100002,0
etc.","Overview
The dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to ""cheat"" by using public labels for predictions. How well does your model perform on truly unseen data?
The data has been split into two groups:
training set (train.csv)
test set (test.csv)
The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.
The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Synthanic.
Data Dictionary",kaggle competitions download -c tabular-playground-series-apr-2021,"['https://www.kaggle.com/code/andreshg/automl-libraries-comparison', 'https://www.kaggle.com/code/remekkinas/ensemble-learning-meta-classifier-for-stacking', 'https://www.kaggle.com/code/subinium/tps-apr-highlighting-the-data', 'https://www.kaggle.com/code/michau96/simple-way-to-evaluate-features-in-the-model', 'https://www.kaggle.com/code/alexryzhkov/n3-tps-april-21-lightautoml-starter']"
137,"This competition challenges data scientists to show how publicly funded data are used to serve science and society. Evidence through data is critical if government is to address the many threats facing society, including; pandemics, climate change, Alzheimer’s disease, child hunger, increasing food production, maintaining biodiversity, and addressing many other challenges. Yet much of the information about data necessary to inform evidence and science is locked inside publications.
Can natural language processing find the hidden-in-plain-sight data citations? Can machine learning find the link between the words used in research articles and the data referenced in the article?
Now is the time for data scientists to help restore trust in data and evidence. In the United States, federal agencies are now mandated to show how their data are being used. The new Foundations of Evidence-based Policymaking Act requires agencies to modernize their data management. New Presidential Executive Orders are pushing government agencies to make evidence-based decisions based on the best available data and science. And the government is working to respond in an open and transparent way.
This competition will build just such an open and transparent approach. The results will show how public data are being used in science and help the government make wiser, more transparent public investments. It will help move researchers and governments from using ad-hoc methods to automated ways of finding out what datasets are being used to solve problems, what measures are being generated, and which researchers are the experts. Previous competitions have shown that it is possible to develop algorithms to automate the search and discovery of references to data. Now, we want the Kaggle community to develop the best approaches to identify critical datasets used in scientific publications.
In this competition, you'll use natural language processing (NLP) to automate the discovery of how scientific data are referenced in publications. Utilizing the full text of scientific publications from numerous research areas gathered from CHORUS publisher members and other sources, you'll identify data sets that the publications' authors used in their work.
If successful, you'll help support evidence in government data. Automated NLP approaches will enable government agencies and researchers to quickly find the information they need. The approach will be used to develop data usage scorecards to better enable agencies to show how their data are used and bring down a critical barrier to the access and use of public data.
The Coleridge Initiative is a not-for-profit that has been established to use data for social good. One way in which the organization does this is by furthering science through publicly available research.
Resources
Coleridge Data Examples
Rich Search and Discovery for Research Datasets
Democratizing Our Data
NSF""Rich Context"" Video
Acknowledgments
United States Department of Agriculture
United States Department of Commerce
United States Geological Survey
National Oceanic and Atmospheric Administration
National Science Foundation
National Institutes of Health
CHORUS
Westat
Alfred P. Sloan Foundation
Schmidt Futures
Overdeck Family Foundation

This is a Code Competition. Refer to Code Requirements for details.","The objective of the competition is to identify the mention of datasets within scientific publications. Your predictions will be short excerpts from the publications that appear to note a dataset.
Submissions are evaluated on a Jaccard-based FBeta score between predicted texts and ground truth texts, with Beta = 0.5 (a micro F0.5 score). Multiple predictions are delineated with a pipe (|) character in the submission file.
The following is Python code for calculating the Jaccard score for a single prediction string against a single ground truth string. Note that the overall score for a sample uses Jaccard to compare multiple ground truth and prediction strings that are pipe-delimited - this code does not handle that process or the final micro F-beta calculation.
def jaccard(str1, str2): 
    a = set(str1.lower().split()) 
    b = set(str2.lower().split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
Note that ALL ground truth texts have been cleaned for matching purposes using the following code:
def clean_text(txt):
    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())
For each publication's set of predictions, a token-based Jaccard score is calculated for each potential prediction / ground truth pair. The prediction with the highest score for a given ground truth is matched with that ground truth.
Predicted strings for each publication are sorted alphabetically and processed in that order. Any scoring ties are resolved on the basis of that sort.
Any matched predictions where the Jaccard score meets or exceeds the threshold of 0.5 are counted as true positives (TP), the remainder as false positives (FP).
Any unmatched predictions are counted as false positives (FP).
Any ground truths with no nearest predictions are counted as false negatives (FN).
All TP, FP and FN across all samples are used to calculate a final micro F0.5 score. (Note that a micro F score does precisely this, creating one pool of TP, FP and FN that is used to calculate a score for the entire set of predictions.)
Submission File
For each publication Id in the test set, you must predict excerpts (multiple excerpts divided by a pipe character) for PredictionString variable. The file should contain a header and have the following format:
Id,PredictionString
000e04d6-d6ef-442f-b070-4309493221ba,space objects dataset|small objects data
0176e38e-2286-4ea2-914f-0583808a98aa,small objects dataset
01860fa5-2c39-4ea2-9124-74458ae4a4b4,large objects
01e4e08c-ffea-45a7-adde-6a0c0ad755fc,space location data|national space objects|national space dataset
01fea149-a6b8-4b01-8af9-51e02f46f03f,a dataset of large objects
etc.","The objective of the competition is to identify the mention of datasets within scientific publications. Your predictions will be short excerpts from the publications that appear to note a dataset. Predictions that more accurately match the precise words used to identify the dataset within the publication will score higher. Predictions should be cleaned using the clean_text function from the Evaluation page to ensure proper matching.
Publications are provided in JSON format, broken up into sections with section titles.
The goal in this competition is not just to match known dataset strings but to generalize to datasets that have never been seen before using NLP and statistical techniques. A percentage of the public test set publications are drawn from the training set - not all datasets have been identified in train, so these unidentified datasets have been used as a portion of the public test labels. These should serve as guides for the difficult task of labeling the private test set.
Note that the hidden test set has roughly ~8000 publications, many times the size of the public test set. Plan your compute time accordingly.
Files
train - the full text of the training set's publications in JSON format, broken into sections with section titles
test - the full text of the test set's publications in JSON format, broken into sections with section titles
train.csv - labels and metadata for the training set
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c coleridgeinitiative-show-us-the-data,"['https://www.kaggle.com/code/prashansdixit/coleridge-initiative-eda-baseline-model', 'https://www.kaggle.com/code/josephassaker/coleridge-initiative-eda-na-ve-submission', 'https://www.kaggle.com/code/manabendrarout/tabular-data-preparation-basic-eda-and-baseline', 'https://www.kaggle.com/code/tungmphung/coleridge-matching-bert-ner', 'https://www.kaggle.com/code/chumajin/coleridge-initiative-eda-for-biginner']"
138,"## Problem Statement Apples are one of the most important temperate fruit crops in the world. Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current process for disease diagnosis in apple orchards is based on manual scouting by humans, which is time-consuming and expensive. Although computer vision-based models have shown promise for plant disease identification, there are some limitations that need to be addressed. Large variations in visual symptoms of a single disease across different apple cultivars, or new varieties that originated under cultivation, are major challenges for computer vision-based disease identification. These variations arise from differences in natural and image capturing environments, for example, leaf color and leaf morphology, the age of infected tissues, non-uniform image background, and different light illumination during imaging etc. Plant Pathology 2020-FGVC7 challenge competition had a pilot dataset of 3,651 RGB images of foliar disease of apples. For Plant Pathology 2021-FGVC8, we have significantly increased the number of foliar disease images and added additional disease categories. This year’s dataset contains approximately 23,000 high-quality RGB images of apple foliar diseases, including a large expert-annotated disease dataset. This dataset reflects real field scenarios by representing non-homogeneous backgrounds of leaf images taken at different maturity stages and at different times of day under different focal camera settings. ## Specific Objectives The main objective of the competition is to develop machine learning-based models to accurately classify a given leaf image from the test dataset to a particular disease category, and to identify an individual disease from multiple disease symptoms on a single leaf image.
## Resources Details and background information on the dataset and Kaggle competition ‘Plant Pathology 2020 Challenge’ were published as a peer-reviewed research article. If you use the dataset for your project, please cite the following [Thapa, Ranjita; Zhang, Kai; Snavely, Noah; Belongie, Serge; Khan, Awais. The Plant Pathology Challenge 2020 data set to classify foliar disease of apples. Applications in Plant Sciences, 8 (9), 2020.](https://bsapubs.onlinelibrary.wiley.com/doi/10.1002/aps3.11390) ### Acknowledgements We acknowledge sponsorship from Cornell Initiative for Digital Agriculture (CIDA).
> **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/plant-pathology-2021-fgvc8/overview/code-requirements) for details.**","The evaluation metric for this competition is Mean F1-Score.
Submission Format
For every author in the dataset, submission files should contain two columns: image and labels. labels should be a space-delimited list.
The file should contain a header and have the following format:
image, labels
85f8cb619c66b863.jpg,healthy
ad8770db05586b59.jpg,healthy
c7b03e718489f3ca.jpg,healthy","Can you help detect farmers detect apple diseases? This competition builds on last year's by challenging you to handle additional diseases and to provide more detailed information about leaves that have multiple infections.
Files
train.csv - the training set metadata.
image - the image ID.
labels - the target classes, a space delimited list of all diseases found in the image. Unhealthy leaves with too many diseases to classify visually will have the complex class, and may also have a subset of the diseases identified.
sample_submission.csv - A sample submission file in the correct format.
image
labels
train_images - The training set images.
test_images - The test set images. This competition has a hidden test set: only three images are provided here as samples while the remaining 5,000 images will be available to your notebook once it is submitted.",kaggle competitions download -c plant-pathology-2021-fgvc8,"['https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases', 'https://www.kaggle.com/code/khoongweihao/insect-augmentation-et-al', 'https://www.kaggle.com/code/praveengovi/plant-pathology-detail-eda-pytorch', 'https://www.kaggle.com/code/nickuzmenkov/pp2021-duplicates-revealing', 'https://www.kaggle.com/code/gargmanas/plant-pathology-2021-fgvc8']"
139,"Hotel Recognition to Combat Human Trafficking
Victims of human trafficking are often photographed in hotel rooms as in the below examples. Identifying these hotels is vital to these trafficking investigations but poses particular challenges due to low quality of images and uncommon camera angles.
Even without victims in the images, hotel identification in general is a challenging fine-grained visual recognition task with a huge number of classes and potentially high intraclass and low interclass variation. In order to support research into this challenging task and create image search tools for human trafficking investigators, we created the TraffickCam mobile application, which allows every day travelers to submit photos of their hotel room. Read more about TraffickCam on TechCrunch.
Example images from one hotel in the TraffickCam dataset are shown below:
In this contest, competitors are tasked with identifying the hotel seen in test images from the TraffickCam dataset, which are based on a large gallery of training images with known hotel IDs.
Our team currently supports an image search system used at the National Center for Missing and Exploited Children in human trafficking investigations. Novel and interesting approaches have the potential to be incorporated in this search system.

This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):
$$MAP@5 = \frac{1}{U} \sum_{u=1}^{U} \sum_{k=1}^{min(n,5)} P(k) \times rel(k)$$
where \( U \) is the number of images, \( P(k) \) is the precision at cutoff \( k \), \( n \) is the number of predictions per image, and \( rel(k) \) is an indicator function equaling 1 if the item at rank \( k \) is a relevant correct label, zero otherwise.
Once a correct label has been scored for an observation, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0.
A B C D E
A A A A A
A B A C A
Submission File
For each image in the test set, you must predict a space-delimited list of hotel IDs that could match that image. The list should be sorted such that the first ID is considered the most relevant one and the last the least relevant one. The file should contain a header and have the following format:
image,hotel_id 
99e91ad5f2870678.jpg,36363 53586 18807 64314 60181
b5cc62ab665591a9.jpg,36363 53586 18807 64314 60181
d5664a972d5a644b.jpg,36363 53586 18807 64314 60181","Identifying the location of a hotel room is a challenging problem of great interest for combating human trafficking. This competition provides a rich dataset of photos of hotel room interiors, without any people present, for this purpose.
Many of the hotels are independent or part of very small chains, where shared decor isn't a concern. However, the shared standards for their interior decoration for the larger chains means that many hotels can look quite similar at first glance. Identifying the chain can narrow the range of possibilities, but only down to a set that is much harder to tell apart and is still scattered across a wide geographic area.
The real value lies in getting the number of candidates to a small enough number that a human investigator could follow up on all of them.
Files
train.csv - The training set metadata.
image - The image ID.
chain - An ID code for the hotel chain. A chain of zero (0) indicates that the hotel is either not part of a chain or the chain is not known. This field is not available for the test set. The number of hotels per chain varies widely.
hotel_id - The hotel ID. The target class.
timestamp - When the image was taken. Provided for the training set only.",kaggle competitions download -c hotel-id-2021-fgvc8,"['https://www.kaggle.com/code/ateplyuk/human-trafficking-2021-baseline', 'https://www.kaggle.com/code/shanmukh05/combat-human-trafficking-2k21-tpu-training', 'https://www.kaggle.com/code/zaber666/human-trafficking-basic-eda-data-analysis', 'https://www.kaggle.com/code/drcapa/human-trafficking-2021-starter', 'https://www.kaggle.com/code/tpmeli/hotel-id-tf-keras-efficientnet-starter']"
140,"Description
Camera traps enable the automatic collection of large quantities of image data. Ecologists all over the world use camera traps to monitor biodiversity and population density of animal species. In order to estimate the abundance and density of species in camera trap data, ecologists need to know not just which species were seen, but also how many of each species were seen. However, because images are taken in motion-triggered bursts to increase the likelihood of capturing the animal(s) of interest, object detection alone is not sufficient as it could lead to over or undercounting. For example, if you get 3 images taken at one frame per second and in the first image you see 3 gazelles, in the second you see 5 gazelles, and in the last you see 4 gazelles, how many total gazelles have you seen? This is more challenging than strictly detecting and categorizing species as it requires reasoning and tracking of individuals across sparse temporal samples.
We have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap but are not identical. The challenge is to categorize species and count the number of individuals across image bursts. To explore multimodal solutions, we allow competitors to train on the following data: (i) our camera trap training set (data provided by WCS), (ii) iNaturalist 2017-2019 data, and (iii) multispectral imagery (from Landsat 8) for each of the camera trap locations. On the competition GitHub page we provide the multispectral data, a taxonomy file mapping our classes into the iNat taxonomy, a subset of iNat data mapped into our class set, and a camera trap detection model (the MegaDetector) along with the corresponding detections.
This is an FGVCx competition as part of the FGVC8 workshop at CVPR 2021 and is sponsored by Microsoft AI for Earth and Wildlife Insights. There is a GitHub page for the competition here. Please open an issue if you have questions or problems with the dataset.
You can find the iWildCam 2018 Competition here, the iWildCam 2019 Competition here, and the
iWildCam 2020 Competition here.
Acknowledgements
We would like to acknowledge WCS for providing the camera trap data, Centaur Labs for generously providing count annotations on the test data, and Microsoft AI4Earth for hosting our external datasets on Azure.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","Evaluation
Submissions will be evaluated using Mean Columnwise Root Mean Squared Error (MCRMSE) where each column j represents a species, each row i represents a sequence, x_ij is the predicted count for that species in that sequence, and y_ij is the ground truth count.
We selected this metric out of the options provided by Kaggle in order to capture both species identification mistakes and count mistakes and to ensure false predictions on empty sequences would contribute to the error. Because many sequences are empty in camera trap data due to false triggers and because many species are rare, the error from this normalized metric looks quite small, while the actual errors in counts are still large. To convert the metric to something more interpretable from an ecological standpoint, you can un-normalize the metric from MCRMSE to the Summed Columnwise Root Summed Squared Error (SCRSSE) by multiplying by the number of categories and the square root of the number of test sequences (i.e. scale the error on the leaderboard by m sqrt(n)).
Submission Format
Submission for the competition is a csv file with the following format:
Id,Predicted2,Predicted3,[...],Predicted571
58857ccf-23d2-11e8-a6a3-ec086b02610b,0,5,[...],0
591e4006-23d2-11e8-a6a3-ec086b02610b,1,0,[...],3
The Id column corresponds to the test sequence id. The Predicted2 holds an integer value that indicates the number of individuals of species 2 in that test sequence. If you predict there are no animals in the sequence, the entire row after the sequence ID should be populated with 0 values.","Data Overview
The iWildCam 2021 WCS training set contains 203,314 images from 323 locations, and the WCS test set contains 60,214 images from 91 locations. These 414 locations are spread across the globe. Additional details about provided metadata are available on the competition github page.
You may also choose to use supplemental training data from the iNaturalist 2017, iNaturalist 2018, iNaturalist 2019, and iNaturalist 2021 competition datasets. As a courtesy, we have curated all the images from iNaturalist 2017-2018 datasets containing classes that might be in the test set and mapped them into the iWildCam categories.
We are providing the same Landsat-8 multispectral imagery for each camera location as was provided in iWildCam 2020 as supplementary data. In particular, each site is associated with a series of patches collected between 2013 and 2019. The patches are extracted from a ""Tier 1"" Landsat product, which consists only of data that meets certain geometric and radiometric quality standards. Consequently, the number of patches per site varies from 39 to 406 (median: 147). Each patch is 200x200x9 pixels, covering an area of 6km^2 at a resolution of 30 meters / pixel across 9 spectral bands. Note that all patches for a given site are registered, but are not centered exactly at the camera location to protect the integrity of the site.
The data can be downloaded from the competition GitHub page.
The MegaDetector Camera Trap Detection Model",kaggle competitions download -c iwildcam2021-fgvc8,"['https://www.kaggle.com/code/khoongweihao/insect-augmentation-et-al', 'https://www.kaggle.com/code/nayuts/creating-mask-from-bbox-with-deepmac', 'https://www.kaggle.com/code/nayuts/iwildcam-2021-starter-notebook', 'https://www.kaggle.com/code/devashishprasad/fastest-way-to-read-and-process-images', 'https://www.kaggle.com/code/nayuts/256-x-256-cropped-images']"
141,"The Herbarium 2021: Half-Earth Challenge is to identify vascular plant specimens provided by the New York Botanical Garden (NY), Bishop Museum (BPBM), Naturalis Biodiversity Center (NL), Queensland Herbarium (BRI), and Auckland War Memorial Museum (AK).
The Herbarium 2021: Half-Earth Challenge dataset includes more than 2.5M images representing nearly 65,000 species from the Americas and Oceania that have been aligned to a standardized plant list (LCVP v1.0.2).
This dataset has a long tail; there are a minimum of 3 images per species. However, some species can be represented by more than 100 images. This dataset only includes vascular land plants which include lycophytes, ferns, gymnosperms, and flowering plants. The extinct forms of lycophytes are the major component of coal deposits, ferns are indicators of ecosystem health, gymnosperms provide major habitats for animals, and flowering plants provide almost all of our crops, vegetables, and fruits.
The teams with the most accurate models will be contacted with the intention of using them on the unnamed plant collections in the NYBG herbarium and then be assessed by the NYBG plant specialists for accuracy.
Background
There are approximately 3,000 herbaria world-wide, and they are massive repositories of plant diversity data. These collections not only represent a vast amount of plant diversity, but since herbarium collections include specimens dating back hundreds of years, they provide snapshots of plant diversity through time. The integrity of the plant is maintained in herbaria as a pressed, dried specimen; a specimen collected nearly two hundred years ago by Darwin looks much the same as one collected a month ago by an NYBG botanist. All specimens not only maintain their morphological features but also include collection dates and locations, their reproductive state, and the name of the person who collected the specimen. This information, multiplied by millions of plant collections, provides the framework for understanding plant diversity on a massive scale and learning how it has changed over time. The models developed during this competition are an integral first step to speed the pace of species discovery and save the plants of the world.
There are approximately 400,000 known vascular plant species with an estimated 80,000 still to be discovered. Herbaria contain an overwhelming amount of unnamed and new specimens, and with the threats of climate change, we need new tools to quicken the pace of species discovery. This is more pressing today as a United Nations report indicates that more than one million species are at risk of extinction, and amid this dire prediction is a recent estimate that suggests plants are disappearing more quickly than animals. This year, we have expanded our curated herbarium dataset to vascular plant diversity in the Americas and Oceania.
The most accurate models will be used on the unidentified plant specimens in our herbarium and assessed by our taxonomists thereby producing a tool to quicken the pace of species discovery.
About
This is an FGVC competition hosted as part of the FGVC8 workshop at CVPR 2021 and sponsored by NYBG.
Details of this competition are mirrored on the github page. Please post in the forum or open an issue if you have any questions or problems with the dataset.
Acknowledgements
The images are provided by the New York Botanical Garden, Bishop Museum, Naturalis Biodiversity Center, Queensland Herbarium, and Auckland War Memorial Museum.","Submissions are evaluated using the macro F1 score.
The F1 score is given by
$$
F_1 = 2\frac{precision \cdot recall}{precision+recall}
$$
where:
$$
precision = \frac{TP}{TP+FP},
$$
$$
recall = \frac{TP}{TP+FN}.
$$
In ""macro"" F1 a separate F1 score is calculated for each species value and then averaged.
Submission Format
For each image Id, you should predict the corresponding image label (category_id) in the Predicted column. The submission file should have the following format:
Id,Predicted
0,1
1,27
2,42
...","Data Overview
The training and test set contain images of herbarium specimens from nearly 65,000 species of vascular plants. Each image contains exactly one specimen. The text labels on the specimen images have been blurred to remove category information in the image.
The data has been approximately split 80%/20% for training/test. Each category has at least 1 instance in both the training and test datasets. Note that the test set distribution is slightly different from the training set distribution. The training set contains species with hundreds of examples, but the test set has the number of examples per species capped at a maximum of 10.
Dataset Details
Each image has different image dimensions, with a maximum of 1000 pixels in the larger dimension. These have been resized from the original image resolution. All images are in JPEG format.
Dataset Format
This dataset uses the COCO dataset format with additional annotation fields. In addition to the species category labels, we also provide region and supercategory information.",kaggle competitions download -c herbarium-2021-fgvc8,"['https://www.kaggle.com/code/khoongweihao/insect-augmentation-et-al', 'https://www.kaggle.com/code/ihelon/herbarium-2021-exploratory-data-analysis', 'https://www.kaggle.com/code/ateplyuk/herb2021-pytorch-starter', 'https://www.kaggle.com/code/debarshichanda/herbarium-2021-pytorch-starter', 'https://www.kaggle.com/code/yasserhessein/herbarium-2021-uing-vgg16']"
142,"Do you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.
Two different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.
Shopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.
In this competition, you’ll apply your machine learning skills to build a model that predicts which items are the same products.
The applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals.","Submissions will be evaluated based on their mean F1 score. The mean is calculated in a sample-wise fashion, meaning that an F1 score is calculated for every predicted row, then averaged.
Submission File
You must create a space-delimited list of all posting_ids that match the posting in the posting_id column. Posts always self-match. Group sizes were capped at 50, so there is no benefit to predict more than 50 matches.
The file should have a header, be named submission.csv, and look like the following:
posting_id,matches
test_123,test_123
test_456,test_456 test_789
You should predict matches for every posting_id. For example, if you believe A matches B and C, A,A B C, you would also include B,B A C and C,C A B.","Finding near-duplicates in large datasets is an important problem for many online businesses. In Shopee's case, everyday users can upload their own images and write their own product descriptions, adding an extra layer of challenge. Your task is to identify which products have been posted repeatedly. The differences between related products may be subtle while photos of identical products may be wildly different!
As this is a code competition, only the first few rows/images of the test set are published; the remainder are only available to your notebook when it is submitted. Expect to find roughly 70,000 images in the hidden test set. The few test rows and images that are provided are intended to illustrate the hidden test set format and folder structure.
Files
[train/test].csv - the training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa.
posting_id - the ID code for the posting.
image - the image id/md5sum.
image_phash - a perceptual hash of the image.
title - the product description for the posting.
label_group - ID code for all postings that map to the same product. Not provided for the test set.",kaggle competitions download -c shopee-product-matching,"['https://www.kaggle.com/code/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700', 'https://www.kaggle.com/code/cdeotte/rapids-cuml-tfidfvectorizer-and-knn', 'https://www.kaggle.com/code/ragnar123/unsupervised-baseline-arcface', 'https://www.kaggle.com/code/ishandutta/v7-shopee-indepth-eda-one-stop-for-all-your-needs', 'https://www.kaggle.com/code/ruchi798/shopee-eda-rapids-preprocessing-w-b']"
143,"In a technology-forward world, sometimes the best and easiest tools are still pen and paper. Organic chemists frequently draw out molecular work with the Skeletal formula, a structural notation used for centuries. Recent publications are also annotated with machine-readable chemical descriptions (InChI), but there are decades of scanned documents that can't be automatically searched for specific chemical depictions. Automated recognition of optical chemical structures, with the help of machine learning, could speed up research and development efforts.
Unfortunately, most public data sets are too small to support modern machine learning models. Existing tools produce 90% accuracy but only under optimal conditions. Historical sources often have some level of image corruption, which reduces performance to near zero. In these cases, time-consuming, manual work is required to reliably convert scanned chemical structure images into a machine-readable format.
Bristol-Myers Squibb is a global biopharmaceutical company working to transform patients' lives through science. Their mission is to discover, develop, and deliver innovative medicines that help patients prevail over serious diseases.
In this competition, you’ll interpret old chemical images. With access to a large set of synthetic image data generated by Bristol-Myers Squibb, you'll convert images back to the underlying chemical structure annotated as InChI text.
Tools to curate chemistry literature would be a significant benefit to researchers. If successful, you'll help chemists expand access to collective chemical research. In turn, this would speed up research and development efforts in many key fields by avoiding repetition of previously published chemistries and identifying novel trends via mining large data sets.
Photo by Terry Vlisidis on Unsplash","Submissions are evaluated on the mean Levenshtein distance between the InChi strings you submit and the ground truth InChi values.
Submission File
For each image_id in the test set, you must predict the InChi string of the molecule in the corresponding image. The file should contain a header and have the following format:
image_id,InChI
00000d2a601c,InChI=1S/H2O/h1H2
00001f7fc849,InChI=1S/H2O/h1H2
000037687605,InChI=1S/H2O/h1H2
etc.","In this competition, you are provided with images of chemicals, with the objective of predicting the corresponding International Chemical Identifier (InChI) text string of the image. The images provided (both in the training data as well as the test data) may be rotated to different angles, be at various resolutions, and have different noise levels.
Note: There are about 4m total images in this dataset. Unzipping the downloaded data will take a non-trivial amount of time.
Files
train/ - the training images, arranged in a 3-level folder structure by image_id
test/ - the test images, arranged in the same folder structure as train/
train_labels.csv - ground truth InChi labels for the training images
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c bms-molecular-translation,"['https://www.kaggle.com/code/yasufuminakama/inchi-resnet-lstm-with-attention-starter', 'https://www.kaggle.com/code/yasufuminakama/inchi-resnet-lstm-with-attention-inference', 'https://www.kaggle.com/code/ihelon/molecular-translation-exploratory-data-analysis', 'https://www.kaggle.com/code/yasufuminakama/inchi-preprocess-2', 'https://www.kaggle.com/code/stainsby/improved-synthetic-data-for-bms-competition-v3']"
144,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
Getting Started
Check out this Starter Notebook which walks you through how to make your very first submission!
For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each row in the test set, you must predict the probability of a binary target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:
id,target
5,0.5
6,0.1
8,0.9
etc.","For this competition, you will be predicting a binary target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat18 are categorical, and the feature columns cont0 - cont10 are continuous.
Files
train.csv - the training data with the target column
test.csv - the test set; you will be predicting the target for each row in this file (the probability of the binary target)
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-mar-2021,"['https://www.kaggle.com/code/gaetanlopez/how-to-make-clean-visualizations', 'https://www.kaggle.com/code/bextuychiev/lgbm-optuna-hyperparameter-tuning-w-understanding', 'https://www.kaggle.com/code/dwin183287/tps-mar-2021-eda-models', 'https://www.kaggle.com/code/craigmthomas/tps-mar-2021-stacked-starter', 'https://www.kaggle.com/code/subinium/dark-mode-visualization-apple-version']"
145,"In addition to the predictive modeling competitions we typically host (NCAA Women's and Men’s), we are hosting two separate, experimental challenges that ask you to not only predict the the winner, but to predict the margin of victor. The mania of March can come down to final second buzzer beaters, upsets, and even a few blowouts. Can you predict big wins as easily as you can predict close calls?
This competition (and the parallel competition for the women's tournament) allows you to explore how data science and machine learning can continue to examine the depths of college basketball. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.
In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the point spreads of the 2021 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results.
Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for it's 8th year!
Acknowledgments
Markus Spiske on Unsplash and The Noun Project","Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the predicted point spread and the observed spread.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2021 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict (64*63)/2  = 2,016 matchups.
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2015_3106_3107"" indicates team 3106 played team 3107 in the year 2015. You must predict the difference in score between both teams, relative to the team with the lower id. For example, if you believe team 3106 will beat 3107 by 7 points, your prediction would be 7. if you believe team 3106 will lose to 3107 by 3 points, your prediction will be -3.
The resulting submission format looks like the following, where `Pred` represents the difference in score between the lower id team and the higher id team:
ID,Pred
2015_3106_3107,7
2015_3106_3110,-1
2015_3106_3113,-9
...",,,
146,,,,,
147,,,,,
148,,,,,
149,,,,,
150,"There are billions of humans on this earth, and each of us is made up of trillions of cells. Just like every individual is unique, even genetically identical twins, scientists observe differences between the genetically identical cells in our bodies.
Differences in the location of proteins can give rise to such cellular heterogeneity. Proteins play essential roles in virtually all cellular processes. Often, many different proteins come together at a specific location to perform a task, and the exact outcome of this task depends on which proteins are present. As you can imagine, different subcellular distributions of one protein can give rise to great functional heterogeneity between cells. Finding such differences, and figuring out how and why they occur, is important for understanding how cells function, how diseases develop, and ultimately how to develop better treatments for those diseases.
To see more, start with less. That may seem counterintuitive, but the study of a single cell enables the discovery of mechanisms too difficult to see with multi-cell research. The importance of studying single cells is reflected in the ongoing revolution in biology centered around technologies for single cell analysis. Microscopy offers an opportunity to study differences in protein localizations within a population of cells. Current machine learning models for classifying protein localization patterns in microscope images gives a summary of the entire population of cells. However, the single-cell revolution in biology demands models that can precisely classify patterns in each individual cell in the image.
The Human Protein Atlas is an initiative based in Sweden that is aimed at mapping proteins in all human cells, tissues, and organs. The data in the Human Protein Atlas database is freely accessible to scientists all around the world that allows them to explore the cellular makeup of the human body. Solving the single-cell image classification challenge will help us characterize single-cell heterogeneity in our large collection of images by generating more accurate annotations of the subcellular localizations for thousands of human proteins in individual cells. Thanks to you, we will be able to more accurately model the spatial organization of the human cell and provide new open-access cellular data to the scientific community, which may accelerate our growing understanding of how human cells functions and how diseases develop.
This is a weakly supervised multi-label classification problem and a code competition. Given images of cells from our microscopes and labels of protein location assigned together for all cells in the image, Kagglers will develop models capable of segmenting and classifying each individual cell with precise labels. If successful, you'll contribute to the revolution of single-cell biology!
The scientific journal Nature Methods is interested in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team, led by Professor Emma Lundberg, would like to invite top performing teams to join as co-authors in writing this paper. Please follow the discussion forum for more details on how you can help.
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated by computing [mAP], with the mean taken over the 19 segmentable classes of the challenge. It is identical to the OpenImages Instance Segmentation Challenge evaluation. The OpenImages version of the metric is described in detail here. See also this tutorial on running the evaluation in Python.
Segmentation is calculated using IoU with a threshold of 0.6.
Submission File
For each image in the test set, you must predict a list of instance segmentation masks and their associated detection score (Confidence). The submission csv file uses the following format:
ImageID,ImageWidth,ImageHeight,PredictionString
ImageAID,ImageAWidth,ImageAHeight,LabelA1 ConfidenceA1 EncodedMaskA1 LabelA2 ConfidenceA2 EncodedMaskA2 ...
ImageBID,ImageBWidth,ImageBHeight,LabelB1 ConfidenceB1 EncodedMaskB1 LabelB2 ConfidenceB2 EncodedMaskB2 …
Note that a mask MAY have more than one class. If that is the case, predict separate detections for each class using the same mask.
ImageID,ImageWidth,ImageHeight,PredictionString
ImageAID,ImageAWidth,ImageAHeight,LabelA1 ConfidenceA1 EncodedMaskA1 LabelA2 ConfidenceA2 EncodedMaskA1 ...
A sample with real values would be:
ID,ImageWidth,ImageHeight,PredictionString
721568e01a744247,1118,1600,0 0.637833 eNqLi8xJM7BOTjS08DT2NfI38DfyM/Q3NMAJgJJ+RkBs7JecF5tnAADw+Q9I
7b018c5e3a20daba,1600,1066,16 0.85117 eNqLiYrLN7DNCjDMMIj0N/Iz9DcwBEIDfyN/QyA2AAsBRfxMPcKTA1MMADVADIo=
The binary segmentation masks are run-length encoded (RLE), zlib compressed, and base64 encoded to be used in text format as EncodedMask. Specifically, we use the Coco masks RLE encoding/decoding (see the encode method of COCO’s mask API), the zlib compression/decompression (RFC1950), and vanilla base64 encoding.
An example python function to encode an instance segmentation mask would be:
import base64
import numpy as np
from pycocotools import _mask as coco_mask
import typing as t
import zlib


def encode_binary_mask(mask: np.ndarray) -> t.Text:
  """"""Converts a binary mask into OID challenge encoding ascii text.""""""

  # check input mask --
  if mask.dtype != np.bool:
    raise ValueError(
        ""encode_binary_mask expects a binary mask, received dtype == %s"" %
        mask.dtype)

  mask = np.squeeze(mask)
  if len(mask.shape) != 2:
    raise ValueError(
        ""encode_binary_mask expects a 2d mask, received shape == %s"" %
        mask.shape)

  # convert input mask to expected COCO API input --
  mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)
  mask_to_encode = mask_to_encode.astype(np.uint8)
  mask_to_encode = np.asfortranarray(mask_to_encode)

  # RLE encode mask --
  encoded_mask = coco_mask.encode(mask_to_encode)[0][""counts""]

  # compress and base64 encoding --
  binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)
  base64_str = base64.b64encode(binary_str)
  return base64_str
(This code is available as a gist here.)","What files do I need?
On the data page below, you will find a set of full size original images (a mix of 1728x1728, 2048x2048 and 3072x3072 PNG files) in train.zip and test.zip. (Please note that since this is a code competition, part of test data will be hidden)
You will also need the image level labels from train.csv and the filenames for the test set from sample_submission.csv.
As many Kagglers made use of all public images in HPA for previous classification challenge, we made the public HPA images available to download as instructed in this notebook. Note also that there are TFRecords available if competitors would like to use TPUs.
The 16-bit version of the training images are available here. Additional training images are available here.
What should I expect the data format to be?
The training image-level labels are provided for each sample in train.csv.
The bulk of the data for images - train.zip. Each sample consists of four files. Each file represents a different filter on the subcellular protein patterns represented by the sample. The format should be [filename]_[filter color].png for the PNG files. Colors are red for microtubule channels, blue for nuclei channels, yellow for Endoplasmic Reticulum (ER) channels, and green for protein of interest.",kaggle competitions download -c hpa-single-cell-image-classification,"['https://www.kaggle.com/code/thedrcat/hpa-single-cell-classification-eda', 'https://www.kaggle.com/code/its7171/mmdetection-for-segmentation-training', 'https://www.kaggle.com/code/lnhtrang/single-cell-patterns', 'https://www.kaggle.com/code/frlemarchand/generate-masks-from-weak-image-level-labels', 'https://www.kaggle.com/code/its7171/mmdetection-for-segmentation-inference']"
151,"Whether it be in an arcade, on a phone, as an app, on a computer, or maybe stumbled upon in a web search, many of us have likely developed fond memories playing a version of Snake. It’s addicting to control a slithering serpent and watch it grow along the grid until you make one… wrong… move. Then you have to try again because surely you won’t make the same mistake twice!
With Hungry Geese, Kaggle has taken this classic in the video game industry and put a multi-player, simulation spin to it. You will create an AI agent to play against others and survive the longest. You must make sure your goose doesn’t starve or run into other geese; it’s a good thing that geese love peppers, donuts, and pizza—which show up across the board.
Extensive research exists in building Snake models using reinforcement learning, Q-learning, neural networks, and more (maybe you’ll use… Python?). Take your grid-based reinforcement learning knowledge to the next level with this exciting new challenge!","Each day, your team is able to submit up to 5 agents (bots) to the competition. Each submission will play episodes (games) against other bots on the ladder that have a similar skill rating. Over time, skill ratings will go up with wins or down with losses. Every bot submitted will continue to play games until the end of the competition. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents our uncertainty of that estimate which will decrease over time.
When you upload a Submission, we first play a Validation Episode where that Submission plays against a copy of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error. Otherwise, we initialize the Submission with μ0=600, and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions and try to pick Submissions with similar ratings for fair matches. We aim to run ~8 Episodes a day per Submission with an additional slight rate increase for the newest-submitted Episodes to give you feedback faster.
After an Episode finishes, we'll update the Rating estimate of both agents in that Episode. If one agent won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
At the submission deadline, additional submissions will be locked. One additional week will be allotted to continue to run games. At the conclusion of this week, the leaderboard is final.",,,"['https://www.kaggle.com/code/ihelon/hungry-geese-agents-comparison', 'https://www.kaggle.com/code/ilialar/risk-averse-greedy-goose', 'https://www.kaggle.com/code/ks2019/handy-rl-training-notebook', 'https://www.kaggle.com/code/alexandersamarin/training-resnet-agent-from-scratch', 'https://www.kaggle.com/code/gabrielmilan/crazy-goose']"
152,,,,,
153,"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions, and thus more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching a month-long tabular Playground competition on the 1st of every month, and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Good luck and have fun!
Getting Started
Check out this Starter Notebook which walks you through how to make your very first submission!
For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.","Submissions are scored on the root mean squared error. RMSE is defined as:
$$ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} $$
where \( \hat{y} \) is the predicted value, \( y \) is the original value, and \( n \) is the number of rows in the test data.
Submission File
For each row in the test set, you must predict the value of the target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:
id,target
0,0.5
2,10.2
6,2.2
etc.","For this competition, you will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cont1 - cont14 are continuous.
Files
train.csv - the training data with the target column
test.csv - the test set; you will be predicting the target for each row in this file
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c tabular-playground-series-jan-2021,"['https://www.kaggle.com/code/hamzaghanmi/xgboost-catboost-using-optuna', 'https://www.kaggle.com/code/springmanndaniel/1st-place-turn-your-data-into-daeta', 'https://www.kaggle.com/code/iamleonie/handling-multimodal-distributions-fe-techniques', 'https://www.kaggle.com/code/somayyehgholami/results-driven-tabular-playground-series-201', 'https://www.kaggle.com/code/inversion/get-started-jan-tabular-playground-competition']"
154,"Serious complications can occur as a result of malpositioned lines and tubes in patients. Doctors and nurses frequently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity.
Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube malpositioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of malpositioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines.
The gold standard for the confirmation of line and tube positions are chest radiographs. However, a physician or radiologist must manually check these chest x-rays to verify that the lines and tubes are in the optimal position. Not only does this leave room for human error, but delays are also common as radiologists can be busy reporting other scans. Deep learning algorithms may be able to automatically detect malpositioned catheters and lines. Once alerted, clinicians can reposition or remove them to avoid life-threatening complications.
The Royal Australian and New Zealand College of Radiologists (RANZCR) is a not-for-profit professional organisation for clinical radiologists and radiation oncologists in Australia, New Zealand, and Singapore. The group is one of many medical organisations around the world (including the NHS) that recognizes malpositioned tubes and lines as preventable. RANZCR is helping design safety systems where such errors will be caught.
In this competition, you’ll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on 40,000 images to categorize a tube that is poorly placed.
The dataset has been labelled with a set of definitions to ensure consistency with labelling. The normal category includes lines that were appropriately positioned and did not require repositioning. The borderline category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position. The abnormal category included lines that required immediate repositioning.
If successful, your efforts may help clinicians save lives. Earlier detection of malpositioned catheters and lines is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients.
This is a Code Competition. Refer to Code Requirements for details.
Koopmann MC, Kudsk KA, Szotkowski MJ, Rees SM. A Team-Based Protocol and Electromagnetic Technology Eliminate Feeding Tube Placement Complications [Internet]. Vol. 253, Annals of Surgery. 2011. p. 297–302. Available from: http://dx.doi.org/10.1097/sla.0b013e318208f550
Sorokin R, Gottlieb JE. Enhancing patient safety during feeding-tube insertion: a review of more than 2,000 insertions. JPEN J Parenter Enteral Nutr. 2006 Sep;30(5):440–5.
Marderstein EL, Simmons RL, Ochoa JB. Patient safety: effect of institutional protocols on adverse events related to feeding tube placement in the critically ill. J Am Coll Surg. 2004 Jul;199(1):39–47; discussion 47–50.
Jemmett ME. Unrecognized Misplacement of Endotracheal Tubes in a Mixed Urban to Rural Emergency Medical Services Setting [Internet]. Vol. 10, Academic Emergency Medicine. 2003. p. 961–5. Available from: http://dx.doi.org/10.1197/s1069-6563(03)00315-4
Lotano R, Gerber D, Aseron C, Santarelli R, Pratter M. Utility of postintubation chest radiographs in the intensive care unit. Crit Care. 2000 Jan 24;4(1):50–3.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
To calculate the final score, AUC is calculated for each of the 11 labels, then averaged. The score is then the average of the individual AUCs of each predicted column.
Submission File
For each ID in the test set, you must predict a probability for all target variables. The file should contain a header and have the following format:
StudyInstanceUID,ETT - Abnormal,ETT - Borderline,ETT - Normal,NGT - Abnormal,NGT - Borderline,NGT - Incompletely Imaged,NGT - Normal,CVC - Abnormal,CVC - Borderline,CVC - Normal,Swan Ganz Catheter Present
1.2.826.0.1.3680043.8.498.62451881164053375557257228990443168843,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.83721761279899623084220697845011427274,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.12732270010839808189235995393981377825,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.11769539755086084996287023095028033598,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.87838627504097587943394933987052577153,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.53211840524738036417560823327351887819,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.93555795394184819372299157360228027866,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.52241894131170494723503100795076463919,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.36500167484503936720548852591033878284,0,0,0,0,0,0,0,0,0,0,0
1.2.826.0.1.3680043.8.498.86199852603457900780565655267977637728,0,0,0,0,0,0,0,0,0,0,0","In this competition, you’ll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on 40,000 images to categorize a tube that is poorly placed.
What files do I need?
You will need the train and test images. This is a code-only competition so there is a hidden test set (approximately 4x larger, with ~14k images) as well.
train.csv contains image IDs, binary labels, and patient IDs.
TFRecords are available for both train and test. (They are also available for the hidden test set.)
We've also included train_annotations.csv. These are segmentation annotations for training samples that have them. They are included solely as additional information for competitors.
Files
train.csv - contains image IDs, binary labels, and patient IDs.
sample_submission.csv - a sample submission file in the correct format
test - test images
train - training images",kaggle competitions download -c ranzcr-clip-catheter-line-classification,"['https://www.kaggle.com/code/yasufuminakama/ranzcr-resnext50-32x4d-starter-training', 'https://www.kaggle.com/code/underwearfitting/resnet200d-public-benchmark-2xtta-lb0-965', 'https://www.kaggle.com/code/yasufuminakama/ranzcr-resnext50-32x4d-starter-inference', 'https://www.kaggle.com/code/ihelon/catheter-position-exploratory-data-analysis', 'https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965']"
155,"When you have a broken arm, radiologists help save the day—and the bone. These doctors diagnose and treat medical conditions using imaging techniques like CT and PET scans, MRIs, and, of course, X-rays. Yet, as it happens when working with such a wide variety of medical tools, radiologists face many daily challenges, perhaps the most difficult being the chest radiograph. The interpretation of chest X-rays can lead to medical misdiagnosis, even for the best practicing doctor. Computer-aided detection and diagnosis systems (CADe/CADx) would help reduce the pressure on doctors at metropolitan hospitals and improve diagnostic quality in rural areas.
Existing methods of interpreting chest X-ray images classify them into a list of findings. There is currently no specification of their locations on the image which sometimes leads to inexplicable results. A solution for localizing findings on chest X-ray images is needed for providing doctors with more meaningful diagnostic assistance.
Established in August 2018 and funded by the Vingroup JSC, the Vingroup Big Data Institute (VinBigData) aims to promote fundamental research and investigate novel and highly-applicable technologies. The Institute focuses on key fields of data science and artificial intelligence: computational biomedicine, natural language processing, computer vision, and medical image processing. The medical imaging team at VinBigData conducts research in collecting, processing, analyzing, and understanding medical data. They're working to build large-scale and high-precision medical imaging solutions based on the latest advancements in artificial intelligence to facilitate effective clinical workflows.
In this competition, you’ll automatically localize and classify 14 types of thoracic abnormalities from chest radiographs. You'll work with a dataset consisting of 18,000 scans that have been annotated by experienced radiologists. You can train your model with 15,000 independently-labeled images and will be evaluated on a test set of 3,000 images. These annotations were collected via VinBigData's web-based platform, VinLab. Details on building the dataset can be found in our recent paper “VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations”.
If successful, you'll help build what could be a valuable second opinion for radiologists. An automated system that could accurately identify and localize findings on chest radiographs would relieve the stress of busy doctors while also providing patients with a more accurate diagnosis.
Acknowledgments
Challenge Organizing Team
Ha Q. Nguyen, PhD - Vingroup Big Data Institute
Hieu H. Pham, PhD - Vingroup Big Data Institute
Nhan T. Nguyen, MSc - Vingroup Big Data Institute
Dung B. Nguyen, BSc - Vingroup Big Data Institute
Minh Dao, PhD - Vingroup Big Data Institute
Van Vu, PhD - Vingroup Big Data Institute
Khanh Lam, MD, PhD - Hospital 108
Linh T. Le, MD, PhD - Hanoi Medical University Hospital
Data Contributors
The dataset used in this competition was created by assembling de-identified Chest X-ray studies provided by two hospitals in Vietnam: the Hospital 108 and the Hanoi Medical University Hospital.","The challenge uses the standard PASCAL VOC 2010 mean Average Precision (mAP) at IoU > 0.4.
Submission File
Images in the test set may contain more than one object. For each object in a given test image, you must predict a class ID, confidence score, and bounding box in format xmin ymin xmax ymax. If you predict that there are NO objects in a given image, you should predict 14 1.0 0 0 1 1, where 14 is the class ID for ""No finding"", 1.0 is the confidence, and 0 0 1 1 is a one-pixel bounding box.
The submission file should contain a header and have the following format:
ID,TARGET
004f33259ee4aef671c2b95d54e4be68,14 1 0 0 1 1
004f33259ee4aef671c2b95d54e4be69,11 0.5 100 100 200 200 13 0.7 10 10 20 20
etc.","In this competition, we are classifying common thoracic lung diseases and localizing critical findings. This is an object detection and classification problem.
For each test image, you will be predicting a bounding box and class for all findings. If you predict that there are no findings, you should create a prediction of ""14 1 0 0 1 1"" (14 is the class ID for no finding, and this provides a one-pixel bounding box with a confidence of 1.0).
The images are in DICOM format, which means they contain additional data that might be useful for visualizing and classifying.
Dataset information
The dataset comprises 18,000 postero-anterior (PA) CXR scans in DICOM format, which were de-identified to protect patient privacy.
All images were labeled by a panel of experienced radiologists for the presence of 14 critical radiographic findings as listed below:
0 - Aortic enlargement
1 - Atelectasis
2 - Calcification
3 - Cardiomegaly
- Consolidation
- ILD
- Infiltration
- Lung Opacity
- Nodule/Mass
- Other lesion
- Pleural effusion
- Pleural thickening
- Pneumothorax
- Pulmonary fibrosis",kaggle competitions download -c vinbigdata-chest-xray-abnormalities-detection,"['https://www.kaggle.com/code/raddar/convert-dicom-to-np-array-the-correct-way', 'https://www.kaggle.com/code/corochann/vinbigdata-detectron2-train', 'https://www.kaggle.com/code/dschettler8845/visual-in-depth-eda-vinbigdata-competition-data', 'https://www.kaggle.com/code/awsaf49/vinbigdata-cxr-ad-yolov5-14-class-train', 'https://www.kaggle.com/code/trungthanhnguyen0502/eda-vinbigdata-chest-x-ray-abnormalities']"
156,"Welcome
The Acea Group is one of the leading Italian multiutility operators. Listed on the Italian Stock Exchange since 1999, the company manages and develops water and electricity networks and environmental services. Acea is the foremost Italian operator in the water services sector supplying 9 million inhabitants in Lazio, Tuscany, Umbria, Molise, Campania.
In this competition we will focus only on the water sector to help Acea Group preserve precious waterbodies. As it is easy to imagine, a water supply company struggles with the need to forecast the water level in a waterbody (water spring, lake, river, or aquifer) to handle daily consumption. During fall and winter waterbodies are refilled, but during spring and summer they start to drain. To help preserve the health of these waterbodies it is important to predict the most efficient water availability, in terms of level and water flow for each day of the year.
Data
The reality is that each waterbody has such unique characteristics that their attributes are not linked to each other. This analytics competition uses datasets that are completely independent from each other. However, it is critical to understand total availability in order to preserve water across the country.
Each dataset represents a different kind of waterbody. As each waterbody is different from the other, the related features are also different. So, if for instance we consider a water spring we notice that its features are different from those of a lake. These variances are expected based upon the unique behavior and characteristics of each waterbody. The Acea Group deals with four different type of waterbodies: water springs, lakes, rivers and aquifers.
Challenge
Can you build a story to predict the amount of water in each unique waterbody? The challenge is to determine how features influence the water availability of each presented waterbody. To be more straightforward, gaining a better understanding of volumes, they will be able to ensure water availability for each time interval of the year.
The time interval is defined as day/month depending on the available measures for each waterbody. Models should capture volumes for each waterbody(for instance, for a model working on a monthly interval a forecast over the month is expected).
The desired outcome is a notebook that can generate four mathematical models, one for each category of waterbody (acquifers, water springs, river, lake) that might be applicable to each single waterbody.





See the Submission Evaluation criteria.","### Can you build a model to predict the amount of water in each waterbody to help preserve this natural resource? This is an Analytics competition where your task is to create a Notebook that best addresses the Evaluation criteria below. Submissions should be shared directly with host and will be judged by the Acea Group based on how well they address:
**Methodology/Completeness (min 0 points, max 5 points)** - Are the statistical models appropriate given the data? - Did the author develop one or more machine learning models? - Did the author provide a way of assessing the performance and accuracy of their solution? - What is the Mean Absolute Error (MAE) of the models? - What is the Root Mean Square Error (RMSE) of the models? **Presentation (min 0 points, max 5 points)** - Does the notebook have a compelling and coherent narrative? - Does the notebook contain data visualizations that help to communicate the author’s main points? - Did the author include a thorough discussion on the intersection between features and their prediction? For example between rainfall and amount/level of water. - Was there discussion of automated insight generation, demonstrating what factors to take into account? - Is the code documented in a way that makes it easy to understand and reproduce? - Were all external sources of data made public and cited appropriately? **Application (min 0 points, max 5 points)** - Is the provided model useful/able to forecast water availability in terms of level or water flow in a time interval of the year? - Is the provided methodology applicable also on new datasets belong to another waterbody? ###Examples of Winning Notebooks from other Analytics Competitions: - [Luca Basanisi](https://www.kaggle.com/lucabasa) - [Luca's submission](https://www.kaggle.com/lucabasa/quantify-the-madness-a-study-of-competitiveness) created a unique metric to label how competitive a game is. - [DS4G: Spatial Panel Data Modeling](https://www.kaggle.com/katemelianova/ds4g-spatial-panel-data-modeling). [@katemelianova](https://www.kaggle.com/katemelianova) and [@artvolgin](https://www.kaggle.com/artvolgin) An excellent submission with comprehensive correction and accounting for confounding factors, with a well reasoned and explained solution and recommendation.","This competition uses nine different datasets, completely independent and not linked to each other. Each dataset can represent a different kind of waterbody. As each waterbody is different from the other, the related features as well are different from each other. So, if for instance we consider a water spring we notice that its features are different from the lake’s one. This is correct and reflects the behavior and characteristics of each waterbody. The Acea Group deals with four different type of waterbodies: water spring (for which three datasets are provided), lake (for which a dataset is provided), river (for which a dataset is provided) and aquifers (for which four datasets are provided).
Let’s see how these nine waterbodies differ from each other.
Waterbody: Auser
Type: Aquifer
Description: This waterbody consists of two subsystems, called NORTH and SOUTH, where the former partly influences the behavior of the latter. Indeed, the north subsystem is a water table (or unconfined) aquifer while the south subsystem is an artesian (or confined) groundwater.
The levels of the NORTH sector are represented by the values of the SAL, PAG, CoS and DIEC wells, while the levels of the SOUTH sector by the LT2 well.
Waterbody: Petrignano",kaggle competitions download -c acea-water-prediction,"['https://www.kaggle.com/code/andreshg/timeseries-analysis-a-complete-guide', 'https://www.kaggle.com/code/iamleonie/intro-to-time-series-forecasting', 'https://www.kaggle.com/code/iamleonie/eda-quenching-the-thirst-for-insights', 'https://www.kaggle.com/code/sandhyakrishnan02/time-series-analysis-and-forecasting', 'https://www.kaggle.com/code/maksymshkliarevskyi/acea-smart-water-full-eda-prediction']"
157,"It's the most wonderful time of the year
With the elves eating candy
They’ll feel super dandy and be of good cheer
It's the most wonderful time of the year
It's the hap-happiest season of all
When spirits are lifted the toys will be gifted
And games to enthrall!
It's the hap-happiest season of all
The party for throwing
Has snow cones a’glowing
With bragging rights out on display.
So now you must plan it,
To beat the armed bandits
who keep all the candy away.
It's the most wonderful time of the year!
Morale has been low at the North Pole this year. But Santa really believes in “making spirits bright!” So he has planned a friendly competition among the elves to keep the Christmas cheer alive and make as many toys as possible! And the winning team gets a snow cone party!
As one of the team leaders, you know that nothing keeps your fellow elves more productive and motivated than a steady supply of candy canes! But all seven levels of the Candy Cane Forest are closed for revegetation, so the only ones available are stuck in the break room vending machines. And even though you receive free snacks on the job, the vending machines are always broken and don’t always give you what you want.
Due to social distancing, only two elves can be in the break room at once. You and another team leader will take turns trying to get candy canes out of the 100 possible vending machines in the room, but each machine is unpredictable in how likely it is to work. You do know, however, that the more often you try to use a machine, the less likely it will give you a candy cane. Plus, you only have time to try 2000 times on the vending machines until you need to get back to the workshop!
If you can collect more candy canes than the other team leaders, you’ll surely be able to help your team win Santa's contest! Try your hand at this multi-armed candy cane challenge!
Image Credit: Photos by Joanna Kosinska and Misty Ladd on Unsplash.","Each day, your team is able to submit up to 5 agents (bots) to the competition. Each submission will play episodes (games) against other bots on the ladder that have a similar skill rating. Over time, skill ratings will go up with wins or down with losses. Every bot submitted will continue to play games until the end of the competition. On the leaderboard, only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each submission has an estimated skill rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents our uncertainty of that estimate which will decrease over time.
When you upload a submission, we first play a validation episode where that submission plays against copies of itself to make sure it works properly. If the episode fails, the submission is marked as error. Otherwise, we initialize the submission with μ0=600 and it joins the pool of all submissions for ongoing evaluation.
We repeatedly run episodes from the pool of all submissions and try to pick submissions with similar ratings for fair matches. We aim to run ~8 episodes a day per submission, with an additional slight rate increase for the newest-submitted episodes to give you feedback faster.
After an episode finishes, we'll update the rating estimate for all submissions in that episode. If one submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values and also relative to each submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an episode does not affect the skill rating updates.
At the submission deadline, additional submissions will be locked. One additional week will be allotted to continue to run episodes. At the conclusion of this week, the leaderboard is final.",,,"['https://www.kaggle.com/code/ilialar/simple-multi-armed-bandit', 'https://www.kaggle.com/code/nagiss/santa2020-leaderboard-analysis', 'https://www.kaggle.com/code/subinium/merry-christmas', 'https://www.kaggle.com/code/a763337092/pull-vegas-slot-machines-add-weaken-rate-continue5', 'https://www.kaggle.com/code/isaienkov/santa-2020-starter']"
158,"Welcome to Kaggle's annual Machine Learning and Data Science Survey competition! You can read our executive summary here.
This year, as in 2017, 2018, and 2019 we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live for 3.5 weeks in October, and after cleaning the data we finished with 20,036 responses!
There's a lot to explore here. The results include raw numbers about who is working with data, what’s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published the data in as raw a format as possible without compromising anonymization, which makes it an unusual example of a survey dataset.
This year Kaggle is once again launching an annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community.
In our fourth year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey. For that reason, we’re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world.
The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A “story” could be defined any number of ways, and that’s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!
Submissions will be evaluated on the following:
Composition - Is there a clear narrative thread to the story that’s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.
Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.
Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible
To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid.
How to Participate
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will review the last (most recent) entry.
No submission is necessary for the Notebook Award. To be eligible, a notebook must be public and use the 2020 Data Science Survey as a data source.
Submission deadline: 11:59PM UTC, January 6th, 2021.","How to Participate
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will review the last (most recent) entry.
No submission is necessary for the Notebook Award. To be eligible, a notebook must be public and use the 2020 Data Science Survey as a data source.
Submissions will be evaluated on the following:
Composition - Is there a clear narrative thread to the story that’s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.
Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.
Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible
To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid.","Main Data:
kaggle_survey_2020_responses.csv: 39+ questions and 20,036 responses
Responses to multiple choice questions (only a single choice can be selected)
were recorded in individual columns. Responses to multiple selection questions
(multiple choices can be selected) were split into multiple columns (with one
column per answer choice).
Supplementary Data:
kaggle_survey_2020_answer_choices.pdf: list of answer choices for every question
With footnotes describing which questions were asked to which respondents.
kaggle_survey_2020_methodology.pdf: a description of how the survey was conducted
You can ask additional questions by posting in the pinned Q&A thread.",kaggle competitions download -c kaggle-survey-2020,"['https://www.kaggle.com/code/subinium/kaggle-2020-visualization-analysis', 'https://www.kaggle.com/code/dwin183287/kagglers-seen-by-continents', 'https://www.kaggle.com/code/corazzon/how-to-use-pandas-filter-in-survey-eda', 'https://www.kaggle.com/code/michau96/education-level-affects-data-analysis', 'https://www.kaggle.com/code/andradaolteanu/treasure-hunt-what-gives-to-be-really-good']"
159,"As the second-largest provider of carbohydrates in Africa, cassava is a key food security crop grown by smallholder farmers because it can withstand harsh conditions. At least 80% of household farms in Sub-Saharan Africa grow this starchy root, but viral diseases are major sources of poor yields. With the help of data science, it may be possible to identify common diseases so they can be treated.
Existing methods of disease detection require farmers to solicit the help of government-funded agricultural experts to visually inspect and diagnose the plants. This suffers from being labor-intensive, low-supply and costly. As an added challenge, effective solutions for farmers must perform well under significant constraints, since African farmers may only have access to mobile-quality cameras with low-bandwidth.
In this competition, we introduce a dataset of 21,367 labeled images collected during a regular survey in Uganda. Most images were crowdsourced from farmers taking photos of their gardens, and annotated by experts at the National Crops Resources Research Institute (NaCRRI) in collaboration with the AI lab at Makerere University, Kampala. This is in a format that most realistically represents what farmers would need to diagnose in real life.
Your task is to classify each cassava image into four disease categories or a fifth category indicating a healthy leaf. With your help, farmers may be able to quickly identify diseased plants, potentially saving their crops before they inflict irreparable damage.
Recommended Tutorial
We highly recommend Jesse Mostipak’s Getting Started Tutorial that walks you through making your very first submission step by step.
Acknowledgements
The Makerere Artificial Intelligence (AI) Lab is an AI and Data Science research group based at Makerere University in Uganda. The lab specializes in the application of artificial intelligence and data science - including for example, methods from machine learning, computer vision and predictive analytics to problems in the developing world. Their mission is: “To advance Artificial Intelligence research to solve real-world challenges.""
We thank the different experts and collaborators from National Crops Resources Research Institute (NaCRRI) for assisting in preparing this dataset.

This is a Code Competition. Refer to Code Requirements for details.","Evaluation
Submissions will be evaluated based on their categorization accuracy.
Submission Format
The submission format for the competition is a csv file with the following format:
image_id,label
1000471002.jpg,4
1000840542.jpg,4
etc.","Can you identify a problem with a cassava plant using a photo from a relatively inexpensive camera? This competition will challenge you to distinguish between several diseases that cause material harm to the food supply of many African countries. In some cases the main remedy is to burn the infected plants to prevent further spread, which can make a rapid automated turnaround quite useful to the farmers.
Files
[train/test]_images the image files. The full set of test images will only be available to your notebook when it is submitted for scoring. Expect to see roughly 15,000 images in the test set.
train.csv
image_id the image file name.
label the ID code for the disease.
sample_submission.csv A properly formatted sample submission, given the disclosed test set content.
image_id the image file name.
label the predicted ID code for the disease.",kaggle competitions download -c cassava-leaf-disease-classification,"['https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline', 'https://www.kaggle.com/code/khyeh0719/pytorch-efficientnet-baseline-train-amp-aug', 'https://www.kaggle.com/code/ihelon/cassava-leaf-disease-exploratory-data-analysis', 'https://www.kaggle.com/code/jessemostipak/getting-started-tpus-cassava-leaf-disease', 'https://www.kaggle.com/code/khyeh0719/pytorch-efficientnet-baseline-inference-tta']"
160,"“Buy low, sell high.” It sounds so easy….
In reality, trading for profit has always been a difficult problem to solve, even more so in today’s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time.
In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their “fair values” and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.
Developing trading strategies to identify and take advantage of inefficiencies is challenging. Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. As a result, it can be hard to distinguish good luck from having made a good trading decision.
In the first three months of this challenge, you will build your own quantitative trading model to maximize returns using market data from a major global stock exchange. Next, you’ll test the predictiveness of your models against future market returns and receive feedback on the leaderboard.
Your challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject.
In general, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to “fair” values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation.
Jane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world.
Admittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there’s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge.
This is a Code Competition. Refer to Code Requirements for details.","This competition is evaluated on a utility score. Each row in the test set represents a trading opportunity for which you will be predicting an action value, 1 to make the trade and 0 to pass on it. Each trade j has an associated weight and resp, which represents a return.
For each date i, we define:
$$
p_i = \sum_j(weight_{ij} * resp_{ij} * action_{ij}),
$$
$$
t = \frac{\sum p_i }{\sqrt{\sum p_i^2}} * \sqrt{\frac{250}{|i|}},
$$
where \(|i|\) is the number of unique dates in the test set. The utility is then defined as:
$$ u = min(max(t,0), 6) \sum p_i. $$
Submission File
You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow the following template in Kaggle Notebooks:
import janestreet
env = janestreet.make_env() # initialize the environment
iter_test = env.iter_test() # an iterator which loops over the test set

for (test_df, sample_prediction_df) in iter_test:
    sample_prediction_df.action = 0 #make your 0/1 prediction here
    env.predict(sample_prediction_df)",,,"['https://www.kaggle.com/code/carlmcbrideellis/jane-street-eda-of-day-0-and-feature-importance', 'https://www.kaggle.com/code/hamditarek/market-prediction-xgboost-with-gpu-fit-in-1min', 'https://www.kaggle.com/code/aimind/bottleneck-encoder-mlp-keras-tuner-8601c5', 'https://www.kaggle.com/code/gogo827jz/jane-street-neural-network-starter', 'https://www.kaggle.com/code/muhammadmelsherbini/jane-street-extensive-eda-pca-starter']"
161,"Who doesn't enjoy the morning chirp of a bird or a frog’s evening croak? Animals bring more than sweet songs and natural ambience to the world. The presence of rainforest species is a good indicator of the impact of climate change and habitat loss. As it's easier to hear these species than see them, it’s important to use acoustic technologies that can work on a global scale. Real-time information, such as provided through machine learning techniques, could enable early-stage detection of human impacts on the environment. This result could drive more effective conservation management decisions.
Traditional methods of assessing the diversity and abundance of species are costly and limited in space and time. And while automatic acoustic identification via deep learning has been successful, models require a large number of training samples per species. This limits applicability to rarer species, which are central to conservation efforts. Thus, methods to automate high-accuracy species detection in noisy soundscapes with limited training data are the solution.
Rainforest Connection (RFCx) created the world’s first scalable, real-time monitoring system for protecting and studying remote ecosystems. Unlike visual-based tracking systems like drones or satellites, RFCx relies on acoustic sensors that monitor the ecosystem soundscape at selected locations year round. RFCx technology has advanced to support a comprehensive biodiversity monitoring program that allows local partners to measure progress of wildlife restoration and recovery through principles of adaptive management. The RFCx monitoring platform also has the capacity to create convolutional neural network (CNN) models for analysis.
In this competition, you’ll automate the detection of bird and frog species in tropical soundscape recordings. You'll create your models with limited, acoustically complex training data. Rich in more than bird and frog noises, expect to hear an insect or two, which your model will need to filter out.
If successful, you'll have a hand in a rapidly expanding field of science: the development of automated eco-acoustic monitoring systems. The resulting real-time information could enable earlier detection of human environmental impacts, making environmental conservation more swift and effective.","The task consists of predicting the species present in each test audio file. Some test audio files contain a single species while others contain multiple. The predictions are to be done at the audio file level, i.e., no start/end timestamps are required.
The competition metric is the label-weighted label-ranking average precision, which is a generalization of the mean reciprocal rank measure for the case where there can be multiple true labels per test item.
The ""label-weighted"" part means that the overall score is the average over all the labels in the test set, where each label receives equal weight (by contrast, plain lrap gives each test observation equal weight, thereby discounting the contribution of individual labels when when an observation has multiple labels). In other words, each test observation is weighted by the number of ground truth labels found in the observation.
Submission File
For each recording_id in the test set, you must predict the probability of each species label being found in the audio sample. The file should contain a header (each species number with an s prefix) and have the following format:
recording_id,s0,...,s23
000316da7,0.1,....,0.3
003bc2cb2,0.0,...,0.8
etc.","In this competition, you are given audio files that include sounds from numerous species. Your task is, for each test audio file, to predict the probability that each of the given species is audible in the audio clip. While the training files contain both the species identification as well as the time the species was heard, the time localization is not part of the test predictions.
Note that the training data also includes false positive label occurrences to assist with training.
Files
train_tp.csv - training data of true positive species labels, with corresponding time localization
train_fp.csv - training data of false positives species labels, with corresponding time localization
sample_submission.csv - a sample submission file in the correct format; note each species column has an s prefix.
train/ - the training audio files
test/ - the test audio files; the task is to predict the species found in each audio file
tfrecords/{train,test} - competition data in the TFRecord format, which includes recording_id, audio_wav (encoded in 16-bit PCM format), and label_info (for train only), which provides a,-delimited string of the columns below (minus recording_id), where multiple labels for a recording_id are ;-delimited.
Columns",kaggle competitions download -c rfcx-species-audio-detection,"['https://www.kaggle.com/code/shreyasdhaware/pretraining-hubert', 'https://www.kaggle.com/code/alpborakirte/rfcx-audio-detection', 'https://www.kaggle.com/code/khoaphamdang/rfcx-v3', 'https://www.kaggle.com/code/sagniksanyal/birdclef-2022-torchaudio-audiomentations-skimpy', 'https://www.kaggle.com/code/rdipiazza/spectral-segmentation-edit-raph']"
162,"The National Football League (NFL) has teamed up with Amazon Web Services (AWS) to develop the “Digital Athlete,” a virtual representation of a composite NFL player that the NFL can use to model game scenarios to try to better predict and prevent player injury. The NFL is actively addressing the need for a computer vision system to detect on-field helmet impacts as part of the “Digital Athlete” platform, and the league is calling on Kagglers to help.
In this competition, you’ll develop a computer vision model that automatically detects helmet impacts that occur on the field. Kick off with a dataset of more than one thousand definitive head impacts from thousands of game images, labeled video from the sidelines and end zones, and player tracking data. This information is sourced from the NFL’s Next Gen Stats (NGS) system, which documents the position, speed, acceleration, and orientation for every player on the field during NFL games.
This competition is part of the NFL’s annual 1st and Future competition, which is designed to spur innovation in athlete safety and performance. For the first time this year, 1st and Future will be broadcast in primetime during Super Bowl LV week on NFL Network, and winning Kagglers may have the opportunity to present their computer vision systems as part of this exciting event.
If successful, you could support the NFL’s research programs in a big way: improving athletes' safety. Backed by this research, the NFL may implement rule changes and helmet design improvements to try to better protect the athletes who play the game millions watch each week.
The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit NFL.com/PlayerHealthandSafety.
This is a Code Competition. Refer to Code Requirements for details.","In this task, you will segment helmet collisions in videos of football plays using bounding boxes. This competition is evaluated using a micro F1 score at an Intersection over Union (IoU) threshold of 0.35.
There are a few important differences from other bounding box metrics:
The main departure from a traditional metric is that some imprecision on the timing of the impact is acceptable. For a given ground truth impact, a prediction within +/- 4 frames (9 frames total) within the same play can be accepted as valid without necessarily degrading the score. Assuming the player is moving over the course of those frames, the exact bounding box predicted to achieve an IoU of 1.0 would also vary depending on the frame.
As one helmet may partially obscure another from the camera's perspective, both predicted and ground truth bounding boxes may overlap. However, at most one prediction will ever be assigned to a given ground truth box.
The two criteria described above mean that one or more predictions could theoretically be assigned to more than one ground truth boxes. If this happens, our metric will optimize for the assignments between your prediction(s) and the ground truth boxes that lead to the highest total number of True Positives (thereby maximizing the F1 score). At most one prediction will be assigned to any ground truth box and vice versa.
The IoU of a proposed bounding box and a ground truth bounding box is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric treats any IoU of at least 0.35 as a true positive.
F1 is calculated as follows:
$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$
where:
$$
precision = \frac{TP}{TP + FP}
$$
$$
recall = \frac{TP}{TP + FN}
$$
Submission File
Due to the custom metric, this competition relies on an evaluation pipeline which is slightly different than a typical code competition. Your notebook must import and submit via the custom nflimpact python module available in Kaggle notebooks. To submit, simply add these three lines at the end of your code:
import nflimpact
env = nflimpact.make_env()
env.predict(df) # df is a pandas dataframe of your entire submission file
The dataframe should be in the following format. Each row in your submission represents a single predicted bounding box for the given frame. Note that it is not required to include labels of which players had an impact, only a bounding box where it occurred.
gameKey,playID,view,video,frame,left,width,top,height
57590,3607,Endzone,57590_003607_Endzone.mp4,1,1,1,1,1
57590,3607,Sideline,57590_003607_Sideline.mp4,1,1,1,1,1
57595,1252,Endzone,57595_001252_Endzone.mp4,1,1,1,1,1
57595,1252,Sideline,57595_001252_Sideline.mp4,1,1,1,1,1
etc.","In this competition, you are tasked with identifying helmet collisions in video files. Each play has two associated videos, showing a sideline and endzone view, and the videos are aligned so that frames correspond between the videos. The training set videos are in train with corresponding labels in train_labels.csv, while the videos for which you must predict are in the test folder.
To aid with helmet detection, you are also provided an ancillary dataset of images showing helmets with labeled bounding boxes. These files are located in images and the bounding boxes in image_labels.csv.
This is a code competition. When you submit, your model will be rerun on a set of 15 unseen plays located in the same test location. The publicly provided test videos are simply a set of mock plays (copied from the training set) which are not used in scoring.
Note: the dataset provided for this competition has been carefully designed for the purposes of training computer vision models and therefore contains plays that have much higher incidence of helmet impacts than is normal. This dataset should not be used to make inferences about the incidence of helmet impact rates during football games, as it is not a representative sample of those rates.
Files
[train/test] mp4 videos of each play. Each play has two copies, one shot from the endzone and the other shot from the sideline. The video pairs are matched frame for frame in time, but different players may be visible in each view. You only need to make predictions for the view that a player is actually visible in.",kaggle competitions download -c nfl-impact-detection,"['https://www.kaggle.com/code/orkatz2/yolo-video', 'https://www.kaggle.com/code/s903124/perspective-transform-of-helmet-and-tracking-data', 'https://www.kaggle.com/code/nikitautin/35th-place-efficientdet-train', 'https://www.kaggle.com/code/nikitautin/35th-place-efficientdet-inference', 'https://www.kaggle.com/code/robin26091991/nfl-helmet-impact-detection-computer-vision-system']"
163,"Our best estimates show there are over 7 billion people on the planet and 300 billion stars in the Milky Way galaxy. By comparison, the adult human body contains 37 *trillion* cells. To determine the function and relationship among these cells is a monumental undertaking. Many areas of human health would be impacted if we better understand cellular activity. A problem with this much data is a great match for the Kaggle community. Just as the Human Genome Project mapped the entirety of human DNA, the [Human BioMolecular Atlas Program](https://hubmapconsortium.org/) (HuBMAP) is a major endeavor. Sponsored by the National Institutes of Health (NIH), HuBMAP is working to catalyze the development of a framework for mapping the human body at a level of glomeruli functional tissue units for the first time in history. Hoping to become one of the world’s largest collaborative biological projects, HuBMAP aims to be an open map of the human body at the cellular level. This competition, “Hacking the Kidney,"" starts by mapping the human kidney at single cell resolution. Your challenge is to detect functional tissue units (FTUs) across different tissue preparation pipelines. An FTU is defined as a “three-dimensional block of cells centered around a capillary, such that each cell in this block is within diffusion distance from any other cell in the same block” ([de Bono, 2013](https://www.ncbi.nlm.nih.gov/pubmed/24103658)). The goal of this competition is the implementation of a successful and robust glomeruli FTU detector. You will also have the opportunity to present your findings to a panel of judges for additional consideration. Successful submissions will construct the tools, resources, and cell atlases needed to determine how the relationships between cells can affect the health of an individual. Advancements in HuBMAP will accelerate the world’s understanding of the relationships between cell and tissue organization and function and human health. These datasets and insights can be used by researchers in cell and tissue anatomy, pharmaceutical companies to develop therapies, or even parents to show their children the magnitude of the human body. > **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/hubmap-kidney-segmentation/overview/code-requirements) for details.**",,"The HuBMAP data used in this hackathon includes 11 fresh frozen and 9 Formalin Fixed Paraffin Embedded (FFPE) PAS kidney images. Glomeruli FTU annotations exist for all 20 tissue samples; some of these will be shared for training, and others will be used to judge submissions.
There are over 600,000 glomeruli in each human kidney (Nyengaard, 1992). Normal glomeruli typically range from 100-350μm in diameter with a roughly spherical shape (Kannan, 2019).
Teams are invited to develop segmentation algorithms that identify glomeruli in the PAS stained microscopy data. They are welcome to use other external data and/or pre-trained machine learning models in support of FTU segmentation. All data and all code used must be released under Attribution 4.0 International (CC BY 4.0).
The Dataset
The dataset is comprised of very large (>500MB - 5GB) TIFF files. The training set has 8, and the public test set has 5. The private test set is larger than the public test set.
The training set includes annotations in both RLE-encoded and unencoded (JSON) forms. The annotations denote segmentations of glomeruli.
Both the training and public test sets also include anatomical structure segmentations. They are intended to help you identify the various parts of the tissue.
File structure",kaggle competitions download -c hubmap-kidney-segmentation,"['https://www.kaggle.com/code/konathamanilkrishna/kidneysegmentationimageprocessing', 'https://www.kaggle.com/code/uitacma/kech-unanalyzed-tiles105k', 'https://www.kaggle.com/code/keehb97/testkee1', 'https://www.kaggle.com/code/sabahesaraki/kidney-tissue-segmentation-mask', 'https://www.kaggle.com/code/ch1119/final-project']"
164,"Rock, Paper, Scissors (sometimes called roshambo) has been a staple to settle playground disagreements or determine who gets to ride in the front seat on a road trip. The game is simple, with a balance of power. There are three options to choose from, each winning or losing to the other two. In a series of truly random games, each player would win, lose, and draw roughly one-third of games. But people are not truly random, which provides a fun opportunity for AI.
Studies have shown that a Rock, Paper, Scissors AI can consistently beat human opponents. With previous games as input, it studies patterns to understand a player’s tendencies. But what happens when we expand the simple “Best-of-3” game to be “Best-of-1000”? How well can artificial intelligence perform?
In this simulation competition, you will create an AI to play against others in many rounds of this classic game. Can you find patterns to make yours win more often than it loses? It’s possible to greatly outperform a random player when the matches involve non-random agents. A strong AI can consistently beat predictable AI.
This problem is fundamental to the fields of machine learning, artificial intelligence, and data compression. There are even potential applications in human psychology and hierarchical temporal memory. Warm up your hands and get ready to Rock, Paper, Scissors in this challenge.
Image acknowledgements:
Photos from The Noun Project: Rock, Paper, Scissors","Each day, your team is able to submit up to 5 agents (bots) to the competition. Each submission will play episodes (games) against other bots on the ladder that have a similar skill rating. Over time, skill ratings will go up with wins or down with losses. Every bot submitted will continue to play games until the end of the competition. On the leaderboard, only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents our uncertainty of that estimate which will decrease over time.
When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions and try to pick Submissions with similar ratings for fair matches. We aim to run ~8 Episodes a day per Submission, with an additional slight rate increase for the newest-submitted Episodes to give you feedback faster.
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
At the submission deadline, additional submissions will be locked. One additional week will be allotted to continue to run episodes. At the conclusion of this week, the leaderboard is final.",,,"['https://www.kaggle.com/code/saabet/rock-paper-scissors-classification-alexnet', 'https://www.kaggle.com/code/faduregis/multi-armed-bandit-vs-deterministic-agents', 'https://www.kaggle.com/code/bryanthogerson/rps-psycho-100-70', 'https://www.kaggle.com/code/brandonnova/working-winning-algo', 'https://www.kaggle.com/code/iamprajapatirahul/simple-and-basic-rock-paper-scissors']"
165,"Winning the fight against the COVID-19 pandemic will require an effective vaccine that can be equitably and widely distributed. Building upon decades of research has allowed scientists to accelerate the search for a vaccine against COVID-19, but every day that goes by without a vaccine has enormous costs for the world nonetheless. We need new, fresh ideas from all corners of the world. Could online gaming and crowdsourcing help solve a worldwide pandemic? Pairing scientific and crowdsourced intelligence could help computational biochemists make measurable progress.
mRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19, but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA). Conventional vaccines (like your seasonal flu shots) are packaged in disposable syringes and shipped under refrigeration around the world, but that is not currently possible for mRNA vaccines.
Researchers have observed that RNA molecules have the tendency to spontaneously degrade. This is a serious limitation--a single cut can render the mRNA vaccine useless. Currently, little is known on the details of where in the backbone of a given RNA is most prone to being affected. Without this knowledge, current mRNA vaccines against COVID-19 must be prepared and shipped under intense refrigeration, and are unlikely to reach more than a tiny fraction of human beings on the planet unless they can be stabilized.
The Eterna community, led by Professor Rhiju Das, a computational biochemist at Stanford’s School of Medicine, brings together scientists and gamers to solve puzzles and invent medicine. Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles. The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules. The Eterna community has previously unlocked new scientific principles, made new diagnostics against deadly diseases, and engaged the world’s most potent intellectual resources for the betterment of the public. The Eterna community has advanced biotechnology through its contribution in over 20 publications, including advances in RNA biotechnology.
In this competition, we are looking to leverage the data science expertise of the Kaggle community to develop models and design rules for RNA degradation. Your model will predict likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position. We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines. These final test sequences are currently being synthesized and experimentally characterized at Stanford University in parallel to your modeling efforts -- Nature will score your models!
Improving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve. Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19. The problem we are trying to solve has eluded academic labs, industry R&D groups, and supercomputers, and so we are turning to you. To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.
                  ","Submissions are scored using MCRMSE, mean columnwise root mean squared error:
$$
\textrm{MCRMSE} = \frac{1}{N_{t}}\sum_{j=1}^{N_{t}}\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_{ij} - \hat{y}_{ij})^2}
$$
where \(N_t\) is the number of scored ground truth target columns, and \(y\) and \(\hat{y}\) are the actual and predicted values, respectively.
From the Data page: There are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: reactivity, deg_Mg_pH10, and deg_Mg_50C.
Submission File
For each sample id in the test set, you must predict targets for each sequence position (seqpos), one per row. If the length of the sequence of an id is, e.g., 107, then you should make 107 predictions. Positions greater than the seq_scored value of a sample are not scored, but still need a value in the solution file.
id_seqpos,reactivity,deg_Mg_pH10,deg_pH10,deg_Mg_50C,deg_50C    
id_00073f8be_0,0.1,0.3,0.2,0.5,0.4
id_00073f8be_1,0.3,0.2,0.5,0.4,0.2
id_00073f8be_2,0.5,0.4,0.2,0.1,0.2
etc.","In this competition, you will be predicting the degradation rates at various locations along RNA sequence.
There are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: reactivity, deg_Mg_pH10, and deg_Mg_50C.
Files
train.json - the training data
test.json - the test set, without any columns associated with the ground truth.
sample_submission.csv - a sample submission file in the correct format
Columns
id - An arbitrary identifier for each sample.
seq_scored - (68 in Train and Public Test, 91 in Private Test) Integer value denoting the number of positions used in scoring with predicted values. This should match the length of reactivity, deg_* and *_error_* columns. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.
seq_length - (107 in Train and Public Test, 130 in Private Test) Integer values, denotes the length of sequence. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.",kaggle competitions download -c stanford-covid-vaccine,"['https://www.kaggle.com/code/mrkmakr/covid-ae-pretrain-gnn-attn-cnn', 'https://www.kaggle.com/code/xhlulu/openvaccine-simple-gru-model', 'https://www.kaggle.com/code/its7171/gru-lstm-with-feature-engineering-and-augmentation', 'https://www.kaggle.com/code/tuckerarrants/openvaccine-gru-lstm', 'https://www.kaggle.com/code/symyksr/openvaccine-deepergcn']"
166,"When a quarterback takes a snap and drops back to pass, what happens next may seem like chaos. As offensive players move in various patterns, the defense works together to prevent successful pass completions and then to quickly tackle receivers that do catch the ball. In this year’s Kaggle competition, your goal is to use data science to better understand the schemes and players that make for a successful defense against passing plays.
In American football, there are a plethora of defensive strategies and outcomes. The National Football League (NFL) has used previous Kaggle competitions to focus on offensive plays, but as the old proverb goes, “defense wins championships.” Though metrics for analyzing quarterbacks, running backs, and wide receivers are consistently a part of public discourse, techniques for analyzing the defensive part of the game trail and lag behind. Identifying player, team, or strategic advantages on the defensive side of the ball would be a significant breakthrough for the game.
This competition uses NFL’s Next Gen Stats data, which includes the position and speed of every player on the field during each play. You’ll employ player tracking data for all drop-back pass plays from the 2018 regular season. The goal of submissions is to identify unique and impactful approaches to measure defensive performance on these plays. There are several different directions for participants to ‘tackle’ (ha)—which may require levels of football savvy, data aptitude, and creativity. As examples:
What are coverage schemes (man, zone, etc) that the defense employs? What coverage options tend to be better performing?
Which players are the best at closely tracking receivers as they try to get open?
Which players are the best at closing on receivers when the ball is in the air?
Which players are the best at defending pass plays when the ball arrives?
Is there any way to use player tracking data to predict whether or not certain penalties – for example, defensive pass interference – will be called?
Who are the NFL’s best players against the pass?
How does a defense react to certain types of offensive plays?
Is there anything about a player – for example, their height, weight, experience, speed, or position – that can be used to predict their performance on defense?
What does data tell us about defending the pass play? You are about to find out.
Note: Are you a university participant? Students have the option to participate in a college-only Competition, where you’ll work on the identical themes above. Students can opt-in for either the Open or College Competitions, but not both.","Your challenge is to generate actionable, practical, and novel insights from player tracking data that corresponds to defensive backs. Suggestions made here represent some of the approaches that football coaches are currently thinking of, but there undoubtedly several others.
An entry to the competition consists of a Notebook submission that is evaluated on the following five components, where 0 is the low score and 10 is the high score.
Note: All notebooks submitted must be made public on or before the submission deadline to be eligible. If submitting as a team, all team members must be listed as collaborators on all notebooks submitted.
Open Competition: The first aim takes on what an NFL defense does once a quarterback drops back to pass. This includes coverage schemes (typically man versus zone), how players (often termed “secondary” defenders) disrupt and prevent the offense from completing passes, and how, once the ball is in the air, the defense works to ensure that a pass falls incomplete. There are several different approaches requiring levels of football savvy, data aptitude, and creativity.
College Competition: Same as the above but only open to college students. Students may only submit to either the Open or College Competition but not both.
Big Data Bowl 2021 scoring sheet
Submissions will be judged by the NFL based on how well they address:
Innovation:
Are the proposed findings actionable?
Is this a way of looking at tracking data that is novel?
Is this project creative?
Accuracy:
Is the work correct?
Are claims backed up by data?
Are the statistical models appropriate given the data?
Relevance:
Would NFL teams (or the league office) be able to use these results on a week-to-week basis?
Does the analysis account for variables that make football data complex?
Clarity:
Evaluate the writing with respect to how clear the writer(s) make findings.
Data visualization/tables:
Are the charts and tables provided accessible, interesting, visually appealing, and accurate?
Judges will consist of analytics staffers that are working for either (i) NFL headquarters, (ii) the 32 NFL teams, or (iii) player tracking vendors that work with NFL teams. Scores will be averaged so that each of the components above are weighed equally. The top scoring entries in each of the two categories will be awarded as winners.
Each of the eight winning teams will be invited to present their results to the NFL for the chance to win an additional top prize of $10,000. Evaluation criteria for that presentation will be provided later.
Notebooks should consist of no more than 2,000 words and no more than 7 tables/figures. Submissions will not be penalized for any number of words or figures under this limit. Participants are encouraged to show statistical code if it helps readers better understand their analyses; most, if not all code, however, should be hidden in the Appendix.","The 2021 Big Data Bowl data contains player tracking, play, game, and player level information for all possible passing plays during the 2018 regular season. For purposes of this event, passing plays are considered to be ones on a pass was thrown, the quarterback was sacked, or any one of five different penalties was called (defensive pass interference, offensive pass interference, defensive holding, illegal contact, or roughing the passer). On each play, linemen (both offensive and defensive) data are not provided. The focus of this year's contest is on pass coverage.
Here, you'll find a summary of each data set in the 2021 Data Bowl, a list of key variables to join on, and a description of each variable.
File descriptions
Game data: The games.csv contains the teams playing in each game. The key variable is gameId.
Player data: The players.csv file contains player-level information from players that participated in any of the tracking data files. The key variable is nflId.
Play data: The plays.csv file contains play-level information from each game. The key variables are gameId and playId.
Tracking data: Files week[week].csv contain player tracking data from all games in week [week]. The key variables are gameId, playId, and nflId. There are 17 weeks to a typical NFL Regular Season, and thus 17 data frames with player tracking data are provided.",kaggle competitions download -c nfl-big-data-bowl-2021,"['https://www.kaggle.com/code/robikscube/nfl-big-data-bowl-plotting-player-position', 'https://www.kaggle.com/code/jpmiller/some-best-practices-for-analytics-reporting', 'https://www.kaggle.com/code/utkarshxy/object-detection-with-yolo-complete-theory-5mins', 'https://www.kaggle.com/code/subinium/tips-for-making-the-informative-visualization', 'https://www.kaggle.com/code/isaienkov/nfl-big-data-bowl-2021-eda']"
167,"If every breath is strained and painful, it could be a serious and potentially life-threatening condition. A pulmonary embolism (PE) is caused by an artery blockage in the lung. It is time consuming to confirm a PE and prone to overdiagnosis. Machine learning could help to more accurately identify PE cases, which would make management and treatment more effective for patients. Currently, CT pulmonary angiography (CTPA), is the most common type of medical imaging to evaluate patients with suspected PE. These CT scans consist of hundreds of images that require detailed review to identify clots within the pulmonary arteries. As the use of imaging continues to grow, constraints of radiologists’ time may contribute to delayed diagnosis. The Radiological Society of North America (RSNA®) has teamed up with the Society of Thoracic Radiology (STR) to help improve the use of machine learning in the diagnosis of PE. In this competition, you’ll detect and classify PE cases. In particular, you'll use chest CTPA images (grouped together as studies) and your data science skills to enable more accurate identification of PE. If successful, you'll help reduce human delays and errors in detection and treatment. With 60,000-100,000 PE deaths annually in the United States, it is among the most fatal cardiovascular diseases. Timely and accurate diagnosis will help these patients receive better care and may also improve outcomes. > **This is a Code Competition. Refer to [Code Requirements](https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection/overview/code-requirements) for details.** ###Acknowledgments The Radiological Society of North America (RSNA®) is an international society of radiologists, medical physicists, and other medical professionals with more than 53,400 members worldwide. RSNA hosts the world’s premier radiology forum and publishes two top peer-reviewed journals: Radiology, the highest-impact scientific journal in the field, and RadioGraphics, the only journal dedicated to continuing education in radiology. The Society of Thoracic Radiology (STR) was founded in 1982. The STR is dedicated to advancing cardiothoracic imaging in clinical application, education, and research in radiology and allied disciplines. Continuing professional development opportunities provided by the STR include educational and scientific meetings, mentorship programs, grant support and award opportunities, our society journal, Journal of Thoracic Imaging, and global collaboration activities. [A full set of acknowledgments can be found on this page](https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection/overview/acknowledgments).","Every study / exam has a row for each label that is scored (detailed in the Data page). It is uniquely indicated by the StudyInstanceUID. Every image, further, has a row for the PE Present on Image label and is uniquely indicated by the SOPInstanceUID. Your prediction file should have a number of rows equal to: (number of images) + (number of studies * number of scored labels).
Metric
The metric used in this competition is weighted log loss. It is weighted to account for the relative importance of some labels. There are 9 study-level labels and one image-level label, detailed further on the Data page.
Exam-level weighted log loss
Let y_ij = 1 if label j was annotated to exam i and y_ij = 0, otherwise. Let p_ij be the predicted probability that y_ij = 1:
i = 1, 2, …, N for N exams in the test set
j = 1, 2, …, 9 labels
Let w_j signify the weight for label j.
The weights are as follows:
Label Weight
Negative for PE 0.0736196319
Indeterminate 0.09202453988
Chronic 0.1042944785
Acute & Chronic 0.1042944785
Central PE 0.1877300613
Left PE 0.06257668712
Right PE 0.06257668712
RV/LV Ratio >= 1 0.2346625767
RV/LV Ratio < 1 0.0782208589
Kaggle uses a binary log loss equation for each label and then takes the mean of the log loss over all labels.
The binary weighted log loss function for label j on exam i is specified as:
$$
L_{ij} = - w_j * [ y_{ij}*log(p_{ij}) + (1-y_{ij})*log(1-p_{ij}) ]
$$
Image-level weighted log loss
Let y_ik = 1 if image k in exam i was annotated as ‘PE Present on Image’; otherwise, y_ik = 0.
Let p_ik be the predicted probability that y_ik = 1.
w = 0.07361963
i = 1, 2, …, N exams
k = 1, 2, …, n_i, where n_i is the number of images in exam i
Then, let m_i = sum_(k = 1 to n_i) y_ik be the number of positive images in exam i such that
q_i = m_i/n_i is the proportion of positive images in exam i
At the image level, we have a binary classification where the image is classified as PE Present on Image or not (image is negative for PE).
The image-level log loss is written as:
The total loss is the average of all image- and exam-level loss, divided by the average of all row (both image- and exam-level) weights. To get the average of all row weights, sum the weights of all images (q_i*w for each image) and all exam-level labels (w_j for each label j in the test set) and divide by the number of rows.
Submission Format
id,label
df06fad17bc3_negative_exam_for_pe,0.5
df06fad17bc3_rv_lv_ratio_gte_1,0.5
df06fad17bc3_rv_lv_ratio_lt_1,0.5
df06fad17bc3_leftsided_pe,0.5
df06fad17bc3_chronic_pe,0.5
df06fad17bc3_rightsided_pe,0.5
df06fad17bc3_acute_and_chronic_pe,0.5
df06fad17bc3_central_pe,0.5
df06fad17bc3_indeterminate,0.5
eb3cbf4180b5,0.5
57b93aeb1b16,0.5
ca48991fcad3,0.5
c72c1f5763d4,0.5
26c67856a1e9,0.5
3c64e5645222,0.5
d3e59334bba4,0.5
be315623c913,0.5
74941ba7b035,0.5
70589c8529fb,0.5
etc.","RSNA Pulmonary Embolism Detection Challenge Terms of Use and Attribution
You may access and use these de-identified imaging datasets and annotations (“the data”) for non-commercial purposes only, including academic research and education, as long as you agree to abide by the following provisions:
Not to make any attempt to identify or contact any individual(s) who may be the subjects of the data.
If you share or re-distribute the data in any form, include a citation to the “RSNA-STR Pulmonary Embolism CT (RSPECT) Dataset, Copyright RSNA, 2020” as follows:
E Colak, FC Kitamura, SB Hobbs, et al. The RSNA Pulmonary Embolism CT Dataset [https://pubs.rsna.org/doi/full/10.1148/ryai.2021200254]. Radiology: Artificial Intelligence 2021;3:2.
Data Overview
In this competition, we are predicting the existence and characteristics of pulmonary embolisms. Please see the Evaluation page for further details about the predictions themselves.
This competition is inference-only, meaning that your submitted kernels will not have access to the training set.",kaggle competitions download -c rsna-str-pulmonary-embolism-detection,"['https://www.kaggle.com/code/pablopalacios2001/detecci-n-de-embolias-pulmonares-eda', 'https://www.kaggle.com/code/pablopalacios2001/transfer-learning-xgboost', 'https://www.kaggle.com/code/pablopalacioslpez/transfer-learning-vgg19', 'https://www.kaggle.com/code/pablopalacioslpez/model-creation-batchnormalization-and-dropout', 'https://www.kaggle.com/code/pablopalacioslpez/data-augmentation-batchnormalization-and-drop-out']"
168,"CDP is a global non-profit that drives companies and governments to reduce their greenhouse gas emissions, safeguard water resources, and protect forests. Each year, CDP takes the information supplied in its annual reporting process and scores companies and cities based on their journey through disclosure and towards environmental leadership.
CDP houses the world’s largest, most comprehensive dataset on environmental action. As the data grows to include thousands more companies and cities each year, there is increasing potential for the data to be utilized in impactful ways. Because of this potential, CDP is excited to launch an analytics challenge for the Kaggle community. Data scientists will scour environmental information provided to CDP by disclosing companies and cities, searching for solutions to our most pressing problems related to climate change, water security, deforestation, and social inequity.
How do you help cities adapt to a rapidly changing climate amidst a global pandemic, but do it in a way that is socially equitable?
What are the projects that can be invested in that will help pull cities out of a recession, mitigate climate issues, but not perpetuate racial/social inequities?
What are the practical and actionable points where city and corporate ambition join, i.e. where do cities have problems that corporations affected by those problems could solve, and vice versa?
How can we measure the intersection between environmental risks and social equity, as a contributor to resiliency?
PROBLEM STATEMENT
Develop a methodology for calculating key performance indicators (KPIs) that relate to the environmental and social issues that are discussed in the CDP survey data. Leverage external data sources and thoroughly discuss the intersection between environmental issues and social issues. Mine information to create automated insight generation demonstrating whether city and corporate ambitions take these factors into account.
HOW TO PARTICIPATE
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will only review the most recent entry. A starter notebook demonstrates how to load and work with the data.
To be valid, a submission must be contained in one or more notebook, and made public on or before the submission deadline. Participants are free to use any datasets in addition to the official Kaggle dataset, but those datasets must be public and hosted on Kaggle for the submission to be valid.","HOW TO PARTICIPATE
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will only review the most recent entry. A starter notebook demonstrates how to load and work with the data.
To be valid, a submission must be contained in one or more notebook, and made public on or before the submission deadline. Participants are free to use any datasets in addition to the official Kaggle dataset, but those datasets must be public and hosted on Kaggle for the submission to be valid.
SUBMISSIONS WILL BE EVALUATED ON THE FOLLOWING
Accuracy/Completeness
Did the author develop one or more key performance indicators (KPIs)?
Did the author provide a way of assessing the performance and accuracy of their solution?
Are the KPIs useful for discussing relationships between social issues and environmental issues and demonstrating whether city and corporate ambitions take these factors into account?
Do the KPIs accurately reflect the underlying data?
Communication
Does the notebook have a compelling and coherent narrative?
Does the notebook contain data visualizations that help to communicate the author’s main points?
Did the author include a thorough discussion on the intersection between environmental issues and social issues?
Was there discussion of automated insight generation, demonstrating whether city and corporate ambitions take these factors into account?
Documentation
Is the code documented in a way that makes it easy to understand and reproduce?
Were all external sources of data made public and cited appropriately?","Overview of the Data:
The CDP dataset consists of publicly available responses to 3 different surveys: (1) corporate climate change disclosures; (2) corporate water security disclosures; and (3) disclosures from cities. Data is available for 2018, 2019, and 2020, along with a small collection of supplementary datasets. A starter notebook demonstrates how to load and work with the data.
Main Data:
2018_Full_Climate_Change_Dataset.csv (298.53 MB)
2019_Full_Climate_Change_Dataset.csv (879.62 MB)
2020_Full_Climate_Change_Dataset.csv (511.43 MB)
2018_Full_Water_Security_Dataset.csv (64.52 MB)
2019_Full_Water_Security_Dataset.csv (128.6 MB)
2020_Full_Water_Security_Dataset.csv(127.32 MB)
2018_Full_Cities_Dataset.csv (71.16 MB)
2019_Full_Cities_Dataset.csv (214.66 MB)",kaggle competitions download -c cdp-unlocking-climate-solutions,"['https://www.kaggle.com/code/jpmiller/some-best-practices-for-analytics-reporting', 'https://www.kaggle.com/code/subinium/tips-for-making-the-informative-visualization', 'https://www.kaggle.com/code/anshumoudgil/untying-climate-s-knots-visualisation', 'https://www.kaggle.com/code/callumr22/cdp-starter-notebook', 'https://www.kaggle.com/code/boltmaud/started-with-dataviz-next-step-is-data-mining']"
169,"This is a synthetic code challenge to sharpen your programming skills. This problem was first released during the 2016 qualification round of Google's annual coding competition, Hash Code. We’ve re-released it as a Playground Code Competition to help you sharpen your skills. Along with the Photo Slideshow Optimization competition, open for late submissions, you can use it as practice in advance of Hash Code 2021.
The Internet has profoundly changed the way we buy things, but the online shopping of today is likely not the end of that change; the expectations for purchase delivery has gone from a week, to two days, to one day, to same day. What about in just a few hours? With drones, this may be possible, and they’ll bring a whole new fleet of problems to solve with data science.
Drones are­ autonomous, electric vehicles often used to deliver online purchases. Current experiments use flying drones, so they’re never stuck in traffic. As drone technology improves every year, there remains a major issue: how would we manage and coordinate all those drones?
In this competition, you are given a hypothetical fleet of drones, a list of customer orders, and availability of the individual products in warehouses. Can you schedule the drone operations so that the orders are completed as soon as possible?
When flying delivery drones become the norm, scheduling is one of the many problems to be solved. Get a head start—and improve your data science skills at the same time.
This is a Code Competition. Refer to Code Requirements for details.
Photo by Ian Usher on Unsplash","Each completed order will earn between 1 and 100 points, depending on the turn in which it is completed.
The order is completed in the first turn at the end of which all items in the order are delivered.
For an order completed in turn t and a simulation taking T turns in total, the score for the order is calculated as (T − t) / T × 100 , rounded up to the next integer.
For example, if the simulation takes 160 turns (T = 160), and an order consists of three items, delivered at turns 5, 15 and 15, then the order is considered completed at t = 15, and the score is calculated as (160 − 15)/160 = 0.90625 , multiplied by 100 and rounded up to 91 points.
For more examples, see the instructions pdf on the data tab.","The Internet has profoundly changed the way we buy things, but the online shopping of today is likely not
the end of that change; after each purchase we still need to wait multiple days for physical goods to be
carried to our doorstep.
This is where drones come in ­ autonomous, electric vehicles delivering online purchases. Flying, so never
stuck in traffic. As drone technology improves every year, there remains a major issue: how do we manage
and coordinate all those drones?
Given a hypothetical fleet of drones, a list of customer orders and availability of the individual products in warehouses, your task is to schedule the drone operations so that the orders are completed as soon as possible. You will need to handle the complications of multiple drones, customer orders, product types and weights, warehouses, and delivery destinations.
This is a more involved optimization problem so for the full details please see the pdf in the dataset. Note that the pdf discusses the possibility of multiple input files but this competition only uses one.",kaggle competitions download -c hashcode-drone-delivery,"['https://www.kaggle.com/code/jpmiller/application-of-google-or-tools', 'https://www.kaggle.com/code/srii96/hashcode-problem-understanding-pre-process', 'https://www.kaggle.com/code/piantic/drone-delivery-basic-eda-and-submission', 'https://www.kaggle.com/code/egrehbbt/greedy-solution-post-processing', 'https://www.kaggle.com/code/spacelx/2020-hc-dd-2nd-place-solution-w-or-tools']"
170,"What if scientists could anticipate volcanic eruptions as they predict the weather? While determining rain or shine days in advance is more difficult, weather reports become more accurate on shorter time scales. A similar approach with volcanoes could make a big impact. Just one unforeseen eruption can result in tens of thousands of lives lost. If scientists could reliably predict when a volcano will next erupt, evacuations could be more timely and the damage mitigated.
Currently, scientists often identify “time to eruption” by surveying volcanic tremors from seismic signals. In some volcanoes, this intensifies as volcanoes awaken and prepare to erupt. Unfortunately, patterns of seismicity are difficult to interpret. In very active volcanoes, current approaches predict eruptions some minutes in advance, but they usually fail at longer-term predictions.
Enter Italy's Istituto Nazionale di Geofisica e Vulcanologia (INGV), with its focus on geophysics and volcanology. The INGV's main objective is to contribute to the understanding of the Earth's system while mitigating the associated risks. Tasked with the 24-hour monitoring of seismicity and active volcano activity across the country, the INGV seeks to find the earliest detectable precursors that provide information about the timing of future volcanic eruptions.
In this competition, using your data science skills, you’ll predict when a volcano's next eruption will occur. You'll analyze a large geophysical dataset collected by sensors deployed on active volcanoes. If successful, your algorithms will identify signatures in seismic waveforms that characterize the development of an eruption.
With enough notice, areas around a volcano can be safely evacuated prior to their destruction. Seismic activity is a good indicator of an impending eruption, but earlier precursors must be identified to improve longer-term predictability. The impact of your participation could be felt worldwide with tens of thousands of lives saved by more predictable volcanic ruptures and earlier evacuations.","Submissions are evaluated on the mean absolute error (MAE) between the predicted loss and the actual loss.
Submission File
For every id in the test set, you should predict the time until the next eruption. The file should contain a header and have the following format:
segment_id,time_to_eruption
1,1
2,2
3,3
etc.","Detecting volcanic eruptions before they happen is an important problem that has historically proven to be a very difficult. This competition provides you with readings from several seismic sensors around a volcano and challenges you to estimate how long it will be until the next eruption. The data represent a classic signal processing setup that has resisted traditional methods.
Identifying the exact sensors may be possible but would not be in the spirit of the competition nor further the scientific objectives. Please respect the importance of the problem and the time invested by the researchers at INGV in making this problem available by not seeking more metadata or information that would be unavailable in a real prediction context.
Files
train.csv Metadata for the train files.
segment_id: ID code for the data segment. Matches the name of the associated data file.
time_to_eruption: The target value, the time until the next eruption.
[train|test]/*.csv: the data files. Each file contains ten minutes of logs from ten different sensors arrayed around a volcano. The readings have been normalized within each segment, in part to ensure that the readings fall within the range of int16 values. If you are using the Pandas library you may find that you still need to load the data as float32 due to the presence of some nulls.",kaggle competitions download -c predict-volcanic-eruptions-ingv-oe,"['https://www.kaggle.com/code/isaienkov/ingv-volcanic-eruption-prediction-eda-modeling', 'https://www.kaggle.com/code/jesperdramsch/introduction-to-volcanology-seismograms-and-lgbm', 'https://www.kaggle.com/code/amanooo/ingv-volcanic-basic-solution-stft', 'https://www.kaggle.com/code/carpediemamigo/ingv-catboost-baseline-tsfresh', 'https://www.kaggle.com/code/ajcostarino/ingv-volcanic-eruption-prediction-lgbm-baseline']"
171,"The Connectivity Map, a project within the Broad Institute of MIT and Harvard, the Laboratory for Innovation Science at Harvard (LISH), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS), present this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms.
What is the Mechanism of Action (MoA) of a drug? And why is it important?
In the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.
How do we determine the MoAs of a new drug?
One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.
In this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells’ responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset.
As is customary, the dataset has been split into testing and training subsets. Hence, your task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.
How to evaluate the accuracy of a solution?
Based on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair.
If successful, you’ll help to develop an algorithm to predict a compound’s MoA given its cellular signature, thus helping scientists advance the drug discovery process.
This is a Code Competition. Refer to Code Requirements for details.","For every sig_id you will be predicting the probability that the sample had a positive response for each <MoA> target. For \(N\) sig_id rows and \(M\) <MoA> targets, you will be making \(N \times M\) predictions. Submissions are scored by the log loss:
$$ \text{score} = - \frac{1}{M}\sum_{m=1}^{M} \frac{1}{N} \sum_{i=1}^{N} \left[ y_{i,m} \log(\hat{y}_{i,m}) + (1 - y_{i,m}) \log(1 - \hat{y}_{i,m})\right] $$
where:
\(N\) is the number of sig_id observations in the test data (\(i=1,…,N\))
\(M\) is the number of scored MoA targets (\(m=1,…,M\))
\( \hat{y}_{i,m} \) is the predicted probability of a positive MoA response for a sig_id
\( y_{i,m} \) is the ground truth, 1 for a positive response, 0 otherwise
\( log() \) is the natural (base e) logarithm
Note: the actual submitted predicted probabilities are replaced with \(max(min(p,1-10^{-15}),10^{-15})\). A smaller log loss is better.
Submission File
You must predict a probability of a positive target for each sig_id-<MoA> pair. The id used for the submission is created by concatenating the sig_id with the MoA target for which you are predicting. The file should have a header and be in the following format:
sig_id,11-beta-hsd1_inhibitor,ace_inhibitor,...,wnt_inhibitor
id_000644bb2,0.32,0.01,...,0.57
id_000a6266a,0.88,0.27,...,0.42
etc...","In this competition, you will be predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data.
Two notes:
the training data has an additional (optional) set of MoA labels that are not included in the test data and not used for scoring.
the re-run dataset has approximately 4x the number of examples seen in the Public test.
Files
train_features.csv - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).
train_drug.csv - This file contains an anonymous drug_id for the training set only.
train_targets_scored.csv - The binary MoA targets that are scored.
train_targets_nonscored.csv - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.
test_features.csv - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.",kaggle competitions download -c lish-moa,"['https://www.kaggle.com/code/headsortails/explorations-of-action-moa-eda', 'https://www.kaggle.com/code/isaienkov/mechanisms-of-action-moa-prediction-eda', 'https://www.kaggle.com/code/namanj27/new-baseline-pytorch-moa', 'https://www.kaggle.com/code/yasufuminakama/moa-pytorch-nn-starter', 'https://www.kaggle.com/code/amiiiney/drugs-moa-classification-eda']"
172,"Riiid AIEd Challenge 2020
Challenge Website
Thank you for all those who attended the AAAI-2021 workshop on AI Education! Prize-winning teams presented their models at the AAAI-2021 Workshop on AI Education - Imagining Post-COVID Education with AI - on February 9, 2021. You can find the model write-ups on the workshop website.
Think back to your favorite teacher. They motivated and inspired you to learn. And they knew your strengths and weaknesses. The lessons they taught were based on your ability. For example, teachers would make sure you understood algebra before advancing to calculus. Yet, many students don’t have access to personalized learning. In a world full of information, data scientists like you can help. Machine learning can offer a path to success for young people around the world, and you are invited to be part of this mission.
In 2018, 260 million children weren't attending school. At the same time, more than half of these young students didn't meet minimum reading and math standards. Education was already in a tough place when COVID-19 forced most countries to temporarily close schools. This further delayed learning opportunities and intellectual development. The equity gaps in every country could grow wider. We need to re-think the current education system in terms of attendance, engagement, and individualized attention.
Riiid Labs, an AI solutions provider delivering creative disruption to the education market, empowers global education players to rethink traditional ways of learning leveraging AI. With a strong belief in equal opportunity in education, Riiid launched an AI tutor based on deep-learning algorithms in 2017 that attracted more than one million South Korean students. This year, the company released EdNet, the world’s largest open database for AI education containing more than 100 million student interactions.
In this competition, your challenge is to create algorithms for ""Knowledge Tracing,"" the modeling of student knowledge over time. The goal is to accurately predict how students will perform on future interactions. You will pair your machine learning skills using Riiid’s EdNet data.
Your innovative algorithms will help tackle global challenges in education. If successful, it’s possible that any student with an Internet connection can enjoy the benefits of a personalized learning experience, regardless of where they live. With your participation, we can build a better and more equitable model for education in a post-COVID-19 world.
Acknowledgements
Academic Advisors
Paul Kim, Stanford Graduate School of Education
Neil Heffernan, WPI & ASSISTments
Partners","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
You must make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.
The kernels environment automatically formats and creates your submission files in this competition. There is no need to manually create your submissions.","Tailoring education to a student's ability level is one of the many valuable things an AI tutor can do. Your challenge in this competition is a version of that overall task; you will predict whether students are able to answer their next questions correctly. You'll be provided with the same sorts of information a complete education app would have: that student's historic performance, the performance of other students on the same question, metadata about the question itself, and more.
This is a time-series code competition, you will receive test set data and make predictions with Kaggle's time-series API. Please be sure to review the Time-series API Details section closely.
Files
train.csv
row_id: (int64) ID code for the row.
timestamp: (int64) the time in milliseconds between this user interaction and the first event completion from that user.
user_id: (int32) ID code for the user.
content_id: (int16) ID code for the user interaction
content_type_id: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.
task_container_id: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a .",kaggle competitions download -c riiid-test-answer-prediction,
173,"This is a relaunch of a previous competition, Conway's Reverse Game of Life, with the following changes:
The grid size is larger (25 vs. 25) and the grid wraps around from top to bottom and left to right
Submissions are solved forward by the appropriate number of steps, so that any correct starting solution will achieve a maximum score. This article contains the stepping function that is used for this competition.
Obligatory Disclaimer: A lot has changed since the original competition was launched 6 years ago. With the change from ""exact starting point"" to ""any correct starting point"", it is possible to get a perfect score. We just don't know how difficult that will be. Use it as a fun learning experience, and don't spoil it for others by posting perfect solutions!
~~~~~~~~~
The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state:
Overpopulation: if a living cell is surrounded by more than three living cells, it dies.
Stasis: if a living cell is surrounded by two or three living cells, it survives.
Underpopulation: if a living cell is surrounded by fewer than two living cells, it dies.
Reproduction: if a dead cell is surrounded by exactly three cells, it becomes a live cell.
These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics. As Wikipedia states
Ever since its publication, Conway's Game of Life has attracted much interest, because of the surprising ways in which the patterns can evolve. Life provides an example of emergence and self-organization. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that ""design"" and ""organization"" can spontaneously emerge in the absence of a designer. For example, philosopher and cognitive scientist Daniel Dennett has used the analogue of Conway's Life ""universe"" extensively to illustrate the possible evolution of complex philosophical constructs, such as consciousness and free will, from the relatively simple set of deterministic physical laws governing our own universe.
The emergence of order from simple rules begs an interesting question—what happens if we set time backwards?
This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse. Is the chaotic start of Life predictable from its orderly ends? We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board.
This is a Code Competition. Refer to Code Requirements for details.","You are evaluated on the mean absolute error of your predictions, stepped forward by the specified, and compared to the provided ending solution.
In this case, this is equivalent to \(1 - \text{classification accuracy}\) across all of the cells. You may only predict 0 (dead) or 1 (alive) for each cell.
Submission File
For every game in the test set, your submission file should list the predicted starting board on a single row. Values are listed in a row-wise order. That is, if you want to predict a matrix,
1 2
3 4
the predicted row would be (1,2,3,4). The submission file should contain a header and have the following format:
id,start_0,start_1,start_2,...,start_624
50000,0,0,0,0,0,0,...,0
50001,0,0,0,0,0,0,...,0
etc.","We have provided 50,000 training games and 50,000 test games, whose starting board you must predict. Each board is 25x25, for a total of 625 cells per board. Values are listed in a row-wise order. You are free to create more training games if you desire.
The provided variables are:
id - unique identifier of each game
delta - the number of steps between the start and stop boards
start_0 - row 1, column 1 of the game's starting board
start_1 - row 1, column 2 of the game's starting board
…
stop_0 - row 1, column 1 of the game's stopping board
…
Your test-set predictions should be the starting board at delta steps before the stopping board. The games were created by the following procedure:
An initial board was chosen by filling the board with a random density between 1% full (mostly zeros) and 99% full (mostly ones).",kaggle competitions download -c conways-reverse-game-of-life-2020,"['https://www.kaggle.com/code/jamesmcguigan/game-of-life-z3-constraint-satisfaction', 'https://www.kaggle.com/code/ulrich07/sample-submission', 'https://www.kaggle.com/code/yakuben/crgl-probability-extension-true-target-problem', 'https://www.kaggle.com/code/ulrich07/quick-neighborhood-fe-mlp-keras', 'https://www.kaggle.com/code/yakuben/crgl2020-iterative-cnn-approach']"
174,"“Every artist dips his brush in his own soul, and paints his own nature into his pictures.”
-Henry Ward Beecher
We recognize the works of artists through their unique style, such as color choices or brush strokes. The “je ne sais quoi” of artists like Claude Monet can now be imitated with algorithms thanks to generative adversarial networks (GANs). In this getting started competition, you will bring that style to your photos or recreate the style from scratch!
Computer vision has advanced tremendously in recent years and GANs are now capable of mimicking objects in a very convincing way. But creating museum-worthy masterpieces is thought of to be, well, more art than science. So can (data) science, in the form of GANs, trick classifiers into believing you’ve created a true Monet? That’s the challenge you’ll take on!
The Challenge:
A GAN consists of at least two neural networks: a generator model and a discriminator model. The generator is a neural network that creates the images. For our competition, you should generate images in the style of Monet. This generator is trained using a discriminator.
The two models will work against each other, with the generator trying to trick the discriminator, and the discriminator trying to accurately classify the real vs. generated images.
Your task is to build a GAN that generates 7,000 to 10,000 Monet-style images.
Getting Started:
Details on the dataset can be found here and an overview of the evaluation process can be found here.
To learn how to submit and answers to other FAQs, review the Frequently Asked Questions.
Recommended Tutorial
We highly recommend Amy Jang's notebook that goes over the basics of loading data from TFRecords, using TPUs, and building a CycleGAN.
Although the competition dataset only includes Monet images, check out this dataset for Cezanne, Ukiyo-e, and Van Gogh paintings to run your GAN on.","MiFID
Submissions are evaluated on MiFID (Memorization-informed Fréchet Inception Distance), which is a modification from Fréchet Inception Distance (FID).
The smaller MiFID is, the better your generated images are.
What is FID?
Originally published here (github), FID, along with Inception Score (IS), are both commonly used in recent publications as the standard for evaluation methods of GANs.
In FID, we use the Inception network to extract features from an intermediate layer. Then we model the data distribution for these features using a multivariate Gaussian distribution with mean µ and covariance Σ. The FID between the real images \(r\) and generated images \(g\) is computed as:
\[
\text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr} (\Sigma_r + \Sigma_g - 2 (\Sigma_r \Sigma_g)^{1/2})
\]
where \(Tr\) sums up all the diagonal elements. FID is calculated by computing the Fréchet distance between two Gaussians fitted to feature representations of the Inception network.
What is MiFID (Memorization-informed FID)?
In addition to FID, Kaggle takes training sample memorization into account.
The memorization distance is defined as the minimum cosine distance of all training samples in the feature space, averaged across all user generated image samples. This distance is thresholded, and it's assigned to 1.0 if the distance exceeds a pre-defined epsilon.
In mathematical form:
\[d_{ij} = 1 - cos(f_{gi}, f_{rj}) = 1 - \frac{f_{gi} \cdot f_{rj}}{|f_{gi}| |f_{rj}|}\]
where \(f_g\) and \(f_r\) represent the generated/real images in feature space (defined in pre-trained networks); and \(f_{gi}\) and \(f_{rj}\) represent the \(i^{th}\) and \(j^{th}\) vectors of \(f_g\) and \(f_r\), respectively.
\[d = \frac{1}{N} \sum_{i} \min_j d_{ij} \]
defines the minimum distance of a certain generated image (\(i\)) across all real images ((\(j\)), then averaged across all the generated images.

defines the threshold of the weight only applies when the (\(d\)) is below a certain empirically determined threshold.
Finally, this memorization term is applied to the FID:
\[ MiFID = FID \cdot \frac{1}{d_{thr}}\]
Kaggle's workflow calculating MiFID for public and private scores
Kaggle calculates public MiFID scores with the pre-train neural network Inception, and the public images used for evaluation are the rest of the TFDS Monet paintings. Note that as a Getting Started competition there is no private leaderboard.
A demo of our MiFID evaluation code can be seen here.
Submission File
You are going to generate 7,000-10,000 Monet-style images that are in jpg format. Their sizes should be 256x256x3 (RGB). Then you need to zip those images and your output from your Kernel should only have ONE output file named images.zip.
Please note that Kaggle Kernels has a number of output files capped at 500. We highly encourage you to either directly write to a zip file as you generate images, or create a folder at ../tmp as your temporary directory.
Honor Code: This competition has a unique format with images expected as submission, rather than predictions. Competitors should be using generative methods to create their submission images and not directly submit images of Monet paintings or altered versions of such images. This defeats purpose of a getting started competition aimed at learning.","The dataset contains four directories: monet_tfrec, photo_tfrec, monet_jpg, and photo_jpg. The monet_tfrec and monet_jpg directories contain the same painting images, and the photo_tfrec and photo_jpg directories contain the same photos.
We recommend using TFRecords as a Getting Started competition is a great way to become more familiar with a new data format, but JPEG images have also been provided.
The monet directories contain Monet paintings. Use these images to train your model.
The photo directories contain photos. Add Monet-style to these images and submit your generated jpeg images as a zip file. Other photos outside of this dataset can be transformed but keep your submission file limited to 10,000 images.
Note: Monet-style art can be created from scratch using other GAN architectures like DCGAN. The submitted image files do not necessarily have to be transformed photos.
Check out the CycleGAN dataset to experiment with the artistic style of other artists.
Files
monet_jpg - 300 Monet paintings sized 256x256 in JPEG format
monet_tfrec - 300 Monet paintings sized 256x256 in TFRecord format
photo_jpg - 7028 photos sized 256x256 in JPEG format",kaggle competitions download -c gan-getting-started,"['https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial', 'https://www.kaggle.com/code/ohseokkim/transfering-style', 'https://www.kaggle.com/code/dimitreoliveira/introduction-to-cyclegan-monet-paintings', 'https://www.kaggle.com/code/imoore/generate-paintings-by-image-style-transfer', 'https://www.kaggle.com/code/dimitreoliveira/improving-cyclegan-monet-paintings']"
175,"Manchester City F.C. and Google Research are proud to present AI football competition using the Google Research Football Environment.
A word from Manchester City F.C.
Brian Prestidge, Director of Data Insights & Decision Technology at City Football Group, the owners of Manchester City F.C., sets out the challenge. “Football is a tough environment to perform in and an even tougher environment to learn in. Learning is all about harnessing failure, but failure in football is seldom accepted. Working with Google Research’s physics based football environment provides us with a new place to learn through simulation and offers us the capabilities to test tactical concepts and refine principles so that they are strong enough for a coach to stake their career on.”
“We are therefore very pleased to be working with Google’s research team in creating this competition and are looking forward to the opportunity to support some of the most creative and successful competitors through funding and exclusive prizes. We hope to establish ongoing collaboration with the winners beyond this competition, and that it will provide us all with the platform to explore and establish fundamental principles of football tactics, thus improving our ability to perform and be successful on the pitch.”
Greg Swimer, Chief Technology Officer at City Football Group added ""Technologies such as Machine Learning and Artificial Intelligence have huge future potential to enhance the understanding and enjoyment of football for players, coaches and fans. We are delighted to be collaborating with Google's research team to help broaden the knowledge, talent, and innovation working in this exciting and transformational area"".
The Google Research football environment competition
The world gets a kick out of football (soccer in the United States). As the most popular sport on the planet, millions of fans enjoy watching Sergio Agüero, Raheem Sterling, and Kevin de Bruyne on the field. Football video games are less lively, but still immensely popular, and we wonder if AI agents would be able to play those properly.
Researchers want to explore AI agents' ability to play in complex settings like football. The sport requires a balance of short-term control, learned concepts such as passing, and high-level strategy, which can be difficult to teach agents. A current environment exists to train and test agents, but other solutions may offer better results.
The teams at Google Research aspire to make discoveries that impact everyone. Essential to their approach is sharing research and tools to fuel progress in the field. Together with Manchester City F.C., Google Research has put forth this competition to get help in reaching their goal.
In this competition, you’ll create AI agents that can play football. Teams compete in “steps,” where agents react to a game state. Each agent in an 11 vs 11 game controls a single active player and takes actions to improve their team’s situation. As with a typical football game, you want your team to score more than the other side. You can optionally see your efforts rendered in a physics-based 3D football simulation.
If controlling 11 football players with code sounds difficult, don't be discouraged! You only need to control one player at a time (the one with the ball on offense, or the one closest to the ball on defense) and your code gets to pick from 1 of 19 possible actions. We have prepared a getting started example to show you how simple a basic strategy can be. Before implementing your own strategy, however, you might want to learn more about the Google Research football environment, especially observations provided to you by the environment and available actions. You can also play the game yourself on your computer locally to get better understanding of the environment's dynamics and explore different scenarios.
If successful, you'll help researchers explore the ability of AI agents to play in complex settings. This could offer new insights into the strategies of the world's most-watched sport. Additionally, this research could pave the way for a new generation of AI agents that can be trained to learn complex skills.","Each day, your team is able to submit up to 5 agents (bots) to the competition. Each submission will play episodes (games) against other bots on the ladder that have a similar skill rating. Over time, skill ratings will go up with wins or down with losses. Every bot submitted will continue to play games until the end of the competition. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents our uncertainty of that estimate which will decrease over time.
When you upload a Submission, we first play a Validation Episode where that Submission plays against a copy of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions and try to pick Submissions with similar ratings for fair matches. We aim to run ~8 Episodes a day per Submission with an additional slight rate increase for the newest-submitted Episodes to give you feedback faster.
After an Episode finishes, we'll update the Rating estimate of both agents in that Episode. If one agent won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
At the submission deadline, additional submissions will be locked. One additional week will be allotted to continue to run games. At the conclusion of this week, the leaderboard is final.",,,"['https://www.kaggle.com/code/yegorbiryukov/gfootball-with-memory-patterns', 'https://www.kaggle.com/code/piotrstanczyk/gfootball-template-bot', 'https://www.kaggle.com/code/sx2154/gfootball-rules-from-environment-exploration', 'https://www.kaggle.com/code/piotrstanczyk/gfootball-train-seed-rl-agent', 'https://www.kaggle.com/code/jaronmichal/human-readable-visualization']"
176,"Autonomous vehicles (AVs) are expected to dramatically redefine the future of transportation. However, there are still significant engineering challenges to be solved before one can fully realize the benefits of self-driving cars. One such challenge is building models that reliably predict the movement of traffic agents around the AV, such as cars, cyclists, and pedestrians.
The ridesharing company Lyft started Level 5 to take on the self-driving challenge and build a full self-driving system (they’re hiring!). Their previous competition tasked participants with identifying 3D objects, an important step prior to detecting their movement. Now, they’re challenging you to predict the motion of these traffic agents.
In this competition, you’ll apply your data science skills to build motion prediction models for self-driving vehicles. You'll have access to the largest Prediction Dataset ever released to train and test your models. Your knowledge of machine learning will then be required to predict how cars, cyclists,and pedestrians move in the AV's environment.
Lyft’s mission is to improve people’s lives with the world’s best transportation. They believe in a future where self-driving cars make transportation safer, environment-friendly and more accessible for everyone. Their goal is to accelerate development across the industry by sharing data with researchers. As a result of your participation, you can have a hand in propelling the industry forward and helping people around the world benefit from self-driving cars sooner.
This is a Code Competition. Refer to Code Requirements for details.","The goal of this competition is to predict the trajectories of other traffic participants. You can employ uni-modal models yielding a single prediction per sample, or multi-modal ones generating multiple hypotheses (up to 3) - further described by a confidence vector.
Due to the high amount of multi-modality and ambiguity in traffic scenes, the used evaluation metric to score this competition is tailored to account for multiple predictions.
Note: The following is a brief excerpt of our metrics page in the L5Kit repository
We calculate the negative log-likelihood of the ground truth data given the multi-modal predictions. Let us take a closer look at this. Assume, ground truth positions of a sample trajectory are


and we predict K hypotheses, represented by means


In addition, we predict confidences c of these K hypotheses. We assume the ground truth positions to be modeled by a mixture of multi-dimensional independent Normal distributions over time, yielding the likelihood









which results in the loss





Submission File
Note: if you're using L5Kit, we provide a function to directly convert your predictions (single and multi-modal) into a valid CSV.
Every agent is identified by its track_id and its timestamp. Each trajectory holds 50 2D (X,Y) predictions.
You can predict up to 3 trajectories for each agent in the test set. Because the format is a CSV file, all 3 trajectories fields must have a value, even if your prediction is single-modal. However, each one of the three trajectory has its own confidence, and you can set it 0 to completely ignore one or more trajectories during evaluation. The 3 confidences must sum to 1.
An example of a valid CSV header:
timestamp, track_id, conf_0, conf_1, conf_2, coord_x00, coord_y_00,...,coord_x049, coord_y_049, coord_x10, coord_y_10,...,coord_x149, coord_y_149, coord_x20, coord_y_20,...,coord_x249, coord_y_249","The Lyft Motion Prediction for Autonomous Vehicles competition is fairly unique, data-wise. In it, a very large amount of data is provided, which can be used in many different ways. Reading the data is also complex - please refer to Lyft's L5Kit module and sample notebooks to properly load the data and use it for training. Further Kaggle-specific sample notebooks will follow shortly.
Note also that this competition requires that submissions be made from kernels, and that internet must be turned off in your submission kernels. For your convenience, Lyft's l5kit module is provided via a utility script called kaggle_l5kit. Just attach it to your kernel, and the latest version of l5kit and all dependencies will be available.
What files do I need?
You can compete with just train.zarr and test.zarr, the other files are optional but will likely be helpful. Please refer to the sample notebooks for help on how to load and iterate over the datasets.
What should I expect the data format to be?
Note: for full details, please refer to the data format page in L5Kit
The data is packaged in .zarr files. These are loaded using the zarr Python module, and are also loaded natively by l5kit. Each .zarr file contains a set of:
scenes: driving episodes acquired from a given vehicle.",kaggle competitions download -c lyft-motion-prediction-autonomous-vehicles,"['https://www.kaggle.com/code/nxrprime/understanding-the-data-catalyst-kekas-baseline', 'https://www.kaggle.com/code/corochann/lyft-comprehensive-guide-to-start-competition', 'https://www.kaggle.com/code/corochann/lyft-deep-into-the-l5kit-library', 'https://www.kaggle.com/code/huanvo/lyft-complete-train-and-prediction-pipeline', 'https://www.kaggle.com/code/pestipeti/pytorch-baseline-inference']"
177,"""…when you have eliminated the impossible, whatever remains, however improbable, must be the truth""
-Sir Arthur Conan Doyle
Our brains process the meaning of a sentence like this rather quickly.
We're able to surmise:
Some things to be true: ""You can find the right answer through the process of elimination.”
Others that may have truth: ""Ideas that are improbable are not impossible!""
And some claims are clearly contradictory: ""Things that you have ruled out as impossible are where the truth lies.""
Natural language processing (NLP) has grown increasingly elaborate over the past few years. Machine learning models tackle question answering, text extraction, sentence generation, and many other complex tasks. But, can machines determine the relationships between sentences, or is that still left to humans? If NLP can be applied between sentences, this could have profound implications for fact-checking, identifying fake news, analyzing text, and much more.
The Challenge:
If you have two sentences, there are three ways they could be related: one could entail the other, one could contradict the other, or they could be unrelated. Natural Language Inferencing (NLI) is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related.
Your task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set include text in fifteen different languages! You can find more details on the dataset by reviewing the Data page.
Today, the most common approaches to NLI problems include using embeddings and transformers like BERT. In this competition, we’re providing a starter notebook to try your hand at this problem using the power of Tensor Processing Units (TPUs). TPUs are powerful hardware accelerators specialized in deep learning tasks, including Natural Language Processing. Kaggle provides all users TPU Quota at no cost, which you can use to explore this competition. Check out our TPU documentation and Kaggle’s YouTube playlist for more information and resources.
Recommended Tutorial
We highly recommend this excellent tutorial on using KerasNLP to solve this problem, from the Keras team as well as Ana Sofia Uzsoy’s Tutorial that walks you through creating your very first submission step by step with TPUs and BERT.
This is a great opportunity to flex your NLP muscles and solve an exciting problem!
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.","Goal
Your goal is to predict whether a given hypothesis is related to its premise by contradiction, entailment, or whether neither of those is true (neutral).
For each sample in the test set, you must predict a 0, 1, or 2 value for the variable.
Those values map to the logical condition as:
0 == entailment
1 == neutral
2 == contradiction
Metric
Your score is the percentage of relationships you correctly predict. This is known as accuracy.
Submission File Format
You should submit a csv file with exactly 5195 entries plus a header row. Your submission will show an error if you have extra columns (beyond id and prediction) or rows.
The file should have exactly 2 columns:
id (sorted in any order)
prediction (contains your predictions: 0 for entailment, 1 for neutral, 2 for contradiction)
id,prediction
c6d58c3f69,1
cefcc82292,1
e98005252c,1
58518c10ba,1
c32b0d16df,1
Etc.
You can download an example submission file (sample_submission.csv) on the Data page.
Code Submission Requirement
In this code competition, your submission.csv file must be generated as an output from a Kaggle notebook. For details on how to submit from a notebook, review the FAQ on ""How do I make a submission?""","In this Getting Started Competition, we’re classifying pairs of sentences (consisting of a premise and a hypothesis) into three categories - entailment, contradiction, or neutral. Let’s take a look at an example of each of these cases for the following premise:
He came, he opened the door and I remember looking back and seeing the expression on his face, and I could tell that he was disappointed.
Hypothesis 1:
Just by the look on his face when he came through the door I just knew that he was let down.
We know that this is true based on the information in the premise. So, this pair is related by entailment.
Hypothesis 2:
He was trying not to make us feel guilty but we knew we had caused him trouble.
This very well might be true, but we can’t conclude this based on the information in the premise. So, this relationship is .",kaggle competitions download -c contradictory-my-dear-watson,"['https://www.kaggle.com/code/alexia/kerasnlp-starter-notebook-contradictory-dearwatson', 'https://www.kaggle.com/code/anasofiauzsoy/tutorial-notebook', 'https://www.kaggle.com/code/nkitgupta/text-representations', 'https://www.kaggle.com/code/rohanrao/tpu-sherlocked-one-stop-for-with-tf', 'https://www.kaggle.com/code/vbookshelf/basics-of-bert-and-xlm-roberta-pytorch']"
178,"Welcome to the third Landmark Recognition competition! This year, we have worked to set this up as a code competition and collected a new set of test images.
Have you ever gone through your vacation photos and asked yourself: What was the name of that temple I visited in China? or Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.
Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.
In the previous editions of this challenge (2018 and 2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring.
This challenge is organized in conjunction with the Landmark Retrieval Challenge 2020, which was launched June 30, 2020. Both challenges are affiliated with the Instance-Level Recognition workshop in ECCV’20.
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated using Global Average Precision (GAP) at \\(k\\), where \\(k=1\\). This metric is also known as micro Average Precision (\\(\mu\\)AP), as per [1,2]. It works as follows:
For each test image, you will predict one landmark label and a corresponding confidence score. The evaluation treats each prediction as an individual data point in a long list of predictions (sorted in descending order by confidence scores), and computes the Average Precision based on this list.
If a submission has \\(N\\) predictions (label/confidence pairs) sorted in descending order by their confidence scores, then the Global Average Precision is computed as:
$$GAP = \frac{1}{M}\sum_{i=1}^N P(i) rel(i)$$
where:
\\(N\\) is the total number of predictions returned by the system, across all queries
\\(M\\) is the total number of queries with at least one landmark from the training set visible in it (note that some queries may not depict landmarks)
\\(P(i)\\) is the precision at rank \\(i\\)
\\(rel(i)\\) denotes the relevance of prediciton \\(i\\): it’s 1 if the \\(i\\)-th prediction is correct, and 0 otherwise
[1] F. Perronnin, Y. Liu, and J.-M. Renders, ""A Family of Contextual Measures of Similarity between Distributions with Application to Image Retrieval,"" Proc. CVPR'09
[2] T. Weyand, A. Araujo, B. Cao and J. Sim, ""Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval,"" Proc. CVPR'20
Submission File
For each id in the test set, you can predict at most one landmark and its corresponding confidence score. Some images contain no landmarks. You may decide not to predict any result for a given query, by submitting an empty prediction. The submission file should contain a header and have the following format (larger scores denote more confident matches):
id,landmarks
000088da12d664db,8815 0.03
0001623c6d808702,
0001bbb682d45002,5328 0.5
etc.","In this competition, you are asked to take test images and recognize which landmarks (if any) are depicted in them. The training set is available in the train/ folder, with corresponding landmark labels in train.csv. The test set images are listed in the test/ folder. Each image has a unique id. Since there are a large number of images, each image is placed within three subfolders according to the first three characters of the image id (i.e. image abcdef.jpg is placed in a/b/c/abcdef.jpg).
This is a synchronous rerun code competition. The provided test set is a representative set of files to demonstrate the format of the private test set. When you submit your notebook, Kaggle will rerun your code on the private dataset. Additionally, this competition also has two unique characteristics:
To facilitate recognition-by-retrieval approaches, the private training set contains only a 100k subset of the total public training set. This 100k subset contains all of the training set images associated with the landmarks in the private test set. You may still attach the full training set as an external data set if you wish.
Submissions are given 12 hours to run, as compared to the site-wide session limit of 9 hours. While your commit must still finish in the 9 hour limit in order to be eligible to submit, the rerun may take the full 12 hours.
GLDv2
The training data for this competition comes from a cleaned version of the Google Landmarks Dataset v2 (GLDv2), which is available here. Please refer to the paper for more details on the dataset construction and how to use it. See this code example for an example of a pretrained model.",kaggle competitions download -c landmark-recognition-2020,"['https://www.kaggle.com/code/camaskew/host-baseline-example', 'https://www.kaggle.com/code/chankhavu/keras-layers-arcface-cosface-adacos', 'https://www.kaggle.com/code/paulorzp/baseline-landmark-recognition-lb-0-48', 'https://www.kaggle.com/code/rhtsingh/pytorch-training-inference-efficientnet-baseline', 'https://www.kaggle.com/code/ks2019/stratified-tfrecords-training-pipeline']"
179,"Imagine one day, your breathing became consistently labored and shallow. Months later you were finally diagnosed with pulmonary fibrosis, a disorder with no known cause and no known cure, created by scarring of the lungs. If that happened to you, you would want to know your prognosis. That’s where a troubling disease becomes frightening for the patient: outcomes can range from long-term stability to rapid deterioration, but doctors aren’t easily able to tell where an individual may fall on that spectrum. Your help, and data science, may be able to aid in this prediction, which would dramatically help both patients and clinicians.
Current methods make fibrotic lung diseases difficult to treat, even with access to a chest CT scan. In addition, the wide range of varied prognoses create issues organizing clinical trials. Finally, patients suffer extreme anxiety—in addition to fibrosis-related symptoms—from the disease’s opaque path of progression.
Open Source Imaging Consortium (OSIC) is a not-for-profit, co-operative effort between academia, industry and philanthropy. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis (IPF), fibrosing interstitial lung diseases (ILDs), and other respiratory diseases, including emphysematous conditions. Its mission is to bring together radiologists, clinicians and computational scientists from around the world to improve imaging-based treatments.
In this competition, you’ll predict a patient’s severity of decline in lung function based on a CT scan of their lungs. You’ll determine lung function based on output from a spirometer, which measures the volume of air inhaled and exhaled. The challenge is to use machine learning techniques to make a prediction with the image, metadata, and baseline FVC as input.
If successful, patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments.

This is a Code Competition. Refer to Code Requirements for details.","This competition is evaluated on a modified version of the Laplace Log Likelihood. In medical applications, it is useful to evaluate a model's confidence in its decisions. Accordingly, the metric is designed to reflect both the accuracy and certainty of each prediction.
For each true FVC measurement, you will predict both an FVC and a confidence measure (standard deviation \( \sigma \)). The metric is computed as:
$$ \sigma_{clipped} = max(\sigma, 70), $$
$$ \Delta = min ( |FVC_{true} - FVC_{predicted}|, 1000 ), $$
$$ metric = - \frac{\sqrt{2} \Delta}{\sigma_{clipped}} - \ln ( \sqrt{2} \sigma_{clipped} ). $$
The error is thresholded at 1000 ml to avoid large errors adversely penalizing results, while the confidence values are clipped at 70 ml to reflect the approximate measurement uncertainty in FVC. The final score is calculated by averaging the metric across all test set Patient_Weeks (three per patient). Note that metric values will be negative and higher is better.
Submission File
For each Patient_Week, you must predict the FVC and a confidence. To avoid potential leakage in the timing of follow up visits, you are asked to predict every patient's FVC measurement for every possible week. Those weeks which are not in the final three visits are ignored in scoring.
The file should contain a header and have the following format:
Patient_Week,FVC,Confidence
ID00002637202176704235138_1,2000,100
ID00002637202176704235138_2,2000,100
ID00002637202176704235138_3,2000,100
etc.","The aim of this competition is to predict a patient’s severity of decline in lung function based on a CT scan of their lungs. Lung function is assessed based on output from a spirometer, which measures the forced vital capacity (FVC), i.e. the volume of air exhaled.
In the dataset, you are provided with a baseline chest CT scan and associated clinical information for a set of patients. A patient has an image acquired at time Week = 0 and has numerous follow up visits over the course of approximately 1-2 years, at which time their FVC is measured.
In the training set, you are provided with an anonymized, baseline CT scan and the entire history of FVC measurements.
In the test set, you are provided with a baseline CT scan and only the initial FVC measurement. You are asked to predict the final three FVC measurements for each patient, as well as a confidence value in your prediction.
There are around 200 cases in the public & private test sets, combined. This is split roughly 15-85 between public-private.
Since this is real medical data, you will notice the relative timing of FVC measurements varies widely. The timing of the initial measurement relative to the CT scan and the duration to the forecasted time points may be different for each patient. This is considered part of the challenge of the competition. To avoid potential leakage in the timing of follow up visits, you are asked to predict every patient's FVC measurement for every possible week. Those weeks which are not in the final three visits are ignored in scoring.
Files",kaggle competitions download -c osic-pulmonary-fibrosis-progression,"['https://www.kaggle.com/code/piantic/osic-pulmonary-fibrosis-progression-basic-eda', 'https://www.kaggle.com/code/allunia/pulmonary-dicom-preprocessing', 'https://www.kaggle.com/code/andradaolteanu/pulmonary-fibrosis-competition-eda-dicom-prep', 'https://www.kaggle.com/code/ulrich07/osic-multiple-quantile-regression-starter', 'https://www.kaggle.com/code/rohanrao/osic-understanding-laplace-log-likelihood']"
180,"Welcome to the third Landmark Retrieval competition! This year, we have worked to set this up as a code competition and we have completely refreshed the test and index image sets.
Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph.
In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the Landmark Recognition Challenge 2020. Both challenges will be discussed at the Instance-Level Recognition workshop in ECCV’20.
In the previous editions of this challenge (2018 and 2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a representation learning format: rather than creating a submission file with retrieved images, you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality with mean average precision.
This is a Code Competition. Refer to Code Requirements for details.","Submissions are evaluated according to mean Average Precision @ 100 (\(mAP@100\)):
$$mAP@100 = \frac{1}{Q} \sum_{q=1}^{Q} \frac{1}{min(m_q, 100)} \sum_{k=1}^{min(n_q,100)} P_q(k) rel_q(k)$$
where:
\(Q\) is the number of query images
\(m_q\) is the number of index images containing a landmark in common with the query image \(q\) (note that \(m_q \gt 0\))
\(n_q\) is the number of predictions made by the system for query \(q\)
\(P_q(k)\) is the precision at rank \(k\) for the \(q\)-th query
\(rel_q(k)\) denotes the relevance of prediciton \(k\) for the \(q\)-th query: it’s 1 if the \(k\)-th prediction is correct, and 0 otherwise
Submission file
Unlike a traditional Kaggle code competition where notebooks are rerun top-to-bottom on a private test set, in this competition you will be submitting a model file. Please refer to the data page for complete details. In most cases, scoring is expected to take a few hours to complete.","In this competition, you are asked to develop models that can efficiently retrieve landmark images from a large database. The training set is available in the train/ folder, with corresponding landmark labels in train.csv. The query images are listed in the test/ folder, while the ""index"" images from which you are retrieving are listed in index/. Each image has a unique id. Since there are a large number of images, each image is placed within three subfolders according to the first three characters of the image id (i.e. image abcdef.jpg is placed in a/b/c/abcdef.jpg).
Unlike a traditional Kaggle code competition where notebooks are rerun top-to-bottom on a private test set, in this competition you will be submitting a model file that Kaggle will use to:
Extract embeddings for the private test and index sets.
Create a kNN (k = 100) lookup for each test sample, using the Euclidean distance between test and index embeddings.
Score the quality of the lookups using the competition metric.
The provided index/ and test/ images in the publicly available dataset are provided to mock the size and structure of the private data, but are otherwise not directly used.
Your model must be named submission.zip and be compatible with TensorFlow 2.2. The submission.zip should contain all files and directories created by the tf.saved_model_save function using Tensorflow's SavedModel format.
The code used to load your SavedModel, create the embeddings, and score your submission is provided here.",kaggle competitions download -c landmark-retrieval-2020,"['https://www.kaggle.com/code/seriousran/google-landmark-retrieval-2020-eda', 'https://www.kaggle.com/code/mayukh18/creating-submission-from-your-own-model', 'https://www.kaggle.com/code/sandy1112/create-and-train-resnet50-from-scratch', 'https://www.kaggle.com/code/camaskew/baseline-submission', 'https://www.kaggle.com/code/waelkh/landmark2020-delf-model-submission-code']"
181,"Learn how to use Tensor Processing Units (TPUs) on Kaggle
TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try.
TPU quotas are available on Kaggle at no cost to users.
Watch the video below to see how to get started! You can follow along with this notebook.
The Challenge
It’s difficult to fathom just how vast and diverse our natural world is.
There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish – and astonishingly, over 400,000 different types of flowers.
In this competition, you’re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we’re sticking to just over 100 types).
Recommended Tutorial
We highly recommend Ryan Holbrook’s Tutorial that walks you through making your very first submission step by step.
Have Questions?
Kaggle Data Scientists will be actively monitoring the competition forum - your fellow data scientists and TPU users will be there too! If you have a question or need help troubleshooting, that’s the best place to find help.
Learn More
Check out Kaggle’s Youtube playlist for more videos introducing TPUs.
Read the TPU documentation for more information and resources.
Many thanks to Martin Görner, Google Developer Advocate and author of Tensorflow without a PhD for his tireless work on the dataset, the notebooks, and the original competition that this Getting Started competition draws from.","Submissions are evaluated on macro F1 score.
F1 is calculated as follows:
$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$
where:
$$
precision = \frac{TP}{TP + FP}
$$
$$
recall = \frac{TP}{TP + FN}
$$
In ""macro"" F1 a separate F1 score is calculated for each class / label and then averaged.
Submission File
For each id in the test set, you must predict a type of flower (or label). The file should contain a header and have the following format:
id,label
a762df180,0
24c5cf439,0
7581e896d,0
eb4b03b29,0
etc.","In this Getting Started competition (what is a Getting Started competition?), we're classifying 104 types of flowers based on their images drawn from five different public datasets. Some classes are very narrow, containing only a particular sub-type of flower (e.g. pink primroses) while other classes contain many sub-types (e.g. wild roses).
The dataset contains imperfections - images of flowers in odd places, or as a backdrop to modern machinery - but that's part of the challenge! Build a classifier than can see past all that, to the flowers at the heart of the images.
Files
This competition provides its files in TFRecord format. The TFRecord format is a container format frequently used in Tensorflow to group and shard data data files for optimal training performace.
Each file contains the id, label (the class of the sample, for training data) and img (the actual pixels in array form) information for many images.
Please see our Getting Started notebook or our Learn exercise for notes on how to load and use them! Additional information is available in the TPU documentation.
train/*.tfrec - training samples, including labels.
val/*.tfrec - pre-split training samples w/ labels intended to help with checking your model's performance on TPU. The split was stratified across labels.",kaggle competitions download -c tpu-getting-started,"['https://www.kaggle.com/code/ryanholbrook/create-your-first-submission', 'https://www.kaggle.com/code/cdeotte/rotation-augmentation-gpu-tpu-0-96', 'https://www.kaggle.com/code/cdeotte/cutmix-and-mixup-on-gpu-tpu', 'https://www.kaggle.com/code/ryanholbrook/tfrecords-basics', 'https://www.kaggle.com/code/georgezoto/computer-vision-petals-to-the-metal']"
182,"Do you hear the birds chirping outside your window? Over 10,000 bird species occur in the world, and they can be found in nearly every environment, from untouched rainforests to suburbs and even cities. Birds play an essential role in nature. They are high up in the food chain and integrate changes occurring at lower levels. As such, birds are excellent indicators of deteriorating habitat quality and environmental pollution. However, it is often easier to hear birds than see them. With proper sound detection and classification, researchers could automatically intuit factors about an area’s quality of life based on a changing bird population.
There are already many projects underway to extensively monitor birds by continuously recording natural soundscapes over long periods. However, as many living and nonliving things make noise, the analysis of these datasets is often done manually by domain experts. These analyses are painstakingly slow, and results are often incomplete. Data science may be able to assist, so researchers have turned to large crowdsourced databases of focal recordings of birds to train AI models. Unfortunately, there is a domain mismatch between the training data (short recording of individual birds) and the soundscape recordings (long recordings with often multiple species calling at the same time) used in monitoring applications. This is one of the reasons why the performance of the currently used AI models has been subpar.
To unlock the full potential of these extensive and information-rich sound archives, researchers need good machine listeners to reliably extract as much information as possible to aid data-driven conservation.
The Cornell Lab of Ornithology’s Center for Conservation Bioacoustics (CCB)’s mission is to collect and interpret sounds in nature. The CCB develops innovative conservation technologies to inspire and inform the conservation of wildlife and habitats globally. By partnering with the data science community, the CCB hopes to further its mission and improve the accuracy of soundscape analyses.
In this competition, you will identify a wide variety of bird vocalizations in soundscape recordings. Due to the complexity of the recordings, they contain weak labels. There might be anthropogenic sounds (e.g., airplane overflights) or other bird and non-bird (e.g., chipmunk) calls in the background, with a particular labeled bird species in the foreground. Bring your new ideas to build effective detectors and classifiers for analyzing complex soundscape recordings!
If successful, your work will help researchers better understand changes in habitat quality, levels of pollution, and the effectiveness of restoration efforts. Reliable machine listeners would also allow conservationists to deploy more recording units worldwide and would enable data-driven conservation at a scale not yet possible. The eventual conservation outcomes could greatly improve the quality of life for many living organisms—birds and human beings included.","Submissions will be evaluated based on their row-wise micro averaged F1 score.
For each row_id/time window, you need to provide a space separated list of the set of unique birds that made a call beginning or ending in that time window. If there are no bird calls in a time window, use the code nocall.
There are three sites in the test set. Sites 1 and 2 are labeled in 5 second increments, while site 3 was labeled per audio file due to the time consuming nature of the labeling process.
The submission file must have a header and should look like the following:
Submission File
row_id,birds
site_1_0a997dff022e3ad9744d4e7bbf923288_5,amecro
site_1_0a997dff022e3ad9744d4e7bbf923288_10,amecro amerob
site_1_0a997dff022e3ad9744d4e7bbf923288_15,nocall","Your challenge in this competition is to identify which birds are calling in long recordings, given training data generated in meaningfully different contexts. This is the exact problem facing scientists trying to automate the remote monitoring of bird populations.
Files
train_audio
The train data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org.
test_audio
The hidden test_audio directory contains approximately 150 recordings in mp3 format, each roughly 10 minutes long. They will not all fit in a notebook's memory at the same time. The recordings were taken at three separate remote locations in North America. Sites 1 and 2 were labeled in 5 second increments and need matching predictions, but due to the time consuming nature of the labeling process the site 3 files are only labeled at the file level. Accordingly, site 3 has relatively few rows in the test set and needs lower time resolution predictions.
Two example soundscapes from another data source are also provided to illustrate how the soundscapes are labeled and the hidden dataset folder structure. The two example audio files are BLKFR-10-CPL_20190611_093000.pt540.mp3 and ORANGE-7-CAP_20190606_093000.pt623.mp3. These soundscapes were kindly provided by Jack Dumbacher of the California Academy of Science's Department of Ornithology and Mammology.",kaggle competitions download -c birdsong-recognition,"['https://www.kaggle.com/code/andradaolteanu/birdcall-recognition-eda-and-audio-fe', 'https://www.kaggle.com/code/hidehisaarai1213/introduction-to-sound-event-detection', 'https://www.kaggle.com/code/hidehisaarai1213/inference-pytorch-birdcall-resnet-baseline', 'https://www.kaggle.com/code/ttahara/training-birdsong-baseline-resnest50-fast', 'https://www.kaggle.com/code/parulpandey/eda-and-audio-processing-with-python']"
183,"Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection—potentially aided by data science—can make treatment more effective.
Currently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or “ugly ducklings” that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account “contextual” images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work.
As the leading healthcare organization for informatics in medical imaging, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. SIIM is joined by the International Skin Imaging Collaboration (ISIC), an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions.
In this competition, you’ll identify melanoma in images of skin lesions. In particular, you’ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists.
Melanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each image_name in the test set, you must predict the probability (target) that the sample is malignant. The file should contain a header and have the following format:
image_name,target
ISIC_0052060,0.7
ISIC_0052349,0.9
ISIC_0058510,0.8
ISIC_0073313,0.5
ISIC_0073502,0.5
etc.","What should I expect the data format to be?
The images are provided in DICOM format. This can be accessed using commonly-available libraries like pydicom, and contains both image and metadata. It is a commonly used medical imaging data format.
Images are also provided in JPEG and TFRecord format (in the jpeg and tfrecords directories, respectively). Images in TFRecord format have been resized to a uniform 1024x1024.
Metadata is also provided outside of the DICOM format, in CSV files. See the Columns section for a description.
What am I predicting?
You are predicting a binary target for each image. Your model should predict the probability (floating point) between 0.0 and 1.0 that the lesion in the image is malignant (the target). In the training data, train.csv, the value 0 denotes benign, and 1 indicates malignant.
Files
train.csv - the training set
test.csv - the test set
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c siim-isic-melanoma-classification,"['https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords', 'https://www.kaggle.com/code/nroman/melanoma-pytorch-starter-efficientnet', 'https://www.kaggle.com/code/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble', 'https://www.kaggle.com/code/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once', 'https://www.kaggle.com/code/andradaolteanu/siim-melanoma-competition-eda-augmentations']"
184,"Ahoy there! There's halite to be had and ships to be deployed! Are you ready to navigate the skies and secure your territory?
Halite by Two Sigma (""Halite"") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.
Created by Two Sigma in 2016, more than 15,000 people around the world have participated in a Halite challenge. Players apply advanced algorithms in a dynamic, open source game setting. The strategic depth and immersive, interactive nature of Halite games make each challenge a unique learning environment.
Halite IV builds on the core game design of Halite III with a number of key changes that shift the focus of the game towards tighter competition on a smaller board. New game features include regenerating halite, shipyard creation, no more ship movement costs, and stealing halite from other players!
So dust off your halite meters and fasten your seatbelts. The fourth season of Halite is about to begin!","Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play episodes (games) against other bots on the ladder that have a similar skill rating. Over time skill ratings will go up with wins or down with losses. Every bot submitted will continue to play games until the end of the competition. On the leaderboard only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.
Each Submission has an estimated Skill Rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents our uncertainty of that estimate which will decrease over time.
When you upload a Submission, we first play a Validation Episode where that Submission plays against copies of itself to make sure it works properly. If the Episode fails, the Submission is marked as Error. Otherwise, we initialize the Submission with μ0=600 and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. We aim to run ~8 Episodes a day per Submission, with an additional slight rate increase for the newest-submitted Episodes to give you feedback faster.
After an Episode finishes, we'll update the Rating estimate for all Submissions in that Episode. If one Submission won, we'll increase its μ and decrease its opponent's μ -- if the result was a draw, then we'll move the two μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each Submission's uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an Episode does not affect the skill rating updates.
Since Halite is a four-player game the resolution is a little more complex. We treat a four-agent episode as 6 two-agent episodes (agent 1 vs agent 2, agent 1 vs agent 3, agent 2 vs agent 3, etc) and calculate the standard two-player score update for each pair, then we average those updates to calculate each agent's final ranking updates.
At the submission deadline, additional submissions will be locked. One additional week will be allotted to continue to run games. At the conclusion of this week, the leaderboard is final.",,,"['https://www.kaggle.com/code/yegorbiryukov/halite-swarm-intelligence', 'https://www.kaggle.com/code/sam/halite-sdk-overview', 'https://www.kaggle.com/code/alexisbcook/getting-started-with-halite', 'https://www.kaggle.com/code/basu369victor/designing-game-ai-with-reinforcement-learning', 'https://www.kaggle.com/code/solverworld/optimus-mine-agent']"
185,"LAUNCHED
This competition was launched and opened for submissions on May 27th 2020. Submissions will close in 1 week at 11:00 AM UTC on June 3rd 2020. The public leaderboard is based on the TREC-COVID Round 2 dataset. The private leaderboard will be based on the Round 3 dataset, which will be evaluated after the competition closes. Review the Data page for more details.
Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presents a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge will identify answers for some of today's questions while building infrastructure to improve tomorrow's search systems.
Kaggle first teamed up with the Allen Institute for AI in the launch of the COVID-19 Open Research Dataset (CORD-19). TREC-COVID builds on the CORD-19 Challenge by using the same document set, a collection of biomedical literature articles that has been updated on a weekly rolling basis.
This is the 3rd Round of the TREC-COVID Challenge. Prior runs were hosted directly on the TREC-COVID Site. For this round, you have the option to submit on Kaggle or directly to the TREC-COVID platform. The organizers have added 5 additional COVID-related topics to the 35 topics from the first two rounds, for a total of 40 topics. You will create a retrieval system that returns ranked lists of documents from CORD-19 for (a) each of these additional Round 3 topics (""runs"") and as well as (b) residual rankings on the completed Round 1 & 2 topics, i.e., for any documents not judged in the CORD-19 dataset (not previously included as a ranked document). The eligible population of documents for Round 3 is anything included in the CORD-19 release up to Round 3's launch date, last updated on May 19th 2020.
Following the close of Round 3, NIST will gather the collective set of participants' runs, to include those participants submitting directly through TREC-COVID. The organizers will then assess some reasonable subset of these submissions for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, will then be used to score the submitted runs. It is important to understand that not all documents will be assessed, and thus the private leaderboard score will be based on partial document assessment.
With your help, the final document and topic sets together with the cumulative relevance judgments will comprise a COVID test collection. The incremental nature of the collection will support research on search systems for dynamic environments.
Acknowledgments
The Text REtrieval Conference (TREC) was founded in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.
The TREC-COVID Challenge is being organized by the Allen Institute for Artificial Intelligence (AI2), the National Institute of Standards and Technology (NIST), the National Library of Medicine (NLM), Oregon Health and Science University (OHSU), and the University of Texas Health Science Center at Houston (UTHealth).
See the NIST press release for more information.","Metric
Submissions will be evaluated using NDCG (Normalized Discounted Cumulative Gain).
Relevance Judgements
The relevance judgments will be made by human annotators that have biomedical expertise. Annotators will use a three-way scale:
Relevant: the article is fully responsive to the information need as expressed by the topic, i.e. answers the Question in the topic. The article need not contain all information on the topic, but must, on its own, provide an answer to the question.
Partially Relevant: the article answers part of the question but would need to be combined with other information to get a complete answer.
Not Relevant: everything else.
Submission File
The submission should be a list of topicid-docid ranked from top to bottom according to their relevance (i.e., the top docid is the most relevant document for a the specified topic). Each line should be a single topicid-docid.
The columns are as follows:
topicid - is the topic number (1..40)
docid - the cord_uid of the document retrieved in this position. It must be a valid cord_id in the May 19 release of CORD-19. If it has already been judged for this topic, it will be removed.
Example
topicid,docid
1,000ajevz
1,000q5l5n
...
1,000tfenb","The task of this competition is to identify relevant documents from the CORD-19 dataset that match specific queries. This data page is an abbreviated version of the TREC-COVID Round 3 page. Please reference it for a more in-depth description of this challenge.
Note: The Public leaderboard is scored from data that has been previously released. We discourage submitting perfect submissions, but it's not unlikely that you will see scores of 1.0 on the leaderboard. This is effectively meaningless, since the final Private evaluation will be based on labels obtained after the competition close.
Files
CORD-19 - folder containing the May 19, 2020 version of CORD-19 documents and metadata
topics-rnd3.csv - file containing the topic-id, query, question, and narrative for each topic.
docids-rnd3.txt - the list of documents that are can be predicted as relevant; these exclude documents that have been scored in previous rounds for a topic
qrels.csv - contains the relevance judgements for documents that have been evaluated in previous rounds
sample_submission.csv - a sample submission file in the correct format (with random document entries)",kaggle competitions download -c trec-covid-information-retrieval,"['https://www.kaggle.com/code/mpwolke/covid-19-ace', 'https://www.kaggle.com/code/khotijahs1/trec-covid-information-retrieval', 'https://www.kaggle.com/code/grapestone5321/trec-covid-information-retrieval-sample-submission', 'https://www.kaggle.com/code/davidmezzetti/trec-covid-search-index', 'https://www.kaggle.com/code/mpwolke/covid-19-interleukin-6']"
186,"Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year the Open Images Instance Segmentation competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details.
Participants are also welcome to submit to this playground competition beyond the context of RVC.
Instance Segmentation Track
In this track of the Challenge, you are asked to provide segmentation masks of objects.
This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.
Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.

The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.","Submissions are evaluated by computing mean Average Precision, with the mean taken over the 300 segmentable classes of the challenge. It follows the same spirit as the Object Detection evaluation, but takes into account mask-to-mask matching. The metric is described in detail here. See also this tutorial on running the evaluation in Python.

Note that the RVC Challenge ranks participants in a separate leaderboard which integrates results across multiple benchmarks, including this competition.
Submission File
For each image in the test set, you must predict a list of instance segmentation masks and their associated detection score (Confidence). The submission csv file uses the following format:
ImageID,ImageWidth,ImageHeight,PredictionString
ImageAID,ImageAWidth,ImageAHeight,LabelA1 ConfidenceA1 EncodedMaskA1 LabelA2 ConfidenceA2 EncodedMaskA2 ...
ImageBID,ImageBWidth,ImageBHeight,LabelB1 ConfidenceB1 EncodedMaskB1 LabelB2 ConfidenceB2 EncodedMaskB2 …
A sample with real values would be:
ImageID,ImageWidth,ImageHeight,PredictionString
721568e01a744247,1118,1600,/m/018xm 0.637833 eNqLi8xJM7BOTjS08DT2NfI38DfyM/Q3NMAJgJJ+RkBs7JecF5tnAADw+Q9I
7b018c5e3a20daba,1600,1066,/m/01g317 0.85117 eNqLiYrLN7DNCjDMMIj0N/Iz9DcwBEIDfyN/QyA2AAsBRfxMPcKTA1MMADVADIo=
The binary segmentation masks are run-length encoded (RLE), zlib compressed, and base64 encoded to be used in text format as EncodedMask. Specifically, we use the Coco masks RLE encoding/decoding (see the encode method of COCO’s mask API), the zlib compression/decompression (RFC1950), and vanilla base64 encoding.
An example python function to encode an instance segmentation mask would be:
import base64
import numpy as np
from pycocotools import _mask as coco_mask
import typing as t
import zlib
def encode_binary_mask(mask: np.ndarray) -> t.Text:
 """"""Converts a binary mask into OID challenge encoding ascii text.""""""

 # check input mask --
 if mask.dtype != np.bool:
   raise ValueError(
       ""encode_binary_mask expects a binary mask, received dtype == %s"" %
       mask.dtype)

 mask = np.squeeze(mask)
 if len(mask.shape) != 2:
   raise ValueError(
       ""encode_binary_mask expects a 2d mask, received shape == %s"" %
       mask.shape)

 # convert input mask to expected COCO API input --
 mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)
 mask_to_encode = mask_to_encode.astype(np.uint8)
 mask_to_encode = np.asfortranarray(mask_to_encode)

 # RLE encode mask --
 encoded_mask = coco_mask.encode(mask_to_encode)[0][""counts""]

 # compress and base64 encoding --
 binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)
 base64_str = base64.b64encode(binary_str)
 return base64_str
(This code is available as a gist here.)
There is a 5Gb file size limit on the submission csv file. This implicitly limits the number of detections to about 50~100 per image (on average).","The train and validation sets of images and their ground truth (instance masks) should be downloaded from the Open Images Challenge page.
The test images used in this competition are independent from those released as part of the Open Images Dataset.
The test images are the same as in the Object Detection track, so you might not need to re-download them.
The challenge test set images can be downloaded from:
Kaggle (test.zip file below)
CVDF
Figure 8
You should expect 99,999 images in total in the challenge test set.
File descriptions
test.zip - the 99,999 images of the challenge test set.
sample_empty_submission.csv - a sample submission file with zero detections.
sample_truncated_submission.csv - a (truncated) sample submission file in the correct format.
It includes 42 detections over (only) 10 images.
A full submission.csv file should cover all 99,999 images and be less than 5Gb in size.",kaggle competitions download -c open-images-instance-segmentation-rvc-2020,"['https://www.kaggle.com/code/jpmiller/open-images-eda', 'https://www.kaggle.com/code/meenakshiramaswamy/open-images-mask-rcnn-starter', 'https://www.kaggle.com/code/shawon10/object-detection-and-image-segmentation-pixellib', 'https://www.kaggle.com/code/vsevicky/open-images-object-detection-rvc-2020-edition', 'https://www.kaggle.com/code/mahmudds/open-images-object-detection-rvc']"
187,"Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year the Open Images Object Detection competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details.
Participants are also welcome to submit to this playground competition beyond the context of RVC.
Object Detection Track
In this track, you are asked to predict a tight bounding box around object instances.
The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).
Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.
The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.","Submissions on Open Images Object Detection competition are evaluated by computing mean Average Precision (mAP), modified to take into account the annotation process of Open Images dataset (mean is taken over per-class APs). The metric is described on the Open Images Challenge website, and is implemented as a part of Tensorflow Object Detection API. See this Tutorial on running the evaluation in Python.
The final mAP is computed as the average AP over the 500 classes. The participants leaderboard will be ranked on this final metric.
Note that the RVC Challenge ranks participants in a separate leaderboard which integrates results across multiple benchmarks, including this competition.
Submission File
For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as
ImageID,PredictionString
ImageID,{Label Confidence XMin YMin XMax YMax} {...}","The train and validation sets of images and their ground truth (bounding boxes and labels) should be downloaded from Open Images Challenge page. Please note that the test images used in this competition is independent from those released as part of the Open Images Dataset. The images can be downloaded from:
Kaggle (test.zip file below)
CVDF
Figure 8
You should expect 99,999 images in total in the test set.
For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as
ImageID,PredictionString
ImageID,{Label Confidence XMin YMin XMax YMax} {...}",kaggle competitions download -c open-images-object-detection-rvc-2020,"['https://www.kaggle.com/code/jpmiller/open-images-eda', 'https://www.kaggle.com/code/imoore/generate-paintings-by-image-style-transfer', 'https://www.kaggle.com/code/ateplyuk/open-images-2020-tfhub', 'https://www.kaggle.com/code/shawon10/google-object-detection-by-pixellib-mask-rcnn', 'https://www.kaggle.com/code/bryanafreeman/image-object-detection']"
188,"That file you downloaded may contain hidden messages that aren’t part of its regular contents. The same technology employed for digital watermarking is also misused by crime rings. Law enforcement must now use steganalysis to detect these messages as part of their investigations. Machine learning is an important tool in the discovery of this secret data.
Current methods produce unreliable results, raising false alarms. One reason for inaccuracy is the many different devices and processing combinations. Yet, detection models are trained on a homogeneous dataset. To increase accuracy, researchers must put data hidden within digital images “into the wild” (hence the name ALASKA) to mimic real world conditions.
In the competition, you’ll create an efficient and reliable method to detect secret data hidden within innocuous-seeming digital images. Rather than limiting the data source, these images have been acquired with as many as 50 different cameras (from smartphone to full-format high end) and processed in different fashions. Successful entries will include robust detection algorithms with minimal false positives.
The IEEE WIFS (Workshop on Information Forensics and Security) is eager to make this happen again, as a follow up to the ALASKA#1 Challenge. WIFS is an annual event where researchers gather to discuss emerging challenges, exchange fresh ideas, and share state-of-the-art results and technical expertise in the areas of information security and forensics. WIFS has teamed up with Troyes University of Technology, CRIStAL Lab, Lille University, and CNRS to enable more accurate steganalysis.
Law enforcement officers need better methods to combat criminals using hidden messages. The data science community and other researchers can help with better automated detection. More accurate methods could help catch criminals whose communications are hidden in plain sight.
The challenge is organized by Rémi COGRANNE (UTT), Patrick BAS (CRIStAL / CNRS) and Quentin Giboulot (UTT) ; in addition to Kaggle, we have been greatly helped by the following sponsors:
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1951250%2F3a24a302d6a43d42087769d57048b566%2Flogo_UTT_CRIStAL.png?generation=1588208625932740&alt=media) ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1951250%2Fa5c6f8c179e51a4048b581191166425d%2FLogoCNRS_SPS.png?generation=1588208648755636&alt=media)","In order to focus on reliable detection with an emphasis on low false-alarm rate, submissions are evaluated on the weighted AUC. To calculate the weighted AUC, each region of the ROC curve is weighted according to these chosen parameters:
tpr_thresholds = [0.0, 0.4, 1.0]
weights = [2, 1]
In other words, the area between the true positive rate of 0 and 0.4 is weighted 2X, the area between 0.4 and 1 is now weighed (1X). The total area is normalized by the sum of weights such that the final weighted AUC is between 0 and 1.
The figure below illustrates the different areas. This example also shows that the submission #2 has the lowest AUC but is ranked first since it outperforms its competitors for low False Positive Rate (FPR). On the opposite, the submission #3 has the highest AUC but would have been ranked third.
Submission File
For each Id (image) in the test set, you must provide a score that indicates how likely this image contains hidden data: the higher the score, the more it is assumed that image contains secret data. The file should contain a header and have the following format:
Id,Label
0001.jpg,0.1
0002.jpg,0.99
0003.jpg,1.2
0004.jpg,-2.2
etc.","This dataset contains a large number of unaltered images, called the ""Cover"" image, as well as corresponding examples in which information has been hidden using one of three steganography algorithms (JMiPOD, JUNIWARD, UERD).
The goal of the competition is to determine which of the images in the test set (Test/) have hidden messages embedded.
Note that in order to make the competition more realistic the length of hidden messages (the payload) will not be provided. The only available information on the test set is:
Each embedding algorithm is used with the same probability.
The payload (message length) is adjusted such that the ""difficulty"" is approximately the same regardless the content of the image. Images with smooth content are used to hide shorter messages while highly textured images will be used to hide more secret bits. The payload is adjusted in the same manner for testing and training sets.
The average message length is 0.4 bit per non-zero AC DCT coefficient.
The images are all compressed with one of the three following JPEG quality factors: 95, 90 or 75.
Files
Cover/ contains 75k unaltered images meant for use in training.
JMiPOD/ contains 75k examples of the JMiPOD algorithm applied to the cover images.
JUNIWARD/contains 75k examples of the JUNIWARD algorithm applied to the cover images.",kaggle competitions download -c alaska2-image-steganalysis,"['https://www.kaggle.com/code/tanulsingh077/steganalysis-complete-understanding-and-model', 'https://www.kaggle.com/code/shonenkov/train-inference-gpu-baseline', 'https://www.kaggle.com/code/prashant111/alaska2-image-steganalysis-all-you-need-to-know', 'https://www.kaggle.com/code/andradaolteanu/alaska2-competition-multiclass-pytorch-effnetb2', 'https://www.kaggle.com/code/xhlulu/alaska2-efficientnet-on-tpus']"
189,"This is week 5 of Kaggle's COVID-19 forecasting series, following the Week 4 competition. This competition has some changes from prior weeks - be sure to check the Evaluation and Data pages for more details. All of the prior discussion forums have been migrated to this competition for continuity.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves developing quantile estimates intervals for confirmed cases and fatalities between May 12 and June 7 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.","Public and Private Leaderboard
To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data prior to 2020-04-27 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.
Public Leaderboard Period: 2020-04-27 - 2020-05-11
Private Leaderboard Period: 2020-05-13 - 2020-06-10
Evaluation
Submissions are scored using the Weighted Pinball Loss.
$$
\text{score} = \frac{1}{N_{f}} \sum_{f} w_{f} \frac{1}{N_{\tau}} \sum_{\tau} L_{\tau}(y_i,\hat{y}_{i})
$$
where:
$$
\begin{eqnarray}
L_{\tau}(y,\hat{y}) & = & (y - \hat{y}) \tau & \textrm{ if } y \geq \hat{y} \\
& = & (\hat{y} - y) (1 - \tau) & \textrm{ if } \hat{y} > y
\end{eqnarray}
$$
and:
\(y \) is the ground truth value
\(\hat{y} \) is the predicted value
\(\tau \) is the quantile to be predicted, e.g., one of [0.05, 0.50, 0.95]
\(N_{f}\) is the total number of forecast (\(f\)) day x target combinations
\(N_{\tau} \) is the total number of quantiles to predict
\( w\) is a weighting factor
Weights are calculated as follows:
ConfirmedCases: \(\log(\text{population}+1)^{-1}\)
Fatalities: \(10 \cdot \log(\text{population}+1)^{-1}\)
Submission File
For each ForecastId in the test set, you'll predict the 0.05, 0.50, and 0.95 quantiles for daily COVID-19 cases and fatalities to date. The file should contain a header and have the following format:
ForecastId_Quantile,TargetValue
1_0.05,1
1_0.50,1
1_0.95,1
2_0.05,1
etc.
You will get the ForecastId_Quantile for the corresponding date and location from the test.csv file.","In this challenge, you will be predicting the daily number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates. This latest challenge includes US state county data.
Files
train.csv - the training data (you are encouraged to join in many more useful external datasets)
test.csv - the dates to predict; there is a week of overlap with the training data for the initial Public leaderboard. Once submissions are paused, the Public leaderboard will update based on last 28 days of predicted data.
submission.csv - a sample submission in the correct format; again, predictions should be daily
Data Source
This evaluation data for this competition comes from John Hopkins CSSE, which is uninvolved in the competition.
See their README for a description of how the data was collected.
They are currently updating the data daily.",kaggle competitions download -c covid19-global-forecasting-week-5,"['https://www.kaggle.com/code/nitishabharathi/the-story-of-covid-19-in-india-eda-and-prediction', 'https://www.kaggle.com/code/nischaydnk/covid19-week5-visuals-randomforestregressor', 'https://www.kaggle.com/code/eswarchandt/geospatial-analysis-on-covid-19-day-to-day-track', 'https://www.kaggle.com/code/dkjung/covid-19-eda-s-korea-forecasting-global', 'https://www.kaggle.com/code/eswarchandt/timeseries-forecasting-of-covid-19-arima']"
190,"Note: Put your heads together to solve programming challenges. Google's coding competition, Hash Code, has just finished for 2020. Use this online qualifier from 2019 to keep your skills sharp for future competitions!
As the saying goes, ""a picture is worth a thousand words."" We agree – photos are an important part of contemporary digital and cultural life. How we experience photos largely depends on the story they’re arranged to tell. The same shots could be a monotonous series of snaps or form a narrative masterpiece.
Approximately 2.5 billion people around the world carry a camera – in the form of a smartphone – in their pocket every day. We tend to make good use of it, too, taking more photos than ever (back in 2017, Google Photos announced it was backing up more than 1.2 billion photos and videos per day)! The rise of digital photography creates an interesting challenge: what should we do with all of these photos? In this competition, you will compose a slideshow out of a photo collection.
Given a list of photos and the tags associated with each photo, you are challenged to arrange the photos into a slideshow that is as interesting as possible (the evaluation section explains what we mean by “interesting”)
Will your slideshow tell a good story or be a major snoozefest?","Scoring
The slideshow is scored based on how interesting the transitions between each pair of subsequent (neighboring) slides are. We want the transitions to have something in common to preserve continuity (the two slides should not be totally different), but we also want them to be different enough to keep the audience interested. The similarity of two vertical photos on a single slide is not taken into account for the scoring function. This means that two photos can, but don't have to, have tags in common.
For two subsequent slides Si and Si+1, the interest factor is the minimum of:
the number of common tags between Si and Si+1
the number of tags in Si but not in Si+1
the number of tags in Si+1 but not in Si.
Submission Format
You must make your submission via a Kaggle notebook, and that notebook must output a submission file named submission.txt. Once the notebook has been saved and committed, you can file your submission from the output section of the results.
The output file must start with a single integer S (1 ≤ S ≤N)— the number of slides in the slideshow. This must be followed by S lines describing the individual slides. Each line should contain either:
A single integer – ID of the single horizontal photo in the slide.
Two integers separated by a single space – IDs of the two vertical photos in the slide in any order.
Each photo can be used only one time or not at all.
Example
To submit an album with three slides in total, including slides 0, 3, and the pair of 1 and 2:
3
0
3
1 2","Photos
A photo is described by a set of tags. For example, a photo with a cat on a beach, during a sunny afternoon could be tagged with the following tags: [cat, beach, sun]. Each photo's orientation is either horizontal or vertical.
Slideshow
A slideshow is an ordered list of slides. Each slide contains either:
a single horizontal photo, or
two vertical photos side-by-side
If the slide contains a single horizontal photo, the tags of the slide are the same as the tags of the single photo it contains.
For example, a slide containing a single horizontal photo with tags [cat, beach, sun], has tags [cat, beach, sun].
If the slide contains two vertical photos, the tags of the slide are all the tags present in any or both of the two photos it contains.",kaggle competitions download -c hashcode-photo-slideshow,"['https://www.kaggle.com/code/group16/greedy-solution-lb-400k', 'https://www.kaggle.com/code/huikang/441k-in-11-mins', 'https://www.kaggle.com/code/mathurinache/googlehashcode2019-starter-code', 'https://www.kaggle.com/code/group16/tsp-swapper', 'https://www.kaggle.com/code/egrehbbt/442k-in-2-hours']"
191,"Human brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.
In this competition, you will predict multiple assessments plus age from multimodal brain MRI features. You will be working from existing results from other data scientists, doing the important work of validating the utility of multimodal features in a normative population of unaffected subjects. Due to the complexity of the brain and differences between scanners, generalized approaches will be essential to effectively propel multimodal neuroimaging research forward.
The Tri-Institutional Georgia State University/Georgia Institute of Technology/Emory University Center for Translational Research in Neuroimaging and Data Science (TReNDS) leverages advanced brain imaging to promote research into brain health. The organization is focused on developing, applying and sharing advanced analytic approaches and neuroinformatics tools. Among its software projects are the GIFT and FIT neuroimaging toolboxes, the COINS data management system, and the COINSTAC toolkit for federated learning, all aimed at supporting data scientists and other neuroimaging researchers.
Making the leap from research to clinical application is particularly difficult in brain health. In order to translate to clinical settings, research findings have to be reproduced consistently and validated in out-of-sample instances. The problem is particularly well-suited for data science, but current approaches typically do not generalize well. With this large dataset and competition, your efforts could directly address an important area of brain research.
Acknowledgments","Submissions are scored using feature-weighted, normalized absolute errors.
$$
\text{score} = \sum_f w_f \left( \frac{\sum_i \lvert y_{f,i} - \hat{y}_{f,i} \rvert}{\sum_i \hat{y}_{f,i}} \right)
$$
where \(y_{f,i}\) is the \(i^{th}\) observation of feature \(f\), \(\hat{y}_{f,i}\) is the corresponding ground truth for that observation, and \(w_f\) is a weighting given to each feature. The weights are [.3, .175, .175, .175, .175] corresponding to features [age, domain1_var1, domain1_var2, domain2_var1, domain2_var2].
A small percentage of values are missing from the ground truth. These are skipped in the calculation. You should, though, make predictions for every row in the submission file.
Submission File
For each Id in the test set, you must make a corresponding prediction for each feature. The Id contains both the sample id as well as feature. The file should contain a header and have the following format:
Id,Predicted
10003_age,0
10003_domain1_var1,50.0
10003_domain1_var2,50.0
10003_domain2_var1,50.0
10003_domain2_var2,50.0
etc.","End of Competition Update: At the request of the competition host, the dataset has been withdrawn from this page at the conclusion of the competition.
In this challenge, participants will predict age and assessment values from two domains using features derived from brain MRI images as inputs.
Models are expected to generalize on data from a different scanner/site (site 2). All subjects from site 2 were assigned to the test set, so their scores are not available. While there are fewer site 2 subjects than site 1 subjects in the test set, the total number of subjects from site 2 will not be revealed until after the end of the competition. To make it more interesting, the IDs of some site 2 subjects have been revealed below. Use this to inform your models about site effects. Site effects are a form of bias. To generalize well, models should learn features that are not related to or driven by site effects.
The .mat files for this competition can be read in python using h5py, and the .nii file can be read in python using nilearn.
Files
fMRI_train - a folder containing 53 3D spatial maps for train samples in .mat format
fMRI_test - a folder containing 53 3D spatial maps for test samples in .mat format
fnc.csv - static FNC correlation features for both train and test samples",kaggle competitions download -c trends-assessment-prediction,"['https://www.kaggle.com/code/rohitsingh9990/trends-eda-visualization-simple-baseline', 'https://www.kaggle.com/code/aerdem4/rapids-svm-on-trends-neuroimaging', 'https://www.kaggle.com/code/gunesevitan/trends-neuroimaging-data-analysis-3d-features', 'https://www.kaggle.com/code/tunguz/rapids-ensemble-for-trends-neuroimaging', 'https://www.kaggle.com/code/soham1024/visualization-using-nilearn']"
192,"With more than 1 million new diagnoses reported every year, prostate cancer (PCa) is the second most common cancer among males worldwide that results in more than 350,000 deaths annually. The key to decreasing mortality is developing more precise diagnostics. Diagnosis of PCa is based on the grading of prostate tissue biopsies. These tissue samples are examined by a pathologist and scored according to the Gleason grading system. In this challenge, you will develop models for detecting PCa on images of prostate tissue samples, and estimate severity of the disease using the most extensive multi-center dataset on Gleason grading yet available.
The grading process consists of finding and classifying cancer tissue into so-called Gleason patterns (3, 4, or 5) based on the architectural growth patterns of the tumor (Fig. 1). After the biopsy is assigned a Gleason score, it is converted into an ISUP grade on a 1-5 scale. The Gleason grading system is the most important prognostic marker for PCa, and the ISUP grade has a crucial role when deciding how a patient should be treated. There is both a risk of missing cancers and a large risk of overgrading resulting in unnecessary treatment. However, the system suffers from significant inter-observer variability between pathologists, limiting its usefulness for individual patients. This variability in ratings could lead to unnecessary treatment, or worse, missing a severe diagnosis.
Automated deep learning systems have shown some promise in accurately grading PCa. Recent research, including two studies independently conducted by the groups hosting this challenge, have shown that these systems can achieve pathologist-level performance. However, these systems/results were not tested with multi-center datasets at scale.
Your work here will improve on these efforts using the most extensive multi-center dataset on Gleason grading yet. The training set consists of around 11,000 whole-slide images of digitized H&E-stained biopsies originating from two centers. This is the largest public whole-slide image dataset available, roughly 8 times the size of the CAMELYON17 challenge, one of the largest digital pathology datasets and best known challenges in the field. Furthermore, in contrast to previous challenges, we are making full diagnostic biopsy images available. Using a sizable multi-center test set, graded by expert uro-pathologists, we will evaluate challenge submissions on their applicability to improve this critical diagnostic function.
Figure 1: An illustration of the Gleason grading process for an example biopsy containing prostate cancer. The most common (blue outline, Gleason pattern 3) and second most common (red outline, Gleason pattern 4) cancer growth patterns present in the biopsy dictate the Gleason score (3+4 for this biopsy), which in turn is converted into an ISUP grade (2 for this biopsy) following guidelines of the International Society of Urological Pathology. Biopsies not containing cancer are represented by an ISUP grade of 0 in this challenge.
Radboud University Medical Center and Karolinska Institute have teamed up to organize this competition in collaboration with colleagues from Tampere University. The Computational Pathology Group (CPG) of the Radboud University Medical Center is a research group that develops computer algorithms to aid clinicians. Karolinska Institute’s Department of Medical Epidemiology and Biostatistics (MEB) includes an interdisciplinary research group to improve the diagnostics and treatment of prostate cancer. Together, they hope to further their existing research to make a significant impact on the healthcare of prostate cancer patients.
Challenge organizer team: Wouter Bulten, Geert Litjens, Hans Pinckaers, Peter Ström, Martin Eklund, Lars Egevad, Henrik Grönberg, Kimmo Kartasalo, Pekka Ruusuvuori, Tomi Häkkinen, Sohier Dane, Maggie Demkin.
Sponsors
The PANDA workshop at MICCAI 2020 is sponsored by ContextVision, Ibex and Google.
Published results
The paper on the PANDA challenge has been published as Open Access in Nature Medicine. In the paper, we took a deep dive into the solutions, tested the methods to see if they generalize well to unseen data, and performed a comparison with pathologists. You can read the full paper and all results here:
https://www.nature.com/articles/s41591-021-01620-2
Bulten, W., Kartasalo, K., Chen, PH.C. et al. Artificial intelligence for diagnosis and Gleason grading of prostate cancer: the PANDA challenge. Nat Med (2022). https://doi.org/10.1038/s41591-021-01620-2
Using the data outside of the competition
With the paper's publication, the embargo on the data is now lifted (see forum post). If you want, you can now use the dataset for further scientific work and publish your results on the dataset. If you do so, please take the license (CC BY-SA-NC 4.0) into account (non-commercial) and make sure you cite the PANDA paper. The test sets will not be made public at this time, to allow further late submissions to be used for benchmarking algorithms. We are looking forward to seeing new scientific projects coming out of this dataset!","Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.
The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that Oi,j
corresponds to the number of isup_grades i (actual) that received a predicted value j. An N-by-N matrix of weights, w,
is calculated based on the difference between actual and predicted values:
$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$
An N-by-N histogram matrix of expected outcomes, E, is calculated assuming that there is no correlation between values. 
This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that E and O have the same sum.
From these three matrices, the quadratic weighted kappa is calculated as: 
$$\kappa=1-\frac{\sum_{i,j}w_{i,j}O_{i,j}}{\sum_{i,j}w_{i,j}E_{i,j}}.$$
Submission File
You must predict the isup_grade for each image_id.
The submission file must have a header and should look like the following:
image_id,isup_grade
test_1,0
test_2,1
etc.","Your challenge in this competition is to classify the severity of prostate cancer from microscopy scans of prostate biopsy samples. There are two unusual twists to this problem relative to most competitions:
Each individual image is quite large. We're excited to see what strategies you come up with for efficiently locating areas of concern to zoom in on.
The labels are imperfect. This is a challenging area of pathology and even experts in the field with years of experience do not always agree on how to interpret a slide. This will make training models more difficult, but increases the potential medical value of having a strong model to provide consistent ratings. All of the private test set images and most of the public test set images were graded by multiple pathologists, but this was not feasible for the training set. You can find additional details about how consistently the pathologist's labels matched here.
Files
[train/test].csv
image_id: ID code for the image.
data_provider: The name of the institution that provided the data. Both the Karolinska Institute and Radboud University Medical Center contributed data. They used different scanners with slightly different maximum microscope resolutions and worked with different pathologists for labeling their images.
isup_grade: Train only. The target variable. The severity of the cancer on a 0-5 scale.",kaggle competitions download -c prostate-cancer-grade-assessment,"['https://www.kaggle.com/code/rohitsingh9990/panda-eda-better-visualization-simple-baseline', 'https://www.kaggle.com/code/tanulsingh077/prostate-cancer-in-depth-understanding-eda-model', 'https://www.kaggle.com/code/iafoss/panda-concat-tile-pooling-starter-0-79-lb', 'https://www.kaggle.com/code/haqishen/train-efficientnet-b0-w-36-tiles-256-lb0-87', 'https://www.kaggle.com/code/iafoss/panda-16x128x128-tiles']"
193,"This is week 4 of Kaggle's COVID-19 forecasting series, following the Week 3 competition. This is the 4th competition we've launched in this series. All of the prior discussion forums have been migrated to this competition for continuity.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 15 and May 14 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.","Public and Private Leaderboard
To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data prior to 2020-04-1 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.
Public Leaderboard Period - 2020-04-01 - 2020-04-15
Private Leaderboard Period - 2020-04-16 - 2020-05-14
Evaluation
Submissions are evaluated using the column-wise root mean squared logarithmic error.
The RMSLE for a single column calculated as
$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },
$$
where:
\\(n\\) is the total number of observations
\\(p_i\\) is your prediction
\\(a_i\\) is the actual value
\\(\log(x)\\) is the natural logarithm of \\(x\\)
The final score is the mean of the RMSLE over all columns (in this case, 2).
Submission File
We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.
For each ForecastId in the test set, you'll predict the cumulative COVID-19 cases and fatalities to date. The file should contain a header and have the following format:
ForecastId,ConfirmedCases,Fatalities
1,10,0
2,10,0
3,10,0
etc.
You will get the ForecastId for the corresponding date and location from the test.csv file.","In this challenge, you will be predicting the cumulative number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates.
We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.
Files
train.csv - the training data (you are encouraged to join in many more useful external datasets)
test.csv - the dates to predict; there is a week of overlap with the training data for the initial Public leaderboard. Once submissions are paused, the Public leaderboard will update based on last 28 days of predicted data.
submission.csv - a sample submission in the correct format; again, predictions should be cumulative
Data Source
This evaluation data for this competition comes from John Hopkins CSSE, which is uninvolved in the competition.
See their README for a description of how the data was collected.
They are currently updating the data daily.",kaggle competitions download -c covid19-global-forecasting-week-4,"['https://www.kaggle.com/code/saga21/covid-global-forecast-sir-model-ml-regressions', 'https://www.kaggle.com/code/nitishabharathi/the-story-of-covid-19-in-india-eda-and-prediction', 'https://www.kaggle.com/code/frlemarchand/covid-19-forecasting-with-an-rnn', 'https://www.kaggle.com/code/anshuls235/covid19-explained-through-visualizations', 'https://www.kaggle.com/code/davidbnn92/weather-data']"
194,"This week 3 forecasting task is now closed for submissions. Click here to visit the week 4 version, and make a submission there.
This is week 3 of Kaggle's COVID19 forecasting series, following the Week 2 competition. This is the 3rd of at least 4 competitions we plan to launch in this series. All of the prior discussion forums have been migrated to this competition for continuity.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.","Public and Private Leaderboard
To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data prior to 2020-03-26 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.
Public Leaderboard Period - 2020-03-26 - 2020-04-08
Private Leaderboard Period - 2020-04-09 - 2020-05-07
Evaluation
Submissions are evaluated using the column-wise root mean squared logarithmic error.
The RMSLE for a single column calculated as
$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },
$$
where:
\\(n\\) is the total number of observations
\\(p_i\\) is your prediction
\\(a_i\\) is the actual value
\\(\log(x)\\) is the natural logarithm of \\(x\\)
The final score is the mean of the RMSLE over all columns (in this case, 2).
Submission File
We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.
For each ForecastId in the test set, you'll predict the cumulative COVID-19 cases and fatalities to date. The file should contain a header and have the following format:
ForecastId,ConfirmedCases,Fatalities
1,10,0
2,10,0
3,10,0
etc.
You will get the ForecastId for the corresponding date and location from the test.csv file.",,,
195,,,,,
196,,,,,
197,,,,,
198,,,,,
199,"This week 1 forecasting task is now closed for submissions. Click here to visit the week 2 version, and make a submission there.
This is one of the two complementary forecasting tasks to predict COVID-19 spread. This task is based on various regions across the world. To start on a single state-level subcomponent, please see the companion forecasting task for California, USA.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching two companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 by region, the primary goal isn't to produce accurate forecasts. It’s to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.","Public and Private Leaderboard
To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data on or prior to 2020-03-11 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.
Public Leaderboard Period - 2020-03-12 - 2020-03-25
Private Leaderboard Period - 2020-03-26 - 2020-04-23
Evaluation
Submissions are evaluated using the column-wise root mean squared logarithmic error.
The RMSLE for a single column calculated as
$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },
$$
where:
\\(n\\) is the total number of observations
\\(p_i\\) is your prediction
\\(a_i\\) is the actual value
\\(\log(x)\\) is the natural logarithm of \\(x\\)
The final score is the mean of the RMSLE over all columns (in this case, 2).
Submission File
We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.
For each ForecastId in the test set, you'll predict the cumulative COVID-19 cases and fatalities to date. The file should contain a header and have the following format:
ForecastId,ConfirmedCases,Fatalities
1,10,0
2,10,0
3,10,0
etc.
You will get the ForecastId for the corresponding date and location from the test.csv file.","In this challenge, you will be predicting the cumulative number of confirmed COVID19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates.
We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.
Files
train.csv - the training data up to Mar 18, 2020.
test.csv - the dates to predict; there is a week of overlap with the training data for the initial Public leaderboard. Once submissions are paused, the Public leaderboard will update based on last 28 days of predicted data.
submission.csv - a sample submission in the correct format; again, predictions should be cumulative
Data Source
This evaluation data for this competition comes from John Hopkins CSSE, which is uninvolved in the competition.
See their README for a description of how the data was collected.
They are currently updating the data daily.",kaggle competitions download -c covid19-global-forecasting-week-1,"['https://www.kaggle.com/code/abhinand05/covid-19-digging-a-bit-deeper', 'https://www.kaggle.com/code/davidbnn92/weather-data', 'https://www.kaggle.com/code/deepakdeepu8978/covid-19-analysis-eda-forecasting', 'https://www.kaggle.com/code/pradeepmuniasamy/covid19-inside-story-of-each-countries', 'https://www.kaggle.com/code/deadskull7/covid-19-inf-lung-segmentation-classification']"
200,,,,,
201,"This is one of the two complementary forecasting tasks to predict COVID-19 spread. This one is based on a single state-level subcomponent in California, USA. Our intent in having this region-specific version is to offer a more manageable starting point for the global forecasting task. To start on the global version, please see the companion forecasting task.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching two companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 in California, the primary goal isn't to produce accurate forecasts. It’s to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.","Public and Private Leaderboard
To have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data on or prior to 2020-03-11 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.
Public Leaderboard Period - 2020-03-12 - 2020-03-25
Private Leaderboard Period - 2020-03-26 - 2020-04-23
Evaluation
Submissions are evaluated using the column-wise root mean squared logarithmic error.
The RMSLE for a single column calculated as
$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },
$$
where:
\\(n\\) is the total number of observations
\\(p_i\\) is your prediction
\\(a_i\\) is the actual value
\\(\log(x)\\) is the natural logarithm of \\(x\\)
The final score is the mean of the RMSLE over all columns (in this case, 2).
Submission File
We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.
For each ForecastId in the test set, you'll predict the cumulative COVID-19 cases and fatalities to date. The file should contain a header and have the following format:
ForecastId,ConfirmedCases,Fatalities
1,10,0
2,10,0
3,10,0
etc.
You will get the ForecastId for the corresponding date and location from the test.csv file.","In this challenge, you will be predicting the cumulative number of confirmed COVID19 cases in California, as well as the number of resulting fatalities, for future dates.
We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.
Files
ca_train.csv - the training data up to Mar 18, 2020.
ca_test.csv - the dates to predict; there is a week of overlap with the training data for the initial Public leaderboard. Once submissions are paused, the Public leaderboard will update based on last 28 days of predicted data.
ca_submission.csv - a sample submission in the correct format; again, predictions should be cumulative
Data Source
This evaluation data for this competition comes from John Hopkins CSSE, which is uninvolved in the competition.
See their README for a description of how the data was collected.
They are currently updating the data daily.",kaggle competitions download -c covid19-local-us-ca-forecasting-week-1,"['https://www.kaggle.com/code/dferhadi/covid-19-predictions-growth-factor-and-calculus', 'https://www.kaggle.com/code/mdmahmudferdous/covid-19-us-ca-forecasting-top-4-notebook-6th', 'https://www.kaggle.com/code/dferhadi/global-forecasting-covid-19-random-forest', 'https://www.kaggle.com/code/panosc/california-curves-vs-other-world-regions', 'https://www.kaggle.com/code/tunguz/simple-covid-19-ca-eda']"
202,"Problem Statement
Misdiagnosis of the many diseases impacting agricultural crops can lead to misuse of chemicals leading to the emergence of resistant pathogen strains, increased input costs, and more outbreaks with significant economic loss and environmental impacts. Current disease diagnosis based on human scouting is time-consuming and expensive, and although computer-vision based models have the promise to increase efficiency, the great variance in symptoms due to age of infected tissues, genetic variations, and light conditions within trees decreases the accuracy of detection.
Specific Objectives
Objectives of ‘Plant Pathology Challenge’ are to train a model using images of training dataset to 1) Accurately classify a given image from testing dataset into different diseased category or a healthy leaf; 2) Accurately distinguish between many diseases, sometimes more than one on a single leaf; 3) Deal with rare classes and novel symptoms; 4) Address depth perception—angle, light, shade, physiological age of the leaf; and 5) Incorporate expert knowledge in identification, annotation, quantification, and guiding computer vision to search for relevant features during learning.
Resources
Details and background information on the dataset and Kaggle competition ‘Plant Pathology 2020 Challenge’ were published. If you use the dataset for your project, please cite the following peer-reviewed research article
Thapa, Ranjita; Zhang, Kai; Snavely, Noah; Belongie, Serge; Khan, Awais. The Plant Pathology Challenge 2020 data set to classify foliar disease of apples. Applications in Plant Sciences, 8 (9), 2020.
Acknowledgments
We acknowledge financial support from Cornell Initiative for Digital Agriculture (CIDA) and special thanks to Zach Guillian for help with data collection.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","Submissions are evaluated on mean column-wise ROC AUC. In other words, the score is the average of the individual AUCs of each predicted column.
Submission File
For each image_id in the test set, you must predict a probability for each target variable. The file should contain a header and have the following format:
image_id,
test_0,0.25,0.25,0.25,0.25
test_1,0.25,0.25,0.25,0.25
test_2,0.25,0.25,0.25,0.25
etc.","Given a photo of an apple leaf, can you accurately assess its health? This competition will challenge you to distinguish between leaves which are healthy, those which are infected with apple rust, those that have apple scab, and those with more than one disease.
Files
train.csv
image_id: the foreign key
combinations: one of the target labels
healthy: one of the target labels
rust: one of the target labels
scab: one of the target labels
images
A folder containing the train and test images, in jpg format.
test.csv
image_id: the foreign key",kaggle competitions download -c plant-pathology-2020-fgvc7,"['https://www.kaggle.com/code/beomsulee/plant-pathology-2020-fgvc7-with-efficientnet', 'https://www.kaggle.com/code/damilareoyediran/notebook602ee1b9f3', 'https://www.kaggle.com/code/kimseokje/plant-pathology-2020-baseline', 'https://www.kaggle.com/code/leemoodong/ch12-notebook', 'https://www.kaggle.com/code/leemoodong/ch12-modeling2']"
203,"Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automatic species classification in camera trap images. However, as we try to expand the scope of these models we are faced with an interesting problem: how do we train models that perform well on new (unseen during training) camera trap locations? Can we leverage data from other modalities, such as citizen science data and remote sensing data?
In order to tackle this problem, we have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to classify species in the test cameras correctly. To explore multimodal solutions, we allow competitors to train on the following data: (i) our camera trap training set (data provided by WCS), (ii) iNaturalist 2017-2019 data, and (iii) multispectral imagery (from Landsat 8) for each of the camera trap locations. On the competition GitHub page we provide the multispectral data, a taxonomy file mapping our classes into the iNat taxonomy, a subset of iNat data mapped into our class set, and a camera trap detection model (the MegaDetector) along with the corresponding detections.
If you use this dataset in publication, please cite:
@article{beery2020iwildcam,
    title={The iWildCam 2020 Competition Dataset},
    author={Beery, Sara and Cole, Elijah and Gjoka, Arvi},
    journal={arXiv preprint arXiv:2004.10340},
    year={2020}
}
This is an FGVCx competition as part of the FGVC7 workshop at CVPR 2020, and is sponsored by Microsoft AI for Earth and Wildlife Insights. There is a GitHub page for the competition here. Please open an issue if you have questions or problems with the dataset.
You can find the iWildCam 2018 Competition here, and the iWildCam 2019 Competition here.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","Evaluation
Submissions will be evaluated based on their categorization accuracy
Submission Format
The submission format for the competition is a csv file with the following format:
Id,Predicted
58857ccf-23d2-11e8-a6a3-ec086b02610b,1
591e4006-23d2-11e8-a6a3-ec086b02610b,5
The Id column corresponds to the test image id. The Category is an integer value that indicates the class of the animal, or 0 to represent the absence of an animal.","Data Overview
The WCS training set contains 217,959 images from 441 locations, and the WCS test set contains 62,894 images from 111 locations. These 552 locations are spread across the globe.
You may also choose to use supplemental training data from the iNaturalist 2017, iNaturalist 2018 and iNaturalist 2019 competition datasets. As a courtesy, we have curated all the images from these datasets containing classes that might be in the test set and mapped them into the iWildCam categories. Note: these curated images come only from the iNaturalist 2017 and iNaturalist 2018 datasets because there are no common classes between the iNaturalist 2019 dataset and the WCS dataset. However, participants are still free to use the iNaturalist 2019 data.
This year we are providing Landsat-8 multispectral imagery for each camera location as supplementary data. In particular, each site is associated with a series of patches collected between 2013 and 2019. The patches are extracted from a ""Tier 1"" Landsat product, which consists only of data that meets certain geometric and radiometric quality standards. Consequently, the number of patches per site varies from 39 to 406 (median: 147). Each patch is 200x200x9 pixels, covering an area of 6km^2 at a resolution of 30 meters / pixel across 9 spectral bands. Note that all patches for a given site are registered, but are not centered exactly at the camera location to protect the integrity of the site.
The data can be downloaded from the competition GitHub page.
Camera Trap Animal Detection Model",kaggle competitions download -c iwildcam-2020-fgvc7,"['https://www.kaggle.com/code/nayuts/iwildcam-2020-overviewing-for-start', 'https://www.kaggle.com/code/ateplyuk/iwildcam2020-pytorch-start', 'https://www.kaggle.com/code/bsridatta/eda-and-object-extraction-for-classifier-training', 'https://www.kaggle.com/code/seriousran/image-pre-processing-for-iwild-2020', 'https://www.kaggle.com/code/seriousran/simple-starter-iwildcam-2020']"
204,"The Herbarium 2020 FGVC7 Challenge is to identify vascular plant species from a large, long-tailed collection herbarium specimens provided by the New York Botanical Garden (NYBG).
The Herbarium 2020 dataset contains over 1M images representing over 32,000 plant species. This is a dataset with a long tail; there are a minimum of 3 specimens per species. However, some species are represented by more than a hundred specimens. This dataset only contains vascular land plants which includes lycophytes, ferns, gymnosperms, and flowering plants. The extinct forms of lycophytes are the major component of coal deposits, ferns are indicators of ecosystem health, gymnosperms provide major habitats for animals, and flowering plants provide all of our crops, vegetables, and fruits.
The teams with the most accurate models will be contacted, with the intention of using them on the un-named plant collections in the NYBG herbarium collection, and assessed by the NYBG plant specialists.
Background
The New York Botanical Garden (NYBG) herbarium contains more than 7.8 million plant and fungal specimens. Herbaria are a massive repository of plant diversity data. These collections not only represent a vast amount of plant diversity, but since herbarium collections include specimens dating back hundreds of years, they provide snapshots of plant diversity through time. The integrity of the plant is maintained in herbaria as a pressed, dried specimen; a specimen collected nearly two hundred years ago by Darwin looks much the same as one collected a month ago by an NYBG botanist. All specimens not only maintain their morphological features but also include collection dates and locations, and the name of the person who collected the specimen. This information, multiplied by millions of plant collections, provides the framework for understanding plant diversity on a massive scale and learning how it has changed over time.
About
This is an FGVC competition hosted as part of the FGVC7 workshop at CVPR 2020 and sponsored by NYBG.
Details of this competition are mirrored on the github page. Please post in the forum or open an issue if you have any questions or problems with the dataset.","Submissions are evaluated using the macro F1 score.
F1 is calculated as follows:
$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$
where:
$$
precision = \frac{TP}{TP + FP}
$$
$$
recall = \frac{TP}{TP + FN}
$$
In ""macro"" F1 a separate F1 score is calculated for each species value and then averaged.
Submission Format
For each image Id, you should predict the corresponding image label (""category_id"") in the Predicted column. The submission file should have the following format:
Id,Predicted
0,0
1,27
2,42
...","Data Overview
The training and test set contain images of herbarium specimens, from over 32,000 species of vascular plants. Each image contains exactly one specimen. The text and barcode labels on the specimen images have been blurred to remove category information in the image.
The data has been approximately split 80%/20% for training/test. Each category has at least 1 instance in both the training and test datasets. Note that the test set distribution is slightly different from the training set distribution. The training set contains species with hundreds of examples, but the test set has the number of examples per species capped at a maximum of 10.
Dataset Details
Each image has different image dimensions, with a maximum of 1000 pixels in the larger dimension. These have been resized from the original image resolution. All images are in JPEG format.
Dataset Format
This dataset uses the COCO dataset format with additional annotation fields. In addition to the species category labels, we also provide region and supercategory information.
The training set metadata (train/metadata.json) and test set metadata (test/metadata.json) are JSON files in the format below. Naturally, the test set metadata file omits the ""annotations"", ""categories"" and ""regions"" elements.",kaggle competitions download -c herbarium-2020-fgvc7,"['https://www.kaggle.com/code/seraphwedd18/herbarium-consolidating-the-details', 'https://www.kaggle.com/code/rsingh99/getting-started-with-herbarium-2020', 'https://www.kaggle.com/code/yasufuminakama/herbarium-2020-pytorch-resnet18-inference', 'https://www.kaggle.com/code/yasufuminakama/herbarium-2020-pytorch-resnet18-train', 'https://www.kaggle.com/code/khotijahs1/identify-plant-species-from-herbarium-specimens']"
205,"Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? If you are interested in estimating the uncertainty distribution of the realized values of the same series, be sure to check out its companion competition
How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy.
The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.
In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.
If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.
Acknowledgements
Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.","This competition uses a Weighted Root Mean Squared Scaled Error (RMSSE). Extensive details about the metric, scaling, and weighting can be found in the M5 Participants Guide.
Submission File
Each row contains an id that is a concatenation of an item_id and a store_id, which is either validation (corresponding to the Public leaderboard), or evaluation (corresponding to the Private leaderboard). You are predicting 28 forecast days (F1-F28) of items sold for each row. For the validation rows, this corresponds to d_1914 - d_1941, and for the evaluation rows, this corresponds to d_1942 - d_1969. (Note: a month before the competition close, the ground truth for the validation rows will be provided.)
The files must have a header and should look like the following:
id,F1,...F28
HOBBIES_1_001_CA_1_validation,0,...,2
HOBBIES_1_002_CA_1_validation,2,...,11
...
HOBBIES_1_001_CA_1_evaluation,3,...,7
HOBBIES_1_002_CA_1_evaluation,1,...,4","In the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide.
Files
calendar.csv - Contains information about the dates on which the products are sold.
sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]
sample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.
sell_prices.csv - Contains information about the price of the products sold per store and date.
sales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)",kaggle competitions download -c m5-forecasting-accuracy,"['https://www.kaggle.com/code/headsortails/back-to-predict-the-future-interactive-m5-eda', 'https://www.kaggle.com/code/robikscube/m5-forecasting-starter-data-exploration', 'https://www.kaggle.com/code/tarunpaparaju/m5-competition-eda-models', 'https://www.kaggle.com/code/kneroma/m5-first-public-notebook-under-0-50', 'https://www.kaggle.com/code/kyakovlev/m5-three-shades-of-dark-darker-magic']"
206,"Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the uncertainty distribution of the unit sales of various products sold in the USA by Walmart? This specific competition is the first of its kind, opening up new directions for both academic research and how uncertainty could be assessed and used in organizations. If you are interested in providing point (accuracy) forecasts for the same series, be sure to check out its companion competition.
How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy.
The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.
In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days and to make uncertainty estimates for these forecasts. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.
If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.
Acknowledgements
Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.","This competition uses a Weighted Scaled Pinball Loss (WSPL). Extensive details about the metric, scaling, and weighting can be found in the M5 Participants Guide.
Submission File
Similar to the point forecast competition, each row contains an id that is a concatenation of an item_id, a store_id, a quartile, and the prediction interval, which is either validation (corresponding to the Public leaderboard), or evaluation (corresponding to the Private leaderboard).
In addition, this competition has rows that have been aggregated at different levels. An X indicates the absence of a second aggregation level.
You are predicting 28 forecast days (F1-F28) of items sold for each row. For the validation rows, this corresponds to d_1914 - d_1941, and for the evaluation rows, this corresponds to d_1942 - d_1969. (Note: a month before the competition close, the ground truth for the validation rows will be provided.)
The files must have a header and should look like the following:
id,F1,...F28
Total_X_0.005_validation,53,...,201
HOBBIES_1_001_CA_1_0.005_validation,0,...,2
HOBBIES_1_002_CA_1_0.005_validation,2,...,11
...
HOBBIES_1_001_CA_1_0.995_evaluation,3,...,7
HOBBIES_1_002_CA_1_0.995_evaluation,1,...,4","In the challenge, you are predicting 9 quartiles of item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide.
Files
calendar.csv - Contains information about the dates on which the products are sold.
sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]
sample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.
sell_prices.csv - Contains information about the price of the products sold per store and date.
sales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)",kaggle competitions download -c m5-forecasting-uncertainty,"['https://www.kaggle.com/code/headsortails/back-to-predict-the-future-interactive-m5-eda', 'https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction', 'https://www.kaggle.com/code/kneroma/from-point-to-uncertainty-prediction', 'https://www.kaggle.com/code/ulrich07/quantile-regression-with-keras', 'https://www.kaggle.com/code/rohanrao/m5-the-weighing-scale']"
207,"Think you can use your data science skills to make big predictions at a submicroscopic level? Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.
When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data. The University of Liverpool’s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you’ll use ion channel data to better model automatic identification methods. If successful, you’ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments. Technology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems. Acknowledgements: This would not be possible without the help of the [Biotechnology and Biological Sciences Research Council (BBSRC)](https://bbsrc.ukri.org/).","Submissions are evaluated using the macro F1 score.
F1 is calculated as follows:
$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$
where:
$$
precision = \frac{TP}{TP + FP}
$$
$$
recall = \frac{TP}{TP + FN}
$$
In ""macro"" F1 a separate F1 score is calculated for each open_channels value and then averaged.
Submission File
For each time value in the test set, you must predict open_channels. The files must have a header and should look like the following:
time,open_channels
500.0000,0
500.0001,2
etc.","In this competition, you will be predicting the number of open_channels present, based on electrophysiological signal data.
IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.
You can find detailed information about the data from the paper Deep-Channel uses deep neural networks to detect single-molecule events from patch-clamp data.
Files
train.csv - the training set
test.csv - the test set; you will be predicting open_channels from the signal data in this file
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c liverpool-ion-switching,"['https://www.kaggle.com/code/cdeotte/one-feature-model-0-930', 'https://www.kaggle.com/code/tarunpaparaju/ion-switching-competition-signal-eda', 'https://www.kaggle.com/code/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr', 'https://www.kaggle.com/code/friedchips/clean-removal-of-data-drift', 'https://www.kaggle.com/code/cdeotte/rapids-knn-30-seconds-0-938']"
208,"Update: this competition has been cancelled on account of the COVID-19 pandemic.
As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset.
In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results.
As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!
This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here. If you want to extend your analysis then try out our Analytics Competition here","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of games played
\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2
\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins
\\( log() \\) is the natural (base e) logarithm
The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2020 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2 = 2,278 matchups.
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2015_1107_1112"" indicates team 1107 potentially played team 1112 in the year 2015. You must predict the probability that the team with the lower id beats the team with the higher id.
The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:
id,pred
2015_1107_1112,0.5
2015_1107_1116,0.5
2015_1107_1124,0.5
...","Data Description:
Each season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness®, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness® game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.
If you are unfamiliar with the format and intricacies of the NCAA® tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.
As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Remember as well that you are required to disclose your external sources of data prior to the start of the tournament",kaggle competitions download -c google-cloud-ncaa-march-madness-2020-division-1-mens-tournament,"['https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda', 'https://www.kaggle.com/code/headsortails/jump-shot-to-conclusions-march-madness-eda', 'https://www.kaggle.com/code/artgor/march-madness-2020-ncaam-eda-and-baseline', 'https://www.kaggle.com/code/vbmokin/mm-ncaam-no-leaks-lgb-xgb-logreg', 'https://www.kaggle.com/code/ratan123/march-madness-2020-ncaam-simple-lightgbm-on-kfold']"
209,"Update: this competition has been cancelled on account of the COVID-19 pandemic.
As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset.
In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results.
As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!
This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here. If you want to extend your analysis then try out our Analytics Competition here","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of games played
\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2
\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins
\\( log() \\) is the natural (base e) logarithm
The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2020 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict (64*63)/2  = 2,016 matchups. 
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2015_3106_3107"" indicates team 3106 played team 3107 in the year 2015. You must predict the probability that the team with the lower id beats the team with the higher id.
The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:
id,pred
2015_3106_3107,0.5
2015_3106_3110,0.5
2015_3106_3113,0.5
...","Each season there are thousands of NCAA® basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness® game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.
If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.
As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Remember as well that you are required to disclose your external sources of data prior to the start of the tournament.
We extend our gratitude to Kenneth Massey for providing much of the historical data.
Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.",kaggle competitions download -c google-cloud-ncaa-march-madness-2020-division-1-womens-tournament,"['https://www.kaggle.com/code/robikscube/2020-march-madness-data-first-look-eda', 'https://www.kaggle.com/code/jaseziv83/applying-pythagorean-expectation-to-major-sports', 'https://www.kaggle.com/code/anshumoudgil/basketball-2020-vectors-feature-engg-strategy', 'https://www.kaggle.com/code/lucabasa/are-men-s-and-women-s-tournaments-different', 'https://www.kaggle.com/code/parulpandey/decoding-march-madness']"
210,"Can a computer learn complex, abstract tasks from just a few examples?
Current machine learning techniques are data-hungry and brittle—they can only make sense of patterns they've seen before. Using current methods, an algorithm can gain new skills by exposure to large amounts of data, but cognitive abilities that could broadly generalize to many tasks remain elusive. This makes it very challenging to create systems that can handle the variability and unpredictability of the real world, such as domestic robots or self-driving cars.
However, alternative approaches, like inductive programming, offer the potential for more human-like abstraction and reasoning. The Abstraction and Reasoning Corpus (ARC) provides a benchmark to measure AI skill-acquisition on unknown tasks, with the constraint that only a handful of demonstrations are shown to learn a complex task. It provides a glimpse of a future where AI could quickly learn to solve new problems on its own. The Kaggle Abstraction and Reasoning Challenge invites you to try your hand at bringing this future into the present!
This competition is hosted by François Chollet, creator of the Keras neural networks library. Chollet’s paper on measuring intelligence provides the context and motivation behind the ARC benchmark.
In this competition, you’ll create an AI that can solve reasoning tasks it has never seen before. Each ARC task contains 3-5 pairs of train inputs and outputs, and a test input for which you need to predict the corresponding output with the pattern learned from the train examples.
If successful, you’ll help bring computers closer to human cognition and you'll open the door to completely new AI applications!","The competition uses top-3 error rate for the evaluation metric. For each task in the test set, you can predict up to 3 outputs for each test input grid. Each task output has one ground truth. For a given task output, if the ground truth is contained in any of the 3 predicted outputs, then the error for that task is 0, otherwise it is 1. The final score is the error averaged across all tasks.
Mathematically, for each task \(i\), your algorithm can make up to 3 predictions \( o_{ij} \), where \(1 \le j \le 3\). The error for task \(i\) with ground truth \( g_i\) is:
$$
e_i = \min_j d(o_{ij}, g_i)
$$
where \( d(x, y) \text{ is } 0 \text{ if } x=y, \text{ otherwise } 1 \). The overall error score is the average over all \(N\) task outputs:
$$
\text{score } = \frac{1}{N} \sum_i e_i
$$
Format of output predictions
The training, evaluation, and as well as test input data all in the same JSON format. The inputs and outputs are stored in 2d python lists. Your output predictions, though, must be flattened into strings, with list rows delimited with an |.
For example, the output [[1,2], [3,4]] should be reformatted to |12|34| as a prediction. You can have up to 3 output predictions per task, and they should be space delimited. See the sample_solution.csv file as an example.
The following python code converts a 2d list pred into the correct format:
def flattener(pred):
    str_pred = str([row for row in pred])
    str_pred = str_pred.replace(', ', '')
    str_pred = str_pred.replace('[[', '|')
    str_pred = str_pred.replace('][', '|')
    str_pred = str_pred.replace(']]', '|')
    return str_pred
Submission File
For each task output output_id in the test set, you can make up to 3 predictions. The output_id is the id of the task, with an indicator of which output you are predicting for that task. Most tasks only have a single output (i.e., 0), although some tasks have two outputs that must be predicted (i.e., 0, 1). The file should contain a header and have the following format:
output_id,output
00576224_0,|32|78| |32|78| |00|00|
...
12997ef3_0,|00000000000|01100000000|11000000000|...
12997ef3_1,|00000000000|01100000000|11000000000|...
etc.","The objective of this competition is to create an algorithm that is capable of solving abstract reasoning tasks. The format is very different than previous competition, so please read this information carefully, and refer to supplementary documentation as needed.
When looking at a task, a ""test-taker"" has access to inputs and outputs of the demonstration pairs (train pairs), plus the input(s) of the test pair(s). The goal is to construct the output grid(s) corresponding to the test input grid(s), using 3 trials for each test input. ""Constructing the output grid"" involves picking the height and width of the output grid, then filling each cell in the grid with a symbol (integer between 0 and 9, which are visualized as colors). Only exact solutions (all cells match the expected answer) can be said to be correct.
A additional information, as well as an interactive app to explore the objective of this competition is found at the Abstraction and Reasoning Corpus github page. It is highly recommended that you download and explore the interactive app, as the best way to understand the objective of the competition.
Task file format
The task files are stored in two directories:
training: contains the task files for training (400 tasks). Use these to prototype your algorithm or to train your algorithm to acquire ARC-relevant cognitive priors.
evaluation: contains the task files for evaluation (400 tasks).",kaggle competitions download -c abstraction-and-reasoning-challenge,"['https://www.kaggle.com/code/arsenynerinovsky/cellular-automata-as-a-language-for-reasoning', 'https://www.kaggle.com/code/boliu0/visualizing-all-task-pairs-with-gridlines', 'https://www.kaggle.com/code/tarunpaparaju/arc-competition-eda-pytorch-cnn', 'https://www.kaggle.com/code/zenol42/dsl-and-genetic-algorithm-applied-to-arc', 'https://www.kaggle.com/code/nagiss/manual-coding-for-the-first-10-tasks']"
211,"PROJECT OVERVIEW
Develop a methodology to calculate an average historical emissions factor of electricity generated for a sub-national region, using remote sensing data and techniques.
The Environmental Insights Explorer team at Google is keen to gather insights on ways to improve calculations of global emissions factors for sub-national regions. The ultimate goal of this challenge is to test if calculations of emissions factors using remote sensing techniques are possible and on par with calculations of emissions factors from current methodologies.
PROBLEM STATEMENT
Current emissions factors methodologies are based on time-consuming data collection and may include errors derived from a lack of access to granular datasets, inability to refresh data on a frequent basis, overly general modeling assumptions, and inaccurate reporting of emissions sources like fuel consumption. This begs the question: What if there was a different way to calculate or measure emissions factors? We’re challenging the Kaggle community to see if it’s possible to use remote sensing techniques to better model emissions factors. You will develop a methodology to calculate an average historical emissions factor for electricity generation in a sub-national region.
We’ve provided an initial list of datasets covering the geographic boundary of Puerto Rico to serve as the foundation for this analysis. As an island, there are fewer confounding factors from nearby areas. Puerto Rico also offers a unique fuel mix and distinctive energy system layout that should make it easier to isolate pollution attributable to power generation in the remote sensing data.
Participants will be tasked with developing a methodology to calculate an average annual historical emissions factor for the sub-national region. Participants will also be asked to provide an explanation of the conditions that would result in a higher/lower emissions factor, as well as a recommendation for how the methodology could be applied to calculate the emissions factor of electricity for another geospatial area using similar techniques. Bonus points will be awarded for smaller time slices of the average historical emissions factors, such as one per month for the 12-month period, and additional bonus points will be awarded for participants that develop methodologies for calculating marginal emissions factors for the sub-national region.
HOW TO PARTICIPATE
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will only review the most recent entry.
To be valid, a submission must be contained in one or more notebook, and made public on or before the submission deadline. Participants are free to use any datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Earth Engine or Kaggle for the submission to be valid.","HOW TO PARTICIPATE
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will only review the most recent entry.
To be valid, a submission must be contained in one or more notebook, and made public on or before the submission deadline. Participants are free to use any datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Earth Engine or Kaggle for the submission to be valid.
SUBMISSIONS WILL BE EVALUATED ON THE FOLLOWING
Documentation
Is the code documented in a way that is easily reproducible (i.e. thorough comments, organized notebook, clear scripts/code)?
Does the notebook narrative clearly state all assumptions that are factored into the value and the potential impact of fluctuations? (i.e. Which plants / fuel types did the author use for their analysis, and why?)
Does the notebook contain data visualizations (e.g. time graphs, etc.) that help convey the author’s findings and/or recommendations?
Did the author upload and properly cite all files for any supporting datasets that were used for their analysis?
Recommendation
Did the author write a compelling and coherent narrative explaining their rationale for the scalability and accuracy of their model and recommendation?
Does the recommendation include an explanation of what data/assumptions could be substituted to produce the value for another geospatial area or location?
Is there documentation about the pros and cons of the model, and the geographic nuances that may have impacted the emissions factor?
Does the explanation convey why/how this model improves current emissions factors calculations?
Does the recommendation indicate other datasets/factors/assumptions (and why) that could be useful to include to make future emissions factor calculation methodologies more robust?
Accuracy
Does the model produce a value for the an annual average historical grid-level electricity emissions factor (based on rolling 12-months of data from July 2018 - July 2019) for the sub-national region?
Bonus points for smaller time slices of the average historical emissions factors, such as one per month for the 12-month period.
Bonus points for participants that develop a methodology to calculate a marginal emissions factor for the sub-national region using the provided datasets.","Context:
Current emissions factors methodologies are based on time-consuming data collection and may include errors derived from a lack of access to granular datasets, inability to refresh data on a frequent basis, overly general modeling assumptions, and inaccurate reporting of emissions sources like fuel consumption. We’re challenging the Kaggle community to see if it’s possible to use remote sensing techniques to develop a methodology to calculate an average historical emissions factor of electricity generated for a sub-national region.
Content:
DATASET SELECTION STARTER PACK
Global Power Plant database by WRI
Sentinel 5P OFFL NO2 by EU/ESA/Copernicus
Global Forecast System 384-Hour Predicted Atmosphere Data by NOAA/NCEP/EMC
Global Land Data Assimilation System by NASA
Participants may also consider using other public datasets related to trade commodities for fuel types, total fuel consumed, and/or data from the US Energy Information Agency (EIA).
Dataset Licenses:",kaggle competitions download -c ds4g-environmental-insights-explorer,"['https://www.kaggle.com/code/parulpandey/understanding-the-data', 'https://www.kaggle.com/code/paultimothymooney/how-to-get-started-with-the-earth-engine-data', 'https://www.kaggle.com/code/caesarlupum/ds4g-go-to-the-green-future', 'https://www.kaggle.com/code/katemelianova/ds4g-spatial-panel-data-modeling', 'https://www.kaggle.com/code/caesarlupum/green-future-anomaly-analysis-time-series']"
212,"Tensor Processing Units (TPUs) are Now Available on Kaggle
Tensor Processing Unit (TPU) quotas are now available on Kaggle, at no cost to you!
TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try.
The latest Tensorflow release (TF 2.1) was focused on TPUs and they’re now supported both through the Keras high-level API and at a lower level, in models using a custom training loop.
We can’t wait to see how your solutions are accelerated by TPUs!
The Challenge
It’s difficult to fathom just how vast and diverse our natural world is.
There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish – and astonishingly, over 400,000 different types of flowers.
In this competition, you’re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we’re sticking to just over 100 types).
To get started with TPUs:
Read the TPU documentation one-pager
Then jump right into the Getting Started Notebook for this competition
Quick note: a TPU is a network-connected accelerator and requires a couple extra lines in your code. Flipping the TPU switch in your notebook will not, by itself, accelerate your code.
Have Questions?
Martin Görner, Google Developer Advocate and author of Tensorflow without a PhD will be actively engaged in the competition forum. If you have a question or need help troubleshooting, that’s the best place to find help.","Submissions are evaluated on macro F1 score.
F1 is calculated as follows:
$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$
where:
$$
precision = \frac{TP}{TP + FP}
$$
$$
recall = \frac{TP}{TP + FN}
$$
In ""macro"" F1 a separate F1 score is calculated for each class / label and then averaged.
Submission File
For each id in the test set, you must predict a type of flower (or label). The file should contain a header and have the following format:
id,label
a762df180,0
24c5cf439,0
7581e896d,0
eb4b03b29,0
etc.","In this competition we're classifying 104 types of flowers based on their images drawn from five different public datasets. Some classes are very narrow, containing only a particular sub-type of flower (e.g. pink primroses) while other classes contain many sub-types (e.g. wild roses).
Files
This competition is different in that images are provided in TFRecord format. The TFRecord format is a container format frequently used in Tensorflow to group and shard data data files for optimal training performace.
Each file contains the id, label (the class of the sample, for training data) and img (the actual pixels in array form) information for many images.
Please see our getting started notebook for notes on how to load and use them! Additional information is available in the TPU documentation.
train/*.tfrec - training samples, including labels.
val/*.tfrec - pre-split training samples w/ labels intended to help with checking your model's performance on TPU. The split was stratified across labels.
test/*.tfrec - samples without labels - you'll be predicting what classes of flowers these fall into.
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c flower-classification-with-tpus,"['https://www.kaggle.com/code/cdeotte/rotation-augmentation-gpu-tpu-0-96', 'https://www.kaggle.com/code/mgornergoogle/getting-started-with-100-flowers-on-tpu', 'https://www.kaggle.com/code/cdeotte/cutmix-and-mixup-on-gpu-tpu', 'https://www.kaggle.com/code/xhlulu/flowers-tpu-concise-efficientnet-b7', 'https://www.kaggle.com/code/saife245/cutmix-vs-mixup-vs-gridmask-vs-cutout']"
213,"We’re excited to announce a beta-version of a brand-new type of ML competition called Simulations. In Simulation Competitions, you’ll compete against a set of rules, rather than against an evaluation metric. To enter, accept the rules and create a python submission file that can “play” against a computer, or another user.
The Challenge
In this game, your objective is to get a certain number of your checkers in a row horizontally, vertically, or diagonally on the game board before your opponent. When it's your turn, you “drop” one of your checkers into one of the columns at the top of the board. Then, let your opponent take their turn. This means each move may be trying to either win for you, or trying to stop your opponent from winning. The default number is four-in-a-row, but we’ll have other options to come soon.
Background History
For the past 10 years, our competitions have been mostly focused on supervised machine learning. The field has grown, and we want to continue to provide the data science community cutting-edge opportunities to challenge themselves and grow their skills.
So, what’s next? Reinforcement learning is clearly a crucial piece in the next wave of data science learning. We hope that Simulation Competitions will provide the opportunity for Kagglers to practice and hone this burgeoning skill.
How is this Competition Different?
Instead of submitting a CSV file, or a Kaggle Notebook, you will submit a Python .py file (more submission options are in development). You’ll also notice that the leaderboard is not based on how accurate your model is but rather how well you’ve performed against other users. See Evaluation for more details.
We’d Love Your Feedback
This competition is a low-stakes, trial-run introduction. We’re considering this a beta launch – there are complicated new mechanics in play and we’re still working on refining the process. We’d love your help testing the experience and want to hear your feedback.
Please note that we may make changes throughout the competition that could include things like resetting the leaderboard, invalidating episodes, making changes to the interface, or changing the environment configuration (e.g. modifying the number of columns, rows, or tokens in a row required to win, etc).","Each Submission has an estimated Skill Rating which is modeled by a Gaussian \(\mathcal N (\mu, \sigma^2)\) where \(\mu\) is the estimated skill and \(\sigma\) represents our uncertainty of that estimate.
When you upload a Submission, we first play a Validation Episode where that Submission plays against itself to make sure it works properly. If the Episode fails, the Submission is marked as Error. Otherwise, we initialize the Submission with \(\mu_0 = 600\) and it joins the pool of All Submissions for ongoing evaluation.
We repeatedly run Episodes from the pool of All Submissions, and try to pick Submissions with similar ratings for fair matches. We aim to run ~8 Episodes a day per Submission, with an additional slight rate increase for newer Episodes to give you feedback faster.
After an Episode finishes, we'll update the Rating estimate for both Submissions. If one Submission won, we'll increase its \(\mu\) and decrease its opponent's \(\mu\) -- if the result was a draw, then we'll move the two \(\mu\) values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous \(\mu\) values, and also relative to each Submission's uncertainty \(\sigma\). We also reduce the \(\sigma\) terms relative to the amount of information gained by the result.
So all valid Submissions will continually play more matches and have dynamically changing scores as the pool increases. The Leaderboard will show the \(\mu\) value of each Team's best Submission.",,,"['https://www.kaggle.com/code/ajeffries/connectx-getting-started', 'https://www.kaggle.com/code/alexisbcook/play-the-game', 'https://www.kaggle.com/code/phunghieu/connectx-with-q-learning', 'https://www.kaggle.com/code/phunghieu/connectx-with-deep-q-learning', 'https://www.kaggle.com/code/alexisbcook/create-a-connectx-agent']"
214,"Can you find more cat in your dat?
We loved the participation and engagement with the first Cat in the Dat competition.
Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:
binary features
low- and high-cardinality nominal features
low- and high-cardinality ordinal features
(potentially) cyclical features
This follow-up competition offers an even more challenging dataset so that you can continue to build your skills with the common machine learning task of encoding categorical variables. This challenge adds the additional complexity of feature interactions, as well as missing data.
This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.
If you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.
Have Fun!","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
id,target
600000,0.5
600001,0.5
600002,0.5
...","In this competition, you will be predicting the probability [0, 1] of a binary target column.
The data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.
Since the purpose of this competition is to explore various encoding strategies. Unlike the first Categorical Feature Encoding Challenge, the data for this challenge has missing values and feature interactions.
Files
train.csv - the training set
test.csv - the test set; you must make predictions against this data
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c cat-in-the-dat-ii,"['https://www.kaggle.com/code/abhishek/same-old-entity-embeddings', 'https://www.kaggle.com/code/warkingleo2000/first-step-on-kaggle', 'https://www.kaggle.com/code/faressayah/data-science-best-practices-with-pandas-part-1', 'https://www.kaggle.com/code/vikassingh1996/don-t-underestimate-the-power-of-a-logistic-reg', 'https://www.kaggle.com/code/caesarlupum/2020-20-lines-target-encoding']"
215,"Santa was thrilled with the Kaggle community for minimizing his workshop costs! He had heard rumors that Kagglers were adept at cracking holiday challenges, but, wow, even Santa was surprised at this one.
Unfortunately, the North Pole accountants were less pleased. It turns out, the accountants didn't like being one-upped by machine learning experts on the internet.
To complicate matters, they've decided to allow an additional 1,000 families attend the workshop. And they've also ""fine tuned"" their accounting formula to try and trip up those fancy solvers some people have at their disposal.
Of course, we know that nothing trips up the Kaggle community! (Well, except for maybe over-fitting. But fortunately, that doesn't apply here!)
So this is a bonus Santa competition for those who want an additional challenge and the opportunity to continue to improve their optimization skills. Since Santa used up all his budget on accounting fees, this is strictly a Playground competition, with the chance to win some coveted Kaggle Swag.
Have fun, and Happy Holidays from the Kaggle Team!
Attribution
Banner/Listing Photo by Helloquence on Unsplash","Your submission is scored according to the penalty cost to Santa for suboptimal scheduling. The constraints and penalties are as follows:
The total number of people attending the workshop each day must be between 125 - 300; if even one day is outside these occupancy constraints, the submission will error and will not be scored.
Santa provides consolation gifts (of varying value) to families according to their assigned day relative to their preferences. These sum up per family, and the total represents the \(preference : cost\).
choice_0: no consolation gifts
choice_1: one $50 gift card to Santa's Gift Shop
choice_2: one $50 gift card, and 25% off Santa's Buffet (value $9) for each family member
choice_3: one $100 gift card, and 25% off Santa's Buffet (value $9) for each family member
choice_4: one $200 gift card, and 25% off Santa's Buffet (value $9) for each family member
choice_5: one $200 gift card, and 50% off Santa's Buffet (value $18) for each family member
choice_6: one $300 gift card, and 50% off Santa's Buffet (value $18) for each family member
choice_7: one $300 gift card, and free Santa's Buffet (value $36) for each family member
choice_8: one $400 gift card, and free Santa's Buffet (value $36) for each family member
choice_9: one $500 gift card, and free Santa's Buffet (value $36) for each family member, and 50% off North Pole Helicopter Ride tickets (value $199) for each family member
otherwise: one $500 gift card, and free Santa's Buffet (value $36) for each family member, and free North Pole Helicopter Ride tickets (value $398) for each family member
Santa's accountants have also developed an empirical equation for cost to Santa that arise from many different effects such as reduced shopping in the Gift Shop when it gets too crowded, extra cleaning costs, a very complicated North Pole tax code, etc. This cost in in addition to the consolation gifts Santa provides above, and is defined as:
$$
accounting: penalty = \sum_{d=100}^{1} \sum_{j=1}^{5} \frac{(N_{d} - 125)}{400} \frac{{N_d}^{( \frac{1}{2} + \frac{\lvert N_d - N_{d+j} \rvert }{50} )}}{j^2}
$$
where \(N_d\) is the occupancy of the current day, and \(N_{d+j}\) is the occupancy of the \(i^{th}\) previous day (since we're counting backwards from Christmas!). All \(N_{d+j > 100}\) are equal to \(N_{100} \).
To be clear on the above summation, it starts on the date 100 days before Christmas and ends on Christmas Eve.
And finally:
$$
score = preference : cost + accounting: penalty
$$","Your task is to schedule the families to Santa's Workshop in a way that minimizes the penalty cost to Santa (as described on the Evaluation page).
Each family has listed their top 10 preferences for the dates they'd like to attend Santa's workshop tour. Dates are integer values representing the days before Christmas, e.g., the value 1 represents Dec 24, the value 2 represents Dec 23, etc. Each family also has a number of people attending, n_people.
Every family must be scheduled for one and only one assigned_day.
File descriptions
family_data.csv - the workshop date preferences for each family, and family size
sample_submission.csv - a sample submission in the correct format",kaggle competitions download -c santa-2019-revenge-of-the-accountants,"['https://www.kaggle.com/code/golubev/baseline', 'https://www.kaggle.com/code/golubev/mip-optimization-preference-cost-santa2019revenge', 'https://www.kaggle.com/code/vipito/fork-of-santa-ip', 'https://www.kaggle.com/code/shrutimechlearn/santa-returns-workshop-explorers-wave-1-vs-wave-2', 'https://www.kaggle.com/code/seshurajup/eda-for-santa-2019-revenge-of-the-accountants']"
216,"Welcome to one of our ""Getting Started"" competitions 👋
This particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don’t have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called Kaggle Notebooks.
If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle
Competition Description
Twitter has become an important communication channel in times of emergency.
The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).
But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:




The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.
In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.
💡Getting Started Notebook
To get started quickly, feel free to take advantage of this starter notebook.
Acknowledgments
This dataset was created by the company figure-eight and originally shared on their ‘Data For Everyone’ website here.
Tweet source: https://twitter.com/AnyOtherAnnaK/status/629195955506708480","Submissions are evaluated using F1 between the predicted and expected answers.
F1 is calculated as follows:
$$
F_1 = 2 * \frac{precision * recall}{precision + recall}
$$
where:
$$
precision = \frac{TP}{TP + FP}
$$
$$
recall = \frac{TP}{TP + FN}
$$
and:
True Positive [TP] = your prediction is 1, and the ground truth is also 1 - you predicted a positive and that's true!
False Positive [FP] = your prediction is 1, and the ground truth is 0 - you predicted a positive, and that's false.
False Negative [FN] = your prediction is 0, and the ground truth is 1 - you predicted a negative, and that's false.
Submission File
For each ID in the test set, you must predict 1 if the tweet is describing a real disaster, and 0 otherwise. The file should contain a header and have the following format:
id,target
0,0
2,0
3,1
9,0
11,0","What files do I need?
You'll need train.csv, test.csv and sample_submission.csv.
What should I expect the data format to be?
Each sample in the train and test set has the following information:
The text of a tweet
A keyword from that tweet (although this may be blank!)
The location the tweet was sent from (may also be blank)
What am I predicting?
You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.
Files
train.csv - the training set
test.csv - the test set
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c nlp-getting-started,"['https://www.kaggle.com/code/alexia/kerasnlp-starter-notebook-disaster-tweets', 'https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert', 'https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove', 'https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk', 'https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial']"
217,"Challenge and dataset summary paper available at https://arxiv.org/abs/2010.00170
Bengali is the 5th most spoken language in the world with hundreds of million of speakers. It’s the official language of Bangladesh and the second most spoken language in India. Considering its reach, there’s significant business and educational interest in developing AI that can optically recognize images of the language handwritten. This challenge hopes to improve on approaches to Bengali recognition.
Optical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to English’s 250 graphemic units).
Bangladesh-based non-profit Bengali.AI is focused on helping to solve this problem. They build and release crowdsourced, metadata-rich datasets and open source them through research competitions. Through this work, Bengali.AI hopes to democratize and accelerate research in Bengali language technologies and to promote machine learning education.
For this competition, you’re given the image of a handwritten Bengali grapheme and are challenged to separately classify three constituent elements in the image: grapheme root, vowel diacritics, and consonant diacritics.
By participating in the competition, you’ll hopefully accelerate Bengali handwritten optical character recognition research and help enable the digitalization of educational resources. Moreover, the methods introduced in the competition will also empower cousin languages in the Indian subcontinent.
Acknowledgements:
Apurba: Apurba is the exclusive sponsor of Bengali.AI for this competition. Apurba Technologies Inc. is founded by a group of technology veterans who have been working at the cutting edge of software development in Silicon Valley for many years. Apart from its many ventures, Apurba is a pioneer in Bengali NLP research today and is accelerating AI research in Bangladesh through its contributions.
Intelligent Machines Limited: Intelligent Machines Limited is the technical partner of Bengali.AI for this competition and is providing compute support to Bangladeshi students. IML is an Artificial Intelligence and Advanced Analytics startup offering customized solutions to businesses in Bangladesh. IML believes in the strength of Bangladeshi talented resources and in the possibility of a far greater and developed Bangladesh in the coming days.
If you use this dataset in your research, please cite this paper
@inproceedings{alam2021large,
title={A Large Multi-target Dataset of Common Bengali Handwritten Graphemes},
author={Alam, Samiul and Reasat, Tahsin and Sushmit, Asif Shahriyar and Siddique, Sadi Mohammad and Rahman, Fuad and Hasan, Mahady and Humayun, Ahmed Imtiaz},
booktitle={International Conference on Document Analysis and Recognition},
pages={383--398},
year={2021},
organization={Springer}
}","Submissions are evaluated using a hierarchical macro-averaged recall. First, a standard macro-averaged recall is calculated for each component (grapheme root, vowel diacritic, or consonant diacritic). The final score is the weighted average of those three scores, with the grapheme root given double weight. You can replicate the metric with the following python snippet:
import numpy as np
import sklearn.metrics

scores = []
for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:
    y_true_subset = solution[solution[component] == component]['target'].values
    y_pred_subset = submission[submission[component] == component]['target'].values
    scores.append(sklearn.metrics.recall_score(
        y_true_subset, y_pred_subset, average='macro'))
final_score = np.average(scores, weights=[2,1,1])
Submission File
For each image ID in the test set, you must classify the grapheme root, vowel diacritic, and consonant diacritic for all images. The prediction for each component goes on a separate row. The submission file should contain a header and have the following format:
row_id,target
Test_0_grapheme_root,3
Test_1_grapheme_root,2
Test_2_grapheme_root,1
...","Challenge and dataset summary available at https://arxiv.org/abs/2010.00170
Published @ ICDAR21
Citation:
@inproceedings{alam2021large, title={A Large Multi-target Dataset of Common Bengali Handwritten Graphemes}, author={Alam, Samiul and Reasat, Tahsin and Sushmit, Asif Shahriyar and Siddique, Sadi Mohammad and Rahman, Fuad and Hasan, Mahady and Humayun, Ahmed Imtiaz}, booktitle={International Conference on Document Analysis and Recognition}, pages={383--398}, year={2021}, organization={Springer} }
This dataset contains images of individual hand-written Bengali characters.
Bengali characters (graphemes) are written by combining three components: a grapheme_root, vowel_diacritic, and consonant_diacritic. Your challenge is to classify the components of the grapheme in each image. There are roughly 10,000 possible graphemes, of which roughly 1,000 are represented in the training set. The test set includes some graphemes that do not exist in train but has no new grapheme components. It takes a lot of volunteers filling out sheets like this to generate a useful amount of real data; focusing the problem on the grapheme components rather than on recognizing whole graphemes should make it possible to assemble a Bengali OCR system without handwriting samples for all 10,000 graphemes.
Files",kaggle competitions download -c bengaliai-cv19,"['https://www.kaggle.com/code/kaushal2896/bengali-graphemes-starter-eda-multi-output-cnn', 'https://www.kaggle.com/code/iafoss/image-preprocessing-128x128', 'https://www.kaggle.com/code/corochann/bengali-seresnext-training-with-pytorch', 'https://www.kaggle.com/code/corochann/bengali-seresnext-prediction-with-pytorch', 'https://www.kaggle.com/code/cdeotte/how-to-compete-with-gpus-workshop']"
218,"**This competition is closed for submissions. Participants' selected code submissions were re-run by the host on a privately-held test set and the [private leaderboard results have been finalized](https://www.kaggle.com/c/deepfake-detection-challenge/discussion/157925). Late submissions will not be opened, due to an inability to replicate the unique design of this competition.** Deepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online. These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rights—especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion. Identifying manipulated media is a technically demanding and rapidly evolving challenge that requires collaborations across the entire tech industry and beyond.
AWS, Facebook, Microsoft, the Partnership on AI’s Media Integrity Steering Committee, and academics have come together to build the Deepfake Detection Challenge (DFDC). The goal of the challenge is to spur researchers around the world to build innovative new technologies that can help detect deepfakes and manipulated media. Challenge participants must submit their code into a black box environment for testing. Participants will have the option to make their submission open or closed when accepting the prize. Open proposals will be eligible for challenge prizes as long as they abide by the open source licensing terms. Closed proposals will be proprietary and not be eligible to accept the prizes. Regardless of which track is chosen, all submissions will be evaluated in the same way. Results will be shown on the leaderboard. The PAI Steering Committee has emphasized the need to ensure that all technical efforts incorporate attention to how the resulting code and products based on it can be made as accessible and useful as possible to key frontline defenders of information quality such as journalists and civic leaders around the world. The DFDC results will be a contribution to this effort and building a robust response to the emergent threat deepfakes pose globally.","Submissions are scored on log loss:
$$ \textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right], $$
where
n is the number of videos being predicted
\( \hat{y}_i \) is the predicted probability of the video being FAKE
\( y_i \) is 1 if the video is FAKE, 0 if REAL
\( log() \) is the natural (base e) logarithm
A smaller log loss is better. The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
For each filename in the test set, you must predict a probability for the label variable. The file should contain a header and have the following format:
filename,label
10000.mp4,0
10001.mp4,0.5
10002.mp4,1
etc.","This competition is closed for submissions. Participants' selected code submissions were re-run by the host on a privately-held test set and the private leaderboard results have been finalized. Late submissions will not be opened, due to an inability to replicate the unique design of this competition.
Training Set
This code competition's training set is not available directly on Kaggle, as its size is prohibitively large to train in Kaggle. Instead, it's strongly recommended that you train offline and load the externally trained model as an external dataset into Kaggle Notebooks to perform inference on the Test Set. Review Getting Started for more detailed information.
The full training set is just over 470 GB. We've made it available as one giant file, as well as 50 smaller files, each ~10 GB in size. You must accept the competition's rules to gain access to any of the links below.

Full dataset:
all.zip (471.84 GB)
Dataset split into smaller chunks:
00.zip (11.52 GB) 01.zip (9.41 GB) 02.zip (9.46 GB) 03.zip (9.45 GB) 04.zip (9.45 GB)",kaggle competitions download -c deepfake-detection-challenge,"['https://www.kaggle.com/code/gpreda/deepfake-starter-kit', 'https://www.kaggle.com/code/robikscube/kaggle-deepfake-detection-introduction', 'https://www.kaggle.com/code/humananalog/inference-demo', 'https://www.kaggle.com/code/timesler/facial-recognition-model-in-pytorch', 'https://www.kaggle.com/code/timesler/guide-to-mtcnn-in-facenet-pytorch']"
219,"Hammers ring, are you listenin’
In the shop, toys are glistenin’
Should they see the sights?
There might be a fight…
Walkin’ ‘round the Workshop Wonderland
Families said, they want to see it
Santa said, he’d guarantee it
They pick a date
But they may have to wait
Walkin’ ‘round the Workshop Wonderland
We told Santa that he was a madman
He just wants to make sure they all smile
He’ll say “Are you flexible?“, They’ll say “Yeah man,
But can you help us make it worth our while?”
“Give them food, or sweater
the more they wait, the gifts get better”
Please help us rank
Or we’ll break the bank!
Walkin’ ’round the Workshop Wonderland
Santa has exciting news! For 100 days before Christmas, he opened up tours to his workshop. Because demand was so strong, and because Santa wanted to make things as fair as possible, he let each of the 5,000 families that will visit the workshop choose a list of dates they'd like to attend the workshop.
Now that all the families have sent Santa their preferences, he's realized it's impossible for everyone to get their top picks, so he's decided to provide extra perks for families that don't get their preferences. In addition, Santa's accounting department has told him that, depending on how families are scheduled, there may be some unexpected and hefty costs incurred.
Santa needs the help of the Kaggle community to optimize which day each family is assigned to attend the workshop in order to minimize any extra expenses that would cut into next years toy budget! Can you help Santa out?
Attribution
Banner/Listing Photo by Nathan Lemon on Unsplash
Description Photo by Markus Spiske on Unsplash","Your submission is scored according to the penalty cost to Santa for suboptimal scheduling. The constraints and penalties are as follows:
The total number of people attending the workshop each day must be between 125 - 300; if even one day is outside these occupancy constraints, the submission will error and will not be scored.
Santa provides consolation gifts (of varying value) to families according to their assigned day relative to their preferences. These sum up per family, and the total represents the \(preference : cost\).
choice_0: no consolation gifts
choice_1: one $50 gift card to Santa's Gift Shop
choice_2: one $50 gift card, and 25% off Santa's Buffet (value $9) for each family member
choice_3: one $100 gift card, and 25% off Santa's Buffet (value $9) for each family member
choice_4: one $200 gift card, and 25% off Santa's Buffet (value $9) for each family member
choice_5: one $200 gift card, and 50% off Santa's Buffet (value $18) for each family member
choice_6: one $300 gift card, and 50% off Santa's Buffet (value $18) for each family member
choice_7: one $300 gift card, and free Santa's Buffet (value $36) for each family member
choice_8: one $400 gift card, and free Santa's Buffet (value $36) for each family member
choice_9: one $500 gift card, and free Santa's Buffet (value $36) for each family member, and 50% off North Pole Helicopter Ride tickets (value $199) for each family member
otherwise: one $500 gift card, and free Santa's Buffet (value $36) for each family member, and free North Pole Helicopter Ride tickets (value $398) for each family member
Santa's accountants have also developed an empirical equation for cost to Santa that arise from many different effects such as reduced shopping in the Gift Shop when it gets too crowded, extra cleaning costs, a very complicated North Pole tax code, etc. This cost in in addition to the consolation gifts Santa provides above, and is defined as:
$$
accounting: penalty = \sum_{d=100}^{1} \frac{(N_{d} - 125)}{400} {N_d}^{( \frac{1}{2} + \frac{\lvert N_d - N_{d+1} \rvert }{50} )}
$$
where \(N_d\) is the occupancy of the current day, and \(N_{d+1}\) is the occupancy of the previous day (since we're counting backwards from Christmas!). For the initial condition of \(d=100\), \(N_{101} = N_{100} \).
To be clear on the above summation, it starts on the date 100 days before Christmas and ends on Christmas Eve.
And finally:
$$
score = preference : cost + accounting: penalty
$$
This may seem complicated, but this nifty-difty starter notebook should get you started fast!","Your task is to schedule the families to Santa's Workshop in a way that minimizes the penalty cost to Santa (as described on the Evaluation page).
Each family has listed their top 10 preferences for the dates they'd like to attend Santa's workshop tour. Dates are integer values representing the days before Christmas, e.g., the value 1 represents Dec 24, the value 2 represents Dec 23, etc. Each family also has a number of people attending, n_people.
Every family must be scheduled for one and only one assigned_day.
File descriptions
family_data.csv - the workshop date preferences for each family, and family size
sample_submission.csv - a sample submission in the correct format",kaggle competitions download -c santa-workshop-tour-2019,
220,"Welcome
In this challenge, you're tasked to investigate the relationship between the playing surface and the injury and performance of National Football League (NFL) athletes and to examine factors that may contribute to lower extremity injuries.
You'll also notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, the NFL’s annual Super Bowl competition designed to spur innovation in player health, safety and performance.
The Challenge
In the NFL, 12 stadiums have fields with synthetic turf. Recent investigations of lower limb injuries among football athletes have indicated significantly higher injury rates on synthetic turf compared with natural turf (Mack et al., 2018; Loughran et al., 2019). In conjunction with the epidemiologic investigations, biomechanical studies of football cleat-surface interactions have shown that synthetic turf surfaces do not release cleats as readily as natural turf and may contribute to the incidence of non-contact lower limb injuries (Kent et al., 2015). Given these differences in cleat-turf interactions, it has yet to be determined whether player movement patterns and other measures of player performance differ across playing surfaces and how these may contribute to the incidence of lower limb injury.
Now, the NFL is challenging Kagglers to help them examine the effects that playing on synthetic turf versus natural turf can have on player movements and the factors that may contribute to lower extremity injuries. NFL player tracking, also known as Next Gen Stats, is the capture of real time location data, speed and acceleration for every player, every play on every inch of the field. As part of this challenge, the NFL has provided full player tracking of on-field position for 250 players over two regular season schedules. One hundred of the athletes in the study data set sustained one or more injuries during the study period that were identified as a non-contact injury of a type that may have turf interaction as a contributing factor to injury. The remaining 150 athletes serve as a representative sample of the larger NFL population that did not sustain a non-contact lower-limb injury during the study period. Details of the surface type and environmental parameters that may influence performance and outcome are also provided.
Your challenge is to characterize any differences in player movement between the playing surfaces and identify specific scenarios (e.g., field surface, weather, position, play type, etc.) that interact with player movement to present an elevated risk of injury. More details on the entry criteria are available in Evaluation Tab.
About The NFL
The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country.
The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played.
As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries.
For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com
Evaluation","Your challenge is to characterize any differences in player movement between the playing surfaces and to identify specific variables (e.g., field surface, weather, position, play type, etc.) that may influence player movement and the risk of injury. A valid submission will include:
Summary Slides: A summary of your findings delivered as either a PDF or PPT file.
Notebook Analysis: At least one notebook containing the analysis to support your proposal. All notebooks submitted must be made public on or before the submission deadline to be eligible. If submitting as a team, all team members must be listed as collaborators on all notebooks submitted.
Submissions will be judged by the NFL based on how well they address:
Representation of player movement, including, but not limited to, the development of novel metrics that characterize player movement on the field:
Speed
Directional changes
Acceleration/Deceleration
Distance
Identification of specific variables that present an elevated risk of injury:
Are there specific movement patterns that correlate with the acute onset of injury (in general or by specific injury location)?
Are there summary metrics of player movement which influence risk of injury?
How do playing surface, game scenario, player movement, and weather interact to influence the risk of injury?
Evaluation of differences in player movement between playing surfaces:
For any metrics of player movement shown to influence risk of injury, do differences in these metrics exist across playing surfaces for the non-injured player population?
Do any of the environmental or game scenario variables influence player movement in the non-injured player population?
Submissions will be scored using the following rubric:
Creativity and Presentation (5 points)
Did the authors develop a creative or novel approach to characterizing player movement patterns?
Did the authors make effective use of data visualizations to communicate their findings?
Are the important results easily understood by the average person?
Methodology (5 points)
Does the author document their methodology appropriately?
Are the statistical and modeling methods used appropriate for the situation?
Do the authors appropriately document the performance of their injury risk models?
Application (5 points)
Are the characterizations of player movement patterns useful for comparison and injury risk assessment?
Do the authors provide insights that would be useful for reducing the incidence of injury in the NFL?",,,
221,"Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.
Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well…yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.
Unfortunately, it’s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That’s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.
In this competition, you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a ""common-sense"" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!
Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.","Submissions are evaluated on the mean column-wise Spearman's correlation coefficient. The Spearman's rank correlation is computed for each target column, and the mean of these values is calculated for the submission score.
Submission File
For each qa_id in the test set, you must predict a probability for each target variable. The predictions should be in the range [0,1]. The file should contain a header and have the following format:
qa_id,question_asker_intent_understanding,...,answer_well_written
6,0.0,...,0.5
8,0.5,...,0.1
18,1.0,...,0.0
etc.",,,
222,,,,,
223,,,,,
224,,,,,
225,,,,,
226,"Q: How much does it cost to cool a skyscraper in the summer?
A: A lot! And not just in dollars, but in environmental impact.
Thankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That’s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don’t work with different building types.
In this competition, you’ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies.
About the Host
Founded in 1894, ASHRAE serves to advance the arts and sciences of heating, ventilation, air conditioning refrigeration and their allied fields. ASHRAE members represent building system design and industrial process professionals around the world. With over 54,000 members serving in 132 countries, ASHRAE supports research, standards writing, publishing and continuing education - shaping tomorrow’s built environment today.
Banner photo by Federico Beccari on Unsplash","Evaluation Metric
The evaluation metric for this competition is Root Mean Squared Logarithmic Error.
The RMSLE is calculated as
$$
\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }
$$
Where:
\\(\epsilon\\) is the RMSLE value (score)
\\(n\\) is the total number of observations in the (public/private) data set,
\\(p_i\\) is your prediction of target, and
\\(a_i\\) is the actual target for \\(i\\).
\\(\log(x)\\) is the natural logarithm of \\(x\\)
Note that not all rows will necessarily be scored.
Notebook Submissions
You can make submissions directly from Kaggle Notebooks. By adding your teammates as collaborators on a notebook, you can share and edit code privately with them.
Submission File
For each id in the test set, you must predict the target variable. The file should contain a header and have the following format:
 id,meter_reading
 0,0
 1,0
 2,0
 etc.","Assessing the value of energy efficiency improvements can be challenging as there's no way to truly know how much energy a building would have used without the improvements. The best we can do is to build counterfactual models. Once a building is overhauled the new (lower) energy consumption is compared against modeled values for the original building to calculate the savings from the retrofit. More accurate models could support better market incentives and enable lower cost financing.
This competition challenges you to build these counterfactual models across four energy types based on historic usage rates and observed weather. The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world.
Files
train.csv
building_id - Foreign key for the building metadata.
meter - The meter id code. Read as {0: electricity, 1: chilledwater, 2: steam, 3: hotwater}. Not every building has all meter types.
timestamp - When the measurement was taken
meter_reading - The target variable. Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error. UPDATE: as discussed here, the site 0 electric meter readings are in kBTU.",kaggle competitions download -c ashrae-energy-prediction,"['https://www.kaggle.com/code/hassnaagamal/425project', 'https://www.kaggle.com/code/nadhif2104/tugas2eda-time-series-visualization', 'https://www.kaggle.com/code/rachmaniaazzahras/notebookd111ec2f4e', 'https://www.kaggle.com/code/theo75/eda-lightgbm-with-optuna-rnn-commented', 'https://www.kaggle.com/code/sahibpreet05/ashrae-additional-eda']"
227,"Bored of MNIST?
The goal of this competition is to provide a simple extension to the classic MNIST competition we're all familiar with. Instead of using Arabic numerals, it uses a recently-released dataset of Kannada digits.
Kannada is a language spoken predominantly by people of Karnataka in southwestern India. The language has roughly 45 million native speakers and is written using the Kannada script. Wikipedia
This competition uses the same format as the MNIST competition in terms of how the data is structured, but it's different in that it is a synchronous re-run Kernels competition. You write your code in a Kaggle Notebook, and when you submit the results, your code is scored on both the public test set, as well as a private (unseen) test set.
Technical Information
All details of the dataset curation has been captured in the paper titled: Prabhu, Vinay Uday. ""Kannada-MNIST: A new handwritten digits dataset for the Kannada language."" arXiv preprint arXiv:1908.01242 (2019)
The github repo of the author can be found here.
On the originally-posted dataset, the author suggests some interesting questions you may be interested in exploring. Please note, although this dataset has been released in full, the purpose of this competition is for practice, not to find the labels to submit a perfect score.
In addition to the main dataset, the author also disseminated an additional real world handwritten dataset (with 10k images), termed as the 'Dig-MNIST dataset' that can serve as an out-of-domain test dataset. It was created with the help of volunteers that were non-native users of the language, authored on a smaller sheet and scanned with different scanner settings compared to the main dataset. This 'dig-MNIST' dataset serves as a more difficult test-set (An accuracy of 76.1% was reported in the paper cited above) and achieving ~98+% accuracy on this test dataset would be rather commendable.
Acknowledgments
Kaggle thanks Vinay Prabhu for providing this interesting dataset for a Playground competition.
Image reference: https://www.researchgate.net/figure/speech-for-Kannada-numbers_fig2_313113588","This competition is evaluated on the categorization accuracy of your predictions (the percentage of images you get correct).
Submission File Format
The file should contain a header and have the following format:
id,label
1,5
2,5
3,5
...","The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine, in the Kannada script.
Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.
The training data set, train.csv, has 785 columns. The first column, called label, is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.
Each pixel column in the training set has a name like pixel{x}, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixel{x} is located on row i and column j of a 28 x 28 matrix, (indexing by zero).
For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.
Visually, if we omit the ""pixel"" prefix, the pixels make up the image like this:
000 001 002 003 ... 026 027
028 029 030 031 ... 054 055
056 057 058 059 ... 082 083
 |   |   |   |  ...  |   |
728 729 730 731 ... 754 755
756 757 758 759 ... 782 783 ",kaggle competitions download -c Kannada-MNIST,"['https://www.kaggle.com/code/shahules/indian-way-to-learn-cnn', 'https://www.kaggle.com/code/benanakca/kannada-mnist-cnn-tutorial-with-app-top-2', 'https://www.kaggle.com/code/parulpandey/visualizing-kannada-mnist-with-t-sne', 'https://www.kaggle.com/code/bustam/cnn-in-keras-for-kannada-digits', 'https://www.kaggle.com/code/marcovasquez/basic-ml-logistic-dt-pca-xgboost']"
228,"We’ve all been there: Stuck at a traffic light, only to be given mere seconds to pass through an intersection, behind a parade of other commuters. Imagine if you could help city planners and governments anticipate traffic hot spots ahead of time and reduce the stop-and-go stress of millions of commuters like you.
Geotab provides a wide variety of aggregate datasets gathered from commercial vehicle telematics devices. Harnessing the insights from this data has the power to improve safety, optimize operations, and identify opportunities for infrastructure challenges.
The dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.
This competition is being hosted in partnership with BigQuery, a data warehouse for manipulating, joining, and querying large scale tabular datasets. BigQuery also offers BigQuery ML, an easy way for users to create and run machine learning models to generate predictions through a SQL query interface.
Kaggle recently released a BigQuery integration within our kernels notebook environment, and this starter kernel gives you a great starting point for how to use BQ & BQML. You’re encouraged to use your data savvy, resourcefulness & intuition to find and join in additional external datasets that will increase your models’ predictive power.
Alright, stop waiting and get started!
Acknowledgments
A big thanks to Geotab for providing the dataset for this competition! Geotab is advancing security, connecting commercial vehicles to the internet and providing web-based analytics to help customers better manage their fleets. Geotab’s open platform and Marketplace, offering hundreds of third-party solution options, allows both small and large businesses to automate operations by integrating vehicle data with their other data assets. As an IoT hub, the in-vehicle device provides additional functionality through IOX Add-Ons. Processing billions of data points a day, Geotab leverages data analytics and machine learning to help customers improve productivity, optimize fleets through the reduction of fuel consumption, enhance driver safety, and achieve strong compliance to regulatory changes. Geotab’s products are represented and sold worldwide through Authorized Geotab Resellers. To learn more, please visit www.geotab.com and follow us @GEOTAB and on LinkedIn.","Submissions are scored on the root mean squared error. RMSE is defined as:
\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]
where \( \hat{y} \) is the predicted value, and \( y \) is the original value.
Submission File
For each row in the test set, you must predict the value of six target outcomes as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:
ID,TARGET
0_1,0
0_2,0
0_3,0
etc.","UPDATE: This dataset is no longer available via BigQuery. However, you can source it directly from the Kaggle dataset on this competition.
The data consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks. The data have been grouped by intersection, month, hour of day, direction driven through the intersection, and whether the day was on a weekend or not.
For each grouping in the test set, you need to make predictions for three different quantiles of two different metrics covering how long it took the group of vehicles to drive through the intersection. Specifically, the 20th, 50th, and 80th percentiles for the total time stopped at an intersection and the distance between the intersection and the first place a vehicle stopped while waiting. You can think of your goal as summarizing the distribution of wait times and stop distances at each intersection.
Each of those six predictions goes on a new row in the submission file. Read the submission TargetId fields, such as 1_1, as the first number being the RowId and the second being the metric id. You can unpack the submission metric id codes with submission_metric_map.json.
The training set includes an optional additional output metric (TimeFromFirstStop) in case you find that useful for building your models. It was only excluded from the test set to limit the number of predictions that must be made.",kaggle competitions download -c bigquery-geotab-intersection-congestion,"['https://www.kaggle.com/code/fayzaalmukharreq/traffic-congestion', 'https://www.kaggle.com/code/ericssy/cs412-project', 'https://www.kaggle.com/code/dtamming/geotab-exploration', 'https://www.kaggle.com/code/dtamming/geotab-part-1-eda', 'https://www.kaggle.com/code/keyongenesis/bq-ntu']"
229,"Intracranial hemorrhage, bleeding that occurs inside the cranium, is a serious health problem requiring rapid and often intensive medical treatment. For example, intracranial hemorrhages account for approximately 10% of strokes in the U.S., where stroke is the fifth-leading cause of death. Identifying the location and type of any hemorrhage present is a critical step in treating the patient. Diagnosis requires an urgent procedure. When a patient shows acute neurological symptoms such as severe headache or loss of consciousness, highly trained specialists review medical images of the patient’s cranium to look for the presence, location and type of hemorrhage. The process is complicated and often time consuming. In this competition, your challenge is to build an algorithm to detect acute intracranial hemorrhage and [its subtypes](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/overview/hemorrhage-types). You’ll develop your solution using a rich image dataset provided by the Radiological Society of North America (RSNA®) in collaboration with members of the American Society of Neuroradiology and MD.ai. If successful, you’ll help the medical community identify the presence, location and type of hemorrhage in order to quickly and effectively treat affected patients. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from December 1-6, 2019. ###Collaborators Four research institutions provided large volumes of de-identified CT studies that were assembled to create the challenge dataset: Stanford University, Thomas Jefferson University, Unity Health Toronto and Universidade Federal de São Paulo (UNIFESP), The American Society of Neuroradiology ([ASNR](https://www.asnr.org/)) organized a cadre of more than 60 volunteers to label over 25,000 exams for the challenge dataset. ASNR is the world’s leading organization for the future of neuroradiology representing more than 5,300 radiologists, researchers, interventionalists, and imaging scientists. MD.ai provided tooling and support for the data annotation process. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for AI to assist in detection and classification of hemorrhages in order to prioritize and expedite their clinical work. [A full set of acknowledgments can be found on this page](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/overview/acknowledgments).","Submissions are evaluated using a weighted multi-label logarithmic loss. Each hemorrhage sub-type is its own row for every image, and you are expected to predict a probability for that sub-type of hemorrhage. There is also an any label, which indicates that a hemorrhage of ANY kind exists in the image. Theany label is weighted more highly than specific hemorrhage sub-types.
For each image Id, you must submit a set of predicted probabilities (a separate row for each sub-type). We then take the log loss for each predicted probability versus its true label. Finally, loss is averaged across all samples.
In order to avoid the extremes of the log function, predicted probabilities are replaced with \(max(min(p,1-10^{-15}),10^{-15})\).
Submission File
There will be 6 rows per image Id. The label indicated by a particular row will look like [Image Id]_[Sub-type Name], as follows
There is also a target column, Label, indicating the probability of whether that type of hemorrhage exists in the indicated image.
For each image Id in the test set, you must predict a probability for each of the different possible sub-types. The file should contain a header and have the following format:
Id,Label
1_epidural,0
1_intraparenchymal,0
1_intraventricular,0
1_subarachnoid,0.6
1_subdural,0
1_any,0.9
2_epidural,0
etc.","What files do I need?
This is a two-stage challenge. You will need the images for the current stage - provided as stage_2_test.zip. You will also need the training data - stage_2_train.csv - and the sample submission stage_2_sample_submission.csv, which provides the IDs for the test set, as well as a sample of what your submission should look like.
Note: The timeline page outlines the two-stage format and deadlines. Stage 2 data is now available in accordance with this timeline. Also review two-stage FAQs for more details.
What should I expect the data format to be?
The training data is provided as a set of image Ids and multiple labels, one for each of five sub-types of hemorrhage, plus an additional label for any, which should always be true if any of the sub-type labels is true.
There is also a target column, Label, indicating the probability of whether that type of hemorrhage exists in the indicated image.
There will be 6 rows per image Id. The label indicated by a particular row will look like [Image Id]_[Sub-type Name], as follows:
Id,Label
1_epidural_hemorrhage,0
1_intraparenchymal_hemorrhage,0
,
,.
,
,.",kaggle competitions download -c rsna-intracranial-hemorrhage-detection,"['https://www.kaggle.com/code/marcovasquez/basic-eda-data-visualization', 'https://www.kaggle.com/code/jhoward/don-t-see-like-a-radiologist-fastai', 'https://www.kaggle.com/code/allunia/rsna-ih-detection-eda', 'https://www.kaggle.com/code/akensert/rsna-inceptionv3-keras-tf1-14-0', 'https://www.kaggle.com/code/jhoward/some-dicom-gotchas-to-be-aware-of-fastai']"
230,"Self-driving technology presents a rare opportunity to improve the quality of life in many of our communities. Avoidable collisions, single-occupant commuters, and vehicle emissions are choking cities, while infrastructure strains under rapid urban growth. Autonomous vehicles are expected to redefine transportation and unlock a myriad of societal, environmental, and economic benefits. You can apply your data analysis skills in this competition to advance the state of self-driving technology.
Lyft, whose mission is to improve people’s lives with the world’s best transportation, is investing in the future of self-driving vehicles. Level 5, their self-driving division, is working on a fleet of autonomous vehicles, and currently has a team of 450+ across Palo Alto, London, and Munich working to build a leading self-driving system (they’re hiring!). Their goal is to democratize access to self-driving technology for hundreds of millions of Lyft passengers.
From a technical standpoint, however, the bar to unlock technical research and development on higher-level autonomy functions like perception, prediction, and planning is extremely high. This implies technical R&D on self-driving cars has traditionally been inaccessible to the broader research community.
This dataset aims to democratize access to such data, and foster innovation in higher-level autonomy functions for everyone, everywhere. By conducting a competition, we hope to encourage the research community to focus on hard problems in this space—namely, 3D object detection over semantic maps.
In this competition, you will build and optimize algorithms based on a large-scale dataset. This dataset features the raw sensor camera inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a restricted geographic area.
If successful, you’ll make a significant contribution towards stimulating further development in autonomous vehicles and empowering communities around the world.","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a set of predicted 3D bounding volumes and ground truth bounding volumes is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.
At each threshold value \(t\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object.
Important note: if there are no ground truth objects at all for a given image, ANY number of predictions (false positives) will result in the image receiving a score of zero, and being included in the mean average precision.
The average precision of a single image is calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
In your submission, you are also asked to provide a confidence level for each bounding box. Bounding boxes will be evaluated in order of their confidence levels in the above process. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.
NOTE: In nearly all cases confidence will have no impact on scoring. It exists primarily to allow for submission boxes to be evaluated in a particular order to resolve extreme edge cases. None of these edge cases are known to exist in the data set. If you do not wish to use or calculate confidence you can use a placeholder value - like 1.0 - to indicate that no particular order applies to the evaluation of your submission boxes.
Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.
Intersection over Union (IoU)
Intersection over Union is a measure of the magnitude of overlap between two bounding boxes (or, in the more general case, two objects). It calculates the size of the overlap between two objects, divided by the total area of the two objects combined.
It can be visualized as the following:
The two boxes in the visualization overlap, but the area of the overlap is insubstantial compared with the area taken up by both objects together. IoU would be low - and would likely not count as a ""hit"" at higher IoU thresholds.
3D Context
The difference between the 2D and 3D bounding volume contexts is small. In the 3D context we reduce the bounding volume to a ground bounding box and a height. The IoU is then the intersection of the ground bounding boxes * the intersection of the height differences, divided by the union of the bounding boxes.
Submission File
The submission format requires a space delimited set of bounding volume parameters. For example:
97ce3ab08ccbc0baae0267cbf8d4da947e1f11ae1dbcb80c3f4408784cd9170c,1.0 2742.15 673.16 -18.65 1.834 4.609 1.648 2.619 car
indicates that sample 97ce3ab08ccbc0baae0267cbf8d4da947e1f11ae1dbcb80c3f4408784cd9170c has a bounding volume with a confidence of 0.5, center_x of 2742.15, center_y of 673.16, center_z of -18.65, width of 1.834, length of 4.609, height of 1.648, yaw of 2.619, and a class_name of car.
The file should contain a header and have the following format. Each row in your submission should contain ALL bounding boxes for a given image.
Id,PredictionString
db8b47bd4ebdf3b3fb21598bb41bd8853d12f8d2ef25ce76edd4af4d04e49341,
97ce3ab08ccbc0baae0267cbf8d4da947e1f11ae1dbcb80c3f4408784cd9170c,1.0 2742.15 673.16 -18.65 1.834 4.609 1.648 2.619 car
etc...","What files do I need?
You will need the LIDAR, image, map and data files for both train and test (test_images.zip, test_lidar.zip, etc.). You may also need the train.csv, which includes the sample annotations in the form expected for submissions. The sample_submission.csv file contains all of the sample Ids for the test set.
The data files (test_data.zip, train_data.zip) are in JSON format.
What should I expect the data format to be?
The data comes in the form of many interlocking tables and formats. The JSON files all contain single tables with identifying tokens that can be used to join with other files / tables. The images and lidar files all correspond to a sample in sample_data.json, and the sample_token from sample_data.json is the primary identifier used for the train and test samples.
The annotations in train.csv are in the following format:
center_x center_y center_z width length height yaw class_name
center_x, center_y and center_z are the world coordinates of the center of the 3D bounding volume.
width, length and height are the dimensions of the volume.
yaw is the angle of the volume around the z axis (where y is forward/back, x is left/right, and z is up/down - making 'yaw' the direction the front of the vehicle / bounding box is pointing at while on the ground).",kaggle competitions download -c 3d-object-detection-for-autonomous-vehicles,"['https://www.kaggle.com/code/tarunpaparaju/lyft-competition-understanding-the-data', 'https://www.kaggle.com/code/gzuidhof/reference-model', 'https://www.kaggle.com/code/rishabhiitbhu/eda-understanding-the-dataset-with-3d-plots', 'https://www.kaggle.com/code/meaninglesslives/lyft3d-inference-kernel', 'https://www.kaggle.com/code/meaninglesslives/lyft3d-inference-prediction-visualization']"
231,"Is there a cat in your dat?
A common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.
Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:
binary features
low- and high-cardinality nominal features
low- and high-cardinality ordinal features
(potentially) cyclical features
This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.
If you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.
Have Fun!","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
id,target
300000,0.5
300001,0.5
300002,0.5
...","In this competition, you will be predicting the probability [0, 1] of a binary target column.
The data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.
Since the purpose of this competition is to explore various encoding strategies, the data has been simplified in that (1) there are no missing values, and (2) the test set does not contain any unseen feature values (See this). (Of course, in real-world settings both of these factors are often important to consider!)
Files
train.csv - the training set
test.csv - the test set; you must make predictions against this data
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c cat-in-the-dat,"['https://www.kaggle.com/code/shahules/an-overview-of-encoding-techniques', 'https://www.kaggle.com/code/kabure/eda-feat-engineering-encode-conquer', 'https://www.kaggle.com/code/subinium/11-categorical-encoders-and-benchmark', 'https://www.kaggle.com/code/peterhurford/why-not-logistic-regression', 'https://www.kaggle.com/code/faressayah/data-science-best-practices-with-pandas-part-1']"
232,"Climate change has been at the top of our minds and on the forefront of important political decision-making for many years. We hope you can use this competition’s dataset to help demystify an important climatic variable. Scientists, like those at Max Planck Institute for Meteorology, are leading the charge with new research on the world’s ever-changing atmosphere and they need your help to better understand the clouds.
Shallow clouds play a huge role in determining the Earth's climate. They’re also difficult to understand and to represent in climate models. By classifying different types of cloud organization, researchers at Max Planck hope to improve our physical understanding of these clouds, which in turn will help us build better climate models.
There are many ways in which clouds can organize, but the boundaries between different forms of organization are murky. This makes it challenging to build traditional rule-based algorithms to separate cloud features. The human eye, however, is really good at detecting features—such as clouds that resemble flowers.
In this challenge, you will build a model to classify cloud organization patterns from satellite images. If successful, you’ll help scientists to better understand how clouds will shape our future climate. This research will guide the development of next-generation models which could reduce uncertainties in climate projections.
Help us remove the haze from climate models and bring clarity to cloud identification.
For more information on the scientific background and how the labels were created see the following paper.","This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:
$$
\frac{2 * |X \cap Y|}{|X| + |Y|}
$$
where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each <Image, Label> pair in the test set.
EncodedPixels
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
IMPORTANT
The predicted encodings should be against images that are scaled by 0.25 per side. In other words, while the images in Train and Test are 1400 x 2100 pixels, the predictions should be scaled down to a 350 x 525 pixel image. The reduction is required to achieve reasonable submission evaluation times.
Submission File Format
Your submission file should be in csv format, with a header and columns names : Image_Label, EncodedPixels. Each row in your submission represents a single predicted cloud type segmentation for the given Image, and predicted Label, and you should have the same number of rows as num_images * num_labels. The segment for cloud type in an image will be encoded into a single row, even if there are several non-contiguous cloud type locations in an image. If there is no area of a certain cloud type for an image, the corresponding EncodedPixels prediction should be left blank.
Image_Label,EncodedPixels
002f507.jpg_Fish,1 1
002f507.jpg_Flower,1 1
002f507.jpg_Gravel,2 183749
etc...","In this competition you will be identifying regions in satellite images that contain certain cloud formations, with label names: Fish, Flower, Gravel, Sugar. For each image in the test set, you must segment the regions of each cloud formation label. Each image has at least one cloud formation, and can possibly contain up to all all four.
The images were downloaded from NASA Worldview. Three regions, spanning 21 degrees longitude and 14 degrees latitude, were chosen. The true-color images were taken from two polar-orbiting satellites, TERRA and AQUA, each of which pass a specific region once a day. Due to the small footprint of the imager (MODIS) on board these satellites, an image might be stitched together from two orbits. The remaining area, which has not been covered by two succeeding orbits, is marked black.
The labels were created in a crowd-sourcing activity at the Max-Planck-Institite for Meteorology in Hamburg, Germany, and the Laboratoire de météorologie dynamique in Paris, France. A team of 68 scientists identified areas of cloud patterns in each image, and each images was labeled by approximately 3 different scientists. Ground truth was determined by the union of the areas marked by all labelers for that image, after removing any black band area from the areas.
The segment for each cloud formation label for an image is encoded into a single row, even if there are several non-contiguous areas of the same formation in an image. If there is no area of a certain cloud type for an image, the corresponding EncodedPixels prediction should be left blank. You can read more about the encoding standard on the Evaluation page.
Files",kaggle competitions download -c understanding_cloud_organization,"['https://www.kaggle.com/code/artgor/segmentation-in-pytorch-using-convenient-tools', 'https://www.kaggle.com/code/dhananjay3/image-segmentation-from-scratch-in-pytorch', 'https://www.kaggle.com/code/mobassir/keras-efficientnetb2-for-classifying-cloud', 'https://www.kaggle.com/code/ekhtiar/eda-find-me-in-the-clouds', 'https://www.kaggle.com/code/aleksandradeis/understanding-clouds-eda']"
233,"Ciphertext Challenge III: Wherefore Art Thou, Simple Ciphers?
We've done the 2010's, the 1990s… now it's time for the 80s.
The 1580s!!
In this new decryption competition's dataset, we've gone from perfectly respectable sources of electronic horror to a time before computers—heck, before calculus was called ""calculus""! Shakespeare's plays are encrypted, and we time travelers must un-encrypt them so people can do innovative stage productions with intricate makeup, costumes, and possibly—possibly!—Leonardo DiCaprio. Think about it, folks: Leo.*
As in previous ciphertext challenges, simple classic ciphers have been used to encrypt this dataset, along with a slightly less simple surprise that expands our definition of ""classic"" into the modern age. The mission is the same: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Meta-puzzles and difficulty await!
Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the leaderboard (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best competition-related kernels, in both visualization and cryptanalysis, based on upvotes. Last, the coveted ""Phil Prize""—for the team that correctly deduces the form AND key of the final cipher—is up for grabs again.
Go ahead. Get cracking!
* - Leo!
Acknowledgements
Many thanks to Kaggler LiamLarson for their excellent Shakespeare dataset.","Submissions are evaluated on Accuracy between the predicted plaintext index and the actual index.
Submission File
For each ciphertext in the test set, you must predict the plaintext index. The file should contain a header and have the following format:
ciphertext_id,index
ID_0827e580b,0
ID_bfcf14ce8,0
ID_aab1e107a,0
ID_061cc38c0,0
ID_0e16534d3,0
ID_b1387ef4a,0
ID_e2275f924,0
ID_d68e90960,0
ID_0a3775e01,0","What terms should I be aware of?
A cipher is an algorithm that transforms a readable piece of information into an unreadable one (and in most cases vice versa). Modern encryption algorithms - AES, DES, etc. - are examples of commonly used ciphers.
In this case, we've used simple (or classic) ciphers. For as long as information has existed, methods have been used to obscure the information's meaning. Before computers were common, they usually consisted of a series of simple operations performed on text. In this data you'll encounter multiple text-transformation ciphers that date back centuries, if not longer.
There is also a newer, well-known cipher that also consists of a series of simple operations performed on data.
Text that has been encrypted is referred to as ciphertext. (Unencrypted text is usually called plaintext.)
The exact cipher(s) used to encrypt this data will be revealed at the end of the competition - if they're not discovered first!
How has the text been encrypted?
Each document has been encrypted with up to 4 layered ciphers (ciphers 1, 2, 3, and 4). We've used a difficulty value here to denote which ciphers were used for a particular piece of text.
Every document in the dataset has been padded to the next hundred characters (95->100, 213->300) with random (in-alphabet) characters, then encrypted based on its difficulty level. A difficulty of 1 means that only cipher #1 was used. A difficulty of means cipher #1 was applied, followed by cipher #2, and so on. The level denotes exactly which ciphers were applied, and in what order.",kaggle competitions download -c ciphertext-challenge-iii,"['https://www.kaggle.com/code/adrianpaul/cipher-challenge-iii-level-1-adrian', 'https://www.kaggle.com/code/elvenmonk/ciphertext-challenge-iii-fast-level-3', 'https://www.kaggle.com/code/elvenmonk/difficulty-4-reverse-engineering', 'https://www.kaggle.com/code/jiaofenx/ciphertext-challenge-iii-simple-eda-and-cracking', 'https://www.kaggle.com/code/elvenmonk/ciphertext-challenge-iii-fast-level-1']"
234,"Steel is one of the most important building materials of modern times. Steel buildings are resistant to natural and man-made wear which has made the material ubiquitous around the world. To help make production of steel more efficient, this competition will help identify defects.
Severstal is leading the charge in efficient steel mining and production. They believe the future of metallurgy requires development across the economic, ecological, and social aspects of the industry—and they take corporate responsibility seriously. The company recently created the country’s largest industrial data lake, with petabytes of data that were previously discarded. Severstal is now looking to machine learning to improve automation, increase efficiency, and maintain high quality in their production.
The production process of flat sheet steel is especially delicate. From heating and rolling, to drying and cutting, several machines touch flat steel by the time it’s ready to ship. Today, Severstal uses images from high frequency cameras to power a defect detection algorithm.
In this competition, you’ll help engineers improve the algorithm by localizing and classifying surface defects on a steel sheet.
If successful, you’ll help keep manufacturing standards for steel high and enable Severstal to continue their innovation, leading to a stronger, more efficient world all around us.","This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:
$$
\frac{2 * |X \cap Y|}{|X| + |Y|}
$$
where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each <ImageId, ClassId> pair in the test set.
EncodedPixels
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
File Format
Your submission file should be in csv format, with a header and columns names : ImageId_ClassId, EncodedPixels. Each row in your submission represents a single predicted defect segmentation for the given ImageId, and predicted ClassId, and you should have the same number of rows as num_images * num_defect_classes. The segment for each defect class will be encoded into a single row, even if there are several non-contiguous defect locations on an image.
ImageId_ClassId,EncodedPixels
004f40c73.jpg_1,1 1
004f40c73.jpg_2,1 1
004f40c73.jpg_3,2 409599
etc...","In this competition you will be predicting the location and type of defects found in steel manufacturing. Images are named with a unique ImageId. You must segment and classify the defects in the test set.
Each image may have no defects, a defect of a single class, or defects of multiple classes. For each image you must segment defects of each class (ClassId = [1, 2, 3, 4]).
The segment for each defect class will be encoded into a single row, even if there are several non-contiguous defect locations on an image. You can read more about the encoding standard on the Evaluation page.
Submissions to this competition must be made through Kernels. After your submission against the Public test set, your kernel will re-run automatically against the entire Public and Private (unseen) test set. Refer to the Kernels Requirement Page for more information.
Files
train_images/ - folder of training images
test_images/ - folder of test images (you are segmenting and classifying these images)
train.csv - training annotations which provide segments for defects (ClassId = [1, 2, 3, 4])
sample_submission.csv - a sample submission file in the correct format; note, each ImageId 4 rows, one for each of the 4 defect classes",kaggle competitions download -c severstal-steel-defect-detection,"['https://www.kaggle.com/code/go1dfish/clear-mask-visualization-and-simple-eda', 'https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch', 'https://www.kaggle.com/code/rishabhiitbhu/unet-starter-kernel-pytorch-lb-0-88', 'https://www.kaggle.com/code/xhlulu/severstal-simple-2-step-pipeline', 'https://www.kaggle.com/code/ekhtiar/resunet-a-baseline-on-tensorflow']"
235,"Build a model to transcribe ancient Kuzushiji into contemporary Japanese characters
Imagine the history contained in a thousand years of books. What stories are in those books? What knowledge can we learn from the world before our time? What was the weather like 500 years ago? What happened when Mt. Fuji erupted? How can one fold 100 cranes using only one piece of paper? The answers to these questions are in those books.
Japan has millions of books and over a billion historical documents such as personal letters or diaries preserved nationwide. Most of them cannot be read by the majority of Japanese people living today because they were written in “Kuzushiji”.
Even though Kuzushiji, a cursive writing style, had been used in Japan for over a thousand years, there are very few fluent readers of Kuzushiji today (only 0.01% of modern Japanese natives). Due to the lack of available human resources, there has been a great deal of interest in using Machine Learning to automatically recognize these historical texts and transcribe them into modern Japanese characters. Nevertheless, several challenges in Kuzushiji recognition have made the performance of existing systems extremely poor. (More information in About Kuzushiji)
This is where you come in. The hosts need help from machine learning experts to transcribe Kuzushiji into contemporary Japanese characters. With your help, Center for Open Data in the Humanities (CODH) will be able to develop better algorithms for Kuzushiji recognition. The model is not only a great contribution to the machine learning community, but also a great help for making millions of documents more accessible and leading to new discoveries in Japanese history and culture.
Hosts
Center for Open Data in the Humanities (CODH) conducts research and development to enhance access to humanities data using state-of-the-art technology in informatics and statistics.
The National Institute of Japanese Literature (NIJL) is an institution which strives to serve researchers in the field of Japanese literature as well as those working in various other humanities, by collecting in one location a vast storage of materials related to Japanese literature gathered from all corners of the country.
The National Institute of Informatics (NII) is Japan's only general academic research institution seeking to create future value in the new discipline of informatics. NII seeks to advance integrated research and development activities in information-related fields, including networking, software, and content.
Official Collaborators
Mikel Bober-Irizar (anokas) Kaggle Grandmaster and Alex Lamb (MILA. Quebec Artificial Intelligence Institute)","Submissions are evaluated on a modified version of the F1 Score. To score a true positive, you must provide center point coordinates that are within the ground truth bounding box and a matching label. The ground truth bounding boxes are defined in the format {label X Y Width Height}, so if the ground truth label is U+003F 1 1 10 10 then a prediction of U+003F 3 3 would pass. You can find a Python version of the metric here.
Submission File
For each image in the test set, you must locate and identify all of the kuzushiji characters. Not every image will necessarily be scored. The file should contain a header and have the following format:
image_id,labels
image_id,{label X Y} {...}
Do not make more than 1,200 predictions per page.","Kuzushiji is a cursive script that was used in Japan for over a thousand years that has fallen out of use in modern times.
Vast portions of Japanese historical documents now cannot be read by most Japanese people. By helping to automate the transcription of kuzushiji you will contribute to unlocking a priceless trove of books and records.
The specific task is to locate and classify each kuzushiji character on a page. While complete bounding boxes are provided for the training set, only a single point within the ground truth bounding box is needed for submissions.
There are a few important things to keep in mind for your submissions:
Some images only contain illustrations, in which case no labels should be submitted for the page.
Kuzushiji text is written such that annotations are placed between the columns of the main text, usually in a slightly smaller font. See the characters with no bounding boxes in this image for examples. The annotation characters should be ignored; labels for them will be scored as false positives.
You can occasionally see through especially thin paper and read characters from the opposite side of the page. Those characters should also be ignored.
Files
train.csv - the training set labels and bounding boxes.",kaggle competitions download -c kuzushiji-recognition,"['https://www.kaggle.com/code/anokas/kuzushiji-visualisation', 'https://www.kaggle.com/code/kmat2019/centernet-keypoint-detector', 'https://www.kaggle.com/code/christianwallenwein/visualization-useful-functions-small-eda', 'https://www.kaggle.com/code/jesucristo/kuzushiji-recognition-complete-guide', 'https://www.kaggle.com/code/hocop1/unet-character-detector']"
236,"Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren’t thinking about the data science that determined your fate.
Embarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. “Press 1 if you really tried to spend $500 on cheddar cheese.”
While perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle.
IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.
In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.
If successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.
Acknowledgements:
Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry. Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually.
Header Photo by Tim Evans on Unsplash","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each TransactionID in the test set, you must predict a probability for the isFraud variable. The file should contain a header and have the following format:
TransactionID,isFraud
3663549,0.5
3663550,0.5
3663551,0.5
etc.","In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.
The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.
Categorical Features - Transaction
ProductCD
card1 - card6
addr1, addr2
P_emaildomain
R_emaildomain
M1 - M9
Categorical Features - Identity
DeviceType
DeviceInfo",kaggle competitions download -c ieee-fraud-detection,"['https://www.kaggle.com/code/artgor/eda-and-models', 'https://www.kaggle.com/code/kabure/extensive-eda-and-modeling-xgb-hyperopt', 'https://www.kaggle.com/code/shahules/tackling-class-imbalance', 'https://www.kaggle.com/code/cdeotte/xgb-fraud-with-magic-0-9600', 'https://www.kaggle.com/code/jesucristo/fraud-complete-eda']"
237,"Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:
Object detection track for detecting bounding boxes around object instances, relaunched from 2018.
Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018.
Instance segmentation track for segmenting masks of objects in images, brand new for 2019.
Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding.
Instance Segmentation Track
In this track of the Challenge, you are asked to provide segmentation masks of objects.
This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.
Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.

The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision.
We are excited to partner with Open Images for this second year of competitions, including this brand new track!","Submissions are evaluated by computing mean Average Precision, with the mean taken over the 300 segmentable classes of the challenge. It follows the same spirit as the Object Detection evaluation, but takes into account mask-to-mask matching. The metric is described in detail here. See also this tutorial on running the evaluation in Python.
Submission File
For each image in the test set, you must predict a list of instance segmentation masks and their associated detection score (Confidence). The submission csv file uses the following format:
ImageID,ImageWidth,ImageHeight,PredictionString
ImageAID,ImageAWidth,ImageAHeight,LabelA1 ConfidenceA1 EncodedMaskA1 LabelA2 ConfidenceA2 EncodedMaskA2 ...
ImageBID,ImageBWidth,ImageBHeight,LabelB1 ConfidenceB1 EncodedMaskB1 LabelB2 ConfidenceB2 EncodedMaskB2 …
A sample with real values would be:
ImageID,ImageWidth,ImageHeight,PredictionString
721568e01a744247,1118,1600,/m/018xm 0.637833 eNqLi8xJM7BOTjS08DT2NfI38DfyM/Q3NMAJgJJ+RkBs7JecF5tnAADw+Q9I
7b018c5e3a20daba,1600,1066,/m/01g317 0.85117 eNqLiYrLN7DNCjDMMIj0N/Iz9DcwBEIDfyN/QyA2AAsBRfxMPcKTA1MMADVADIo=
The binary segmentation masks are run-length encoded (RLE), zlib compressed, and base64 encoded to be used in text format as EncodedMask. Specifically, we use the Coco masks RLE encoding/decoding (see the encode method of COCO’s mask API), the zlib compression/decompression (RFC1950), and vanilla base64 encoding.
An example python function to encode an instance segmentation mask would be:
import base64
import numpy as np
from pycocotools import _mask as coco_mask
import typing as t
import zlib
def encode_binary_mask(mask: np.ndarray) -> t.Text:
 """"""Converts a binary mask into OID challenge encoding ascii text.""""""

 # check input mask --
 if mask.dtype != np.bool:
   raise ValueError(
       ""encode_binary_mask expects a binary mask, received dtype == %s"" %
       mask.dtype)

 mask = np.squeeze(mask)
 if len(mask.shape) != 2:
   raise ValueError(
       ""encode_binary_mask expects a 2d mask, received shape == %s"" %
       mask.shape)

 # convert input mask to expected COCO API input --
 mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)
 mask_to_encode = mask_to_encode.astype(np.uint8)
 mask_to_encode = np.asfortranarray(mask_to_encode)

 # RLE encode mask --
 encoded_mask = coco_mask.encode(mask_to_encode)[0][""counts""]

 # compress and base64 encoding --
 binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)
 base64_str = base64.b64encode(binary_str)
 return base64_str
(This code is available as a gist here.)
There is a 5Gb file size limit on the submission csv file. This implicitly limits the number of detections to about 50~100 per image (on average).","The train and validation sets of images and their ground truth (instance masks) should be downloaded from the Open Images Challenge page.
The test images used in this competition are independent from those released as part of the Open Images Dataset.
The test images are the same as in the Object Detection and Visual Relationship tracks, so you might not need to re-download them.
The challenge test set images can be downloaded from:
Kaggle (test.zip file below)
CVDF
Figure 8
You should expect 99,999 images in total in the challenge test set.
File descriptions
test.zip - the 99,999 images of the challenge test set.
sample_empty_submission.csv - a sample submission file with zero detections.
sample_truncated_submission.csv - a (truncated) sample submission file in the correct format.
It includes 42 detections over (only) 10 images.
A full submission.csv file should cover all 99,999 images and be less than 5Gb in size.",kaggle competitions download -c open-images-2019-instance-segmentation,"['https://www.kaggle.com/code/iiyamaiiyama/how-to-submit-prediction', 'https://www.kaggle.com/code/saravanakumar123/goolge-instance-segmentation-with-sample-intro', 'https://www.kaggle.com/code/gowrishankarin/basic-eda-with-plotly-and-images', 'https://www.kaggle.com/code/elvinagammed/segmentation-metrics-object-detection', 'https://www.kaggle.com/code/drobchak1988/competitions-open-images2019-instance-segmentation']"
238,"This competition is closed and no longer accepting submissions. The private leaderboard has been finalized as of 8/28/2019.
Important Warning: This competition has an experimental format and submission style (images as submission). Competitors must use generative methods to create their submission images and are not permitted to make submissions that include any images already classified as dogs or altered versions of such images.
To enforce and prevent cheating, we reserve the right to: (a) Visually inspect all participants' submitted images, (b) review any submitted source code, (c) use these reviews to identify violators or determine winners, and (d) disqualify participants from the competition who are found in violation. This is also specified in the competition's rules
Use your training skills to create images, rather than identify them. You’ll be using GANs, which are at the creative frontier of machine learning. You might think of GANs as robot artists in a sense—able to create eerily lifelike images, and even digital worlds.
""You might not think that programmers are artists, but programming is an extremely creative profession. It’s logic-based creativity. '' -
John Romero
A generative adversarial network (GAN) is a class of machine learning system invented by Ian Goodfellow in 2014. Two neural networks compete with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set.
In this competition, you’ll be training generative models to create images of dogs. Only this time… there’s no ground truth data for you to predict. Here, you’ll submit the images and be scored based on how well those images are classified as dogs from pre-trained neural networks. Take these images, for example. Can you tell which are real vs. generated?
Trick question; they are all generated!
Why dogs? We chose dogs because, well, who doesn’t love looking at photos of adorable pups? Moreover, dogs can be classified into many sub-categories (breed, color, size), making them ideal candidates for image generation.
Generative methods (in particular, GANs) are currently used in various places on Kaggle for data augmentation. Their potential is vast; they can learn to mimic any distribution of data across any domain: photographs, drawings, music, and prose. If successful, not only will you help advance the state of the art in generative image creation, but you’ll enable us to create more experiments across a variety of domains in the future.
This is a Kernels-only competition. Refer to Kernels Requirements for details.","Important Warning: This competition has an experimental format and submission style (images as submission). Competitors must use generative methods to create their submission images and are not permitted to make submissions that include any images already classified as dogs or altered versions of such images.
To enforce and prevent cheating, we reserve the right to: (a) Visually inspect all participants' submitted images, (b) review any submitted source code, (c) use these reviews to identify violators or determine winners, and (d) disqualify participants from the competition who are found in violation. This is also specified in the competition's rules
MiFID
Submissions are evaluated on MiFID (Memorization-informed Fréchet Inception Distance), which is a modification from Fréchet Inception Distance (FID).
The smaller MiFID is, the better your generated images are.
What is FID?
Originally published here (github), FID, along with Inception Score (IS), are both commonly used in recent publications as the standard for evaluation methods of GANs.
In FID, we use the Inception network to extract features from an intermediate layer. Then we model the data distribution for these features using a multivariate Gaussian distribution with mean µ and covariance Σ. The FID between the real images \(r\) and generated images \(g\) is computed as:
\[
\text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr} (\Sigma_r + \Sigma_g - 2 (\Sigma_r \Sigma_g)^{1/2})
\]
where \(Tr\) sums up all the diagonal elements. FID is calculated by computing the Fréchet distance between two Gaussians fitted to feature representations of the Inception network.
What is MiFID (Memorization-informed FID)?
In addition to FID, Kaggle takes training sample memorization into account.
The memorization distance is defined as the minimum cosine distance of all training samples in the feature space, averaged across all user generated image samples. This distance is thresholded, and it's assigned to 1.0 if the distance exceeds a pre-defined epsilon.
In mathematical form:
\[d_{ij} = 1 - cos(f_{gi}, f_{rj}) = 1 - \frac{f_{gi} \cdot f_{rj}}{|f_{gi}| |f_{rj}|}\]
where \(f_g\) and \(f_r\) represent the generated/real images in feature space (defined in pre-trained networks); and \(f_{gi}\) and \(f_{rj}\) represent the \(i^{th}\) and \(j^{th}\) vectors of \(f_g\) and \(f_r\), respectively.
\[d = \frac{1}{N} \sum_{i} \min_j d_{ij} \]
defines the minimum distance of a certain generated image (\(i\)) across all real images ((\(j\)), then averaged across all the generated images.

defines the threshold of the weight only applies when the (\(d\)) is below a certain empirically determined threshold.
Finally, this memorization term is applied to the FID:
\[ MiFID = FID \cdot \frac{1}{d_{thr}}\]
Kaggle's workflow calculating MiFID for public and private scores
Kaggle calculates public and private MiFID scores with the same code, but with different pre-trained models and evaluation images. The public pre-train neural network is Inception, and the public images used for evaluation are the ImageNet Dogs (all 120 breeds). We will not be sharing what private model or dataset is used for the private MiFID score.
A demo of our MiFID evaluation code can be seen here.
Our workflow of computing the public/private MiFID is demonstrated below:
Submission File
You are going to generate 10,000 images that are in PNG format. Their sizes should be 64x64x3 (RGB). Then you need to zip those images and your output from your Kernel should only have ONE output file named images.zip.
Please note that Kaggle Kernels has a number of output files capped at 500. We highly encourage you to either directly write to a zip file as you generate images, or create a folder at ../tmp as your temporary directory.","The only eligible dataset to use in this competition is the Stanford Dogs Dataset.
Important Warning: This competition has an experimental format and submission style (images as submission). Competitors must use generative methods to create their submission images and are not permitted to make submissions that include any images already classified as dogs or altered versions of such images.
To enforce and prevent cheating, we reserve the right to: (a) Visually inspect all participants' submitted images, (b) review any submitted source code, (c) use these reviews to identify violators or determine winners, and (d) disqualify participants from the competition who are found in violation. This is also specified in the competition's rules
Files
all-dogs.zip - All dog images contained in the Stanford Dogs Dataset
Annotations.zip - Class labels, Bounding boxes
Submission Format
Your kernel's output must be called images.zip and contain 10,000 images sized 64x64. Review the Overview Page for more details.",kaggle competitions download -c generative-dog-images,"['https://www.kaggle.com/code/jesucristo/gan-introduction', 'https://www.kaggle.com/code/cdeotte/dog-memorizer-gan', 'https://www.kaggle.com/code/cdeotte/supervised-generative-dog-net', 'https://www.kaggle.com/code/wendykan/gan-dogs-starter', 'https://www.kaggle.com/code/phoenix9032/gan-dogs-starter-24-jul-custom-layers']"
239,"Imagine being able to detect blindness before it happened.
Millions of people suffer from diabetic retinopathy, the leading cause of blindness among working aged adults. Aravind Eye Hospital in India hopes to detect and prevent this disease among people living in rural areas where medical screening is difficult to conduct. Successful entries in this competition will improve the hospital’s ability to identify potential patients. Further, the solutions will be spread to other Ophthalmologists through the 4th Asia Pacific Tele-Ophthalmology Society (APTOS) Symposium
Currently, Aravind technicians travel to these rural areas to capture images and then rely on highly trained doctors to review the images and provide diagnosis. Their goal is to scale their efforts through technology; to gain the ability to automatically screen images for disease and provide information on how severe the condition may be.
In this synchronous Kernels-only competition, you'll build a machine learning model to speed up disease detection. You’ll work with thousands of images collected in rural areas to help identify diabetic retinopathy automatically. If successful, you will not only help to prevent lifelong blindness, but these models may be used to detect other sorts of diseases in the future, like glaucoma and macular degeneration.
Get started today!","Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, this metric may go below 0. The quadratic weighted kappa is calculated between the scores assigned by the human rater and the predicted scores.
Images have five possible ratings, 0,1,2,3,4.  Each image is characterized by a tuple (e,e), which corresponds to its scores by Rater A (human) and Rater B (predicted).  The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that O corresponds to the number of images that received a rating i by A and a rating j by B. An N-by-N matrix of weights, w, is calculated based on the difference between raters' scores:
An N-by-N histogram matrix of expected ratings, E, is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that E and O have the same sum.
Submissions should be formatted like:
id_code,diagnosis
0005cfc8afb6,0
003f0afdcd15,0
etc.","You are provided with a large set of retina images taken using fundus photography under a variety of imaging conditions.
A clinician has rated each image for the severity of diabetic retinopathy on a scale of 0 to 4:
0 - No DR
1 - Mild
2 - Moderate
3 - Severe
4 - Proliferative DR
Like any real-world data set, you will encounter noise in both the images and labels. Images may contain artifacts, be out of focus, underexposed, or overexposed. The images were gathered from multiple clinics using a variety of cameras over an extended period of time, which will introduce further variation.
Files
In a synchronous Kernels-only competition, the files you can observe and download will be different than the private test set and sample submission. The files may have different ids, may be a different size, and may vary in other ways, depending on the problem. You should structure your code so that it returns predictions for the public test set images in the format specified by the public sample_submission.csv, but does not hard code aspects like the id or number of rows. When Kaggle runs your Kernel privately, it substitutes the private test set and sample submission in place of the public ones.
You can plan on the private test set consisting of 20GB of data across 13,000 images (approximately).",kaggle competitions download -c aptos2019-blindness-detection,"['https://www.kaggle.com/code/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy', 'https://www.kaggle.com/code/tanlikesmath/intro-aptos-diabetic-retinopathy-eda-starter', 'https://www.kaggle.com/code/abhishek/pytorch-inference-kernel-lazy-tta', 'https://www.kaggle.com/code/xhlulu/aptos-2019-densenet-keras-starter', 'https://www.kaggle.com/code/drhabib/starter-kernel-for-0-79']"
240,"The cost of some drugs and medical treatments has risen so high in recent years that many patients are having to go without. You can help with a classification project that could make researchers more efficient.
One of the more surprising reasons behind the cost is how long it takes to bring new treatments to market. Despite improvements in technology and science, research and development continues to lag. In fact, finding new treatments takes, on average, more than 10 years and costs hundreds of millions of dollars.
Recursion Pharmaceuticals, creators of the industry’s largest dataset of biological images, generated entirely in-house, believes AI has the potential to dramatically improve and expedite the drug discovery process. More specifically, your efforts could help them understand how drugs interact with human cells.
This competition will have you disentangling experimental noise from real biological signals. Your entry will classify images of cells under one of 1,108 different genetic perturbations. You can help eliminate the noise introduced by technical execution and environmental variation between experiments.
If successful, you could dramatically improve the industry’s ability to model cellular images according to their relevant biology. In turn, applying AI could greatly decrease the cost of treatments, and ensure these treatments get to patients faster.
This competition is a part of the NeurIPS 2019 competition track. Winners will be invited to contribute their solutions towards the workshop presentation.
Acknowledgments
Thank you to the following sponsors & supporters of this competition:
Google Cloud: Google Cloud is widely recognized as a global leader in delivering a secure, open and intelligent enterprise cloud platform. Our technology is built on Google’s private network and is the product of nearly 20 years of innovation in security, network architecture, collaboration, artificial intelligence and open source software. We offer a simply engineered set of tools and unparalleled technology across Google Cloud Platform and G Suite that help bring people, insights and ideas together. Customers across more than 150 countries trust Google Cloud to modernize their computing environment for today’s digital world.
DoiT: You have the cloud and we have your back. For nearly a decade, we’ve been helping businesses build and scale cloud solutions with our world-class cloud engineering support. We help our customers with technical support and consulting on building and operating complex large-scale distributed systems, developing better machine learning models and setting up big data solutions using Google Cloud, Amazon AWS and Microsoft Azure.
NVIDIA: NVIDIA’s (NASDAQ: NVDA) invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined modern computer graphics and revolutionized parallel computing. More recently, GPU deep learning ignited modern AI — the next era of computing — with the GPU acting as the brain of computers, robots and self-driving cars that can perceive and understand the world. More information at http://nvidianews.nvidia.com.
Lambda: Lambda provides Deep Learning workstations, servers, and GPU cloud services. Lambda Deep Learning infrastructure is used by the world's leading AI research & development organizations including Apple, Microsoft, MIT, Stanford, and the US Government. To learn more, visit www.lambdalabs.com.","Submissions are evaluated on Multiclass Accuracy, which is simply the average number of observations with the correct label.
Submission File
For each id_code in the test set, you must predict the correct sirna. The file should contain a header and have the following format:
id_code,sirna
HEPG2-08_1_B03,911
HEPG2-08_1_B04,911
etc.","One of the main challenges for applying AI to biological microscopy data is that even the most careful replicates of a process will not look identical. This dataset challenges you to develop a model for identifying replicates that is robust to experimental noise.
The same siRNAs (effectively genetic perturbations) have been applied repeatedly to multiple cell lines, for a total of 51 experimental batches. Each batch has four plates, each of which has 308 filled wells. For each well, microscope images were taken at two sites and across six imaging channels. Not every batch will necessarily have every well filled or every siRNA present.
We've condensed this description down to the essentials; for additional details please see RxRx.ai.
File Description
[train/test].zip: the image data. The image paths, such as U2OS-01/Plate1/B02_s2_w3.png, can be read as:
Cell line and batch number (U2OS batch 1)
Plate number (1)
Well location on plate (column B, row 2)
Site (2)",kaggle competitions download -c recursion-cellular-image-classification,"['https://www.kaggle.com/code/jesucristo/quick-visualization-eda', 'https://www.kaggle.com/code/zaharch/keras-model-boosted-with-plates-leak', 'https://www.kaggle.com/code/tanlikesmath/rcic-fastai-starter', 'https://www.kaggle.com/code/leighplt/densenet121-pytorch', 'https://www.kaggle.com/code/xhlulu/recursion-2019-load-resize-and-save-images']"
241,"Imagine being able to search for the moment in any video where an adorable kitten sneezes, even though the uploader didn’t title or describe the video with such descriptive metadata. Now, apply that same concept to videos that cover important or special events like a baby’s first steps or a game-winning goal -- and now we have the ability to quickly find and share special video moments. This technology is called temporal concept localization within video and Google Research can use your help to advance the state of the art in this area.
An example of the detected action ""blowing out candles""
In most web searches, video retrieval and ranking is performed by matching query terms to metadata and other video-level signals. However, we know that videos can contain an array of topics that aren’t always characterized by the uploader, and many of these miss localizations to brief but important moments within the video. Temporal localization can enable applications such as improved video search (including search within video), video summarization and highlight extraction, action moment detection, improved video content safety, and many others.
In previous years, participants worked on advancements in video-level annotations, building both unconstrained and constrained models. In this third challenge based on the YouTube 8M dataset, Kagglers will localize video-level labels to the precise time in the video where the label actually appears, and do this at an unprecedented scale. To put it another way: at what point in the video does the cat sneeze?
If successful, your new machine learning models will significantly improve video understanding for all, by not only identifying the topics relevant to a video, but also pinpointing where in the video they appear.
This competition is being hosted by Google Research as a part of the International Conference on Computer Vision (ICCV) 2019 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.","Submissions are evaluated according to the Mean Average Precision @ K (MAP@K), where \( K = 100,000 \).
$$MAP@100,000 = \frac{1}{C} \sum_{c=1}^{C} \frac{\sum_{k=1}^{n} P(k) \times rel(k)}{N_{c}}$$
where \( C \) is the number of Classes, \( P(k) \) is the precision at cutoff \( k \), \( n \) is the number of Segments predicted per Class, \( rel(k) \) is an indicator function equaling 1 if the item at rank \( k \) is a relevant (correct) Class, or zero otherwise, and \( N_{c} \) is the number of positively-labeled segments for the each Class.
IMPORTANT: The evaluation for this competition is different in some important ways:
As noted above, for each Class you are predicting relevant Segments (and not the other way around).
Not all test segments have been human-rated, and only human-rated segments are used in scoring. All segments that were not explicitly rated (as either containing a Class, or not containing a Class) are removed from the prediction list before scoring.
The Public/Private Test split is performed at the Segment level, not the Class level. In other words, all classes (i.e., submission rows) are evaluated for the Public and Private leaderboard, but only segments for the particular split will be used in the prediction and ground truth.
A python implementation of MAP@K can be found at youtube-8m's github.
Submission File
For each of the 1,000 Class values in the sample_submission.csv file, you may predict up to 100,000 Segment IDs that are predicted to contain that Class, sorted in order of confidence.
IMPORTANT: In order to minimize submission file sizes, for segment predictions, you should only include the video id and the segment start time, but not the segment end time. (These are not needed, since all segments are 5 seconds in duration.)
The file should contain a header and have the following format:
Class,Segments 
3,002G:35 002G:40 002G:60 ... 
7,002G:35 002G:40 002G:60  ...
...","In this competition, you will predict the Class labels of YouTube video segments. We provide you extracted frame-level features. The feature data and detailed feature information can be found on the YouTube-8M dataset webpage. 
The training dataset in this competition contains videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition.
File descriptions
frame-level data
You may download to your local computer with instructions here
Total size of 1.53TB (Large file warning!)
Each video has
id: unique id for the video, in train set it is a YouTube video id, and in test/validation they are anonymized.
labels: list of labels of that video.
Each frame has rgb: float array of length 1024,
Each frame has audio: float array of length 128
A subset of the validation set videos are provided with segment-level labels. In addition to id, labels and the frame level features described above, they come with",kaggle competitions download -c youtube8m-2019,"['https://www.kaggle.com/code/jesucristo/analysis-youtube8m-2019', 'https://www.kaggle.com/code/inversion/starter-kernel-yt8m-2019-sample-data', 'https://www.kaggle.com/code/artyomp/stronger-baseline-with-pytorch', 'https://www.kaggle.com/code/senorcampos/the-peak-muffin-moment', 'https://www.kaggle.com/code/frostxu/annotation-analysis-help-to-understand']"
242,"Imagine suddenly gasping for air, helplessly breathless for no apparent reason. Could it be a collapsed lung? In the future, your entry in this competition could predict the answer.
Pneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifying—it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event.
Pneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more confident diagnosis for non-radiologists.
The Society for Imaging Informatics in Medicine (SIIM) is the leading healthcare organization for those interested in the current and future use of informatics in medical imaging. Their mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community. Today, they need your help.
In this competition, you’ll develop a model to classify (and if present, segment) pneumothorax from a set of chest radiographic images. If successful, you could aid in the early recognition of pneumothoraces and save lives.
If you’re up for the challenge, take a deep breath, and get started now.
Note: As specified on the Data Page, the dataset must be retrieved from Cloud Healthcare. Review this tutorial (or in pdf format) for instructions on how to do so.
Acknowledgments
SIIM Machine Learning Committee Co-Chairs, Steven G. Langer, PhD, CIIP and George Shih, MD, MS for tirelessly leading this effort and making the challenge possible in such a short period of time.
SIIM Machine Learning Committee Members for their dedication in annotating the dataset, helping to define the most useful metrics and running tests to prepare the challenge for launch.
SIIM Hackathon Committee, especially Mohannad Hussain, for their crucial technical support with data conversion.
American College of Radiology (ACR), @RadiologyACR: For Co-hosting the challenge and Co-sponsoring the Prizes
Society of Thoracic Radiology (STR), @thoracicrad: For their unparalleled expertise in adjudicating the dataset
MD.ai: For providing the annotation tool and helping with the first layer of annotations","This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:
$$ \frac{2 * |X \cap Y|}{|X| + |Y|},$$
where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.
Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.  Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space delimited list of pairs. Note that this competition uses relative pixel positions, meaning that after the first pixel position, the remaining pixel values are simply offsets. For example, '1 3 10 5' implies pixels 1,2,3 are to be included in the mask, as well as 14,15,16,17,18. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
The file should contain a header and have the following format:
ImageId,EncodedPixels
0004d4463b50_01,1 1 5 1
0004d4463b50_02,1 1
0004d4463b50_03,1 1
etc.
Submission files may take several minutes to process due to the size.","UPDATE: This dataset is no longer available via the Cloud Healthcare API. Others have shared the dataset on Kaggle, if you're interested in accessing it through those methods.
Stage 2 Note: the stage 1 files (if needed) should be downloaded using the special downloading instructions. The stage 2 files must be downloaded directly from Kaggle. See below.
Note: this is a two-stage competition with special downloading instructions. Please read carefully and ask any questions you might have!
What files do I need?
The stage 2 training set consists of the stage 1 training set and the stage 1 test set combined. If you need to download their images, please follow these instructions for both train AND test, using this tutorial page or in .pdf format here.
The annotations for the stage 2 training set should be downloaded below - the filename is stage_2_train.csv. The stage 2 test images can be downloaded directly from this page - contained in stage_2_images.zip.
You may also need the sample submission (which contains all of the test IDs for stage 2) to aid you in making predictions.
What should I expect the data format to be?",kaggle competitions download -c siim-acr-pneumothorax-segmentation,"['https://www.kaggle.com/code/jesperdramsch/intro-chest-xray-dicom-viz-u-nets-full-data', 'https://www.kaggle.com/code/meaninglesslives/nested-unet-with-efficientnet-encoder', 'https://www.kaggle.com/code/abhishek/mask-rcnn-with-augmentation-and-multiple-masks', 'https://www.kaggle.com/code/iafoss/hypercolumns-pneumothorax-fastai-0-831-lb', 'https://www.kaggle.com/code/ekhtiar/finding-pneumo-part-1-eda-and-unet']"
243,"Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:
Object detection track for detecting bounding boxes around object instances, relaunched from 2018.
Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018.
Instance segmentation track for segmenting masks of objects in images, brand new for 2019.
Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding.
Object Detection Track
In this track of the Challenge, you are asked to predict a tight bounding box around object instances.
The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).

Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.
Please refer to the Open Images 2019 Challenge page for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you.
The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision.
We are excited to partner with Open Images for this second year of competitions. See link here for last year’s Object Detection competition.","Submissions are evaluated by computing mean Average Precision (mAP), modified to take into account the annotation process of Open Images dataset (mean is taken over per-class APs). The metric is described on the Open Images Challenge website.
The final mAP is computed as the average AP over the 500 classes. The participants will be ranked on this final metric.
The metric is implemented as a part of Tensorflow Object Detection API. See this Tutorial on running the evaluation in Python.
Submission File
For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as
ImageID,PredictionString
ImageID,{Label Confidence XMin YMin XMax YMax} {...}","The train and validation sets of images and their ground truth (bounding boxes and labels) should be downloaded from Open Images Challenge page. Please note that the test images used in this competition is independent from those released as part of the Open Images Dataset. The images can be downloaded from:
Kaggle (test.zip file below)
CVDF
Figure 8
Note: The images are the same as in the Visual Relationship Track so you do not need to re-download them. You should expect 99,999 images in total in the test set.
File descriptions
test.zip - the test set of 99,999 images
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c open-images-2019-object-detection,"['https://www.kaggle.com/code/xhlulu/intro-to-tf-hub-for-object-detection', 'https://www.kaggle.com/code/abhishek/training-fast-rcnn-using-torchvision', 'https://www.kaggle.com/code/vikramtiwari/baseline-predictions-using-inception-resnet-v2', 'https://www.kaggle.com/code/thanatoz/understanding-open-image-v5-classes-hierarchy', 'https://www.kaggle.com/code/nhlr21/tf-hub-bounding-boxes-coordinates-corrected']"
244,"Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:
Object detection track for detecting bounding boxes around object instances, relaunched from 2018.
Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018.
Instance segmentation track for segmenting masks of objects in images, brand new for 2019.
Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding.
Visual Relationship Track
In this track of the Challenge, you are asked to detect pairs of objects and the relationships that connect them.
The training set contains 329 relationship triplets with 375k training samples. These include both human-object relationships (e.g. ""woman playing guitar"", ""man holding microphone""), object-object relationships (e.g. ""beer on table"", ""dog inside car""), and also considers object-attribute relationships (e.g.""handbag is made of leather"" and ""bench is wooden"").

Left: Example of ‘man playing guitar’ - Radiofiera - Villa Cordellina Lombardi, Montecchio Maggiore (VI) - agosto 2010 by Andrea Sartorati. Right: Example of ‘chair at table’ - Epic Fireworks - Loads A Room by Epic Fireworks
Please refer to the Open Images 2019 Challenge page for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you.
The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision.
We are excited to partner with Open Images for this second year of competitions. See link here for last year’s Visual Representation Detection competition.","Submissions are evaluated by computing the weighted mean of three metrics: mean Average Precision (mAP) on relationships detection, Recall@N (where N=50), mean Average Precision on phrase detection (mean in mean Average Precision is taken over per-relationship APs).
See more details about the metric on Open Images Challenge website.
The weights applied to each of the 3 metrics are [0.4, 0.2, 0.4].
The metric is implemented as a part of Tensorflow Object Detection API. See this Tutorial on running the evaluation in Python.
Submission File
For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as
  ImageId, PredictionString 
  ImageId, {Confidence Label1 XMin1 YMin1 XMax1 YMax1 Label2 XMin2 YMin2 XMax2 YMax2 RelationLabel}, {...}","The train and validation sets of images and their ground truth (visual relationships annotations, bounding boxes and labels) can be downloaded via Open Images Challenge website . Please note that the test images used in this competition is independent from those released as part of the Open Images Dataset. The images can be downloaded from:
Kaggle (test.zip file below.)
CVDF
Figure 8
Note: The images are the same as in the Object Detection Track so you do not need to re-download them. You should expect 99,999 images.
File descriptions
test.zip - the test set of 99,999 images
VRD_sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c open-images-2019-visual-relationship,"['https://www.kaggle.com/code/seriousran/googld-ai-visual-relationship-data-exploration', 'https://www.kaggle.com/code/danish788/beat-the-lb', 'https://www.kaggle.com/code/jian1201/revised-beat-the-benchmark', 'https://www.kaggle.com/code/anshuljdhingra/visual-relationship', 'https://www.kaggle.com/code/sumanthmeenan/open-images-visual-relationship']"
245,"Think you can use your data science smarts to make big predictions at a molecular level?
This challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules.
Researchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science.
This competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. Winning teams will have an opportunity to partner with this multi-university research program on an academic publication
Your Challenge
In this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant).
Once the competition finishes, CHAMPS would like to invite the top teams to present their work, discuss the details of their models, and work with them to write a joint research publication which discusses an open-source implementation of the solution.
About Scalar Coupling
Using NMR to gain insight into a molecule’s structure and dynamics depends on the ability to accurately predict so-called “scalar couplings”. These are effectively the magnetic interactions between a pair of atoms. The strength of this magnetic interaction depends on intervening electrons and chemical bonds that make up a molecule’s three-dimensional structure.
Using state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows.
A fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior.
Ultimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease.
Join the CHAMPS Scalar Coupling challenge to apply predictive analytics to chemistry and chemical biology.","Submissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type.
$$score = \frac{1}{T} \sum_{t=1}^{T} \log \left( \frac{1}{n_{t}} \sum_{i=1}^{n_t} \lvert y_i - \hat{y_i} \rvert \right) $$
Where:
\( T \) is the number of scalar coupling types
\( n_{t}\) is the number of observations of type \( t \)
\( y_{i}\) is the actual scalar coupling constant for the observation
\( \hat{y_i}\) is the predicted scalar coupling constant for the observation
For this metric, the MAE for any group has a floor of 1e-9, so that the minimum (best) possible score for perfect predictions is approximately -20.7232.
Submission File
For each id in the test set, you must predict the scalar_coupling_constant variable. The file should contain a header and have the following format:
id,scalar_coupling_constant
4659076,0.0
4659077,0.0
4659078,0.0
etc.","In this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.
For this competition, you will not be predicting all the atom pairs in each molecule rather, you will only need to predict the pairs that are explicitly listed in the train and test files. For example, some molecules contain Fluorine (F), but you will not be predicting the scalar coupling constant for any pair that includes F.
The training and test splits are by molecule, so that no molecule in the training data is found in the test data.
Files
train.csv - the training set, where the first column (molecule_name) is the name of the molecule where the coupling constant originates (the corresponding XYZ file is located at ./structures/.xyz), the second (atom_index_0) and third column (atom_index_1) is the atom indices of the atom-pair creating the coupling and the fourth column (scalar_coupling_constant) is the scalar coupling constant that we want to be able to predict
test.csv - the test set; same info as train, without the target variable
sample_submission.csv - a sample submission file in the correct format
structures.zip - folder containing molecular structure (xyz) files, where the first line is the number of atoms in the molecule, followed by a blank line, and then a line for every atom, where the first column contains the atomic element (H for hydrogen, C for carbon etc.) and the remaining columns contain the X, Y and Z cartesian coordinates (a standard format for chemists and molecular visualization programs)",kaggle competitions download -c champs-scalar-coupling,
246,"Data Science for Good: City of Los Angeles
Help the City of Los Angeles to structure and analyze its job descriptions
The City of Los Angeles faces a big hiring challenge: 1/3 of its 50,000 workers are eligible to retire by July of 2020. The city has partnered with Kaggle to create a competition to improve the job bulletins that will fill all those open positions.
Problem Statement
The content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.
The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to: (1) identify language that can negatively bias the pool of applicants; (2) improve the diversity and quality of the applicant pool; and/or (3) make it easier to determine which promotions are available to employees in each job class.
How to Participate
Accept the Rules
Accept the competition rules.
Make Your Submission
Follow the submission instructions.
WIth your help, Los Angeles will overcome a wave of retirements and fill those jobs with a strong and diverse workforce. Good luck and happy Kaggling!
Do you think companies can find better candidates by improving their job postings? We hope to create an open-sourced body of work focused on this topic by hosting another Data Science for Good competition, this time in partnership with the City of Los Angeles.","Grading Rubric
The bulk of the grade will be determined by the contents of your kernel notebook, including: (1) the creation of a single structured CSV file and data dictionary that contains all of the information from the plain text job postings; and (2) the quality of the insights and data-driven recommendations that you make to the City of LA.
The possible elements contained within each job bulletin are described in Kaggle_data_dictionary.csv, Sample job class export template.csv, and Job_titles.csv. If competitors extract additional data elements, they should provide an updated data dictionary, which builds on the one provided but includes at a minimum greater detail about allowable values, additionally identified variables, data types, and explanatory notes as appropriate.
Performances will be scored on:
Accuracy (5 points)
Documentation (5 points)
Recommendation (5 points)
Accuracy (5 points):
Did the user convert the folder of job postings into a structured CSV?
Job class structures will be evaluated on how accurately each prepared field matches manually-structured fields for a randomized set of classes.
Documentation (5 points):
Does the user follow good coding practices?
Useful comments
Description of coding language used/modules used
Are the modules open sourced? (prefer open sourced modules)
Documentation of script automation. (Fully-automated scripts are preferred)
Does the user document their methodology?
Is there documentation about how the solution was evaluated?
Is there documentation about the pros and cons of their solution?
Recommendation (5 points)
Did the authors use the structured data to make an original insight?
Did the authors identify and communicate details about something that they discovered in the data?
Did the authors make an actionable recommendation to the City of LA?
Did the authors make effective use of data visualizations to communicate their recommendations to the City of LA?
Do the recommendations give helpful solutions to one or more of the following issues?
- (1) identify language that can bias the pool of applicants
- (2) improve the diversity and quality of the applicant pool; and/or
- (3) increase the discoverability of promotional pathways",,,
247,"Designers know what they are creating, but what, and how, do people really wear their products? What combinations of products are people using? In this competition, we challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign segmentations and attribute labels for fashion images.
Visual analysis of clothing is a topic that has received increasing attention in recent years. Being able to recognize apparel products and associated attributes from pictures could enhance the shopping experience for consumers, and increase work efficiency for fashion professionals.
We present a new clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.
While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders’ needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 92 related fine-grained attributes. Secondly, a total of 50K clothing images (10K with both segmentation and fine-grained attributes, 40K with apparel instance segmentation) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation.
Individuals/Teams with top submissions will be invited to present their work live at the FGVC6 workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) 2019
Checkout the iMaterialist-Fashion Competition Github repo for the specifics of the dataset.
Acknowledgments
The iMat-Fashion Challenge 2019 is sponsored by Google AI, CVDF, Samasource and Fashionpedia.","Submissions are evaluated on the mean average precision at different intersection over union (IoU) thresholds. Similar to the 2018 science bowl competition, the IoU of a proposed set of object pixels and a set of true object pixels is calculated as:
$$
IoU(A, B) = \frac{A \cap B}{A \cup B}
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.
At each threshold value t, a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single ClassId and a single image is then calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|threshold|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}
$$
The score returned by the competition metric is the mean taken over the individual average precisions of each image and each ClassId in the test dataset.
It is important to distinguish both fine-grained classes of the object as well as object instances. If the mask is correct but the class associated with the mask is wrong then it is considered a false positive.
Submission File
Downsize your submissions
The size of our test images and masks are relatively large. In order to keep each submission file's processing time under several minutes, each submission must downsize its predicted masks (EncodedPixels) such that every image size is (512, 512). An example (python) to get the resized mask from the predicted binary mask is:
import cv2
resized_binary_mask = cv2.resize(binary_mask, (512, 512), cv2.INTER_NEAREST)
# then convert resized_binary_mask to EncodedPixels
# for example, https://www.kaggle.com/stainsby/fast-tested-rle-and-input-routines
File Format
Your submission file should be in csv format, with a header and columns names : ImageId,EncodedPixels,ClassId.
Each row in your submission represents a single predicted apparel object/part segmentation for the given ImageId, and predicted ClassId.
ImageId,EncodedPixels,ClassId
ffebe7df5bfb5974878870b8cb3ebed4,1 1,35_24_51_69_88_195_210_306
0046f98599f05fd7233973e430d6d04d,1 1,24  
004e9e21cd1aca568a8ffc77a54638ce,2 3 8 9,34_218  
etc...
Submission files may take several minutes to process due to the size.
EncodedPixels
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values, similar to the 2018 science bowl competition. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space-delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same class and the same image are overlapping.
ClassId
ClassId refers to a unique class index for an apparel category with its associated fine-grained attributes (if any).
$$\text{ClassId}_i = \{ \text{category}_c, \text{attribute}_a \} $$
$$\forall a \in [0, \text{total attributes number}), c \in [0, \text{total category number}), i \in [0, \text{total unique labels})$$
The ClassId is obtained by concatenating both predicted apparel object index and predicted attributes' indices for this object. An example python function to get ClassId is:
def get_class_id(cat_id, att_ids):
    """"""
    Get concatenated ClassId
    Args:
        cat_id: int, index for the apparel category
        att_ids: [int], list of apparel attributes
    Returns:
        class_id: string, e.g. ""2_10_91_55""
    """"""
    class_id = []
    class_id.append(cat_id)
    att_ids.sort()  # need to be sorted before concatenation
    class_id.extend(att_ids)
    return ""_"".join([str(i) for i in class_id])
Examples of ClassId are:
35_24_51_69_88_195_210_306
24  
34_218  
etc...",,,
248,,,,,
249,,,,,
250,"As part of the FGVC6 workshop at CVPR 2019 we are conducting the iNat Challenge 2019 large scale species classification competition, sponsored by Microsoft. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features a large number of fine-grained categories.
Previous versions of the challenge have focused on classifying large numbers of species. This year features a smaller number of highly similar categories captured in a wide variety of situations, from all over the world. In total, the iNat Challenge 2019 dataset contains 1,010 species, with a combined training and validation set of 268,243 images that have been collected and verified by multiple users from iNaturalist.
Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC6 workshop. Participants who make a submission that beats the sample submission can fill out this form to receive $150 in Google Cloud credits.

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","We use top-1 classification error as the metric for this competition. For each image, an algorithm will produce 1 label. If the predicted label matches the ground truth label then the error for that image is 0, otherwise it is 1. The final score is the error averaged across all images.
Submission File
For each image in the test set, you must predict 1 category label. However, we encourage you to predict more categories labels (sorted by confidence) so that we can analyze top-3 and top-5 performances. The csv file should contain a header and have the following format:
id,predicted  
268243,71 108 339 341 560  
268244,333 729 838 418 785  
268245,690 347 891 655 755
The id column corresponds to the test image id. The predicted column corresponds to 1 category id. The first category id will be used to compute the metric. You should have one row for each test image.","File descriptions
train_val2019.tar.gz - Contains the training and validation images in a directory structure following {iconic category name}/{category name}/{image id}.jpg .
train2019.json - Contains the training annotations.
val2019.json - Contains the validation annotations.
test2019.tar.gz - Contains a single directory of test images.
test2019.json - Contains test image information.
kaggle_sample_submission.csv - A sample submission file in the correct format.
Image Format
All images have been saved in the JPEG format and have been resized to have a maximum dimension of 800 pixels.
Annotation Format
We follow the annotation format of the [COCO dataset][2] and add additional fields. The annotations are stored in the [JSON format][3] and are organized as follows:
{
   : ,
   : [],
   : [],
   : [],
   : []
}

info{
   : ,
   : ,
   : ,
   : ,
   : ,
   : ,
}

image{
   : ,
   : ,
   : ,
   : ,
   : ,
   : 
}

category{
   : ,
   : ,
   : ,
   : ,
   : ,
   : ,
   : ,
   : 
}

annotation{
   : ,
   : ,
   : 
}

license{
   : ,
   : ,
   : 
}",kaggle competitions download -c inaturalist-2019-fgvc6,"['https://www.kaggle.com/code/ateplyuk/inat2019-starter-keras-efficientnet', 'https://www.kaggle.com/code/khursani8/fast-ai-ootb-cutout-efficientnet', 'https://www.kaggle.com/code/feichin/inception3-last-years-baseline', 'https://www.kaggle.com/code/hsinwenchang/keras-data-augmentation-visualize', 'https://www.kaggle.com/code/macaodha/basic-inat2019-data-exploration']"
251,,,,,
252,"Can you help detect toxic comments ― and minimize unintended model bias? That's your challenge in this competition.
The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.
Last year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.
Here’s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. ""gay""), even when those comments were not actually toxic (such as ""I am a gay woman""). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users.
In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.
Acknowledgments
The Conversation AI team would like to thank Civil Comments for making this dataset available publicly and the Online Hate Index Research Project at D-Lab, University of California, Berkeley, whose labeling survey/instrument informed the dataset labeling. We'd also like to thank everyone who has contributed to Conversation AI's research, especially those who took part in our last competition, the success of which led to the creation of this challenge.
This is a Kernels-only competition. Refer to Kernels Requirements for details.","Competition Evaluation
This competition will use a newly developed metric that combines several submetrics to balance overall performance with various aspects of unintended bias.
First, we'll define each submetric.
Overall AUC
This is the ROC-AUC for the full evaluation set.
Bias AUCs
To measure unintended bias, we again calculate the ROC-AUC, this time on three specific subsets of the test set for each identity, each capturing a different aspect of unintended bias. You can learn more about these metrics in Conversation AI's recent paper Nuanced Metrics for Measuring Unintended Bias with Real Data in Text Classification.
Subgroup AUC: Here, we restrict the data set to only the examples that mention the specific identity subgroup. A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity.
BPSN (Background Positive, Subgroup Negative) AUC: Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.
BNSP (Background Negative, Subgroup Positive) AUC: Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.
Generalized Mean of Bias AUCs
To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:
$$M_p(m_s) = \left(\frac{1}{N} \sum_{s=1}^{N} m_s^p\right)^\frac{1}{p}$$
where:
\( M_p \) = the \(p\)th power-mean function

\( m_s \) = the bias metric \( m \) calulated for subgroup \( s \)

\( N \) = number of identity subgroups
For this competition, we use a \( p \) value of -5 to encourage competitors to improve the model for the identity subgroups with the lowest model performance.
Final Metric
We combine the overall AUC with the generalized mean of the Bias AUCs to calculate the final model score:
$$score = w_0 AUC_{overall} + \sum_{a=1}^{A} w_a M_p(m_{s,a})$$
where:
A = number of submetrics (3)

\( m_{s,a} \) = bias metric for identity subgroup \( s \) using submetric \( a \)

\( w_a \) = a weighting for the relative importance of each submetric; all four \(w\) values set to 0.25
While the leaderboard will be determined by this single number, we highly recommend looking at the individual submetric results, as shown in this kernel, to guide you as you develop your models.
Submission File
id,prediction
7000000,0.0
7000001,0.0
etc.","Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.
UPDATE (Nov 18, 2019): The following files have been added post-competition close to facilitate ongoing research. See the File Description section for details.
test_public_expanded.csv
test_private_expanded.csv
toxicity_individual_annotations.csv
identity_individual_annotations.csv
Background
At the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.
In the data supplied for this competition, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (target), and models should predict the target toxicity for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment. For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic).",kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification,"['https://www.kaggle.com/code/thousandvoices/simple-lstm', 'https://www.kaggle.com/code/bminixhofer/simple-lstm-pytorch-version', 'https://www.kaggle.com/code/yuval6967/toxic-bert-plain-vanila', 'https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-for-glove-part2-usage', 'https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-for-glove-part1-eda']"
253,"The Metropolitan Museum of Art in New York, also known as The Met, has a diverse collection of over 1.5M objects of which over 200K have been digitized with imagery. The online cataloguing information is generated by Subject Matter Experts (SME) and includes a wide range of data. These include, but are not limited to: multiple object classifications, artist, title, period, date, medium, culture, size, provenance, geographic location, and other related museum objects within The Met’s collection. While the SME-generated annotations describe the object from an art history perspective, they can also be indirect in describing finer-grained attributes from the museum-goer’s understanding. Adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects.
About
This is an FGVCx competition hosted as part of the FGVC6 workshop at CVPR 2019. View the github page for more details.
This is a Kernels-only competition. Refer to Kernels Requirements for details.","Submissions will be evaluated based on their mean F2 score. The F score, commonly used in information retrieval, measures accuracy using the precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F2 score is given by:
$$\frac{(1 + \beta^2) pr}{\beta^2 p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn},\ \beta = 2.$$
Note that the F2 score weights recall higher than precision. The mean F2 score is formed by averaging the individual F2 scores for each id in the test set.
Submission File
For each image in the test set, predict a space-delimited list of tags which you believe are associated with the image. The file should contain a header and have the following format:
id,attribute_ids
10023b2cc4ed5f68,0 1 2
100fbe75ed8fd887,0 1 2
101b627524a04f19,0 1 2
etc...","In this dataset, you are presented with a large number of artwork images and associated attributes of the art. Multiple modalities can be expected and the camera sources are unknown. The photographs are often centered for objects, and in the case where the museum artifact is an entire room, the images are scenic in nature.
Each object is annotated by a single annotator without a verification step. Annotators were advised to add multiple labels from an ontology provided by The Met, and additionally are allowed to add free-form text when they see fit. They were able to view the museum's online collection pages and advised to avoid annotating labels already present. The attributes can relate to what one ""sees"" in the work or what one infers as the object's ""utility.""
While we have made efforts to make the attribute labels as high quality as possible, you should consider these annotations noisy. There may be a small number of attributes with similar meanings. The competition metric, F2 score, was intentionally chosen to provide some robustness against noisy labels, favoring recall over precision.
This is a kernels-only competition with two stages. After the deadline, Kaggle will rerun your selected kernels on an unseen test set. The second-stage test set is approximately five times the size of the first. You should plan your kernel's memory, disk, and runtime footprint accordingly.
Files
The filename of each image is its id.
train.csv gives the attribute_ids for the train images in /train",kaggle competitions download -c imet-2019-fgvc6,"['https://www.kaggle.com/code/chewzy/eda-weird-images-with-new-updates', 'https://www.kaggle.com/code/lopuhin/imet-2019-submission', 'https://www.kaggle.com/code/ateplyuk/keras-starter', 'https://www.kaggle.com/code/mathormad/resnet50-v2-keras-focal-loss-mix-up', 'https://www.kaggle.com/code/mnpinto/imet-fastai-starter']"
254,"Ciphertext Challenge II: The Challengening!
It's baaaaaaack!
In our first ciphertext competition, we hunted the wilds of the '90s-era internet. This time around, we're exploring the dark slow-broadband-y wastelands of 2011, with the Movie Review Dataset. In 2011 most of the internet hadn't even been invented yet*, so wow, you're in for a treat.
Again, simple classic ciphers have been used to encrypt this dataset. Your mission this time: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Also, there are some new ciphers in play this time, which will involve some meta-puzzling. Enjoy!
Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the LB (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best competition-related kernels, in both visualization and cryptanalysis, based on upvotes.
Go ahead. Get cracking!
* - This is not true.
Acknowledgements
Maas, A., Daly, R., Pham, P., Huang, D., Ng, A. and Potts, C. (2011). Learning Word Vectors for Sentiment Analysis: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. [online] Portland, Oregon, USA: Association for Computational Linguistics, pp. 142–150. Available here.","Submissions are evaluated on Accuracy between the predicted plaintext index and the actual index.
Submission File
For each ciphertext in the test set, you must predict the plaintext index. The file should contain a header and have the following format:
ciphertext_id,index
ID_0827e580b,0
ID_bfcf14ce8,0
ID_aab1e107a,0
ID_061cc38c0,0
ID_0e16534d3,0
ID_b1387ef4a,0
ID_e2275f924,0
ID_d68e90960,0
ID_0a3775e01,0","What terms should I be aware of?
A cipher is an algorithm that transforms a readable piece of information into an unreadable one (and in most cases vice versa). Modern encryption algorithms - AES, DES, etc. - are examples of commonly used ciphers.
In this case, we've used simple (or classic) ciphers. For as long as information has existed, methods have been used to obscure the information's meaning. Before computers were common, they usually consisted of a series of simple operations performed on text. In this data you'll encounter multiple text-transformation ciphers that date back centuries, if not longer.
Text that has been encrypted is referred to as ciphertext. (Unencrypted text is usually called plaintext.)
The exact cipher(s) used to encrypt this data will be revealed at the end of the competition - if they're not discovered first!
How has the text been encrypted?
Each document has been encrypted with up to 4 layered ciphers (ciphers 1, 2, 3, and 4). We've used a difficulty value here to denote which ciphers were used for a particular piece of text.
Every document in the dataset has been padded to the next hundred characters (95->100, 213->300) with random (in-alphabet) characters, then encrypted based on its difficulty level. A difficulty of 1 means that only cipher #1 was used. A difficulty of 2 means cipher #1 was applied, followed by cipher #2, and so on. The difficulty level denotes exactly which ciphers were applied, and in what order.",kaggle competitions download -c ciphertext-challenge-ii,"['https://www.kaggle.com/code/group16/cracking-the-code-difficulty-1', 'https://www.kaggle.com/code/group16/cracking-the-code-difficulty-3', 'https://www.kaggle.com/code/bsteenwi/cracking-the-code-difficulty-2', 'https://www.kaggle.com/code/jazivxt/difficulty-2', 'https://www.kaggle.com/code/jazivxt/the-crypto-keeper']"
255,"Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to nearby areas we are faced with an interesting probem: how do you classify a species in a new region that you may not have seen in previous training data?
In order to tackle this problem, we have prepared a challenge where the training data and test data are from different regions, namely The American Southwest and the American Northwest. The species seen in each region overlap, but are not identical, and the challenge is to classify the test species correctly. To this end, we will allow training on our American Southwest data (from CaltechCameraTraps), on iNaturalist 2017/2018 data, and on simulated data generated from Microsoft AirSim. We have provided a taxonomy file mapping our classes into the iNat taxonomy.
This is an FGVCx competition as part of the FGVC6 workshop at CVPR 2019, and is sponsored by Microsoft AI for Earth. There is a github page for the competition here. Please open an issue if you have questions or problems with the dataset.
If you use this dataset in publication, please cite:
@article{beery2019iwildcam,
 title={The iWildCam 2019 Challenge Dataset},
 author={Beery, Sara and Morris, Dan and Perona, Pietro},
 journal={arXiv preprint arXiv:1907.07617},
 year={2019}
}

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","Evaluation
Submissions will be evaluated based on their macro F1 score - i.e. F1 will be calculated for each class of animal (including ""empty"" if no animal is present), and the submission's final score will be the unweighted mean of all class F1 scores.
Submission Format
The submission format for the competition is a csv file with the following format:
Id,Predicted
58857ccf-23d2-11e8-a6a3-ec086b02610b,1
591e4006-23d2-11e8-a6a3-ec086b02610b,5
The Id column corresponds to the test image id. The Category is an integer value that indicates the class of the animal, or 0 to represent the absence of an animal.","Data Overview
The training set contains 196,157 images from 138 different locations in Southern California. You may also choose to use supplemental training data from iNaturalist 2017, iNaturalist 2018, iNaturalist 2019, and images simulated with Microsoft AirSim. As a courtesy, we have curated all the images from iNaturalist 2017/2018 containing classes that might be in the test set and mapped them into the iWildCam categories. This data (which we call ""iNat Idaho"") can be downloaded from our git page here.
The test set contains 153,730 images from 100 locations in Idaho.
The competition task is to label each image with one of the following label ids:
name, id
empty, 0
deer, 1
moose, 2
squirrel, 3
rodent, 4
small_mammal, 5
elk, 6
pronghorn_antelope, 7
rabbit, 8",kaggle competitions download -c iwildcam-2019-fgvc6,"['https://www.kaggle.com/code/seriousran/image-pre-processing-for-wild-images', 'https://www.kaggle.com/code/gpreda/iwildcam-2019-eda-and-prediction', 'https://www.kaggle.com/code/ateplyuk/iwildcam2019-pytorch-starter', 'https://www.kaggle.com/code/ateplyuk/iwildcam2019-keras-efficientnet', 'https://www.kaggle.com/code/xhlulu/densenet-transfer-learning-iwildcam-2019']"
256,"CareerCon 2019 is upon us!
CareerCon is a digital event all about landing your first data science job — and registration is now open! Ahead of the event, we have a fun competition to get you started. See below for a unique challenge and opportunity to share your resume with select CareerCon sponsors.
___________________________________
The Competition
Robots are smart… by design. To fully understand and properly navigate a task, however, they need input about their environment.
In this competition, you’ll help robots recognize the floor surface they’re standing on using data collected from Inertial Measurement Units (IMU sensors).
We’ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity. Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won’t fall down on the job.
Special thanks for making this competition possible:
The data for this competition has been collected by Heikki Huttunen and Francesco Lomio from the Department of Signal Processing and Damoon Mohamadi, Kaan Celikbilek, Pedram Ghazi and Reza Ghabcheloo from the Department of Automation and Mechanical Engineering both from Tampere University, Finland. We at Kaggle would like thank them all for kindly donating the data that has made this competition possible!","Submissions are evaluated on Multiclass Accuracy, which is simply the average number of observations with the correct label.
Submission File
For each series_id in the test set, you must predict a value for the surface variable. The file should have the following format:
series_id,surface
0,fine_concrete
1,concrete
2,concrete
etc.","X_[train/test].csv - the input data, covering 10 sensor channels and 128 measurements per time series plus three ID columns:

-row_id: The ID for this row.

-series_id: ID number for the measurement series. Foreign key to y_train/sample_submission.

-measurement_number: Measurement number within the series.
The orientation channels encode the current angles how the robot is oriented as a quaternion (see Wikipedia). Angular velocity describes the angle and speed of motion, and linear acceleration components describe how the speed is changing at different times. The 10 sensor channels are:
orientation_X
orientation_Y
orientation_Z
orientation_W
angular_velocity_X",kaggle competitions download -c career-con-2019,"['https://www.kaggle.com/code/peterilberg/notebook062c919140', 'https://www.kaggle.com/code/danilopaula/cross-val-comparison-for-help-navigate-robots', 'https://www.kaggle.com/code/wolfy73/robots-need-help', 'https://www.kaggle.com/code/lawrencehe/robots-floor-surface-lstm', 'https://www.kaggle.com/code/abhishektyagi001/robot-navigation-help']"
257,"To assess the impact of climate change on Earth's flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the VIGIA project, which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery.
This is a kernels-only competition, meaning you must submit predictions using Kaggle Kernels. Read the basics here.
Acknowledgments
Kaggle is hosting this competition for the machine learning community to use for fun and practice. The original version of this data can be found here, with details in the following paper:
Efren López-Jiménez, Juan Irving Vasquez-Gomez, Miguel Angel Sanchez-Acevedo, Juan Carlos Herrera-Lozada, Abril Valeria Uriarte-Arcia, Columnar Cactus Recognition in Aerial Images using a Deep Learning Approach. Ecological Informatics. 2019.
Acknowledgements to Consejo Nacional de Ciencia y Tecnología. Project cátedra 1507. Instituto Politècnico Nacional. Universidad de la Cañada. Contributors: Eduardo Armas Garca, Rafael Cano Martnez and Luis Cresencio Mota Carrera. J.I. Vasquez-Gomez, JC. Herrera Lozada. Abril Uriarte, Miguel Sanchez.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each ID in the test set, you must predict a probability for the has_cactus variable. The file should contain a header and have the following format:
id,has_cactus
000940378805c44108d287872b2f04ce.jpg,0.5
0017242f54ececa4512b4d7937d1e21e.jpg,0.5
001ee6d8564003107853118ab87df407.jpg,0.5
etc.","This dataset contains a large number of 32 x 32 thumbnail images containing aerial photos of a columnar cactus (Neobuxbaumia tetetzo). Kaggle has resized the images from the original dataset to make them uniform in size. The file name of an image corresponds to its id.
You must create a classifier capable of predicting whether an images contains a cactus.
Files
train/ - the training set images
test/ - the test set images (you must predict the labels of these)
train.csv - the training set labels, indicates whether the image has a cactus (has_cactus = 1)
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c aerial-cactus-identification,"['https://www.kaggle.com/code/shahules/getting-started-with-cnn-and-vgg16', 'https://www.kaggle.com/code/ateplyuk/pytorch-efficientnet', 'https://www.kaggle.com/code/ratan123/in-depth-guide-to-different-cnn-architectures', 'https://www.kaggle.com/code/abhinand05/in-depth-guide-to-convolutional-neural-networks', 'https://www.kaggle.com/code/ateplyuk/keras-transfer-vgg16']"
258,"Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. If homes can be found for them, many precious lives can be saved — and more happy families created.
PetFinder.my has been Malaysia’s leading animal welfare platform since 2008, with a database of more than 150,000 animals. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.
Animal adoption rates are strongly correlated to the metadata associated with their online profiles, such as descriptive text and photo characteristics. As one example, PetFinder is currently experimenting with a simple AI tool called the Cuteness Meter, which ranks how cute a pet is based on qualities present in their photos.
In this competition you will be developing algorithms to predict the adoptability of pets - specifically, how quickly is a pet adopted? If successful, they will be adapted into AI tools that will guide shelters and rescuers around the world on improving their pet profiles' appeal, reducing animal suffering and euthanization.
Top participants may be invited to collaborate on implementing their solutions into AI tools for assessing and improving pet adoption performance, which will benefit global animal welfare.
Important Note
Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output.
Photo by Krista Mangulsone on Unsplash","As we will be switching out test data to re-evaluate kernels on stage 2 data to populate the private leaderboard, submissions must be named submission.csv
Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, the metric may go below 0. The quadratic weighted kappa is calculated between the scores which are expected/known and the predicted scores.
Results have 5 possible ratings, 0,1,2,3,4.  The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that Oi,j corresponds to the number of adoption records that have a rating of i (actual) and received a predicted rating j. An N-by-N matrix of weights, w, is calculated based on the difference between actual and predicted rating scores:
$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$
An N-by-N histogram matrix of expected ratings, E, is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between the actual rating's histogram vector of ratings and the predicted rating's histogram vector of ratings, normalized such that E and O have the same sum.
From these three matrices, the quadratic weighted kappa is calculated as: 
$$\kappa=1-\frac{\sum_{i,j}w_{i,j}O_{i,j}}{\sum_{i,j}w_{i,j}E_{i,j}}.$$
Submission Format
You must submit a csv file with the product id and a predicted search relevance for each search record. The order of the rows does not matter. The file must have a header and should look like the following:
PetID,AdoptionSpeed
378fcc4fc,3
73c10e136,2
72000c4c5,1
e147a4b9f,4
etc..","In this competition you will predict the speed at which a pet is adopted, based on the pet’s listing on PetFinder. Sometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted. The data included text, tabular, and image data. See below for details.
This is a Kernels-only competition. At the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data.
File descriptions
train.csv - Tabular/text data for the training set
test.csv - Tabular/text data for the test set
sample_submission.csv - A sample submission file in the correct format
breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.
color_labels.csv - Contains ColorName for each ColorID
state_labels.csv - Contains StateName for each StateID
Data Fields
PetID - Unique hash ID of pet profile",kaggle competitions download -c petfinder-adoption-prediction,"['https://www.kaggle.com/code/artgor/exploration-of-data-step-by-step', 'https://www.kaggle.com/code/bminixhofer/how-bestpetting-cheated', 'https://www.kaggle.com/code/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps', 'https://www.kaggle.com/code/wrosinski/baselinemodeling', 'https://www.kaggle.com/code/jakubwasikowski/stratified-group-k-fold-cross-validation']"
259,"Welcome
In this competition you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition.
CareerVillage.org is a nonprofit that crowdsources career advice for underserved youth. Founded in 2011 in four classrooms in New York City, the platform has now served career advice from 25,000 volunteer professionals to over 3.5M online learners. The platform uses a Q&A style similar to StackOverflow or Quora to provide students with answers to any question about any career.
In this Data Science for Good challenge, CareerVillage.org, in partnership with Google.org, is inviting you to help recommend questions to appropriate volunteers. To support this challenge, CareerVillage.org has supplied five years of data.
Problem Statement
The U.S. has almost 500 students for every guidance counselor. Underserved youth lack the network to find their career role models, making CareerVillage.org the only option for millions of young people in America and around the globe with nowhere else to turn.
To date, 25,000 volunteers have created profiles and opted in to receive emails when a career question is a good fit for them. This is where your skills come in. To help students get the advice they need, the team at CareerVillage.org needs to be able to send the right questions to the right volunteers. The notifications sent to volunteers seem to have the greatest impact on how many questions are answered.
Your objective: develop a method to recommend relevant questions to the professionals who are most likely to answer them.
Criteria for Measuring Solutions
Performance: How well does the solution match professionals to the questions they would be motivated to answer? CareerVillage.org will not be able to live-test every submission, so a strong entry will clearly articulate why it will be effective at motivating answers.
Easy to implement: The CareerVillage.org team wants to put the winning submissions to work, quickly. A good entry will be well documented and easy to test in production.
Extensibility: In the future, CareerVillage.org aims to add more data features and to accommodate new objectives. Winning submissions should allow for this and other augmentations to be added in the future.","Evaluation
Your objective is to develop a method for recommending career advice questions to relevant volunteers that are most likely to answer them.
A valid submission will include:
Kernel Notebook: At least one kernel containing your method that recommends questions to volunteers. All kernels submitted must be made public on or before the submission deadline to be eligible. If submitting as a team, all team members must be listed as collaborators on all kernels submitted.
Criteria for Measuring Solutions
Performance: How well does the solution match questions to volunteers that would be most likely to answer? CareerVillage.org will not be able to live-test every submission, so a strong entry will clearly articulate why it is effective.
Easy to implement: The CareerVillage.org team wants to put the winning submissions to work quickly. A good entry will be well documented and easy to test in production.
Extensibility: In the future, CareerVillage.org aims to add more data features. The best submissions will make it easy to add new features and data not presently available.
Submissions are scored on these major sections:
Quality of documentation and implementation feasibility (0-5)
Quality of data insights and overall communication (0-5)
Quality of model, model features, and model insights (0-5)
Recommendations for future explorations and implementations (0-5)
Each section will be scored on a scale of 0-5 and then summed for each judge. The scale of summed scores is 0-20 where 20 is the highest score. Final scores will be the average of the summed scores from each judge.","CareerVillage.org has provided several years of anonymized data and each file comes from a table in their database.
answers.csv: Answers are what this is all about! Answers get posted in response to questions. Answers can only be posted by users who are registered as Professionals. However, if someone has changed their registration type after joining, they may show up as the author of an Answer even if they are no longer a Professional.
comments.csv: Comments can be made on Answers or Questions. We refer to whichever the comment is posted to as the ""parent"" of that comment. Comments can be posted by any type of user. Our favorite comments tend to have ""Thank you"" in them :)
emails.csv: Each email corresponds to one specific email to one specific recipient. The frequency_level refers to the type of email template which includes immediate emails sent right after a question is asked, daily digests, and weekly digests.
group_memberships.csv: Any type of user can join any group. There are only a handful of groups so far.
groups.csv: Each group has a ""type"". For privacy reasons we have to leave the group names off.
matches.csv: Each row tells you which questions were included in emails. If an email contains only one question, that email's ID will show up here only once. If an email contains 10 questions, that email's ID would show up here 10 times.
professionals.csv: We call our volunteers ""Professionals"", but we might as well call them Superheroes. They're the grown ups who volunteer their time to answer questions on the site.",kaggle competitions download -c data-science-for-good-careervillage,"['https://www.kaggle.com/code/niyamatalmass/lightfm-hybrid-recommendation-system', 'https://www.kaggle.com/code/danielbecker/careervillage-org-recommendation-engine', 'https://www.kaggle.com/code/ididur/nn-based-recommender-engine', 'https://www.kaggle.com/code/arjundas/when-i-grow-up-i-want-to-be', 'https://www.kaggle.com/code/ioohooi/eda-with-some-insights-data-er-diagram']"
260,"Medium voltage overhead power lines run for hundreds of miles to supply power to cities. These great distances make it expensive to manually inspect the lines for damage that doesn't immediately lead to a power outage, such as a tree branch hitting the line or a flaw in the insulator. These modes of damage lead to a phenomenon known as partial discharge — an electrical discharge which does not bridge the electrodes between an insulation system completely. Partial discharges slowly damage the power line, so left unrepaired they will eventually lead to a power outage or start a fire.
Your challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the ENET Centre at VŠB. Effective classifiers using this data will make it possible to continuously monitor power lines for faults.
ENET Centre researches and develops renewable energy resources with the goal of reducing or eliminating harmful environmental impacts. Their efforts focus on developing technology solutions around transportation and processing of energy raw materials.
By developing a solution to detect partial discharge you’ll help reduce maintenance costs, and prevent power outages.","Submissions are evaluated on the Matthews correlation coefficient (MCC) between the predicted and the observed response. The MCC is given by:
$$ MCC = \frac{(TP*TN) - (FP * FN)}{\sqrt{(TP+FP)(TP+FN)(TN + FP)(TN+FN)}}, $$
where TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives.
Submission File
For each signal in the test set, you must predict a binary prediction for the target variable. The file should contain a header and have the following format:
signal_id,target
0,0
1,1
2,0
etc.","Data Description
Faults in electric transmission lines can lead to a destructive phenomenon called partial discharge.
If left alone, partial discharges can damage equipment to the point that it stops functioning entirely. Your challenge is to detect partial discharges so that repairs can be made before any lasting harm occurs.
Each signal contains 800,000 measurements of a power line's voltage, taken over 20 milliseconds. As the underlying electric grid operates at 50 Hz, this means each signal covers a single complete grid cycle. The grid itself operates on a 3-phase power scheme, and all three phases are measured simultaneously.
File Descriptions
metadata_[train/test].csv
id_measurement: the ID code for a trio of signals recorded at the same time.
signal_id: the foreign key for the signal data. Each signal ID is unique across both train and test, so the first ID in train is '0' but the first ID in test is '8712'.
phase: the phase ID code within the signal trio. The phases may or may not all be impacted by a fault on the line.
target: 0 if the power line is undamaged, 1 if there is a fault.
[train/test].parquet - The signal data. Each column contains one signal; 800,000 int8 measurements as exported with pyarrow.parquet version 0.11. Please note that this is different than our usual data orientation of one row per observation; the switch makes it possible loading a subset of the signals efficiently. If you haven't worked with before, please refer to either .",kaggle competitions download -c vsb-power-line-fault-detection,"['https://www.kaggle.com/code/braquino/5-fold-lstm-attention-fully-commented-0-694', 'https://www.kaggle.com/code/afajohn/cnn-lstm-for-signal-classification-lb-0-513', 'https://www.kaggle.com/code/jackvial/dwt-signal-denoising', 'https://www.kaggle.com/code/mark4h/vsb-1st-place-solution', 'https://www.kaggle.com/code/sohier/reading-the-data-with-python']"
261,"As a result of the continued collaboration between Google Cloud and the NCAA, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.
As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!
This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of games played
\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2
\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins
\\( log() \\) is the natural (base e) logarithm
The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2019 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2 = 2,278 matchups.
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2014_1107_1110"" indicates team 1107 potentially played team 1110 in the year 2014. You must predict the probability that the team with the lower id beats the team with the higher id.
The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:
id,pred
2014_1107_1110,0.5
2014_1107_1112,0.5
2014_1107_1113,0.5
...","Data Description:
Note for Stage 2:
All of the data files have been updated through the end of the current regular season. You can find everything you need in these four files:

1) Stage2DataFiles.zip (this contains the same type of information as the DataFiles.zip did for Stage 1, but it now includes 2019 data as well)

2) MasseyOrdinals_thru_2019_day_128.zip (this contains the same type of information as MasseyOrdinals.zip did for Stage 1, but it now includes 2019 data as well). For the absolute latest version of the Massey Ordinals, see the Discussion thread ""Massey Ordinals Day 133 Thread"".

3) PlayByPlay_2019.zip (this contains the same type of information as PlayByPlay_2018.zip, etc., but it is play-by-play for the current 2019 season games)

4) SampleSubmissionStage2.csv (this has the proper number of rows, and the proper teams for the 2019 tourney only, which is all you predict in Stage 2)",kaggle competitions download -c mens-machine-learning-competition-2019,"['https://www.kaggle.com/code/jaseziv83/a-recent-deep-look-at-the-men-s-ncaab', 'https://www.kaggle.com/code/raddar/team-power-rankings', 'https://www.kaggle.com/code/addisonhoward/basic-starter-kernel-ncaa-men-s-dataset-2019', 'https://www.kaggle.com/code/ateplyuk/lgbm-str', 'https://www.kaggle.com/code/pozz13/ncaa-2k19-eda-zion-s-kingdom']"
262,"Most flight-related fatalities stem from a loss of “airplane state awareness.” That is, ineffective attention management on the part of pilots who may be distracted, sleepy or in other dangerous cognitive states.
Your challenge is to build a model to detect troubling events from aircrew’s physiological data. You'll use data acquired from actual pilots in test situations, and your models should be able to run calculations in real time to monitor the cognitive states of pilots. With your help, pilots could then be alerted when they enter a troubling state, preventing accidents and saving lives.
Reducing aircraft fatalities is just one of the complex problems that Booz Allen Hamilton has been solving for business, government, and military leaders for over 100 years. Through devotion, candor, courage, and character, they produce original solutions where there are no roadmaps. Now you can help them find answers, save lives, and change the world.","Submissions are evaluated on the Multi Class Log Loss between the predicted probabilities and the observed target.
The submitted probabilities are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \(max(min(p,1-10^{-15}),10^{-15})\).
Submission File
For each id in the test set, you must predict a probability for each of the 4 possible cognitive states ( A = baseline / no event, B = SS, C = CA, D = DA). The file must have a header and should look like the following:
id,A,B,C,D
0,1,0,0,0
1,1,0,0,0
2,1,0,0,0
etc.","In this dataset, you are provided with real physiological data from eighteen pilots who were subjected to various distracting events. The benchmark training set is comprised of a set of controlled experiments collected in a non-flight environment, outside of a flight simulator. The test set (abbreviated LOFT = Line Oriented Flight Training) consists of a full flight (take off, flight, and landing) in a flight simulator.
The pilots experienced distractions intended to induce one of the following three cognitive states:
Channelized Attention (CA) is, roughly speaking, the state of being focused on one task to the exclusion of all others. This is induced in benchmarking by having the subjects play an engaging puzzle-based video game.
Diverted Attention (DA) is the state of having one’s attention diverted by actions or thought processes associated with a decision. This is induced by having the subjects perform a display monitoring task. Periodically, a math problem showed up which had to be solved before returning to the monitoring task.
Startle/Surprise (SS) is induced by having the subjects watch movie clips with jump scares.
For each experiment, a pair of pilots (each with its own crew id) was recorded over time and subjected to the CA, DA, or SS cognitive states. The training set contains three experiments (one for each state) in which the pilots experienced just one of the states. For example, in the experiment = CA, the pilots were either in a baseline state (no event) or the CA state. The test set contains a full flight simulation during which the pilots could experience any of the states (but never more than one at a time). The goal of this competition is to predict the probability of each state for each time in the test set.
Each sensor operated at a sample rate of 256 Hz. Please note that since this is physiological data from real people, there will be
noise and artifacts in the data.",kaggle competitions download -c reducing-commercial-aviation-fatalities,"['https://www.kaggle.com/code/theoviel/starter-code-eda-and-lgbm-baseline', 'https://www.kaggle.com/code/stuartbman/introduction-to-physiological-data', 'https://www.kaggle.com/code/ashishpatel26/smote-with-model-lightgbm', 'https://www.kaggle.com/code/shahaffind/reducing-commercial-aviation-fatalities-11th', 'https://www.kaggle.com/code/vbookshelf/baseline-best-constant-to-predict-for-logloss']"
263,"As a result of the continued collaboration between Google Cloud and the NCAA®, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.
As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!
This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of games played
\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2
\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins
\\( log() \\) is the natural (base e) logarithm
The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2019 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict (64*63)/2  = 2,016 matchups. 
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_3104_3129"" indicates team 3104 played team 3129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id.
The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:
id,pred
2013_3103_3107,0.5
2013_3103_3112,0.5
2013_3103_3125,0.5
...","Data Description:
Note for Stage 2:
All of the data files have been updated through the end of the current regular season. You can find everything you need in these two files:
1) Stage2WDataFiles.zip (this contains the same type of information as the WDataFiles.zip did for Stage 1, but it now includes 2019 data as well)

2) WSampleSubmissionStage2.csv (this has the proper number of rows, and the proper teams for the 2019 tourney only, which is all you predict in Stage 2)
Each season there are thousands of NCAA® basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness® game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.",kaggle competitions download -c womens-machine-learning-competition-2019,"['https://www.kaggle.com/code/ateplyuk/lgbm-str-w', 'https://www.kaggle.com/code/jazivxt/courtside-seat-2019w-competitiveness', 'https://www.kaggle.com/code/akash14/google-march-madness', 'https://www.kaggle.com/code/akash14/google-cloud-ncaa-ml-competition-2019-women-s', 'https://www.kaggle.com/code/hamidhaghshenas/adaboostclassifier']"
264,"This isn't your classic decoder ring puzzle found in a cereal box. There's a twist.
Welcome to the Ciphertext Challenge! In this competition, we've encrypted parts of a well-known dataset -- the 20 Newsgroups dataset -- with several simple, classic ciphers. This dataset is commonly used as a multi-class and NLP sample set, noted for its small size, varied nature, and the first-hand look it offers into the deep existential horrors of the 90s-era internet. With 20 fairly distinct classes and lots of clues, it allows for a wide variety of successful approaches.
We've made the problem a little harder to solve.
Fabulous Kaggle swag will go to the top competitors - the highest-scoring teams (which might be the first to crack the code!), and the most popular kernel. Note that this is a short competition, so use your submissions wisely.
* = Note: It is possible to apply a number of techniques using ONLY the ciphertext.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice.
You can view and download the unencrypted dataset from Jason Rennie's homepage. In the words of the host:
The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection.
If you use the dataset in a scientific publication, please reference (at a minimum) the above website.
Photo by U.S. Air Force photo/Don Branum","Submissions will be evaluated based on their macro F1 score.
Submission File
For each ID in the test set, you must predict which newsgroup a particular item came from. The file should contain a header and have the following format:
Id,Predicted
ID_e93d1d4c6,0
ID_f5f7560ec,0
ID_6ebe8f07f,0
ID_26222d9b7,0
ID_a0632653b,0
ID_641f090da,0
ID_888b6c7a5,0
ID_7b5b3bb47,0
ID_70f08430c,0
etc.","What terms should I be aware of?
A cipher is an algorithm that transforms a readable piece of information into an unreadable one (and in most cases vice versa). Modern encryption algorithms - AES, DES, etc. - are examples of commonly used ciphers.
In this case, we've used simple (or classic) ciphers. For as long as information has existed, methods have been used to obscure the information's meaning. Before computers were common, they usually consisted of a series of simple operations performed on text. In this data you'll encounter multiple text-transformation ciphers that date back centuries, if not longer.
Text that has been encrypted is referred to as ciphertext. (Unencrypted text is usually called plaintext.)
The exact cipher(s) used to encrypt this data will be revealed at the end of the competition - if they're not discovered first!
How has the text been encrypted?
Each document has been encrypted with up to 4 layered ciphers (ciphers 1, 2, 3, and 4). We've used a difficulty value here to denote which ciphers were used for a particular piece of text.
Every document in the dataset was broken into sequential 300-character chunks, and all chunks for the document were then encrypted based on its difficulty level. A difficulty of 1 means that only cipher #1 was used. A difficulty of 2 means cipher #1 was applied, followed by cipher #2, and so on. The difficulty level denotes exactly which ciphers were applied, and in what order.",kaggle competitions download -c 20-newsgroups-ciphertext-challenge,"['https://www.kaggle.com/code/ashishpatel26/beginner-to-intermediate-nlp-tutorial', 'https://www.kaggle.com/code/ashishpatel26/attension-layer-basic-for-nlp', 'https://www.kaggle.com/code/rturley/a-first-crack-tools-tips-3-cipher-solutions', 'https://www.kaggle.com/code/ashishpatel26/everything-you-want-to-know-about-20-ngctc', 'https://www.kaggle.com/code/suicaokhoailang/one-model-for-each-difficulty-0-3691-lb']"
265,"At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.
Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?
In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each Id in the test set, you must make a binary prediction of the target variable. The file should contain a header and have the following format:
 ID_code,target
 test_0,0
 test_1,1
 test_2,0
 etc.","You are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.
The task is to predict the value of target column in the test set.
File descriptions
train.csv - the training set.
test.csv - the test set. The test set contains some rows which are not included in scoring.
sample_submission.csv - a sample submission file in the correct format.",kaggle competitions download -c santander-customer-transaction-prediction,"['https://www.kaggle.com/code/gpreda/santander-eda-and-prediction', 'https://www.kaggle.com/code/yag320/list-of-fake-samples-and-public-private-lb-split', 'https://www.kaggle.com/code/jiweiliu/lgb-2-leaves-augment', 'https://www.kaggle.com/code/cdeotte/200-magical-models-santander-0-920', 'https://www.kaggle.com/code/mjbahmani/santander-ml-explainability']"
266,"The malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways.
With more than one billion enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security.
As one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences.
Can you help protect more than one billion machines from damage BEFORE it happens?
Acknowledgements
This competition is hosted by Microsoft, Windows Defender ATP Research, Northeastern University College of Computer and Information Science, and Georgia Tech Institute for Information Security & Privacy.
Microsoft contacts
Rob McCann (Robert.McCann@microsoft.com)
Christian Seifert (chriseif@microsoft.com)
Susan Higgs (Susan.Higgs@microsoft.com)
Matt Duncan (Matthew.Duncan@microsoft.com)
Northeastern University contact
Mansour Ahmadi (m.ahmadi@northeastern.edu)
Georgia Tech contacts
Brendan Saltaformaggio (brendan@ece.gatech.edu)
Taesoo Kim (taesoo@gatech.edu)","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed label.
Submission File
For each MachineIdentifier in the test set, you must predict a probability for the HasDetections column. The file should contain a header and have the following format:
MachineIdentifier,HasDetections
1,0.5
6,0.5
14,0.5
etc.","The goal of this competition is to predict a Windows machine’s probability of getting infected by various families of malware, based on different properties of that machine. The telemetry data containing these properties and the machine infections was generated by combining heartbeat and threat reports collected by Microsoft's endpoint protection solution, Windows Defender.
Each row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine. Using the information and labels in train.csv, you must predict the value for HasDetections for each machine in test.csv.
The sampling methodology used to create this dataset was designed to meet certain business constraints, both in regards to user privacy as well as the time period during which the machine was running. Malware detection is inherently a time-series problem, but it is made complicated by the introduction of new machines, machines that come online and offline, machines that receive patches, machines that receive new operating systems, etc. While the dataset provided here has been roughly split by time, the complications and sampling requirements mentioned above may mean you may see imperfect agreement between your cross validation, public, and private scores! Additionally, this dataset is not representative of Microsoft customers’ machines in the wild; it has been sampled to include a much larger proportion of malware machines.
Columns
Unavailable or self-documenting column names are marked with an ""NA"".",kaggle competitions download -c microsoft-malware-prediction,"['https://www.kaggle.com/code/bogorodvo/lightgbm-baseline-model-using-sparse-matrix', 'https://www.kaggle.com/code/theoviel/load-the-totality-of-the-data', 'https://www.kaggle.com/code/artgor/is-this-malware-eda-fe-and-lgb-updated', 'https://www.kaggle.com/code/fabiendaniel/detecting-malwares-with-lgbm', 'https://www.kaggle.com/code/youhanlee/my-eda-i-want-to-see-all']"
267,"Long ago, in the distant, fragrant mists of time, there was a competition…
It was not just any competition.
It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples… without overfitting.
Data scientists ― including Kaggle's very own Will Cukierski ― competed by the hundreds. Legends were made. (Will took 5th place, and eventually ended up working at Kaggle!) People overfit like crazy. It was a Kaggle-y, data science-y madhouse.
So… we're doing it again.
Don't Overfit II: The Overfittening
This is the next logical step in the evolution of weird competitions. Once again we have 20,000 rows of continuous variables, and a mere handful of training samples. Once again, we challenge you not to overfit. Do your best, model without overfitting, and add, perhaps, to your own legend.
In addition to bragging rights, the winner also gets swag. Enjoy!
Acknowledgments
We hereby salute the hard work that went into the original competition, created by Phil Brierly. Thank you!","Submissions are evaluated using AUCROC between the predicted target and the actual target value.
Submission File
For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
 id,target
 300,0
 301,0
 302,0
 303,0
 304,0
 305,0
 306,0
 307,0
 308,0","What am I predicting?
You are predicting the binary target associated with each row, without overfitting to the minimal set of training examples provided.
Files
train.csv - the training set. 250 rows.
test.csv - the test set. 19,750 rows.
sample_submission.csv - a sample submission file in the correct format
Columns
id- sample id
target- a binary target of mysterious origin.
0-299- continuous variables.",kaggle competitions download -c dont-overfit-ii,"['https://www.kaggle.com/code/artgor/how-to-not-overfit', 'https://www.kaggle.com/code/rafjaa/dealing-with-very-small-datasets', 'https://www.kaggle.com/code/cdeotte/lb-probing-strategies-0-890-2nd-place', 'https://www.kaggle.com/code/featureblind/robust-lasso-patches-with-rfe-gs', 'https://www.kaggle.com/code/cdeotte/fun-data-animation']"
268,"Welcome
In this challenge you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. Instead, this challenge asks you to use data to propose specific rule modifications for the NFL that aim to reduce the occurrence of concussions during punt plays. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, presented by Arrow Electronics – the NFL’s annual Super Bowl competition designed to spur innovation in player health, safety and performance.
The Challenge
For the 2018 season, the NFL revised their kickoff rules in an effort to reduce the risk of injury during those plays. By examining injury reports, player position and velocity data, and game video, they were able to understand the game-play circumstances that may exacerbate the risk of injury to players.
This comprehensive review showed that over the course of all games during the 2015-2017 seasons, the kickoff represented only six percent of plays but 12 percent of concussions. Players had approximately four times the risk of concussion on returned kickoffs compared to running or passing plays. The changes to the kickoff rule aim to address the components that posed the most risk, like the use of a two-man wedge.
Now, the NFL is challenging Kagglers to help them perform the same examination, this time on punt play rules. They have provided data for all punt plays from the 2016 and 2017 NFL seasons that includes player rosters, on-field position data and video data, including the plays in which a player suffered a concussion.
Your challenge is to propose specific rule modifications (e.g. changes to the initial formation, tackling techniques, blocking rules etc.), supported by data, that may reduce the occurrence of concussions during punt plays. More details on the entry criteria are available in Overview tab > Evaluation.
About The NFL
The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country.
The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to look at anything and everything to protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played.
As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries.
For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com.
Evaluation","Evaluation
Your challenge is to propose specific rule modifications (e.g. changes to the initial formation, tackling techniques, blocking etc.), supported by data, that may reduce the occurrence of concussions during punt plays.
A valid submission will include:
Summary Slides: A summary of the proposed rule change presented in slide format and delivered as either a PDF or PPT file.
Kernels Analysis: At least one kernel containing the analysis to support your proposal. All kernels submitted must be made public on or before the submission deadline to be eligible. If submitting as a team, all team members must be listed as collaborators on all kernels submitted.
Submissions will be judged by the NFL on the following criteria:
Solution Efficacy - Have you clearly demonstrated, through your data analysis, that you have an understanding of what play features may be associated with concussions and how your proposed rule change(s) will reduce these injuries? Your kernels should be easy to understand and the analysis should be reproducible.
Game Integrity - Is your proposal actionable by the NFL? Could they implement your rule change and still maintain the integrity of the game? Have you considered the way your proposed changes to game dynamics could introduce new risks to player safety? Strong submissions will demonstrate an understanding for the game overall.","The following data is provided for NFL seasons 2016 to 2017.
Each dataset can be merged on the game, play or player level using the provided key variables. GameKey provides a unique identifier for a specific game which is unique across NFL seasons. PlayID identifies a unique play within a specified GameKey. GSISID provides a unique identifier for a player across all seasons.
File descriptions
Game Data: Game level data that specifies the type of season (pre, reg, post), week, and hosting city and team. Each game is uniquely identified across all seasons using GameKey.
Play Information: Play level data that describes the type of play, possession team, score and a brief narrative of each play. Plays are uniquely identified using a its PlayID along with the corresponding GameKey. PlayIDs are not unique.
Player Punt Data: Player level data that specifies the traditional football position for each player. Each player is identified using his GSISID.
Play Player Role Data: Play and player level data that specifies a punt specific player role. This dataset will specify each player that played in each play. A player’s role in a play is uniquely defined by the Gamekey PlayID and GSISID.
Video Review: Injury level data that provides a detailed description of the concussion-producing event. Video Review data are only available in cases in which the injury play can be identified. Each video review case can be identified using a combination of GameKey, PlayID, and GSISID. A brief narrative of the play events is provided.",kaggle competitions download -c NFL-Punt-Analytics-Competition,"['https://www.kaggle.com/code/jpmiller/nfl-punt-analytics', 'https://www.kaggle.com/code/robikscube/evolving-the-punt-play-nfl-data-formal-report', 'https://www.kaggle.com/code/crawford/nfl-punt-analytics-starter-kernel', 'https://www.kaggle.com/code/robikscube/nfl-punt-data-interactive-plots-using-bokeh', 'https://www.kaggle.com/code/gaborfodor/nfl-rules-wordcloud']"
269,"We're going to make you an offer you can't refuse: a Kaggle competition!
In a world… where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's ""You had me at 'Hello.'"" For others, the trailer falls short of expectations and you think ""What we have here is a failure to communicate.""
In this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release.
Join in, ""make our day"", and then ""you've got to ask yourself one question: 'Do I feel lucky?'""","It is your job to predict the international box office revenue for each movie. For each id in the test set, you must predict the value of the revenue variable. 
Submissions are evaluated on Root-Mean-Squared-Logarithmic-Error (RMSLE) between the predicted value and the actual revenue. Logs are taken to not overweight blockbuster revenue movies.
Submission File Format
The file should contain a header and have the following format:
id,revenue
1461,1000000
1462,50000
1463,800000000
etc.
You can download an example submission file (sample_submission.csv) on the Data page.","In this dataset, you are provided with 7398 movies and a variety of metadata obtained from The Movie Database (TMDB). Movies are labeled with id. Data points include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries.
You are predicting the worldwide revenue for 4398 movies in the test file.
Note - many movies are remade over the years, therefore it may seem like multiple instance of a movie may appear in the data, however they are different and should be considered separate movies. In addition, some movies may share a title, but be entirely unrelated.
E.g. The Karate Kid (id: 5266) was released in 1986, while a clearly (or maybe just subjectively) inferior remake (id: 1987) was released in 2010.
Also, while the Frozen (id: 5295) released by Disney in 2013 may be the household name, don't forget about the less-popular Frozen (id: 139) released three years earlier about skiers who are stranded on a chairlift…
Acknowledgements
This dataset has been collected from TMDB. The movie details, credits and keywords have been collected from the TMDB Open API. This competition uses the TMDB API but is not endorsed or certified by TMDB. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself .",kaggle competitions download -c tmdb-box-office-prediction,"['https://www.kaggle.com/code/artgor/eda-feature-engineering-and-model-interpretation', 'https://www.kaggle.com/code/kamalchhirang/eda-feature-engineering-lgb-xgb-cat', 'https://www.kaggle.com/code/tavoosi/predicting-box-office-revenue-with-random-forest', 'https://www.kaggle.com/code/ashishpatel26/now-you-see-me', 'https://www.kaggle.com/code/somang1418/eda-lgb-xgb-modelings-with-a-cute-panda-meme']"
270,"After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.
To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.
In this competition, you’re challenged to build an algorithm to identify individual whales in images. You’ll analyze Happywhale’s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you’ll help to open rich fields of understanding for marine mammal population dynamics around the globe.
Note, this competition is similar in nature to this competition with an expanded and updated dataset.
We'd like to thank Happywhale for providing this data and problem. Happywhale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):
$$MAP@5 = \frac{1}{U} \sum_{u=1}^{U} \sum_{k=1}^{min(n,5)} P(k) \times rel(k)$$
where \( U \) is the number of images, \( P(k) \) is the precision at cutoff \( k \), \( n \) is the number predictions per image, and \( rel(k) \) is an indicator function equaling 1 if the item at rank \( k \) is a relevant (correct) label, zero otherwise.
Once a correct label has been scored for an observation, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0.
[A, B, C, D, E]
[A, A, A, A, A]
[A, B, A, C, A]
Submission File
For each Image in the test set, you may predict up to 5 labels for the whale Id. Whales that are not predicted to be one of the labels in the training data should be labeled as new_whale. The file should contain a header and have the following format:
Image,Id 
00028a005.jpg,new_whale w_23a388d w_9b5109b w_9c506f6 w_0369a5c 
000dcf7d8.jpg,new_whale w_23a388d w_9b5109b w_9c506f6 w_0369a5c 
...","This training data contains thousands of images of humpback whale flukes. Individual whales have been identified by researchers and given an Id. The challenge is to predict the whale Id of images in the test set. What makes this such a challenge is that there are only a few examples for each of 3,000+ whale Ids.
File descriptions
train.zip - a folder containing the training images
train.csv - maps the training Image to the appropriate whale Id. Whales that are not predicted to have a label identified in the training data should be labeled as new_whale.
test.zip - a folder containing the test images to predict the whale Id
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c humpback-whale-identification,
271,"Imagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner!
Right now, Elo, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key.
Elo has built machine learning models to understand the most important aspects and preferences in their customers’ lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in.
In this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers’ lives and help Elo reduce unwanted campaigns, to create the right experience for customers.","Root Mean Squared Error (RMSE)
Submissions are scored on the root mean squared error. RMSE is defined as:
\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]
where \( \hat{y} \) is the predicted loyalty score for each card_id, and \( y \) is the actual loyalty score assigned to a card_id.
Submission File
card_id, target
C_ID_9e86007114,0
C_ID_1c9f77086c,0.5
C_ID_07b20e9908,0
C_ID_63d6bac69a,0
C_ID_bbc26a86eb,0
C_ID_f749aad790,0
C_ID_7b5c15ff41,-0.25
C_ID_ec6b0f2d30,0
C_ID_0a11e759c5,0","Note: All data is simulated and fictitious, and is not real customer data
What files do I need?
You will need, at a minimum, the train.csv and test.csv files. These contain the card_ids that we'll be using for training and prediction.
The historical_transactions.csv and new_merchant_transactions.csv files contain information about each card's transactions. historical_transactions.csv contains up to 3 months' worth of transactions for every card at any of the provided merchant_ids. new_merchant_transactions.csv contains the transactions at new merchants (merchant_ids that this particular card_id has not yet visited) over a period of two months.
merchants.csv contains aggregate information for each merchant_id represented in the data set.
What should I expect the data format to be?
The data is formatted as follows:
train.csv and test.csv contain card_ids and information about the card itself - the first month the card was active, etc. train.csv also contains the target.
historical_transactions.csv and new_merchant_transactions.csv are designed to be joined with train.csv, test.csv, and merchants.csv. They contain information about transactions for each card, as described above.",kaggle competitions download -c elo-merchant-category-recommendation,
272,,,,,
273,,,,,
274,,,,,
275,,,,,
276,"So, where we droppin' boys and girls?
Battle Royale-style video games have taken the world by storm. 100 players are dropped onto an island empty-handed and must explore, scavenge, and eliminate other players until only one is left standing, all while the play zone continues to shrink.
PlayerUnknown's BattleGrounds (PUBG) has enjoyed massive popularity. With over 50 million copies sold, it's the fifth best selling game of all time, and has millions of active monthly players.
The team at PUBG has made official game data available for the public to explore and scavenge outside of ""The Blue Circle."" This competition is not an official or affiliated PUBG site - Kaggle collected data made possible through the PUBG Developer API.
You are given over 65,000 games' worth of anonymized player data, split into training and testing sets, and asked to predict final placement from final in-game stats and initial player ratings.
What's the best strategy to win in PUBG? Should you sit in one spot and hide your way into victory, or do you need to be the top shot? Let's let the data do the talking!","Submissions are evaluated on Mean Absolute Error between your predicted winPlacePerc and the observed winPlacePerc.
Submission File
For each Id in the test set, you must predict their placement as a percentage (0 for last, 1 for first place) for the winPlacePerc variable. The file should contain a header and have the following format:
  Id,winPlacePerc
  47734,0
  47735,0.5
  47736,0
  47737,1
  etc.
See sample_submission.csv on the data page for a full sample submission.","In a PUBG game, up to 100 players start in each match (matchId). Players can be on teams (groupId) which get ranked at the end of the game (winPlacePerc) based on how many other teams are still alive when they are eliminated. In game, players can pick up different munitions, revive downed-but-not-out (knocked) teammates, drive vehicles, swim, run, shoot, and experience all of the consequences -- such as falling too far or running themselves over and eliminating themselves.
You are provided with a large number of anonymized PUBG game stats, formatted so that each row contains one player's post-game stats. The data comes from matches of all types: solos, duos, squads, and custom; there is no guarantee of there being 100 players per match, nor at most 4 player per group.
You must create a model which predicts players' finishing placement based on their final stats, on a scale from 1 (first place) to 0 (last place).
File descriptions
train_V2.csv - the training set
test_V2.csv - the test set
sample_submission_V2.csv - a sample submission file in the correct format
Data fields
DBNOs - Number of enemy players knocked.",kaggle competitions download -c pubg-finish-placement-prediction,"['https://www.kaggle.com/code/deffro/eda-is-fun', 'https://www.kaggle.com/code/carlolepelaars/pubg-data-exploration-rf-funny-gifs', 'https://www.kaggle.com/code/rejasupotaro/effective-feature-engineering', 'https://www.kaggle.com/code/plasticgrammer/pubg-finish-placement-prediction-playground', 'https://www.kaggle.com/code/chocozzz/pubg-data-description-a-to-z-fe-with-python']"
277,"In this competition, Kagglers will develop models capable of classifying mixed patterns of proteins in microscope images. The Human Protein Atlas will use these models to build a tool integrated with their smart-microscopy system to identify a protein's location(s) from a high-throughput image.
Proteins are “the doers” in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells.
Images visualizing proteins in cells are commonly used for biomedical research, and these cells could hold the key for the next breakthrough in medicine. However, thanks to advances in high-throughput microscopy, these images are generated at a far greater pace than what can be manually evaluated. Therefore, the need is greater than ever for automating biomedical image analysis to accelerate the understanding of human cells and disease.
  Nature Methods has indicated interest in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team would like to invite top performing teams to join as co-authors in the writing of this paper.
Top performing teams will also be eligible to compete for the special prize. Additional information for both the special prize and co-authoring for Nature Methods will become available through the Discussion posts once the main competition is complete.
  Acknowledgements
The Human Protein Atlas is a Sweden-based initiative aimed at mapping all human proteins in cells, tissues and organs. All the data in the knowledge resource is open access to allow anyone to pursue exploration of the human proteome. In a recent publication, the Human Protein Atlas team has demonstrated the promise of both citizen science and artificial intelligence approaches in describing the location of human proteins in images, however current results are yet to approach expert-level annotations (Sullivan et al, Nature Biotechnology, Oct 2018).","Submissions will be evaluated based on their macro F1 score.
Submission File
For each Id in the test set, you must predict a class for the Target variable as described in the data page. Note that multiple labels can be predicted for each sample.
The file should contain a header and have the following format:
Id,Predicted  
00008af0-bad0-11e8-b2b8-ac1f6b6435d0,0 1  
0000a892-bacf-11e8-b2b8-ac1f6b6435d0,2 3
0006faa6-bac7-11e8-b2b7-ac1f6b6435d0,0  
0008baca-bad7-11e8-b2b9-ac1f6b6435d0,0  
000cce7e-bad4-11e8-b2b8-ac1f6b6435d0,0  
00109f6a-bac8-11e8-b2b7-ac1f6b6435d0,1 28  
...","What files do I need?
You will need to download a copy of the images. Due to size, we have provided two versions of the same images. On the data page below, you will find a scaled set of 512x512 PNG files in train.zip and test.zip. Alternatively, if you wish to work with full size original images (a mix of 2048x2048 and 3072x3072 TIFF files) you may download train_full_size.7z and test_full_size.7z from here (warning: these are ~250 GB total).
You will also need the training labels from train.csv and the filenames for the test set from sample_submission.csv.
What should I expect the data format to be?
The data format is two-fold - first, the labels are provided for each sample in train.csv.
The bulk of the data is in the images - train.zip and test.zip. Within each of these is a folder containing four files per sample. Each file represents a different filter on the subcellular protein patterns represented by the sample. The format should be [filename]_[filter color].png for the PNG files, and [filename]_[filter color].tif for the TIFF files.
What am I predicting?
You are predicting protein organelle localization labels for each sample. There are in total 28 different labels present in the dataset. The dataset is acquired in a highly standardized way using one imaging modality (confocal microscopy). However, the dataset comprises 27 different cell types of highly different morphology, which affect the protein patterns of the different organelles. All image samples are represented by four filters (stored as individual files), the protein of interest (green) plus three cellular landmarks: nucleus (blue), microtubules (red), endoplasmic reticulum (yellow). The green filter should hence be used to predict the label, and the other filters are used as references.",kaggle competitions download -c human-protein-atlas-image-classification,"['https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline', 'https://www.kaggle.com/code/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb', 'https://www.kaggle.com/code/rejpalcz/best-loss-function-for-f1-score-metric', 'https://www.kaggle.com/code/allunia/uncover-target-correlations-with-bernoulli-mixture', 'https://www.kaggle.com/code/byrachonok/pretrained-inceptionresnetv2-base-classifier']"
278,"In this competition, you’re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.
Here’s the backstory and why solving this problem matters.
Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country.
While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis.
CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift.
To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA®) has reached out to Kaggle’s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge.
The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.
Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018.
Acknowledgements
Thank you to the National Institutes of Health Clinical Center for publicly providing the Chest X-Ray dataset [5].
NIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community
Original source files and documents
Also, a big thank you to the competition organizers!
References
Rui P, Kang K. National Ambulatory Medical Care Survey: 2015 Emergency Department Summary Tables. Table 27. Available from: www.cdc.gov/nchs/data/nhamcs/web_tables/2015_ed_web_tables.pdf
Deaths: Final Data for 2015. Supplemental Tables. Tables I-21, I-22. Available from: www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_06_tables.pdf
Franquet T. Imaging of community-acquired pneumonia. J Thorac Imaging 2018 (epub ahead of print). PMID 30036297
Kelly B. The Chest Radiograph. Ulster Med J 2012;81(3):143-148
Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a set of predicted bounding boxes and ground truth bounding boxes is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.4 to 0.75 with a step size of 0.05: (0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75). In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.
At each threshold value \(t\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object.
Important note: if there are no ground truth objects at all for a given image, ANY number of predictions (false positives) will result in the image receiving a score of zero, and being included in the mean average precision.
The average precision of a single image is calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
In your submission, you are also asked to provide a confidence level for each bounding box. Bounding boxes will be evaluated in order of their confidence levels in the above process. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.
NOTE: In nearly all cases confidence will have no impact on scoring. It exists primarily to allow for submission boxes to be evaluated in a particular order to resolve extreme edge cases. None of these edge cases are known to exist in the data set. If you do not wish to use or calculate confidence you can use a placeholder value - like 1.0 - to indicate that no particular order applies to the evaluation of your submission boxes.
Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.
Intersection over Union (IoU)
Intersection over Union is a measure of the magnitude of overlap between two bounding boxes (or, in the more general case, two objects). It calculates the size of the overlap between two objects, divided by the total area of the two objects combined.
It can be visualized as the following:
The two boxes in the visualization overlap, but the area of the overlap is insubstantial compared with the area taken up by both objects together. IoU would be low - and would likely not count as a ""hit"" at higher IoU thresholds.
Submission File
The submission format requires a space delimited set of bounding boxes. For example:
0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100
indicates that image 0004cfab-14fd-4e49-80ba-63a80b6bddd6 has a bounding box with a confidence of 0.5, at x == 0 and y == 0, with a width and height of 100.
The file should contain a header and have the following format. Each row in your submission should contain ALL bounding boxes for a given image.
patientId,PredictionString
0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100
00313ee0-9eaa-42f4-b0ab-c148ed3241cd,
00322d4d-1c29-4943-afc9-b6754be640eb,0.8 10 10 50 50 0.75 100 100 5 5
etc...","STAGE 2 UPDATE
Note that new files are available to download! The training set now contains both the train and test set from stage 1. The test set is comprised of new, unseen images. The metric and file formats remain the same, but you'll now be making predictions using the updated train and test sets. We have an FAQ about two-stage competitions that provides some context for how this all works. Please give it a read.
Good luck!
What files do I need?
This is a two-stage challenge. You will need the images for the current stage - provided as stage_2_train_images.zip and stage_2_test_images.zip. You will also need the training data - stage_2_train_labels.csv - and the sample submission stage_2_sample_submission.csv, which provides the IDs for the test set, as well as a sample of what your submission should look like. The file stage_2_detailed_class_info.csv contains detailed information about the positive and negative classes in the training set, and may be used to build more nuanced models.
What should I expect the data format to be?
The training data is provided as a set of patientIds and bounding boxes. Bounding boxes are defined as follows:
x-min y-min width height",kaggle competitions download -c rsna-pneumonia-detection-challenge,"['https://www.kaggle.com/code/peterchang77/exploratory-data-analysis', 'https://www.kaggle.com/code/kmader/lung-opacity-overview', 'https://www.kaggle.com/code/jonnedtc/cnn-segmentation-connected-components', 'https://www.kaggle.com/code/hmendonca/mask-rcnn-and-coco-transfer-learning-lb-0-155', 'https://www.kaggle.com/code/seohyeondeok/yolov3-rsna-starting-notebook']"
279,"Help some of the world's leading astronomers grasp the deepest properties of the universe.
The human eye has been the arbiter for the classification of astronomical sources in the night sky for hundreds of years. But a new facility -- the Large Synoptic Survey Telescope (LSST) -- is about to revolutionize the field, discovering 10 to 100 times more astronomical sources that vary in the night sky than we've ever known. Some of these sources will be completely unprecedented!
The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) asks Kagglers to help prepare to classify the data from this new survey. Competitors will classify astronomical sources that vary with time into different classes, scaling from a small training set to a very large test set of the type the LSST will discover.
More background information is available here.
Acknowledgements
PLAsTiCC is funded through LSST Corporation Grant Award # 2017-03 and administered by the University of Toronto. Financial support for LSST comes from the National Science Foundation (NSF) through Cooperative Agreement No. 1258333, the Department of Energy (DOE) Office of Science under Contract No. DE-AC02-76SF00515, and private funding raised by the LSST Corporation. The NSF-funded LSST Project Office for construction was established as an operating center under management of the Association of Universities for Research in Astronomy (AURA). The DOE-funded effort to build the LSST camera is managed by the SLAC National Accelerator Laboratory (SLAC).
The National Science Foundation (NSF) is an independent federal agency created by Congress in 1950 to promote the progress of science. NSF supports basic research and people to create knowledge that transforms the future.


Photo Credit: M. Park/Inigo Films/LSST/AURA/NSF
[1]: https://arxiv.org/abs/1810.00001","Submissions are evaluated using a weighted multi-class logarithmic loss. The overall effect is such that each class is roughly equally important for the final score.
Each object has been labeled with one type. For each object, you must submit a set of predicted probabilities (one for every category). The formula is then
$$\text{Log Loss} = - \left( \frac{\sum^{M}_{i=1} w_{i} \cdot \sum_{j=1}^{N_{i}} \frac{y_{ij}}{N_{i}} \cdot \ln p_{ij} }{\sum^{M}_{i=1} w_{i}} \right)$$
where N is the number of objects in the class set, M is the number of classes,  \\(ln\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation (i) belongs to class (j) and 0 otherwise, \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given object are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission File
For each object ID in the test set, you must predict a probability for each of the different possible classes. The file should contain a header and have the following format:
object_id,class_6,class_15,class_16,class_42,class_52,class_53,class_62,class_64,class_65,class_67,class_88,class_90,class_92,class_95,class_99
13,0,0.1,0,0.1,0,0.3,0,0,0,0,0,0.5,0,0,0
14,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0
17,0.75,0.23,0,0,0.01,0,0,0,0,0.01,0,0,0,0,0
etc.","File Descriptions
data_note: Start here for a more extensive context than we can provide in this space. You'll get helpful background on both this dataset and the relevant astronomy. This is also available on arxiv.org. We've also posted kernels with additional context about the relevant astronomy and the approaches astronomers have taken in the past.
[training/test]_set_metadata: Information about objects in the that doesn't change over time, like the coordinates of the object.
[training/test]_set: Time series of observations of the objects. Maps to the metadata via object_id.
test_set_sample: The first million rows of the test set.
sample_submission: A valid submission file. You can submit probabilities of precision float64, but we highly recommend sticking to float16 for your initial iterations to minimize upload time.
Metadata File Column Descriptions
object_id: unique object identifier. Int32
ra: right ascension, sky coordinate: co-longitude in degrees. Float32
decl: declination, sky coordinate: co-latitude in degrees. Float32
gal_l: galactic longitude in degrees. Float32",kaggle competitions download -c PLAsTiCC-2018,"['https://www.kaggle.com/code/billstam123/convolutional-autoencoding-of-the-time-seri-a76ec1', 'https://www.kaggle.com/code/zongzelee/quantum-test', 'https://www.kaggle.com/code/alexandrelacour/plasticc-lulu', 'https://www.kaggle.com/code/almamun4r/plasticc-2018', 'https://www.kaggle.com/code/sherrysheng97/plasticc-dataset-investigation']"
280,"Airbus is excited to challenge Kagglers to build a model that detects all ships in satellite images as quickly as possible. Can you find them even in imagery with clouds or haze?
Here’s the backstory: Shipping traffic is growing fast. More ships increase the chances of infractions at sea like environmentally devastating ship accidents, piracy, illegal fishing, drug trafficking, and illegal cargo movement. This has compelled many organizations, from environmental protection agencies to insurance companies and national government authorities, to have a closer watch over the open seas.
Airbus offers comprehensive maritime monitoring services by building a meaningful solution for wide coverage, fine details, intensive monitoring, premium reactivity and interpretation response. Combining its proprietary-data with highly-trained analysts, they help to support the maritime industry to increase knowledge, anticipate threats, trigger alerts, and improve efficiency at sea.
A lot of work has been done over the last 10 years to automatically extract objects from satellite images with significative results but no effective operational effects. Now Airbus is turning to Kagglers to increase the accuracy and speed of automatic ship detection.
Algorithm Speed Prize: After the Kaggle challenge is complete, competitors may submit their model via a private Kaggle kernel for a speed evaluation based upon the inference time on over 40.000 images chips (typical size of a full satellite image) to win a special algorithm speed prize.
  If you're interested to explore more Airbus data, you are welcomed to check out the OneAtlas Sandbox. And for more insights on our Maritime Surveillance capabilities, have a look at Airbus Intelligence page.","This competition is evaluated on the F2 Score at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an F2 Score. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.
At each threshold value \(t\), the F2 Score value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects. The following equation is equivalent to F2 Score when \( \beta \) is set to 2:
$$
F_\beta(t) = \frac{(1 + \beta^2) \cdot TP(t)}{(1 + \beta^2) \cdot TP(t) + \beta^2 \cdot FN(t) + FP(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average F2 Score of a single image is then calculated as the mean of the above F2 Score values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t F_2(t).
$$
Lastly, the score returned by the competition metric is the mean taken over the individual average F2 Scores of each image in the test dataset.
Kernel Submissions
You can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.
Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed
and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc. A prediction of of ""no ship in image"" should have a blank value in the EncodedPixels column.
The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.
The file should contain a header and have the following format. Each row in your submission represents a single predicted ship segmentation for the given image.
  ImageId,EncodedPixels
  00002bd58.jpg,1 3
  00015efb6.jpg,
  00023d5fc.jpg,1 3 10 5
  etc.","In this competition, you are required to locate ships in images, and put an aligned bounding box segment around the ships you locate. Many images do not contain ships, and those that do may contain multiple ships. Ships within and across images may differ in size (sometimes significantly) and be located in open sea, at docks, marinas, etc.
For this metric, object segments cannot overlap. There were a small percentage of images in both the Train and Test set that had slight overlap of object segments when ships were directly next to each other. Any segments overlaps were removed by setting them to background (i.e., non-ship) encoding. Therefore, some images have a ground truth may be an aligned bounding box with some pixels removed from an edge of the segment. These small adjustments will have a minimal impact on scoring, since the scoring evaluates over increasing overlap thresholds.
The train_ship_segmentations.csv file provides the ground truth (in run-length encoding format) for the training images. The sample_submission files contains the images in the test images.
Please click on each file / folder in the Data Sources section to get more information about the files.",kaggle competitions download -c airbus-ship-detection,"['https://www.kaggle.com/code/inversion/run-length-decoding-quick-start', 'https://www.kaggle.com/code/kmader/baseline-u-net-model-part-1', 'https://www.kaggle.com/code/meaninglesslives/airbus-ship-detection-data-visualization', 'https://www.kaggle.com/code/iafoss/unet34-submission-tta-0-699-new-public-lb', 'https://www.kaggle.com/code/iafoss/unet34-dice-0-87']"
281,"""Quick, Draw!"" was released as an experimental game to educate the public in a playful way about how AI works. The game prompts users to draw an image depicting a certain category, such as ”banana,” “table,” etc. The game generated more than 1B drawings, of which a subset was publicly released as the basis for this competition’s training set. That subset contains 50M drawings encompassing 340 label categories.
Sounds fun, right? Here's the challenge: since the training data comes from the game itself, drawings can be incomplete or may not match the label. You’ll need to build a recognizer that can effectively learn from this noisy data and perform well on a manually-labeled test set from a different distribution.
Your task is to build a better classifier for the existing Quick, Draw! dataset. By advancing models on this dataset, Kagglers can improve pattern recognition solutions more broadly. This will have an immediate impact on handwriting recognition and its robust applications in areas including OCR (Optical Character Recognition), ASR (Automatic Speech Recognition) & NLP (Natural Language Processing).","Submissions are evaluated according to the Mean Average Precision @ 3 (MAP@3):
$$MAP@3 = \frac{1}{U} \sum_{u=1}^{U} \sum_{k=1}^{min(n,3)} P(k)$$
where U is the number of scored drawings in the test data, P(k) is the precision at cutoff k, and n is the number predictions per drawing.
You can learn more about this metric works from this kernel, and from this python code.
Kernel Submissions
You can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.
Submission File
For each key_id in the test set, you should predict up to 3 word values. The file should contain a header and have the following format. IMPORTANT: Some ""words"" are actually more than one word! The training data aligns to the Quick Draw dataset that that was previously released, and uses spaces to delimit multi-word labels. The Kaggle metric for this competition requires labels with no spaces, so you will need to adjust your label predictions to replace spaces with underscores. For example, ""roller coaster"" should be predicted as ""roller_coaster"".
key_id,word
9000003627287624,The_Eiffel_Tower airplane donut
9000010688666847,The_Eiffel_Tower airplane donut
etc.","The Quick Draw Dataset is a collection of millions of drawings across 300+ categories, contributed by players of Quick, Draw! The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located.
Two versions of the data are given. The raw data is the exact input recorded from the user drawing, while the simplified version removes unnecessary points from the vector information. (For example, a straight line may have been recorded with 8 points, but since you only need 2 points to uniquely identify a line, 6 points can be dropped.) The simplified files are much smaller and provide effectively the same information.
For this competition, you may use the raw files, the simplified files, or both. You can find out more details about the drawing format on the quickdraw-dataset github page.
Your models should predict the correct ""word"" of the drawing. IMPORTANT: Some ""words"" are actually more than one word! The training data aligns to the Quick Draw dataset that that was previously released, and uses spaces to delimit multi-word labels. The Kaggle metric for this competition requires labels with no spaces, so you will need to adjust your label predictions to replace spaces with underscores. For example, ""roller coaster"" should be predicted as ""roller_coaster"". You may predict up to 3 guesses per drawing. See the Evaluation page for the correct format.
File descriptions
sample_submission.csv - a sample submission file in the correct format
test_raw.csv - the test data in the raw vector format",kaggle competitions download -c quickdraw-doodle-recognition,"['https://www.kaggle.com/code/gaborfodor/greyscale-mobilenet-lb-0-892', 'https://www.kaggle.com/code/kmader/quickdraw-baseline-lstm-reading-and-submission', 'https://www.kaggle.com/code/gaborfodor/shuffle-csvs', 'https://www.kaggle.com/code/jpmiller/image-based-cnn', 'https://www.kaggle.com/code/gaborfodor/how-to-draw-an-owl-lb-0-002']"
282,"In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!
To learn how to handle large datasets with ease and solve this problem using TensorFlow, consider taking the Machine Learning with TensorFlow on Google Cloud Platform specialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To make this easier, head to Coursera.org/NEXTextended to claim this specialization for free for the first month!","The evaluation metric for this competition is the root mean-squared error or RMSE. RMSE measures the difference between the predictions of a model, and the corresponding ground truth. A large RMSE is equivalent to a large average error, so smaller values of RMSE are better. One nice property of RMSE is that the error is given in the units being measured, so you can tell very directly how incorrect the model might be on unseen data.
RMSE is given by:
\[ \text{RMSE} = \sqrt{\frac{1}{n} \sum^{n}_{i=1} (\hat{y}_i - y_i)^2} \]
where \\( y_i \\) is the ith observation and $$\hat{y}_i $$ is the prediction for that observation. 
Example 1. Suppose we have one observation, with an actual value of 12.5 and a prediction of 12.5 (good job!). The RMSE will be:
\[ \text{RMSE}_\text{example1} = \sqrt{\frac{1}{1} (12.5 - 12.5)^2} = 0\]
Example 2. We'll add another data point. Your prediction for the second data point is 11.0 and the actual value is 14.0. The RMSE will be:
\[ \text{RMSE}_\text{example2} = \sqrt{\frac{1}{2}((12.5 - 12.5)^{2} + (11.0-14.0)^{2})} = \sqrt{\frac{9}{2}} \approx 2.12 \]
Kernel Submissions
You can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.
Submission File
For each key in the test set, you must predict a value for the fare_amount variable. The file should contain a header and have the following format:
key,fare_amount
2015-01-27 13:08:24.0000002,11.00
2015-02-27 13:08:24.0000002,12.05
2015-03-27 13:08:24.0000002,11.23
2015-04-27 13:08:24.0000002,14.17
2015-05-27 13:08:24.0000002,15.12
etc","File descriptions
train.csv - Input features and target fare_amount values for the training set (about 55M rows).
test.csv - Input features for the test set (about 10K rows). Your goal is to predict fare_amount for each row.
sample_submission.csv - a sample submission file in the correct format (columns key and fare_amount). This file 'predicts' fare_amount to be $11.35 for all rows, which is the mean fare_amount from the training set.
Data fields
ID
key - Unique string identifying each row in both the training and test sets. Comprised of pickup_datetime plus a unique integer, but this doesn't matter, it should just be used as a unique ID field.
Required in your submission CSV. Not necessarily needed in the training set, but could be useful to simulate a 'submission file' while doing cross-validation within the training set.
Features
pickup_datetime - timestamp value indicating when the taxi ride started.",kaggle competitions download -c new-york-city-taxi-fare-prediction,"['https://www.kaggle.com/code/breemen/nyc-taxi-fare-data-exploration', 'https://www.kaggle.com/code/alexisbcook/intro-to-automl', 'https://www.kaggle.com/code/dster/nyc-taxi-fare-starter-kernel-simple-linear-model', 'https://www.kaggle.com/code/szelee/how-to-import-a-csv-file-of-55-million-rows', 'https://www.kaggle.com/code/madhurisivalenka/cleansing-eda-modelling-lgbm-xgboost-starters']"
283,"**August 2019 Update: this competition is closed and is no longer accepting submissions. The data has been removed from this competition and is not available for use. Thanks for participating!**
Can we use the content of news analytics to predict stock price performance? The ubiquity of data today enables investors at any scale to make better investment decisions. The challenge is ingesting and interpreting the data to determine which data is useful, finding the signal in this sea of information. Two Sigma is passionate about this challenge and is excited to share it with the Kaggle community.
As a scientifically driven investment manager, Two Sigma has been applying technology and data science to financial forecasts for over 17 years. Their pioneering advances in big data, AI, and machine learning have pushed the investment industry forward. Now, they're eager to engage with Kagglers in this continuing pursuit of innovation.
By analyzing news data to predict stock prices, Kagglers have a unique opportunity to advance the state of research in understanding the predictive power of the news. This power, if harnessed, could help predict financial outcomes and generate significant economic impact all over the world.
Data for this competition comes from the following sources:
Market data provided by Intrinio.
News data provided by Thomson Reuters. Copyright Thomson Reuters, 2017. All Rights Reserved. Use, duplication, or sale of this service, or data contained herein, except as described in the Competition Rules, is strictly prohibited.
The THOMSON REUTERS Kinesis Logo and THOMSON REUTERS are trademarks of Thomson Reuters and its affiliated companies in the United States and other countries and used herein under license.","In this competition, you must predict a signed confidence value, \( \hat{y}_{ti} \in [-1,1] \) , which is multiplied by the market-adjusted return of a given assetCode over a ten day window. If you expect a stock to have a large positive return--compared to the broad market--over the next ten days, you might assign it a large, positive confidenceValue (near 1.0). If you expect a stock to have a negative return, you might assign it a large, negative confidenceValue (near -1.0). If unsure, you might assign it a value near zero.
For each day in the evaluation time period, we calculate:
$$
x_t = \sum_i \hat{y}_{ti} r_{ti} u_{ti},
$$
where \( r_{ti} \) is the 10-day market-adjusted leading return for day t for instrument i, and \( u_{ti} \) is a 0/1 universe variable (see the data description for details) that controls whether a particular asset is included in scoring on a particular day.
Your submission score is then calculated as the mean divided by the standard deviation of your daily \(x_t\) values:
$$
\text{score} = \frac{\bar{x}_t}{\sigma(x_t)}.
$$
If the standard deviation of predictions is 0, the score is defined as 0.
Submission File
You must make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.
The kernels environment automatically formats and creates your submission files in this competition when calling env.write_submission_file(). There is no need to manually create your submissions. Submissions will have the following format:
time,assetCode,confidenceValue  
2017-01-03,RPXC.O,0.1
2017-01-04,RPXC.O,0.02
2017-01-05,RPXC.O,-0.3
etc.","THE DATA HAS BEEN REMOVED FROM THIS COMPETITION AND IS NOT AVAILABLE FOR USE.
In this competition, you will be predicting future stock price returns based on two sources of data:
Market data (2007 to present) provided by Intrinio - contains financial market information such as opening price, closing price, trading volume, calculated returns, etc.
News data (2007 to present) Source: Thomson Reuters - contains information about news articles/alerts published about assets, such as article details, sentiment, and other commentary.
Each asset is identified by an assetCode (note that a single company may have multiple assetCodes). Depending on what you wish to do, you may use the assetCode, assetName, or time as a way to join the market data to news data.
Since this is a Kernels-only, time-based competition, you will not interact directly with the data files as you would in a standard Kaggle competition. You should refer to the submission instructions for details on how to fetch data and make predictions. As noted in the instructions, you will encounter synthetic future data within competition data. This is included to simulate the volume, timeline, and the computational burden that real future data will introduce.
The custom python module also makes it simple to understand what steps are necessary to participate, telling you which assetsCodes to forecast at what time and, by extenstion, which days are market trading days. During stage one, the leaderboard will show performance on a historical period from 2017-01-01 to 2018-07-31. During stage two, Kaggle will re-run participants' selected Kernels on approximately six months of future data.",kaggle competitions download -c two-sigma-financial-news,"['https://www.kaggle.com/code/artgor/eda-feature-engineering-and-everything', 'https://www.kaggle.com/code/dster/two-sigma-news-official-getting-started-kernel', 'https://www.kaggle.com/code/bguberfain/a-simple-model-using-the-market-and-news-data', 'https://www.kaggle.com/code/qqgeogor/eda-script-67', 'https://www.kaggle.com/code/christofhenkel/market-data-nn-baseline']"
284,"Several areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface.
But unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers.
To create the most accurate seismic images and 3D renderings, TGS (the world’s leading geoscience data company) is hoping Kaggle’s machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not.","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.
At each threshold value \(t\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.
Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed
and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.
The file should contain a header and have the following format. Each row in your submission represents a single predicted salt segmentation for the given image.
id,rle_mask
3e06571ef3,1 1
a51b08d882,1 1
c32590b06f,1 1
etc.","Background
Seismic data is collected using reflection seismology, or seismic reflection. The method requires a controlled seismic source of energy, such as compressed air or a seismic vibrator, and sensors record the reflection from rock interfaces within the subsurface. The recorded data is then processed to create a 3D view of earth’s interior. Reflection seismology is similar to X-ray, sonar and echolocation.
A seismic image is produced from imaging the reflection coming from rock boundaries. The seismic image shows the boundaries between different rock types. In theory, the strength of reflection is directly proportional to the difference in the physical properties on either sides of the interface. While seismic images show rock boundaries, they don't say much about the rock themselves; some rocks are easy to identify while some are difficult.
There are several areas of the world where there are vast quantities of salt in the subsurface. One of the challenges of seismic imaging is to identify the part of subsurface which is salt. Salt has characteristics that makes it both simple and hard to identify. Salt density is usually 2.14 g/cc which is lower than most surrounding rocks. The seismic velocity of salt is 4.5 km/sec, which is usually faster than its surrounding rocks. This difference creates a sharp reflection at the salt-sediment interface. Usually salt is an amorphous rock without much internal structure. This means that there is typically not much reflectivity inside the salt, unless there are sediments trapped inside it. The unusually high seismic velocity of salt can create problems with seismic imaging.
Data
The data is a set of images chosen at various locations chosen at random in the subsurface. The images are 101 x 101 pixels and each pixel is classified as either salt or sediment. In addition to the seismic images, the depth of the imaged location is provided for each image. The goal of the competition is to segment regions that contain salt.",kaggle competitions download -c tgs-salt-identification-challenge,"['https://www.kaggle.com/code/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics', 'https://www.kaggle.com/code/phoenigs/u-net-dropout-augmentation-stratification', 'https://www.kaggle.com/code/shaojiaxin/u-net-with-simple-resnet-blocks-v2-new-loss', 'https://www.kaggle.com/code/pestipeti/explanation-of-scoring-metric', 'https://www.kaggle.com/code/bguberfain/unet-with-depth']"
285,"The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.
RStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have.
In this competition, you’re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.","Root Mean Squared Error (RMSE)
Submissions are scored on the root mean squared error. RMSE is defined as:
\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]
where y hat is the natural log of the predicted revenue for a customer and y is the natural log of the actual summed revenue value plus one.
Submission File
For each fullVisitorId in the test set, you must predict the natural log of their total revenue in PredictedLogRevenue. The submission file should contain a header and have the following format:
fullVisitorId,PredictedLogRevenue
0000000259678714014,0
0000049363351866189,0
0000053049821714864,0
etc.","Important Note
We have now updated the data to work with the new forward-looking problem formulation. Note that in this competition you will be predicting the target for ALL users in the posted test set: test_v2.csv, for their transactions in the future time period of December 1st 2018 through January 31st 2019.
What files do I need?
You will need to download train_v2.csv and test_v2.csv. These contain the data necessary to make predictions for each fullVisitorId listed in sample_submission_v2.csv.
Unfortunately, due to time constraints, the BigQuery version of this data will not be made available immediately.
What should I expect the data format to be?
Both train_v2.csv and test_v2.csv contain the columns listed under Data Fields. Each row in the dataset is one visit to the store. Because we are predicting the log of the total revenue per user, be aware that not all rows in test_v2.csv will correspond to a row in the submission, but all unique fullVisitorIds will correspond to a row in the submission.
IMPORTANT: Due to the formatting of fullVisitorId you must load the Id's as strings in order for all Id's to be properly unique!
There are multiple columns which contain JSON blobs of varying depth. In one of those JSON columns, totals, the sub-column transactionRevenue contains the revenue information we are trying to predict. This sub-column exists only for the training data.",kaggle competitions download -c ga-customer-revenue-prediction,"['https://www.kaggle.com/code/anmolkohli1/google-analytics-revenue-prediction', 'https://www.kaggle.com/code/shroukali/google-analytics-customer-revenue-predict-sa', 'https://www.kaggle.com/code/tequiero739/ga-revenue-prediction-0725', 'https://www.kaggle.com/code/minhvngc/exploration', 'https://www.kaggle.com/code/oceands/basics-of-google-analytics']"
286,"The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?
Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.
In Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.
While this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.
To improve on PMT, the IDB (the largest source of development financing for Latin America and the Caribbean) has turned to the Kaggle community. They believe that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance.
Beyond Costa Rica, many countries face this same problem of inaccurately assessing social need. If Kagglers can generate an improvement, the new algorithm could be implemented in other countries around the world.
This is a Kernels-Only Competition, so you must submit your code through Kernels, rather than uploading .csv predictions. You can create private Kernels and even share/edit your work with teammates by adding them as collaborators.","Submissions will be evaluated based on their macro F1 score.
Kernel Submissions
As this is a Kernels-Only Competition, you must make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.
Submission File
For each Id in the test set, you must predict a class for the Target variable as described in the data page.
The file should contain a header and have the following format:
Id,Target
ID_2f6873615,1
ID_1c78846d2,2
ID_e5442cf6a,3
ID_a8db26a79,4
ID_a62966799,4 
etc","File descriptions
{train|test}.csv - the training set
This is the main table, broken into two files for Train (with a Target column) and Test (without the Target column).
One row represents one person in our data sample.
Multiple people can be part of a single household. Only predictions for heads of household are scored.
sample_submission.csv - a sample submission file in the correct format
This file contains all test IDs and a default value.
Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored.
Core Data fields
Id - a unique identifier for each row.
Target - the target is an ordinal variable indicating groups of income levels.
1 = extreme poverty
2 = moderate poverty
3 = vulnerable households
4 = non vulnerable households",kaggle competitions download -c costa-rican-household-poverty-prediction,"['https://www.kaggle.com/code/willkoehrsen/a-complete-introduction-and-walkthrough', 'https://www.kaggle.com/code/shivamb/costa-rica-poverty-exploration-kernel', 'https://www.kaggle.com/code/willkoehrsen/featuretools-for-good', 'https://www.kaggle.com/code/taindow/predicting-poverty-levels-with-r', 'https://www.kaggle.com/code/ashishpatel26/feature-importance-of-lightgbm']"
287,"Introduction
Google AI (Google’s AI research arm, tasked with advancing AI for everyone) is challenging you to build an algorithm that detects objects automatically using an absolutely massive training dataset ― one with more varied and complex bounding-box annotations and object classes than ever before.
Here's the background. Computers are getting better and better at vision. But in a few critical ways, they still can't match a human’s intuitive perception.
For example, what do you see when you look at this photo?
Most of us would answer, “a sandy beach, the ocean, a few people walking, some trees, grass, and buildings…a woman walking her dog right there! Oh yeah, and there is a man holding a plastic cup.”
Can a computer provide as precise an image description? Google AI wants to further push the capabilities of computer vision. We hope that providing very large training set will stimulate research into more sophisticated object and relationship detection models that will exceed current state-of-the-art performance.
The results of this Challenge will be presented at a workshop at the European Conference on Computer Vision 2018.
Object Detection Track
Object detection is a central task in computer vision, with applications ranging across search, robotics, self-driving cars, and many others. As deep network solutions become deeper and more complex, they are often limited by the amount of training data available.
With this in mind, to spur advances in analyzing and understanding images, Google AI has publicly released the Open Images dataset. Open Images follows the tradition of PASCAL VOC, ImageNet and COCO, now at an unprecedented scale.
The Open Images Challenge is based on Open Images dataset. The training set of the Challenge contains:
12M bounding-box annotations for 500 object classes on 1.7M training images
Images of complex scenes with several objects–an average of 7 boxes per image
Highly varied images that contain brand new objects like “fedora” and “snowman”
Class hierarchy that reflects the relationships between classes of Open Images.
In this track of the Challenge, you are asked to build the best performing algorithm for automatically detecting objects.
Please refer to the Open Images Challenge page for additional details on the dataset. In addition to this Object Detection track, the Challenge also includes a Visual Relationship Detection track to detect pairs of objects in particular relations, e.g. ""woman playing guitar,"" ""beer on table,"" ""dog inside car"", ""man holding coffee"", etc. The Visual Relationship Detection track is available here.
Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.","Submissions are evaluated by computing mean Average Precision (AP), modified to take into account the annotation process of Open Images dataset (mean is taken over per-class APs). The metric is described on the Open Images Challenge website.
The final mAP is computed as the average AP over the 500 classes. The participants will be ranked on this final metric.
Kaggle's production code in C# can be viewed here. The metric is also implemented as a part of Tensorflow Object Detection API. See this Tutorial on running the evaluation in Python.
Kernel Submissions
You can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.
Submission File
For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as
ImageID,PredictionString
ImageID,{Label Confidence XMin YMin XMax YMax},{...}","The train and validation sets of images and their ground truth (bounding boxes and labels) should be downloaded from Open Images Challenge page . Please note that the test images used in this competition is independent from previously released part of Open Images V4. The images can be downloaded from:
Kaggle (test.zip file below)
CVDF (see instructions here)
Figure 8
Note: The images are the same as in the Visual Relationship Track so you do not need to re-download them. You should expect 99,999 images in total.
File descriptions
test.zip - the test set of 99,999 images
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c google-ai-open-images-object-detection-track,"['https://www.kaggle.com/code/mihaskalic/classifier-as-a-bounding-box-predictor', 'https://www.kaggle.com/code/shivamb/objects-bounding-boxes-using-resnet50-imageai', 'https://www.kaggle.com/code/sajinpgupta/object-detection-using-yolov3', 'https://www.kaggle.com/code/vsmolyakov/tensorflow-object-detection-api-apache-spark', 'https://www.kaggle.com/code/sajinpgupta/object-detection-yolov2']"
288,"This competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset.
You are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.
What's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost?
This is a great competition to explore different models and improve your skills in forecasting.","Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.
Kernel Submissions
You can only make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.
Submission File
For each id in the test set, you must predict a probability for the sales variable. The file should contain a header and have the following format:
id,sales
0,35
1,22
2,5
etc.","The objective of this competition is to predict 3 months of item-level sales data at different store locations.
File descriptions
train.csv - Training data
test.csv - Test data (Note: the Public/Private split is time based)
sample_submission.csv - a sample submission file in the correct format
Data fields
date - Date of the sale data. There are no holiday effects or store closures.
store - Store ID
item - Item ID
sales - Number of items sold at a particular store on a particular date.",kaggle competitions download -c demand-forecasting-kernels-only,"['https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting', 'https://www.kaggle.com/code/arindamgot/eda-prophet-mlp-neural-network-forecasting', 'https://www.kaggle.com/code/sumi25/understand-arima-and-tune-p-d-q', 'https://www.kaggle.com/code/adityaecdrid/my-first-time-series-comp-added-prophet', 'https://www.kaggle.com/code/konradb/ts-2-linear-vision']"
289,"Introduction
Google AI (Google’s AI research arm, tasked with advancing AI for everyone) is challenging you to build an algorithm that detects objects automatically using an absolutely massive training dataset ― one with more varied and complex bounding-box annotations and object classes than ever before.
Here's the background. Computers are getting better and better at vision. But in a few critical ways, they still can't match a human’s intuitive perception.
For example, what do you see when you look at this photo?
Most of us would answer, “a sandy beach, the ocean, a few people walking, some trees, grass, and buildings…a woman walking her dog right there! Oh yeah, and there is a man holding a plastic cup.”
Can a computer provide as precise an image description? Google AI wants to further push the capabilities of computer vision. We hope that providing very large training set will stimulate research into more sophisticated object and relationship detection models that will exceed current state-of-the-art performance.
The results of this Challenge will be presented at a workshop at the European Conference on Computer Vision 2018.
Visual Relationship Detection Track
Identifying different objects (man and cup) is an important problem on its own, but identifying the relationship between them (holding) is critical for many real world use cases.
In this Visual Relationship Detection track Challenge you’re asked to build an algorithm that detects pairs of objects in particular relations: things like ""woman playing guitar,"" ""beer on table,"" or ""dog inside car.""
The Challenge dataset includes both object bounding boxes and visual relationship annotations. The training set contains annotations for 329 distinct relationship triplets, occurring a total of 374,768 times.
In this track of the Challenge, you are asked to build the best performing algorithm for automatically detecting relationships triplets. Please refer to the Open Images Challenge page for additional details on the dataset.
This competition is one of two tracks in the Open Images Challenge. Find the Object Detection track of this competition using the entire training set here.
Example of ‘man playing guitar’
Radiofiera - Villa Cordellina Lombardi, Montecchio Maggiore (VI) - agosto 2010 by Andrea Sartorati
Example of ‘chair at table’
Epic Fireworks - Loads A Room by Epic Fireworks","Submissions are evaluated by computing the weighted mean of three metrics: mean Average Precision (AP) on relationships detection, Recall@N (where N=50), mean Average Precision on phrase detection (mean in mean Average Precision is taken over per-relationship APs).
See more details about the metric on Open Images Challenge website.
The weights applied to each of the 3 metrics are [0.4, 0.2, 0.4].
Kaggle's production code in C# can be viewed here. The metric is also implemented as a part of Tensorflow Object Detection API. See this Tutorial on running the evaluation in Python.
Kernel Submissions
You can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.
Submission File
For each image in the test set, you must predict a list of boxes describing objects in the image. Each box is described as
  ImageId, PredictionString 
  ImageId, {Confidence Label1 XMin1 YMin1 XMax1 YMax1 Label2 XMin2 YMin2 XMax2 YMax2 RelationLabel}, {...}","The train and validation sets of images and their ground truth (visual relationships annotations, bounding boxes and labels) can be downloaded via Open Images Challenge website . Please note that the test images used in this competition is independent from previously released part of Open Images V4. The images can be downloaded from:
Kaggle (test.zip file below)
CVDF (see instructions here)
Figure 8
Note: The images are the same as in the Object Detection Track so you do not need to re-download them. You should expect 99,999 images.
File descriptions
test.zip - the test set of 99,999 images
VRD_sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c google-ai-open-images-visual-relationship-track,"['https://www.kaggle.com/code/titericz/beat-the-benchmark-lb-0-0032', 'https://www.kaggle.com/code/dinuuu/no-script']"
290,"The aim of this playground challenge is to find a phenomenon that is not already known to exist – charged lepton flavour violation – thereby helping to establish ""new physics"". 
Flavours of Physics 101
The laws of nature ensure that some physical quantities, such as energy or momentum, are conserved. From Noether’s theorem, we know that each conservation law is associated with a fundamental symmetry. For example, conservation of energy is due to the time-invariance (the outcome of an experiment would be the same today or tomorrow) of physical systems. The fact that physical systems behave the same, regardless of where they are located or how they are oriented, gives rise to the conservation of linear and angular momentum.
Symmetries are also crucial to the structure of the Standard Model of particle physics, our present theory of interactions at microscopic scales. Some are built into the model, while others appear accidentally from it. In the Standard Model, lepton flavour, the number of electrons and electron-neutrinos, muons and muon-neutrinos, and tau and tau-neutrinos, is one such conserved quantity.
Interestingly, in many proposed extensions to the Standard Model, this symmetry doesn’t exist, implying decays that do not conserve lepton flavour are possible. One decay searched for at the LHC is τ- → μ+μ-μ- (or τ → 3μ). Observation of this decay would be a clear indication of the violation of lepton flavour and a sign of long-sought new physics.
Competition Design
You will be working with real data from the LHCb experiment at the LHC, mixed with simulated datasets of the decay. The metric used in this challenge includes checks that physicists do in their analysis to make sure the results are unbiased. These checks have been built into the competition design to help ensure that the results will be useful for physicists in future studies. 
To get started, review the Data Page, and be sure to download the Starter Kit. The Starter Kit will help you to get used to the unique submission procedure for this competition.
Competition Video Tutorial
You've got lots of questions. Researchers at CERN & LCHb have the answers. open_in_newhttps://player.vimeo.com/video/134110342
- What is the goal of this competition? (1:56)
- Why is finding τ → μμμ exciting? (2:18)
- What are flavours? (4:10)
- Why use machine learning to find τ → μμμ? (4:57)
- How did you decide on the size of the dataset? (5:31)
- Why is weighted AUC the evaluation metric? (6:09)
- Why use Ds → φπ data for the Agreement Test? (7:53)
- Why do we need a Correlation Check? (8:44)
- How will the competition results impact what you do? (11:38)
- How will the competition results be used at CERN? (12:17)
Resources
Flavour of Physics, Research Documentation
Roel Aaij et al., Search for the lepton flavour violating decay τ → µµµ, 2015, JHEP, 1502:121, 2015
New approaches for boosting to uniformity
Acknowledgements
This competition is brought to you by: 
                          Co-sponsored by:
Additional support from: 
                       ","The evaluation metric for this competition is Weighted Area Under the ROC Curve. The ROC curve is divided into sections based on the True Positive Rate (TPR). To calculate the total area, multiply the area with TPR in [0., 0.2] by weight 2.0, the area with TPR in [0.2, 0.4] by 1.5, the area with TPR [0.4, 0.6] with weight 1.0, and the area with TPR [0.6, 0.8] with weight 0.5. Anything above a TPR of 0.8 has weight 0.
These weights were chosen to match the evaluation methodology used by CERN scientists. Note that the weighted AUC is calculated only for events (simulated signal events for tau->µµµ and real background events for tau->µµµ) with min_ANNmuon > 0.4 (see details in section 2.2 Physics background).
Before your predictions are scored with weighted AUC, they also must pass two addition checks: first an agreement test and then the correlation test. Please refer to their respective pages to learn about these tests and what is needed to pass them.
Submission File
For every event in the dataset, submission files should contain two columns: id and prediction. The prediction should be a floating point value between 0 and 1.0, indicating the probability that this event is τ → 3μ decay. 
The file should contain a header and have the following format:
id,prediction
14711831,0.3
16316387,0.3
6771382,0.3
686045,0.3
8755882,0.3
10247299,0.3
etc.","In this competition, you are given a list of collision events and their properties. You will then predict whether a τ → 3μ decay happened in this collision. This τ → 3μ is currently assumed by scientists not to happen, and the goal of this competition is to discover τ → 3μ happening more frequently than scientists currently can understand.
It is challenging to design a machine learning problem for something you have never observed before. Scientists at CERN developed the following designs to achieve the goal.
training.csv
This is a labelled dataset (the label ‘signal’ being ‘1’ for signal events, ‘0’ for background events) to train the classifier. Signal events have been simulated, while background events are real data.
This real data is collected by the LHCb detectors observing collisions of accelerated particles with a specific mass range in which τ → 3μ can’t happen. We call these events “background” and label them 0.
FlightDistance - Distance between τ and PV (primary vertex, the original protons collision point).
FlightDistanceError - Error on FlightDistance.
mass - reconstructed τ candidate invariant mass, which is absent in the test samples.
LifeTime - Life time of tau candidate.
IP - Impact Parameter of tau candidate.
IPSig - Significance of Impact Parameter.",kaggle competitions download -c flavours-of-physics-kernels-only,"['https://www.kaggle.com/code/yasserhessein/finding-with-svm-knn-gnb-xgb', 'https://www.kaggle.com/code/prashantmudgal/a-physics-example-of-ensemble-modelling-and-eda', 'https://www.kaggle.com/code/cieske/lightgbm-model-magic-features', 'https://www.kaggle.com/code/rahulbagga/neural-networks-in-keras', 'https://www.kaggle.com/code/scirpus/fork-of-complex-math']"
291,"Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight?
If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see.
Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients. 
Acknowledgements
We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.","Submissions are evaluated on the categorization accuracy (the percent of dishes that you correctly classify).
Submission File
Your submission file should predict the cuisine for each recipe in the test set. The file should contain a header and have the following format:
id,cuisine
35203,italian
17600,italian
35200,italian
17602,italian
...
etc.","In the dataset, we include the recipe id, the type of cuisine, and the list of ingredients of each recipe (of variable length). The data is stored in JSON format. 
An example of a recipe node in train.json:
 {
 ""id"": 24717,
 ""cuisine"": ""indian"",
 ""ingredients"": [
     ""tumeric"",
     ""vegetable stock"",
     ""tomatoes"",
     ""garam masala"",
     ""naan"",
     ""red lentils"",
     ""red chili peppers"",
     ""onions"",
     ""spinach"",
     ""sweet potatoes""
 ]
 },",kaggle competitions download -c whats-cooking-kernels-only,"['https://www.kaggle.com/code/heliaritiana/helia-what-s-cooking', 'https://www.kaggle.com/code/ryzhokhina/what-s-cooking', 'https://www.kaggle.com/code/floriancelary/what-s-cooking-fc-le4-2023', 'https://www.kaggle.com/code/simontang02/what-s-cooking', 'https://www.kaggle.com/code/encode0/cuisine-classification-with-tensorflow']"
292,"""There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.""
The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of Socher et al [2]. We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper:
http://nlp.stanford.edu/sentiment/
There you will find have source code, a live demo, and even an online interface to help train the model.
[1] Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124.
[2] Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).
Image credits: Popcorn - Maura Teague, http://www.flickr.com/photos/93496438@N06/","Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly) for every parsed phrase. The sentiment labels are:
0 - negative
1 - somewhat negative
2 - neutral
3 - somewhat positive
4 - positive
Submission Format
For each phrase in the test set, predict a label for the sentiment. Your submission should have a header and look like the following:
PhraseId,Sentiment
156061,2
156062,2
156063,2
...","The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.
train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.
test.tsv contains just phrases. You must assign a sentiment label to each phrase.
The sentiment labels are:
0 - negative
1 - somewhat negative
2 - neutral
3 - somewhat positive
4 - positive",kaggle competitions download -c movie-review-sentiment-analysis-kernels-only,"['https://www.kaggle.com/code/artgor/movie-review-sentiment-analysis-eda-and-models', 'https://www.kaggle.com/code/nafisur/keras-models-lstm-cnn-gru-bidirectional-glove', 'https://www.kaggle.com/code/aerdem4/fast-basic-lstm-with-proper-k-fold-sentimentembed', 'https://www.kaggle.com/code/hamishdickson/cnn-for-sentence-classification-by-yoon-kim', 'https://www.kaggle.com/code/parth05rohilla/bi-lstm-and-cnn-model-top-10']"
293,"Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing.
In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from cartographic variables. The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.
This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.
This competition originally ran in 2015. We are relaunching it as a kernels-only version here.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:
Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science","Submissions are evaluated on multi-class classification accuracy.
Submission File
Your submission file should have the observation Id and a predicted cover type (an integer between 1 and 7, inclusive). The file should contain a header and have the following format:
Id,Cover_Type
15121,1
15122,1
15123,1
...","The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:
1 - Spruce/Fir
2 - Lodgepole Pine
3 - Ponderosa Pine
4 - Cottonwood/Willow
5 - Aspen
6 - Douglas-fir
7 - Krummholz
The training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations).
Data Fields
Elevation - Elevation in meters
- Aspect in degrees azimuth
- Slope in degrees
- Horz Dist to nearest surface water features
- Vert Dist to nearest surface water features
- Horz Dist to nearest roadway
(0 to 255 index) - Hillshade index at 9am, summer solstice
(0 to 255 index) - Hillshade index at noon, summer solstice
(0 to 255 index) - Hillshade index at 3pm, summer solstice
- Horz Dist to nearest wildfire ignition points
(4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation
(40 binary columns, 0 = absence or 1 = presence) - Soil Type designation
(7 types, integers 1 to 7) - Forest Cover Type designation",kaggle competitions download -c forest-cover-type-kernels-only,"['https://www.kaggle.com/code/moghazy/ensemble-learning-with-feature-engineering', 'https://www.kaggle.com/code/codename007/forest-cover-type-eda-baseline-model', 'https://www.kaggle.com/code/justfor/ensembling-and-stacking-with-heamy', 'https://www.kaggle.com/code/nicapotato/multi-class-lgbm-cv-and-seed-diversification', 'https://www.kaggle.com/code/rohandx1996/features-hybrid-classifier-pls-vote-if-u-like']"
294,"According to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception.
The digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner… and often before they´ve even realized they need the service. In their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.
In this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.","The evaluation metric for this competition is Root Mean Squared Logarithmic Error.
The RMSLE is calculated as
$$
\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }
$$
Where:
\\(\epsilon\\) is the RMSLE value (score)
\\(n\\) is the total number of observations in the (public/private) data set,
\\(p_i\\) is your prediction of target, and
\\(a_i\\) is the actual target for \\(i\\).
\\(\log(x)\\) is the natural logarithm of \\(x\\)
Submission File
For every row in the test.csv, submission files should contain two columns: ID and target.  The ID corresponds to the column of that ID in the test.tsv. The file should contain a header and have the following format:
ID,target
000137c73,5944923.322036332
00021489f,5944923.322036332
0004d7953,5944923.322036332
etc.","You are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column.
The task is to predict the value of target column in the test set.
File descriptions
train.csv - the training set
test.csv - the test set
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c santander-value-prediction-challenge,"['https://www.kaggle.com/code/sudalairajkumar/simple-exploration-baseline-santander-value', 'https://www.kaggle.com/code/headsortails/breaking-bank-santander-eda', 'https://www.kaggle.com/code/titericz/the-property-by-giba', 'https://www.kaggle.com/code/samratp/lightgbm-xgboost-catboost', 'https://www.kaggle.com/code/tezdhar/breaking-lb-fresh-start']"
295,"The world is generating and consuming an enormous amount of video content. Currently on YouTube, people watch over 1 billion hours of video every single day.
To spur advances in analyzing and understanding video, Google AI has publicly released a large-scale video dataset that consists of millions of YouTube video features and associated labels from a diverse vocabulary of 3,700+ visual entities called the YouTube-8M Dataset. Last year, we successfully hosted Google Cloud & YouTube-8M Video Understanding Challenge, with 742 participating teams with 946 individual competitors from 60 countries. This competition is the second Kaggle competition based on YouTube 8M dataset, and is focused on learning video representation under budget constraints.
For a lot of video tasks where there are a large number of classes, like recommending new videos or automatic video classification, compact models need to meet memory and computational requirements. This is true even if working in cloud computational environments. Also, compact models make it possible to have limited-memory or catalog indexes on devices in order to do personalized and privacy-preserving computation on user’s personal mobile phones.
In this competition, you’re challenged to produce a compact video classification model. Your model size must not exceed 1 GB (this is strictly enforced, through model upload). We encourage participants to train a model that most efficiently uses this budget, rather than ensembles of lots of models.
This competition is being hosted by Google AI (previously known as Google Research) as a part of the European Conference on Computer Vision (ECCV) 2018 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.","Submissions are evaluated the Global Average Precision (GAP) at k, where k=20. For each video, you will submit a list of predicted labels and their corresponding confidence scores. The evaluation takes the predicted labels that have the highest k confidence scores for each video, then treats each prediction and the confidence score as an individual data point in a long list of global predictions, to compute the Average Precision across all of the predictions and all the videos. 
If a submission has N predictions (label/confidence pairs) sorted by its confidence score, then the Global Average Precision is computed as:
$$GAP = \sum_{i=1}^N p(i) \Delta r(i)$$
where N is the number of final predictions (if there are 20 predictions for each video, then N = 20 * #Videos ), p(i) is the precision, and r(i) is the recall.  
A python implementation of GAP can be found at youtube-8m's github.
Submission File
For each VideoId in the test set, you must predict a list of labels and their corresponding confidence scores. The file should contain a header and have the following format:
VideoId,LabelConfidencePairs
000c,1 0.5 2 0.3 3 0.1 4 0.05 5 0.05
etc.","In this competition, you will predict the labels of a YouTube video. We provide you extracted frame-level and video-level features. The feature data and detailed feature information can be found on the YouTube-8M dataset webpage. 
The training dataset in this competition contain videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition.
File descriptions
video-level data
You may download to your local computer with instructions here
Each video has
id: unique id for the video, in train set it is a Youtube video id, and in test/validation they are anonymized
labels: list of labels of that video
mean_rgb: float array of length 1024
mean_audio: float array of length 128
Files are in TFRecords format, TensorFlow python readers are available in the github repo, and simple reader available in Kaggle Kernels",kaggle competitions download -c youtube8m-2018,
296,"Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.
Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.
While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each SK_ID_CURR in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:
SK_ID_CURR,TARGET
100001,0.1
100005,0.9
100013,0.2
etc.",,,
297,"To explore what our universe is made of, scientists at CERN are colliding protons, essentially recreating mini big bangs, and meticulously observing these collisions with intricate silicon detectors.
While orchestrating the collisions and observations is already a massive scientific accomplishment, analyzing the enormous amounts of data produced from the experiments is becoming an overwhelming challenge.
Event rates have already reached hundreds of millions of collisions per second, meaning physicists must sift through tens of petabytes of data per year. And, as the resolution of detectors improve, ever better software is needed for real-time pre-processing and filtering of the most promising events, producing even more data.
To help address this problem, a team of Machine Learning experts and physics scientists working at CERN (the world largest high energy physics laboratory), has partnered with Kaggle and prestigious sponsors to answer the question: can machine learning assist high energy physics in discovering and characterizing new particles?
Specifically, in this competition, you’re challenged to build an algorithm that quickly reconstructs particle tracks from 3D points left in the silicon detectors. This challenge consists of two phases:
The Accuracy phase has run on Kaggle from May to 13th August 2018 (Winners to be announced by end September). Here we’ll be focusing on the highest score, irrespective of the evaluation time. This phase is an official IEEE WCCI competition (Rio de Janeiro, Jul 2018).
The Throughput phase will run on Codalab starting in September 2018. Participants will submit their software which is evaluated by the platform. Incentive is on the throughput (or speed) of the evaluation while reaching a good score. This phase is an official NIPS competition (Montreal, Dec 2018).
All the necessary information for the Accuracy phase is available here on Kaggle site. The overall TrackML challenge web site is there.","Custom metric
The evaluation metric for this competition is a custom metric. In one line : it is the intersection between the reconstructed tracks and the ground truth particles, normalized to one for each event, and averaged on the events of the test set.
First, each hit is assigned a weight:
the few first (starting from the center of the detector) and last hits have a larger weight
hits from the more straight tracks (more rare, but more interesting) have a larger weight
random hits or hits from very short tracks have weight zero
the sum of the weights of all the hits of one event is 1 by construction
the hit weights are available in the truth file. They are not revealed for the test dataset
Then, the score is constructed as follows:
tracks are uniquely matched to particles by the double majority rule:
for a given track, the matching particle is the one to which the absolute majority (strictly more that 50%) of the track points belong.
the track should have the absolute majority of the points of the matching particle.
If any of these constraints is not met, the score for this track is zero
the score of a surviving track is the sum of the weights of the points of the intersection between the track and the matching particle.
the score of an event is the sum of the score of all its tracks.
the final score is the average on the events of the public and private leaderboard test respectively.
A perfect algorithm will have a score of 1, while a random one will have a score 0.
An example implementation can be found in the trackml python library.
Submission Format
The submission file should contain three columns: event_id, hit_id, track_id, and should have exactly one line for every hit of every event.
event_id is the event number
hit_id is the hit number, within that event
track_id is the user defined numerical identifier (non negative integer) of the track (the track being the group or cluster of hits).
The file should contain a header and have the following format:
event_id,hit_id,track_id
0,0,21
0,1,49
0,3,32
0,4,0
0,5,21
etc...",,,
298,,,,,
299,,,,,
300,,,,,
301,,,,,
302,"Fraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest
mobile market in the world and therefore suffers from huge volumes of fradulent traffic.
TalkingData, China’s largest independent big data service platform, covers over 70% of active mobile devices nationwide. They handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user’s click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist.
While successful, they want to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. In their 2nd competition with Kaggle, you’re challenged to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support your modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each click_id in the test set, you must predict a probability for the target is_attributed variable. The file should contain a header and have the following format:
click_id,is_attributed
1,0.003
2,0.001
3,0.000
etc.","For this competition, your objective is to predict whether a user will download an app after clicking a mobile app advertisement.
File descriptions
train.csv - the training set
train_sample.csv - 100,000 randomly-selected rows of training data, to inspect data before downloading full set
test.csv - the test set
sampleSubmission.csv - a sample submission file in the correct format
UPDATE: test_supplement.csv - This is a larger test set that was unintentionally released at the start of the competition. It is not necessary to use this data, but it is permitted to do so. The official test data is a subset of this data.
Data fields
Each row of the training data contains a click record, with the following features.
ip: ip address of click.
app: app id for marketing.
device: device type id of user mobile phone (e.g., iphone 6 plus, iphone 7, huawei mate 7, etc.)",kaggle competitions download -c talkingdata-adtracking-fraud-detection,"['https://www.kaggle.com/code/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask', 'https://www.kaggle.com/code/yuliagm/talkingdata-eda-plus-time-patterns', 'https://www.kaggle.com/code/nanomathias/feature-engineering-importance-testing', 'https://www.kaggle.com/code/kailex/talkingdata-eda-and-class-imbalance', 'https://www.kaggle.com/code/pranav84/talkingdata-eda-to-model-evaluation-lb-0-9683']"
303,"Founded in 2000 by a high school teacher in the Bronx, DonorsChoose.org empowers public school teachers from across the country to request much-needed materials and experiences for their students. At any given time, there are thousands of classroom requests that can be brought to life with a gift of any amount.
DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website.
Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they need to solve:
How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as efficiently as possible
How to increase the consistency of project vetting across different volunteers to improve the experience for teachers
How to focus volunteer time on the applications that need the most assistance
The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval.
With an algorithm to pre-screen applications, DonorsChoose.org can auto-approve some applications quickly so that volunteers can spend their time on more nuanced and detailed project vetting processes, including doing more to help teachers develop projects that qualify for specific funding opportunities.
Your machine learning algorithm can help more teachers get funded more quickly, and with less cost to DonorsChoose.org, allowing them to channel even more funding directly to classrooms across the country.
Getting Started with Kernels
Get familiar with the competition data and the machine learning objective quickly using Kernels. Google's engineering education team has put together a starter tutorial implementing benchmark linear classification model.
Acknowledgments
Machine Learning Crash Course was created by Google's engineering education team in partnership with numerous Machine Learning subject matter experts across Google.","The goal of this competition is to predict whether an application to DonorsChoose is accepted. Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the project_is_approved variable. The file should contain a header and have the following format (order does not matter):
id,project_is_approved
p233245,0.84
p096795,0.84
p236235,0.84
etc.",,,"['https://www.kaggle.com/code/fizzbuzz/beginner-s-guide-to-capsule-networks', 'https://www.kaggle.com/code/headsortails/an-educated-guess-update-feature-engineering', 'https://www.kaggle.com/code/artgor/eda-feature-engineering-and-xgb-lgb', 'https://www.kaggle.com/code/vlasoff/beginner-s-guide-nn-with-multichannel-input', 'https://www.kaggle.com/code/codename007/a-very-extensive-end-to-end-project-donorschoose']"
304,"Google Cloud and NCAA® have teamed up to bring you this year’s version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness® during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results.
This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of games played
\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2
\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins
\\( log() \\) is the natural (base e) logarithm
A smaller log loss is better. Games which are not played are ignored in the scoring. The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2018 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict (64*63)/2  = 2,016 matchups. 
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_3104_3129"" indicates team 3104 played team 3129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id.
The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:
id,pred
2013_1103_1107,0.5
2013_1103_1112,0.5
2013_1103_1125,0.5
...","March 2 Update:
Preliminary Data through 2018 now available
Sample Tourney for 2018 now available
Data Description:
Each season there are thousands of NCAA® basketball games played between Division I women's teams, culminating in March Madness®, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness® game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.
If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.
As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.",kaggle competitions download -c womens-machine-learning-competition-2018,"['https://www.kaggle.com/code/juliaelliott/basic-starter-kernel-ncaa-women-s-dataset', 'https://www.kaggle.com/code/tejasrinivas/preprocessing-code-to-join-all-the-tables-eda', 'https://www.kaggle.com/code/taffeylewis/for-fun-my-original-synthetic-spreads-womens', 'https://www.kaggle.com/code/robikscube/eda-of-women-s-ncaa-bracket-data-in-progress', 'https://www.kaggle.com/code/maxphilipp/creating-tidy-dataset-with-dplyr']"
305,"Google Cloud and NCAA® have teamed up to bring you this year’s version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness® during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results.
This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of games played
\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2
\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins
\\( log() \\) is the natural (base e) logarithm
A smaller log loss is better. Games which are not played are ignored in the scoring. Play-in games are also ignored (only the games among the final 64 teams are scored). The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2018 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2  = 2278 matchups. 
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_1104_1129"" indicates team 1104 played team 1129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id.
The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:
id,pred
2013_1103_1107,0.5
2013_1103_1112,0.5
2013_1103_1125,0.5
...","March 5 Update:
Preliminary Data through 2018 now available
Sample Tourney for 2018 now available
Data Description:
Each season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness®, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness® game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.
If you are unfamiliar with the format and intricacies of the NCAA® tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears.
As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The TeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.",kaggle competitions download -c mens-machine-learning-competition-2018,"['https://www.kaggle.com/code/captcalculator/a-very-extensive-ncaa-exploratory-analysis', 'https://www.kaggle.com/code/juliaelliott/basic-starter-kernel-ncaa-men-s-dataset', 'https://www.kaggle.com/code/lpkirwin/fivethirtyeight-elo-ratings', 'https://www.kaggle.com/code/lnatml/feature-engineering-with-advanced-stats', 'https://www.kaggle.com/code/taffeylewis/for-fun-my-original-synthetic-spreads']"
306,"[UPDATE] 2019 challenge launched: https://kaggle.com/c/landmark-retrieval-2019
Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph.
In this competition, Kagglers are given query images and, for each query, are expected to retrieve all database images containing the same landmarks (if any).
The new dataset is the largest worldwide dataset for image retrieval research, comprising more than a million images of 15K unique landmarks. We hope that this release will accelerate progress in this important research problem.
This challenge is organized in conjunction with the Landmark Recognition Challenge (https://www.kaggle.com/c/landmark-recognition-challenge). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.","Submissions are evaluated according to mean Average Precision @ 100 (\(mAP@100\)):
$$mAP@100 = \frac{1}{Q} \sum_{q=1}^{Q} \frac{1}{min(m_q, 100)} \sum_{k=1}^{min(n_q,100)} P_q(k) rel_q(k)$$
where:
\(Q\) is the number of query images that depict landmarks from the index set
\(m_q\) is the number of index images containing a landmark in common with the query image \(q\) (note that this is only for queries which depict landmarks from the index set, so \(m_q \neq 0\))
\(n_q\) is the number of predictions made by the system for query \(q\)
\(P_q(k)\) is the precision at rank \(k\) for the \(q\)-th query
\(rel_q(k)\) denotes the relevance of prediciton \(k\) for the \(q\)-th query: it’s 1 if the \(k\)-th prediction is correct, and 0 otherwise
Some query images will have no associated index images to retrieve. These queries are ignored in scoring.
Submission File
For each query id in the test set, you must predict a space-delimited list of index images that depict the same landmarks as the query. The list should be sorted, such that the first index image is considered the most relevant one, and the last the least relevant one. The file should contain a header and have the following format:
id,images
000088da12d664db,0370c4c856f096e8 766677ab964f4311 e3ae4dcee8133159...
etc.","Note: The Google Landmarks Dataset v1 is deprecated and no longer available. Please consider using the Google Landmarks Dataset v2 instead.
Note: Since this challenge is finalized, the data files (test.csv and index.csv) were transferred to the Google-Landmarks dataset webpage.
In this competition, you are asked to take a query image and retrieve a set of images that depict a landmark contained in the query image. The query images are listed in test.csv, while the ""index"" images from which you are retrieving are listed in index.csv. Each image has a unique id.
Due to restrictions on distributing the actual files, the dataset contains URLs that point to each image (this Python script may be useful to download the images). Since these images are not directly in our control, a small percentage of them will be unavailable (and the availability may change over time). To account for this, the organizers will download the images near the competition end date and adjust the solution file to ignore any missing query images.
You should expect this challenge to involve a small amount of leakage on account of image metadata. The hosts have intentionally selected images that do not contain geolocation information, but there may be predictive information present in the Exif data. Since Kaggle can not detect whether this metadata is being used, consider its use to be permitted.
Dataset construction
The index set was constructed by clustering photos with respect to their geolocation and visual similarity using an algorithm similar to the one described in [1]. Matches between index images were established using local feature matching. Note that there may be multiple clusters per landmark, which typically correspond to different views or different parts of the landmark. To avoid bias, no computer vision algorithms were used for the ground truth generation. Instead, we established ground truth correspondences between query images and landmarks using human annotators.",kaggle competitions download -c landmark-retrieval-challenge,"['https://www.kaggle.com/code/codename007/google-landmark-retrieval-exploratory-analysis', 'https://www.kaggle.com/code/wesamelshamy/tutorial-image-feature-extraction-and-matching', 'https://www.kaggle.com/code/anokas/py3-image-downloader-w-progress-bar', 'https://www.kaggle.com/code/tobwey/landmark-retrieval-challenge-image-downloader', 'https://www.kaggle.com/code/bestaar/deep-metric-learning-with-pretrained-keras-models']"
307,"[UPDATE] 2019 challenge launched: https://kaggle.com/c/landmark-recognition-2019
Did you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections.
Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. In this competition, we present the largest worldwide dataset to date, to foster progress in this problem. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.
Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are a total of 15K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.
This challenge is organized in conjunction with the Landmark Retrieval Challenge ( https://www.kaggle.com/c/landmark-retrieval-challenge ). In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge to train models which could be useful for the retrieval challenge. Note, however, that there are no landmarks in common between the training/index sets of the two challenges.","Submissions are evaluated using Global Average Precision (GAP) at \\(k\\), where \\(k=1\\). This metric is also known as micro Average Precision (microAP), as per [1]. It works as follows:
For each query image, you will predict one landmark label and a corresponding confidence score. The evaluation treats each prediction as an individual data point in a long list of predictions (sorted in descending order by confidence scores), and computes the Average Precision based on this list.
If a submission has \\(N\\) predictions (label/confidence pairs) sorted in descending order by their confidence scores, then the Global Average Precision is computed as:
$$GAP = \frac{1}{M}\sum_{i=1}^N P(i) rel(i)$$
where:
\\(N\\) is the total number of predictions returned by the system, across all queries
\\(M\\) is the total number of queries with at least one landmark from the training set visible in it (note that some queries may not depict landmarks)
\\(P(i)\\) is the precision at rank \\(i\\)
\\(rel(i)\\) denotes the relevance of prediciton \\(i\\): it’s 1 if the \\(i\\)-th prediction is correct, and 0 otherwise
[1] F. Perronnin, Y. Liu, and J.-M. Renders, ""A Family of Contextual Measures of Similarity between Distributions with Application to Image Retrieval,"" Proc. CVPR'09
Submission File
For each id in the test set, you can predict at most one landmark and its corresponding confidence score. Some query images may contain no landmarks. You may decide not to predict any result for a given query, by submitting an empty prediction. The submission file should contain a header and have the following format (larger scores denote more confident matches):
id,landmarks
000088da12d664db,8815 0.03
0001623c6d808702,
0001bbb682d45002,5328 0.5
etc.","Note: The Google Landmarks Dataset v1 is deprecated and no longer available. Please consider using the Google Landmarks Dataset v2 instead.
Note: Since this challenge is finalized, the data files (test.csv and train.csv) were transferred to the Google-Landmarks dataset webpage.
In this competition, you are asked to take test images and recognize which landmarks (if any) are depicted in them. The test images are listed in test.csv, while train.csv contains a large number of images labeled with their associated landmarks. Test images may depict no landmark, one landmark, or more than one landmark. The training set images each depict exactly one landmark. Each image has a unique id (a hash) and each landmark has a unique id (an integer).
Due to restrictions on distributing the actual files, the dataset contains a url for each image (this Python script may be useful to download the images). Since these images are not directly in our control, a small percentage of them will be unavailable (and the availability may change over time). To account for this, the organizers will download the images near the competition end date and adjust the solution file to ignore any missing test images.
You should expect this challenge to involve a small amount of leakage on account of image metadata. The hosts have intentionally selected images that do not contain geolocation information, but there may be predictive information present in the Exif data. Since Kaggle cannot detect whether this metadata is being used, consider its use to be permitted.
Dataset construction",kaggle competitions download -c landmark-recognition-challenge,"['https://www.kaggle.com/code/codename007/a-very-extensive-landmark-exploratory-analysis', 'https://www.kaggle.com/code/wesamelshamy/image-feature-extraction-and-matching-for-newbies', 'https://www.kaggle.com/code/anokas/python3-dataset-downloader-with-progress-bar', 'https://www.kaggle.com/code/tobwey/landmark-recognition-challenge-image-downloader', 'https://www.kaggle.com/code/gpreda/google-landmark-recogn-challenge-data-exploration']"
308,"Spot Nuclei. Speed Cures.
Imagine speeding up research for almost every disease, from lung cancer and heart disease to rare disorders. The 2018 Data Science Bowl offers our most ambitious mission yet: create an algorithm to automate nucleus detection.
We’ve all seen people suffer from diseases like cancer, heart disease, chronic obstructive pulmonary disease, Alzheimer’s, and diabetes. Many have seen their loved ones pass away. Think how many lives would be transformed if cures came faster.
By automating nucleus detection, you could help unlock cures faster—from rare disorders to the common cold. Want a snapshot about the 2018 Data Science Bowl? View this video.
Why nuclei?
Identifying the cells’ nuclei is the starting point for most analyses because most of the human body’s 30 trillion cells contain a nucleus full of DNA, the genetic code that programs each cell. Identifying nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work.
By participating, teams will work to automate the process of identifying nuclei, which will allow for more efficient drug testing, shortening the 10 years it takes for each new drug to come to market. Check out this video overview to find out more.
What will participants do?
Teams will create a computer model that can identify a range of nuclei across varied conditions. By observing patterns, asking questions, and building a model, participants will have a chance to push state-of-the-art technology farther.
Visit DataScienceBowl.com to:
• Sign up to receive news about the competition
• Learn about the history of the Data Science Bowl and past competitions
• Read our latest insights on emerging analytics techniques","This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:
$$
IoU(A,B) = \frac{A \cap B}{ A \cup B}.
$$
The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.
At each threshold value \(t\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:
$$
\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:
$$
\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}.
$$
Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.
Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed
and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.
The file should contain a header and have the following format. Each row in your submission represents a single predicted nucleus segmentation for the given ImageId.
ImageId,EncodedPixels  
0114f484a16c152baa2d82fdd43740880a762c93f436c8988ac461c5c9dbe7d5,1 1  
0999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,1 1  
0999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,2 3 8 9  
etc...
Submission files may take several minutes to process due to the size.","This dataset contains a large number of segmented nuclei images. The images were acquired under a variety of conditions and vary in the cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset is designed to challenge an algorithm's ability to generalize across these variations.
Each image is represented by an associated ImageId. Files belonging to an image are contained in a folder with this ImageId. Within this folder are two subfolders:
images contains the image file.
masks contains the segmented masks of each nucleus. This folder is only included in the training set. Each mask contains one nucleus. Masks are not allowed to overlap (no pixel belongs to two masks).
The second stage dataset will contain images from unseen experimental conditions. To deter hand labeling, it will also contain images that are ignored in scoring. The metric used to score this competition requires that your submissions are in run-length encoded format. Please see the evaluation page for details.
As with any human-annotated dataset, you may find various forms of errors in the data. You may manually correct errors you find in the training set. The dataset will not be updated/re-released unless it is determined that there are a large number of systematic errors. The masks of the stage 1 test set will be released with the release of the stage 2 test set.
File descriptions",kaggle competitions download -c data-science-bowl-2018,"['https://www.kaggle.com/code/keegil/keras-u-net-starter-lb-0-277', 'https://www.kaggle.com/code/stkbailey/teaching-notebook-for-total-imaging-newbies', 'https://www.kaggle.com/code/kmader/nuclei-overview-to-submission', 'https://www.kaggle.com/code/wcukierski/example-metric-implementation', 'https://www.kaggle.com/code/raoulma/nuclei-dsb-2018-tensorflow-u-net-score-0-352']"
309,"As part of the FGVC5 workshop at CVPR 2018 we are conducting the iNat Challenge 2018 large scale species classification competition. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features a large number of fine-grained categories with high class imbalance.
The iNat Challenge 2018 dataset contains over 8,000 species, with a combined training and validation set of 450,000 images that have been collected and verified by multiple users from iNaturalist. The dataset features many visually similar species, captured in a wide variety of situations, from all over the world.
Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC5 workshop. The iNat Challenge 2018 is sponsored by Microsoft.","We follow a similar metric to the classification tasks of the ILSVRC. For each image, an algorithm will produce 3 labels. We allow 3 labels because some categories are disambiguated with additional data provided by the observer, such as latitude, longitude and date. It might also be the case that multiple categories occur in an image (e.g. a photo of a bee on a flower). For this competition each image has one ground truth label. For a given image, if the ground truth label is found among the 3 predicted labels, then the error for that image is 0, otherwise it is 1. The final score is the error averaged across all images. See here for additional information.
Submission File
For each image in the test set, you must predict 3 class labels. The csv file should contain a header and have the following format:
id,predicted
12345,0 78 23
67890,83 13 42
The id column corresponds to the test image id. The predicted column corresponds to 3 category ids, separated by spaces. You should have one row for each test image. Please sort your predictions from most confident to least, from left to right, this will allow us to study top-1, top-2, and top-3 accuracy.","Data
Download the images and annotations here.
File descriptions
train_val2018.tar.gz - Contains the training and validation images in a directory structure following //.jpg .
train2018.json.tar.gz - Contains the training annotations.
val2018.json.tar.gz - Contains the validation annotations.
test2018.tar.gz - Contains a single directory of test images.
test2018.json.tar.gz - Contains test image information.
kaggle_sample_submission.csv - A sample submission file in the correct format.
Image Format
All images have been saved in the JPEG format and have been resized to have a maximum dimension of 800 pixels.
Annotation Format
We follow the annotation format of the COCO dataset and add additional fields. The annotations are stored in the JSON format and are organized as follows:",kaggle competitions download -c inaturalist-2018,[]
310,"After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.
To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.
In this competition, you’re challenged to build an algorithm to identifying whale species in images. You’ll analyze Happy Whale’s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you’ll help to open rich fields of understanding for marine mammal population dynamics around the globe.
We'd like to thank Happy Whale for providing this data and problem. Happy Whale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):
$$MAP@5 = \frac{1}{U} \sum_{u=1}^{U} \sum_{k=1}^{min(n,5)} P(k)$$
where U is the number of images, P(k) is the precision at cutoff k, and n is the number predictions per image.
Submission File
For each Image in the test set, you may predict up to 5 labels for the whale Id. Whales that are not predicted to be one of the labels in the training data should be labeled as new_whale. The file should contain a header and have the following format:
Image,Id 
00029b3a.jpg,new_whale w_1287fbc w_98baff9 w_7554f44 w_1eafe46
0003c693.jpg,new_whale w_1287fbc w_98baff9 w_7554f44 w_1eafe46
...","This training data contains thousands of images of humpback whale flukes. Individual whales have been identified by researchers and given an Id. The challenge is to predict the whale Id of images in the test set. What makes this such a challenge is that there are only a few examples for each of 3,000+ whale Ids.
File descriptions
train.zip - a folder containing the training images
train.csv - maps the training Image to the appropriate whale Id. Whales that are not predicted to have a label identified in the training data should be labeled as new_whale.
test.zip - a folder containing the test images to predict the whale Id
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c whale-categorization-playground,"['https://www.kaggle.com/code/martinpiotte/whale-recognition-model-with-score-0-78563', 'https://www.kaggle.com/code/martinpiotte/bounding-box-model', 'https://www.kaggle.com/code/lextoumbourou/humpback-whale-id-data-and-aug-exploration', 'https://www.kaggle.com/code/awsaf49/happywhale-boundingbox-yolov5', 'https://www.kaggle.com/code/gimunu/data-augmentation-with-keras-into-cnn']"
311,"While It's pretty easy for people to identify subtle differences in photos, computers still have a ways to go. Visually similar items are tough for computers to count, like this overlapping bunch of bananas:
Or, consider this photo of a family of foxes camouflaged in the wild - where do the foxes end and where does the grass begin?
To solve this problem and enhance the state of the art in object detection and classification, the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) began in 2010. Kaggle is excited and honored to be the new home of the official ImageNet Object Localization competition. Participants are challenged with identifying all objects within an image so those images can then be classified and annotated.
Already, because of this competition, there’s been a 4.2× reduction in image classification error (from 28.2% to 6.7%) and a 1.7× reduction in localization error (from 42.5% to 25.3%) between 2010 and 2014 alone. Can you improve the accuracy even further?
Competition Overview
The validation and test data will consist of 150,000 photographs, collected from Flickr and other search engines, hand labeled with the presence or absence of 1000 object categories. The 1000 object categories contain both internal nodes and leaf nodes of ImageNet, but do not overlap with each other.
A random subset of 50,000 of the images with labels will be released as the training set along with a list of the 1000 categories. The remaining images will be used as the test set.
The validation and test data for this competition are not contained in the ImageNet training data.","Evaluation
In this competition, the error for each image is defined as
$$e=min_i(min_j(max(d_{ij},f_{ij})))$$
where
\(d=0\) if the labels of the two boxes are the same, and \(d=1\) otherwise;
\(f=0\) if the overlap of the two boxes >= 50%, and \(f=1\) otherwise;
\(i\) is the predicted labels/bounding boxes, and \(j\) is the ground truth labels/bounding boxes.
For example, let's assume for a given image, there are 2 boxes as ground truth (g_0, g_1), and you predict 3 boxes in your prediction (p_0, p_1, p_2). We iterate through your prediction boxes, and see if they can find a ""match"" with any of the ground truth boxes. If there's a match, then the min error for this image is 0, otherwise, the min error is 1.
A match is defined as
the prediction box has a class label that is the same as the ground truth box, and
the prediction bounding box has over 50% match in the area with the ground truth bounding box.
The psudo-code:
min_error_list = []
for prediction_box in [p_0, p_1, p_2]:
    max_error_list = []
    for ground_truth_box in [g_0, g_1]:
        if label(prediction_box) == label(ground_truth_box):
            d = 0
        else:
            d = 1
        if overlap(prediction_box, ground_truth_box) > 0.5:
            f = 0
        else:
            f = 1
        max_error_list.append(max(d,f))   # the first max
    min_error_list.append(min(max_list))  # the first min

return min(min_error_list)  # the second min
Note the min error is either 0 or 1 for each image.
The total error is then computed as the average of all min errors of all the images in the test dataset.
Submission File
For each image in the test dataset, you will predict a list of label and bounding boxes.
It contains two columns:
ImageId: the id of the test image, for example ILSVRC2012_test_00000001
PredictionString: the prediction string should be a space delimited of 5 integers. For example, 1000 240 170 260 240 means it's label 1000, with a bounding box of coordinates (x_min, y_min, x_max, y_max). We accept up to 5 predictions. For example, if you submit 862 42 24 170 186 862 292 28 430 198 862 168 24 292 190 862 299 238 443 374 862 160 195 294 357 862 3 214 135 356 which contains 6 bounding boxes, we will only take the first 5 into consideration.
ImageId,PredictionString
ILSVRC2012_test_00000001,1000 240 170 260 240
ILSVRC2012_test_00000002,825 240 170 260 240 829 152 331 246 415
ILSVRC2012_test_00000003,862 42 24 170 186 862 292 28 430 198 862 168 24 292 190 862 299 238 443 374 862 160 195 294 357","File descriptions
ILSVRC/ contains the image data and ground truth for the train and validation sets, and the image data for the test set.
The image annotations are saved in XML files in PASCAL VOC format. Users can parse the annotations using the PASCAL Development Toolkit.
Annotations are ordered by their synsets (for example, ""Persian cat"", ""mountain bike"", or ""hot dog"") as their wnid. These id's look like n00141669. Each image's name has direct correspondence with the annotation file name. For example, the bounding box for n02123394/n02123394_28.xml is n02123394_28.JPEG.
You can download all the bounding boxes of a particular synset from http://www.image-net.org/api/download/imagenet.bbox.synset?wnid=[wnid]
The training images are under the folders with the names of their synsets. The validation images are all in the same folder. The test images are also all in the same folder.
ImageSet folder contains text files specifying lists of images for the main localization task.
LOC_sample_submission.csv is the correct format of the submission file. It contains two columns:
ImageId: the id of the test image, for example ILSVRC2012_test_00000001
PredictionString: the prediction string should be a space delimited of 5 integers. For example, 1000 240 170 260 240 means it's label 1000, with a bounding box of coordinates (x_min, y_min, x_max, y_max). We accept up to 5 predictions. For example, if you submit 862 42 24 170 186 862 292 28 430 198 862 168 24 292 190 862 299 238 443 374 862 160 195 294 357 862 3 214 135 356 which contains 6 bounding boxes, we will only take the first 5 into consideration.",kaggle competitions download -c imagenet-object-localization-challenge,"['https://www.kaggle.com/code/alraviemutiarmahesa/kaggle-download-subfolder-spesific-folder', 'https://www.kaggle.com/code/neerajnair/kernel76721cc377', 'https://www.kaggle.com/code/fanbyprinciple/beginner-efficientnet-simple-inference', 'https://www.kaggle.com/code/codgiw/vgg16-pre-trained-on-imagenet-transfer-learning', 'https://www.kaggle.com/code/luckad/comparison-pre-trained-cnn-models']"
312,"Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments.
The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).
In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.
Disclaimer: the dataset for this competition contains text that may be considered profane, vulgar, or offensive.","Update: Jan 30, 2018. Due to changes in the competition dataset, we have changed the evaluation metric of this competition.
Submissions are now evaluated on the mean column-wise ROC AUC. In other words, the score is the average of the individual AUCs of each predicted column.
Submission File
For each id in the test set, you must predict a probability for each of the six possible types of comment toxicity (toxic, severe_toxic, obscene, threat, insult, identity_hate). The columns must be in the same order as shown below. The file should contain a header and have the following format:
id,toxic,severe_toxic,obscene,threat,insult,identity_hate
00001cee341fdb12,0.5,0.5,0.5,0.5,0.5,0.5
0000247867823ef7,0.5,0.5,0.5,0.5,0.5,0.5
etc.","You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:
toxic
severe_toxic
obscene
threat
insult
identity_hate
You must create a model which predicts a probability of each type of toxicity for each comment.
File descriptions
train.csv - the training set, contains comments with their binary labels
test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c jigsaw-toxic-comment-classification-challenge,"['https://www.kaggle.com/code/jhoward/nb-svm-strong-linear-baseline', 'https://www.kaggle.com/code/jagangupta/stop-the-s-toxic-comments-eda', 'https://www.kaggle.com/code/sbongo/for-beginners-tackling-toxic-using-keras', 'https://www.kaggle.com/code/tunguz/logistic-regression-with-words-and-char-n-grams', 'https://www.kaggle.com/code/jhoward/improved-lstm-baseline-glove-dropout']"
313,"Finding footage of a crime caught on tape is an investigator's dream. But even with crystal clear, damning evidence, one critical question always remains–is the footage real?
Today, one way to help authenticate footage is to identify the camera that the image was taken with. Forgeries often require splicing together content from two different cameras. But, unfortunately, the most common way to do this now is using image metadata, which can be easily falsified itself.
This problem is actively studied by several researchers around the world. Many machine learning solutions have been proposed in the past: least-squares estimates of a camera's color demosaicing filters as classification features, co-occurrences of pixel value prediction errors as features that are passed to sophisticated ensemble classifiers, and using CNNs to learn camera model identification features. However, this is a problem yet to be sufficiently solved.
For this competition, the IEEE Signal Processing Society is challenging you to build an algorithm that identifies which camera model captured an image by using traces intrinsically left in the image. Helping to solve this problem would have a big impact on the verification of evidence used in criminal and civil trials and even news reporting.","This competition is evaluated on the weighted categorization accuracy of your predictions (the percentage of camera models correctly predicted).
$$\text{weighted_accuracy}(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} \frac{w_i (y_i = \hat{y}_i)}{\sum{w_i}} $$
where n is the number of samples in the test set, y is the true camera label, y_hat is the predicted camera label, and w_i is 0.7 for unaltered images, and 0.3 for altered images.
Submission File
For each image fname in the test set, you must predict a the correct camera model. The submission file should contain a header and have the following format:
fname,camera
img_0002a04_manip.tif,iPhone-6
img_001e31c_unalt.tif,iPhone-6
img_00275cf_manip.tif,iPhone-6
img_0034113_unalt.tif,iPhone-6","How the data was collected
Images in the training set were captured with 10 different camera models, a single device per model, with 275 full images from each device.
The list of camera models is as follows:
Sony NEX-7
Motorola Moto X
Motorola Nexus 6
Motorola DROID MAXX
LG Nexus 5x
Apple iPhone 6
Apple iPhone 4s
HTC One M7
Samsung Galaxy S4
Samsung Galaxy Note 3
Images in the test set were captured with the same 10 camera models, but using a second device. For example, if the images in the train data for the iPhone 6 were taken with Ben Hamner's device (Camera 1), the images in the test data were taken with Ben Hamner's device (Camera 2), since he lost the first device in the Bay while kite-surfing.",kaggle competitions download -c sp-society-camera-model-identification,"['https://www.kaggle.com/code/zeemeen/i-have-a-clue-what-i-am-doing-noise-patterns', 'https://www.kaggle.com/code/kmader/transfer-learning-with-inceptionv3', 'https://www.kaggle.com/code/inversion/i-have-no-clue-what-i-m-doing-benchmark', 'https://www.kaggle.com/code/CVxTz/keras-simple-cnn-starter', 'https://www.kaggle.com/code/toshif/ieee-image-manipulation-kernel']"
314,"Innovative materials design is needed to tackle some of the most important health, environmental, energy, social, and economic challenges of this century. In particular, improving the properties of materials that are intrinsically connected to the generation and utilization of energy is crucial if we are to mitigate environmental damage due to a growing global demand. Transparent conductors are an important class of compounds that are both electrically conductive and have a low absorption in the visible range, which are typically competing properties. A combination of both of these characteristics is key for the operation of a variety of technological devices such as photovoltaic cells, light-emitting diodes for flat-panel displays, transistors, sensors, touch screens, and lasers. However, only a small number of compounds are currently known to display both transparency and conductivity suitable enough to be used as transparent conducting materials.
Aluminum (Al), gallium (Ga), indium (In) sesquioxides are some of the most promising transparent conductors because of a combination of both large bandgap energies, which leads to optical transparency over the visible range, and high conductivities. These materials are also chemically stable and relatively inexpensive to produce. Alloying of these binary compounds in ternary or quaternary mixtures could enable the design of a new material at a specific composition with improved properties over what is current possible. These alloys are described by the formula \((Al_{x}Ga_{y}In_{z})_{2N}O_{3N}\); where x, y, and z can vary but are limited by the constraint x+y+z = 1. The total number of atoms in the unit cell, \(N_{total} = 2N+3N\)(where N is an integer), is typically between 5 and 100. However, the main limitation in the design of compounds is that identification and discovery of novel materials for targeted applications requires an examination of enormous compositional and configurational degrees of freedom (i.e., many combinations of x, y, and z). To avoid costly and inefficient trial-and-error of synthetic routes, computational data-driven methods can be used to guide the discovery of potentially more efficient materials to aid in the development of advanced (or totally new) technologies. In computational material science, the standard tool for computing these properties is the quantum-mechanical method known as density-functional theory (DFT). However, DFT calculations are expensive, requiring hundreds or thousands of CPU hours on supercomputers for large systems, which prohibits the modeling of a sizable number of possible compositions and configurations. As a result, potential \((Al_{x}Ga_{y}In_{z})_{2N}O_{3N}\) materials remain relatively unexplored. Data-driven models offer an alternative approach to efficiently search for new possible compounds in targeted applications but at a significantly reduced computational cost.
This competition aims to accomplish this goal by asking participants to develop or apply data analytics/data mining/machine-learning models for the prediction of two target properties: the formation energy (which is an indication of the stability of a new material) and the bandgap energy (which is an indication of the potential for transparency over the visible range) to facilitate the discovery of new transparent conductors and allow for advancements in the above-mentioned technologies.","Submissions are evaluated on the column-wise root mean squared logarithmic error.
The RMSLE for a single column calculated as
$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },
$$
where:
\\(n\\) is the total number of observations
\\(p_i\\) is your prediction
\\(a_i\\) is the actual value
\\(\log(x)\\) is the natural logarithm of \\(x\\)
The final score is the mean of the RMSLE over all columns (in this case, 2).
Submission File
For each id in the test set, you must predict a value for both formation_energy_ev_natom and bandgap_energy_ev. The file should contain a header and have the following format:
id,formation_energy_ev_natom,bandgap_energy_ev
1,0.1779,1.8892
2,0.1779,1.8892
3,0.1779,1.8892
...","High-quality data are provided for 3,000 materials that show promise as transparent conductors. The following information has been included:
Spacegroup (a label identifying the symmetry of the material)
Total number of Al, Ga, In and O atoms in the unit cell \((N_{total}\))
Relative compositions of Al, Ga, and In (x, y, z)
Lattice vectors and angles: lv1, lv2, lv3 (which are lengths given in units of angstroms (\(10^{-10}\) meters) and α, β, γ (which are angles in degrees between 0° and 360°)
A domain expert will understand the physical meaning of the above information but those with a data mining background may simply use the data as input for their models.
The task for this competition is to predict two target properties:
Formation energy (an important indicator of the stability of a material)
Bandgap energy (an important property for optoelectronic applications)
File Descriptions
Note: For each line of the CSV file, the corresponding spatial positions of all of the atoms in the unit cell (expressed in Cartesian coordinates) are provided as a separate file.",kaggle competitions download -c nomad2018-predict-transparent-conductors,"['https://www.kaggle.com/code/headsortails/resistance-is-futile-transparent-conductors-eda', 'https://www.kaggle.com/code/tonyyy/how-to-get-atomic-coordinates', 'https://www.kaggle.com/code/cbartel/random-forest-using-elemental-properties', 'https://www.kaggle.com/code/kemuel/python-exploration-with-domain-knowledge', 'https://www.kaggle.com/code/haimfeld87/simple-catboost']"
315,"‘Tis the night before Christmas
year: two thousand seventeen.
Santa’s grown grouchy,
borderline mean.
What used to be simple for Old St. Nick,
is now too puzzling, it’s making him sick!
See, Santa always knew, deep down in his gut,
what toy each kid wanted–no ifs, ands, or buts.
But fierce population growth, more twins, and toy innovation,
has left too complex a problem, in dire need of optimization.
“Don’t worry, Mr. Santa”, said an Elf named McMaggle,
“I have a solution! Have you heard of Kaggle?”
As she explained Kaggle in-depth, Santa’s doubt began turning,
he became a believer in the magic of...machine learning.
So, Santa’s team needs YOU more than ever this year,
to solve this painful problem and save Christmas cheer.
The Challenge
In this playground competition, you’re challenged to build a toy matching algorithm that maximizes happiness by pairing kids with toys they want. In the dataset, each kid has 10 preferences for their gift (from 1000) and Santa has 1000 preferred kids for every gift available. What makes this extra difficult is that 0.4% of the kids are twins, and by their parents’ request, require the same gift.","Your goal is to maximize the
Average Normalized Happiness (ANH) = (AverageNormalizedChildHappiness (ANCH) ) ^ 3 + (AverageNormalizedSantaHappiness (ANSH) ) ^ 3
where NormalizedChildHappiness is the happiness of each child, divided by the maximum possible happiness, and NormalizedSantaHappiness is the happiness of each gift, divided by the maximum possible happiness.
Note the cubic terms with ANCH and ANSH.
in the equation form:
$$ANCH = \frac{1}{n_c} \sum_{i=0}^{n_c-1} \frac{ChildHappiness}{MaxChildHappiness},$$
$$ANSH = \frac{1}{n_g} \sum_{i=0}^{n_g-1} \frac{GiftHappiness}{MaxGiftHappiness}.$$
\(n_c\) is the number of children.
\(n_g\) is the number of gifts
MaxChildHappiness = len(ChildWishList) * 2,
MaxGiftHappiness = len(GiftGoodKidsList) * 2.
ChildHappiness = 2 * GiftOrder if the gift is found in the wish list of the child.
ChildHappiness = -1 if the gift is out of the child's wish list.
Similarly,
GiftHappiness = 2 * ChildOrder if the child is found in the good kids list of the gift.
GiftHappiness = -1 if the child is out of the gift's good kids list.
For example, if a child has a preference of gifts [5,2,3,1,4], and is given gift 3, then
ChildHappiness = [len(WishList)-indexOf(gift_3)] * 2 = [5 - 2] * 2 = 6
If this child is given gift 4, then
ChildHappiness = [5-4] * 2 = 2
Code sample of Average Normalized Happiness can be seen from this Kernel.
Submission File
For each child in the dataset, you will match it with a gift. Remember, the first 0.5% of rows (ChildId 0 to 5000) are triplets, and the following 4% (ChildId 5001-45000) are twins.
ChildId,GiftId
0,669
1,669
2,669
3,8
4,8
5,8
6,689
7,689
8,689","In this competition, you are given a list of 1,000,000 children and their wish lists of 100 gifts. You are also given a list of 1000 gifts, and their list of 1000 good kids that they prefer to give to.
Your goal is to match the list of 1,000,000 children with a gift for each child, and try to make everyone happy. Both the kids and Santa need to be happy - for kids, the higher the gift is on their wish list, the happier, for Santa and his gifts, the higher the child is on the good kids list, the happier Santa is.
A few details to notice:
The first 0.5% (ChildId 0-5000) children are triplets. More particularly, 0, 1, 2 are triplets, 3, 4, 5, are triplets …. 4998, 4999, 5000 are triplets. Triplets need to be given the same gift even though they might have different preferences.
The next 4% (ChildId 5001-45000) children are twins. More particularly, 5001 and 5002 are twins, 5003 and 5004 are twins, …. 44999 and 45000 are twins. Twins need to be given the same gift even though they might have different preferences.
For each GiftId, there are 1000 of them available. There are exactly the same number of gifts available (1000 * 1000 = 1,000,000). You shall not exceed the quantity of 1000 for each GiftId.
You are scored on the Average Normalized Happiness. Please refer to the Evaluation page to see detailed descriptions.
File descriptions",kaggle competitions download -c santa-gift-matching,"['https://www.kaggle.com/code/zfturbo/happiness-vs-gift-popularity-v2-0-89', 'https://www.kaggle.com/code/zfturbo/infinite-probabilistic-improver-0-931', 'https://www.kaggle.com/code/zfturbo/max-flow-with-min-cost-v2-0-9267', 'https://www.kaggle.com/code/gaborfodor/improve-with-the-hungarian-method-0-9375', 'https://www.kaggle.com/code/golubev/simple-example-min-cost-flow']"
316,"Running a thriving local restaurant isn't always as charming as first impressions appear. There are often all sorts of unexpected troubles popping up that could hurt business.
One common predicament is that restaurants need to know how many customers to expect each day to effectively purchase ingredients and schedule staff members. This forecast isn't easy to make because many unpredictable factors affect restaurant attendance, like weather and local competition. It's even harder for newer restaurants with little historical data.
Recruit Holdings has unique access to key datasets that could make automated future customer prediction possible. Specifically, Recruit Holdings owns Hot Pepper Gourmet (a restaurant review service), AirREGI (a restaurant point of sales service), and Restaurant Board (reservation log management software).
In this competition, you're challenged to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be much more efficient and allow them to focus on creating an enjoyable dining experience for their customers.","Submissions are evaluated on the root mean squared logarithmic error.
The RMSLE is calculated as
$$\sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 },
$$
where:
\\(n\\) is the total number of observations
\\(p_i\\) is your prediction of visitors
\\(a_i\\) is the actual number of visitors
\\(\log(x)\\) is the natural logarithm of \\(x\\)
Submission File
For every store and date combination in the test set, submission files should contain two columns: id and visitors.  The id is formed by concatenating the air_store_id and visit_date with an underscore. The file should contain a header and have the following format:
id,visitors
air_00a91d42b08b08d9_2017-04-23,0  
air_00a91d42b08b08d9_2017-04-24,0  
air_00a91d42b08b08d9_2017-04-25,0  
etc.","In this competition, you are provided a time-series forecasting problem centered around restaurant visitors. The data comes from two separate sites:
Hot Pepper Gourmet (hpg): similar to Yelp, here users can search restaurants and also make a reservation online
AirREGI / Restaurant Board (air): similar to Square, a reservation control and cash register system
You must use the reservations, visits, and other information from these sites to forecast future restaurant visitor totals on a given date. The training data covers the dates from 2016 until April 2017. The test set covers the last week of April and May of 2017. The test set is split based on time (the public fold coming first, the private fold following the public) and covers a chosen subset of the air restaurants. Note that the test set intentionally spans a holiday week in Japan called the ""Golden Week.""
There are days in the test set where the restaurant were closed and had no visitors. These are ignored in scoring. The training set omits days where the restaurants were closed.
File Descriptions
This is a relational dataset from two systems. Each file is prefaced with the source (either air_ or hpg_) to indicate its origin. Each restaurant has a unique air_store_id and hpg_store_id. Note that not all restaurants are covered by both systems, and that you have been provided data beyond the restaurants for which you must forecast. Latitudes and Longitudes are not exact to discourage de-identification of restaurants.",kaggle competitions download -c recruit-restaurant-visitor-forecasting,"['https://www.kaggle.com/code/headsortails/be-my-guest-recruit-restaurant-eda', 'https://www.kaggle.com/code/pureheart/1st-place-lgb-model-public-0-470-private-0-502', 'https://www.kaggle.com/code/tunguz/surprise-me-2', 'https://www.kaggle.com/code/asindico/a-japanese-journey', 'https://www.kaggle.com/code/plantsgo/solution-public-0-471-private-0-505']"
317,"Can you differentiate a weed from a crop seedling?
The ability to do so effectively can mean better crop yields and better stewardship of the environment.
The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.
We're hosting this dataset as a Kaggle competition in order to give it wider exposure, to give the community an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas.
Acknowledgments
We extend our appreciation to the Aarhus University Department of Engineering Signal Processing Group for hosting the original data.
Citation
A Public Image Database for Benchmark of Plant Seedling Classification Algorithms","Submissions are evaluated on MeanFScore,
which at Kaggle is actually a micro-averaged F1-score.
Given positive/negative rates for each class k, the resulting score is computed this way:
$$Precision_{micro} = \frac{\sum_{k \in C} TP_k}{\sum_{k \in C} TP_k + FP_k}$$
$$Recall_{micro} = \frac{\sum_{k \in C} TP_k}{\sum_{k \in C} TP_k + FN_k}$$
F1-score is the harmonic mean of precision and recall
$$MeanFScore = F1_{micro}= \frac{2 Precision_{micro} Recall_{micro}}{Precision_{micro} + Recall_{micro}}$$
Submission File
For eachfile in the test set, you must predict a probability for the species variable. The file should contain a header and have the following format:
file,species
0021e90e4.png,Maize
003d61042.png,Sugar beet
007b3da8b.png,Common wheat
etc.","You are provided with a training set and a test set of images of plant seedlings at various stages of grown. Each image has a filename that is its unique id. The dataset comprises 12 plant species. The goal of the competition is to create a classifier capable of determining a plant's species from a photo. The list of species is as follows:
Black-grass
Charlock
Cleavers
Common Chickweed
Common wheat
Fat Hen
Loose Silky-bent
Maize
Scentless Mayweed
Shepherds Purse
Small-flowered Cranesbill
Sugar beet
File descriptions
train.csv - the training set, with plant species organized by folder",kaggle competitions download -c plant-seedlings-classification,"['https://www.kaggle.com/code/gaborfodor/seedlings-pretrained-keras-models', 'https://www.kaggle.com/code/gaborvecsei/plant-seedlings-fun-with-computer-vision', 'https://www.kaggle.com/code/nikkonst/plant-seedlings-with-cnn-and-image-processing', 'https://www.kaggle.com/code/miklgr500/keras-simple-model-0-97103-best-public-score', 'https://www.kaggle.com/code/matrixb/cnn-svm-xgboost']"
318,"It can be hard to know how much something’s really worth. Small details can mean big differences in pricing. For example, one of these sweaters cost $335 and the other cost $9.99. Can you guess which one’s which?
Product pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs.
Mercari, Japan’s biggest community-powered shopping app, knows this problem deeply. They’d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari's marketplace.
In this competition, Mercari’s challenging you to build an algorithm that automatically suggests the right product prices. You’ll be provided user-inputted text descriptions of their products, including details like product category name, brand name, and item condition.
Note that, because of the public nature of this data, this competition is a “Kernels Only” competition. In the second stage of the challenge, files will only be available through Kernels and you will not be able to modify your approach in response to new data. Read more details in the data tab and Kernels FAQ page.","The evaluation metric for this competition is Root Mean Squared Logarithmic Error.
The RMSLE is calculated as
$$
\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }
$$
Where:
\\(\epsilon\\) is the RMSLE value (score)
\\(n\\) is the total number of observations in the (public/private) data set,
\\(p_i\\) is your prediction of price, and
\\(a_i\\) is the actual sale price for \\(i\\).
\\(\log(x)\\) is the natural logarithm of \\(x\\)
Submission File
For every row in the dataset, submission files should contain two columns: test_id and price.  The id corresponds to the column of that id in the test.tsv. The file should contain a header and have the following format:
test_id,price
0,1.50
1,50
2,500
3,100
etc.","In this competition, you will predict the sale price of a listing based on information a user provides for this listing. This is a Kernels-only competition, the files in this Data section are downloadable just for your reference in Stage 1. Stage 2 files will only be available in Kernels and not available for download here.
Data fields
train.tsv, test.tsv
The files consist of a list of product listings. These files are tab-delimited.
train_id or test_id - the id of the listing
name - the title of the listing. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm]
item_condition_id - the condition of the items provided by the seller
category_name - category of the listing
brand_name
price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn't exist in test.tsv since that is what you will predict.
shipping - 1 if shipping fee is paid by seller and 0 by buyer",kaggle competitions download -c mercari-price-suggestion-challenge,"['https://www.kaggle.com/code/thykhuely/mercari-interactive-eda-topic-modelling', 'https://www.kaggle.com/code/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s', 'https://www.kaggle.com/code/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl', 'https://www.kaggle.com/code/lopuhin/eli5-for-mercari', 'https://www.kaggle.com/code/captcalculator/a-very-extensive-mercari-exploratory-analysis']"
319,"We might be on the verge of too many screens. It seems like everyday, new versions of common objects are “re-invented” with built-in wifi and bright touchscreens. A promising antidote to our screen addiction are voice interfaces.
But, for independent makers and entrepreneurs, it’s hard to build a simple speech detector using free, open data and code. Many voice recognition datasets require preprocessing before a neural network model can be built on them. To help with this, TensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people.
In this competition, you're challenged to use the Speech Commands Dataset to build an algorithm that understands simple spoken commands. By improving the recognition accuracy of open-sourced voice interface tools, we can improve product effectiveness and their accessibility.","Submissions are evaluated on Multiclass Accuracy, which is simply the average number of observations with the correct label.
Note: There are only 12 possible labels for the Test set: yes, no, up, down, left, right, on, off, stop, go, silence, unknown.
The unknown label should be used for a command that is not one one of the first 10 labels or that is not silence.
Submission File
For audio clip in the test set, you must predict the correct label. The submission file should contain a header and have the following format:
fname,label
clip_000044442.wav,silence
clip_0000adecb.wav,left
clip_0000d4322.wav,unknown
etc.","The following is a high level overview of the data for this competition. It is highly recommended that you read the supplementary materials found on the tabs of the Overview page.
File descriptions
train.7z - Contains a few informational files and a folder of audio files. The audio folder contains subfolders with 1 second clips of voice commands, with the folder name being the label of the audio clip. There are more labels that should be predicted. The labels you will need to predict in Test are yes, no, up, down, left, right, on, off, stop, go. Everything else should be considered either unknown or silence. The folder _background_noise_ contains longer clips of ""silence"" that you can break up and use as training input.

The files contained in the training audio are not uniquely named across labels, but they are unique if you include the label folder. For example, 00f0204f_nohash_0.wav is found in 14 folders, but that file is a different speech command in each folder.

The files are named so the first element is the subject id of the person who gave the voice command, and the last element indicated repeated commands. Repeated commands are when the subject repeats the same word multiple times. Subject id is not provided for the test data, and you can assume that the majority of commands in the test data were from subjects not seen in train.

You can expect some inconsistencies in the properties of the training data (e.g., length of the audio).",kaggle competitions download -c tensorflow-speech-recognition-challenge,"['https://www.kaggle.com/code/davids1992/speech-representation-and-data-exploration', 'https://www.kaggle.com/code/alexozerin/end-to-end-baseline-tf-estimator-lb-0-72', 'https://www.kaggle.com/code/CVxTz/audio-data-augmentation', 'https://www.kaggle.com/code/timolee/audio-data-conversion-to-images-eda', 'https://www.kaggle.com/code/haqishen/augmentation-methods-for-audio']"
320,"Drifting icebergs present threats to navigation and activities in areas such as offshore of the East Coast of Canada.
Currently, many institutions and companies use aerial reconnaissance and shore-based support to monitor environmental conditions and assess risks from icebergs. However, in remote areas with particularly harsh weather, these methods are not feasible, and the only viable monitoring option is via satellite.
Statoil, an international energy company operating worldwide, has worked closely with companies like C-CORE. C-CORE have been using satellite data for over 30 years and have built a computer vision based surveillance system. To keep operations safe and efficient, Statoil is interested in getting a fresh new perspective on how to use machine learning to more accurately detect and discriminate against threatening icebergs as early as possible.
In this competition, you’re challenged to build an algorithm that automatically identifies if a remotely sensed target is a ship or iceberg. Improvements made will help drive the costs down for maintaining safe working conditions.","Submissions are evaluated on the log loss between the predicted values and the ground truth.
Submission File
For each id in the test set, you must predict the probability that the image contains an iceberg (a number between 0 and 1). The file should contain a header and have the following format:
id,is_iceberg
809385f7,0.5
7535f0cd,0.4
3aa99a38,0.9
etc.","In this competition, you will predict whether an image contains a ship or an iceberg. The labels are provided by human experts and geographic knowledge on the target. All the images are 75x75 images with two bands.
Data fields
train.json, test.json
The data (train.json, test.json) is presented in json format.
The files consist of a list of images, and for each image, you can find the following fields:
id - the id of the image
band_1, band_2 - the flattened image data. Each band has 75x75 pixel values in the list, so the list has 5625 elements. Note that these values are not the normal non-negative integers in image files since they have physical meanings - these are float numbers with unit being dB. Band 1 and Band 2 are signals characterized by radar backscatter produced from different polarizations at a particular incidence angle. The polarizations correspond to HH (transmit/receive horizontally) and HV (transmit horizontally and receive vertically). More background on the satellite imagery can be found here.
inc_angle - the incidence angle of which the image was taken. Note that this field has missing data marked as ""na"", and those images with ""na"" incidence angles are all in the training data to prevent leakage.
is_iceberg - the target variable, set to 1 if it is an iceberg, and 0 if it is a ship. This field only exists in train.json.",kaggle competitions download -c statoil-iceberg-classifier-challenge,"['https://www.kaggle.com/code/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d', 'https://www.kaggle.com/code/muonneutrino/exploration-transforming-images-in-python', 'https://www.kaggle.com/code/dongxu027/explore-stacking-lb-0-1463', 'https://www.kaggle.com/code/dimitrif/domain-knowledge', 'https://www.kaggle.com/code/devm2024/transfer-learning-with-vgg-16-cnn-aug-lb-0-1712']"
321,"Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming.
The problem becomes more complex as retailers add new locations with unique needs, new products, ever transitioning seasonal tastes, and unpredictable product marketing. Corporación Favorita, a large Ecuadorian-based grocery retailer, knows this all too well. They operate hundreds of supermarkets, with over 200,000 different products on their shelves.
Corporación Favorita has challenged the Kaggle community to build a model that more accurately forecasts product sales. They currently rely on subjective forecasting methods with very little data to back them up and very little automation to execute plans. They’re excited to see how machine learning could better ensure they please customers by having just enough of the right products at the right time.","Submissions are evaluated on the Normalized Weighted Root Mean Squared Logarithmic Error (NWRMSLE), calculated as follows:
$$ NWRMSLE = \sqrt{ \frac{\sum_{i=1}^n w_i \left( \ln(\hat{y}i + 1) - \ln(y_i +1)  \right)^2  }{\sum{i=1}^n w_i}} $$
where for row i, \(\hat{y}_i\) is the predicted unit_sales of an item and \(y_i\) is the actual unit_sales; n is the total number of rows in the test set.
The weights, \(w_i\), can be found in the items.csv file (see the Data page). Perishable items are given a weight of 1.25 where all other items are given a weight of 1.00.
This metric is suitable when predicting values across a large range of orders of magnitudes. It avoids penalizing large differences in prediction when both the predicted and the true number are large: predicting 5 when the true value is 50 is penalized more than predicting 500 when the true value is 545.
Submission File
For eachid in the test set, you must predict theunit_sales. Because the metric uses ln(y+1), submissions are validated to ensure there are no negative predictions.
The file should contain a header and have the following format:
id,unit_sales
125497040,2.5
125497041,0.0
125497042,27.9
etc.","In this competition, you will be predicting the unit sales for thousands of items sold at different Favorita stores located in Ecuador. The training data includes dates, store and item information, whether that item was being promoted, as well as the unit sales. Additional files include supplementary information that may be useful in building your models.
File Descriptions and Data Field Information
train.csv
Training data, which includes the target unit_sales by date, store_nbr, and item_nbr and a unique id to label rows.
The target unit_sales can be integer (e.g., a bag of chips) or float (e.g., 1.5 kg of cheese).
Negative values of unit_sales represent returns of that particular item.
The onpromotion column tells whether that item_nbr was on promotion for a specified date and store_nbr.
Approximately 16% of the onpromotion values in this file are NaN.
NOTE: The training data does not include rows for items that had zero unit_sales for a store/date combination. There is no information as to whether or not the item was in stock for the store on the date, and teams will need to decide the best way to handle that situation. Also, there are a small number of items seen in the training data that aren't seen in the test data.",kaggle competitions download -c favorita-grocery-sales-forecasting,"['https://www.kaggle.com/code/headsortails/shopping-for-insights-favorita-eda', 'https://www.kaggle.com/code/arthurtok/comprehensive-python-and-d3-js-favorita-analytics', 'https://www.kaggle.com/code/shixw125/1st-place-lgb-model-public-0-506-private-0-511', 'https://www.kaggle.com/code/paulorzp/log-ma-and-days-of-week-means-lb-0-529', 'https://www.kaggle.com/code/ceshine/lgbm-starter']"
322,"Who's a good dog? Who likes ear scratches? Well, it seems those fancy deep neural networks don't have all the answers. However, maybe they can answer that ubiquitous question we all ask when meeting a four-legged stranger: what kind of good pup is that?
In this playground competition, you are provided a strictly canine subset of ImageNet in order to practice fine-grained image categorization. How well you can tell your Norfolk Terriers from your Norwich Terriers? With 120 breeds of dogs and a limited number training images per class, you might find the problem more, err, ruff than you anticipated.
Acknowledgments
We extend our gratitude to the creators of the Stanford Dogs Dataset for making this competition possible: Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li.","Submissions are evaluated on Multi Class Log Loss between the predicted probability and the observed target.
Submission File
For each image in the test set, you must predict a probability for each of the different breeds. The file should contain a header and have the following format:
id,affenpinscher,afghan_hound,..,yorkshire_terrier
000621fb3cbb32d8935728e48679680e,0.0083,0.0,...,0.0083
etc.","You are provided with a training set and a test set of images of dogs. Each image has a filename that is its unique id. The dataset comprises 120 breeds of dogs. The goal of the competition is to create a classifier capable of determining a dog's breed from a photo. The list of breeds is as follows:
affenpinscher
afghan_hound
african_hunting_dog
airedale
american_staffordshire_terrier
appenzeller
australian_terrier
basenji
basset
beagle
bedlington_terrier
bernese_mountain_dog
black-and-tan_coonhound
blenheim_spaniel
bloodhound
bluetick
border_collie
border_terrier
cairn
cardigan
chesapeake_bay_retriever
chihuahua
chow
clumber
cocker_spaniel
collie
curly-coated_retriever
dandie_dinmont
dhole
doberman
english_foxhound
english_setter
english_springer
entlebucher
eskimo_dog
flat-coated_retriever
french_bulldog
german_shepherd
german_short-haired_pointer
giant_schnauzer
golden_retriever
gordon_setter
great_dane
great_pyrenees
greater_swiss_mountain_dog
groenendael
ibizan_hound
irish_setter
irish_terrier
irish_water_spaniel
irish_wolfhound
italian_greyhound
keeshond
kelpie
kerry_blue_terrier
komondor
kuvasz
labrador_retriever
lakeland_terrier
leonberg
malamute
malinois
maltese_dog
mexican_hairless
miniature_pinscher
miniature_poodle
miniature_schnauzer
newfoundland
old_english_sheepdog
otterhound
papillon
pekinese
pembroke
pomeranian
pug
redbone
rhodesian_ridgeback
rottweiler
saint_bernard
saluki
samoyed
sealyham_terrier
siberian_husky
silky_terrier
soft-coated_wheaten_terrier
staffordshire_bullterrier
standard_poodle
standard_schnauzer
sussex_spaniel
tibetan_mastiff
tibetan_terrier
toy_poodle
toy_terrier
vizsla
walker_hound
weimaraner
welsh_springer_spaniel
west_highland_white_terrier
whippet
wire-haired_fox_terrier
yorkshire_terrier",kaggle competitions download -c dog-breed-identification,"['https://www.kaggle.com/code/dansbecker/tensorflow-programming', 'https://www.kaggle.com/code/gaborfodor/dog-breed-pretrained-keras-models-lb-0-3', 'https://www.kaggle.com/code/dansbecker/exercise-intro-to-dl-for-computer-vision', 'https://www.kaggle.com/code/pvlima/use-pretrained-pytorch-models', 'https://www.kaggle.com/code/yangpeiwen/keras-inception-xception-0-47']"
323,"The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build a better music recommendation system using a donated dataset from KKBOX. WSDM (pronounced ""wisdom"") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models.
Not many years ago, it was inconceivable that the same person would listen to the Beatles, Vivaldi, and Lady Gaga on their morning commute. But, the glory days of Radio DJs have passed, and musical gatekeepers have been replaced with personalizing algorithms and unlimited streaming services.
While the public’s now listening to all kinds of music, algorithms still struggle in key areas. Without enough historical data, how would an algorithm know if listeners will like a new song or a new artist? And, how would it know what songs to recommend brand new users?
WSDM has challenged the Kaggle ML community to help solve these problems and build a better music recommendation system. The dataset is from KKBOX, Asia’s leading music streaming service, holding the world’s most comprehensive Asia-Pop music library with over 30 million tracks. They currently use a collaborative filtering based algorithm with matrix factorization and word embedding in their recommendation system but believe new techniques could lead to better results.
Winners will present their findings at the conference February 6-8, 2018 in Los Angeles, CA. For more information on the conference, click here, and don't forget to check out the other KKBox/WSDM competition: KKBox Music Churn Prediction Challenge","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:
id,target
2,0.3
5,0.1
6,1
etc.",,,
324,,,,,
325,,,,,
326,,,,,
327,"As many of us can attest, learning another language is tough. Picking up on nuances like slang, dates and times, and local expressions, can often be a distinguishing factor between proficiency and fluency. This challenge is even more difficult for a machine.
Many speech and language applications, including text-to-speech synthesis (TTS) and automatic speech recognition (ASR), require text to be converted from written expressions into appropriate ""spoken"" forms. This is a process known as text normalization, and helps convert 12:47 to ""twelve forty-seven"" and $3.16 into ""three dollars, sixteen cents."" 
However, one of the biggest challenges when developing a TTS or ASR system for a new language is to develop and test the grammar for all these rules, a task that requires quite a bit of linguistic sophistication and native speaker intuition.
A    <self>
baby    <self>
giraffe    <self>
is    <self>
6ft    six feet
tall    <self>
and    <self>
weighs    <self>
150lb    one hundred fifty pounds
.    sil
In this competition, you are challenged to automate the process of developing text normalization grammars via machine learning. This track will focus on English, while a separate will focus on Russian here: Russian Text Normalization Challenge
About the sponsor
Google's Text Normalization Research Group conducts research and creates tools for the detection, normalization and denormalization of non-standard words such as abbreviations, numbers or currency expressions; and semiotic classes -- text tokens and token sequences that represent particular entities that are semantically constrained, such as measure phrases, addresses or dates. Applications of this work include text-to-speech synthesis, automatic speech recognition, and information extraction/retrieval.","Submissions are evaluated on prediction accuracy (the total percent of correct tokens). The predicted and actual string must match exactly in order to count as correct. In other words, we are measuring sequence accuracy, in that any error in the output for a given token in the input sequence means that that error is wrong. For example, if the input is ""145"" and the predicted output is ""one forty five"" but the correct output is ""one hundred forty five"", this is counted as a single error.
Submission File
For each token (id) in the test set, you must predict the normalized text. The file should contain a header and have the following format:
id,after
0_0,""the""
0_1,""quick""
0_2,""fox""
...","You are provided with a large corpus of text. Each sentence has a sentence_id. Each token within a sentence has a token_id. The before column contains the raw text, the after column contains the normalized text. The aim of the competition is to predict the after column for the test set. The training set contains an additional column, class, which is provided to show the token type. This column is intentionally omitted from the test set. In addition, there is an id column used in the submission format. This is formed by concatenating the sentence_id and token_id with an underscore (e.g. 123_5).
File descriptions
en_sample_submission.csv - a submission file showing the correct format
en_test.csv - the test set, does not contain the normalized text
en_train.csv - the training set, contains the normalized text",kaggle competitions download -c text-normalization-challenge-english-language,"['https://www.kaggle.com/code/alphasis/xgboost-with-context-label-data-acc-99-637', 'https://www.kaggle.com/code/headsortails/watch-your-language-update-feature-engineering', 'https://www.kaggle.com/code/arthurtok/zoomable-circle-packing-via-d3-js-in-ipython', 'https://www.kaggle.com/code/neerjad/class-wise-regex-functions-l-b-0-995', 'https://www.kaggle.com/code/allunia/eda-en-text-normalization']"
328,"As with any big purchase, full information and transparency are key. While most everyone describes buying a used car as frustrating, it’s just as annoying to sell one, especially online. Shoppers want to know everything about the car but they must rely on often blurry pictures and little information, keeping used car sales a largely inefficient, local industry.
Carvana, a successful online used car startup, has seen opportunity to build long term trust with consumers and streamline the online buying process.
An interesting part of their innovation is a custom rotating photo studio that automatically captures and processes 16 standard images of each vehicle in their inventory. While Carvana takes high quality photos, bright reflections and cars with similar colors as the background cause automation errors, which requires a skilled photo editor to change.


In this competition, you’re challenged to develop an algorithm that automatically removes the photo studio background. This will allow Carvana to superimpose cars on a variety of backgrounds. You’ll be analyzing a dataset of photos, covering different vehicles with a wide variety of year, make, and model combinations.","This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:
$$ \frac{2 * |X \cap Y|}{|X| + |Y|},$$
where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.
Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.  Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
The file should contain a header and have the following format:
img,rle_mask
0004d4463b50_01,1 1 5 1
0004d4463b50_02,1 1
0004d4463b50_03,1 1
etc.
Submission files may take several minutes to process due to the size.","This dataset contains a large number of car images (as .jpg files). Each car has exactly 16 images, each one taken at different angles. Each car has a unique id and images are named according to id_01.jpg, id_02.jpg … id_16.jpg. In addition to the images, you are also provided some basic metadata about the car make, model, year, and trim.
For the training set, you are provided a .gif file that contains the manually cutout mask for each image. The competition task is to automatically segment the cars in the images in the test set folder. To deter hand labeling, we have supplemented the test set with car images that are ignored in scoring.
The metric used to score this competition requires that your submissions are in run-length encoded format. Please see the evaluation page for details, and train_masks.csv for a real example of what the encoding looks like.
File descriptions
/train/ - this folder contains the training set images
/test/ - this folder contains the test set images. You must predict the mask (in run-length encoded format) for each of the images in this folder
/train_masks/ - this folder contains the training set masks in .gif format
train_masks.csv - for convenience, this files gives a run-length encoded version of the training set masks.
sample_submission.csv - shows the correct submission format
metadata.csv - contains basic information about all the cars in the dataset. Note that some values are missing.",kaggle competitions download -c carvana-image-masking-challenge,"['https://www.kaggle.com/code/vfdev5/data-visualization', 'https://www.kaggle.com/code/paulorzp/run-length-encode-and-decode', 'https://www.kaggle.com/code/vfdev5/pil-vs-opencv', 'https://www.kaggle.com/code/stainsby/fast-tested-rle', 'https://www.kaggle.com/code/gaborfodor/augmentation-methods']"
329,"In this competition, Kaggle is challenging you to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables.
Longtime Kagglers will recognize that this competition objective is similar to the ECML/PKDD trip time challenge we hosted in 2015. But, this challenge comes with a twist. Instead of awarding prizes to the top finishers on the leaderboard, this playground competition was created to reward collaboration and collective learning.
We are encouraging you (with cash prizes!) to publish additional training data that other participants can use for their predictions. We also have designated bi-weekly and final prizes to reward authors of kernels that are particularly insightful or valuable to the community.","The evaluation metric for this competition is Root Mean Squared Logarithmic Error.
The RMSLE is calculated as
$$
\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }
$$
Where:
\\(\epsilon\\) is the RMSLE value (score)
\\(n\\) is the total number of observations in the (public/private) data set,
\\(p_i\\) is your prediction of trip duration, and
\\(a_i\\) is the actual trip duration for \\(i\\).
\\(\log(x)\\) is the natural logarithm of \\(x\\)
Submission File
For every row in the dataset, submission files should contain two columns: id and trip_duration.  The id corresponds to the column of that id in the test.csv. The file should contain a header and have the following format:
id,trip_duration
id00001,978
id00002,978
id00003,978
id00004,978
etc.","The competition dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform. The data was originally published by the NYC Taxi and Limousine Commission (TLC). The data was sampled and cleaned for the purposes of this playground competition. Based on individual trip attributes, participants should predict the duration of each trip in the test set.
File descriptions
train.csv - the training set (contains 1458644 trip records)
test.csv - the testing set (contains 625134 trip records)
sample_submission.csv - a sample submission file in the correct format
Data fields
id - a unique identifier for each trip
vendor_id - a code indicating the provider associated with the trip record
pickup_datetime - date and time when the meter was engaged
dropoff_datetime - date and time when the meter was disengaged
passenger_count - the number of passengers in the vehicle (driver entered value)",kaggle competitions download -c nyc-taxi-trip-duration,"['https://www.kaggle.com/code/headsortails/nyc-taxi-eda-update-the-fast-the-curious', 'https://www.kaggle.com/code/maheshdadhich/strength-of-visualization-python-visuals-tutorial', 'https://www.kaggle.com/code/gaborfodor/from-eda-to-the-top-lb-0-367', 'https://www.kaggle.com/code/drgilermo/dynamics-of-new-york-city-animation', 'https://www.kaggle.com/code/karelrv/nyct-from-a-to-z-with-xgboost-tutorial']"
330,"This competition focuses on the problem of forecasting the future values of multiple time series, as it has always been one of the most challenging problems in the field. More specifically, we aim the competition at testing state-of-the-art methods designed by the participants, on the problem of forecasting future web traffic for approximately 145,000 Wikipedia articles.
Sequential or temporal observations emerge in many key real-world problems, ranging from biological data, financial markets, weather forecasting, to audio and video processing. The field of time series encapsulates many different problems, ranging from analysis and inference to classification and forecast. What can you do to help predict future views?
This competition will run as two stages and involves prediction of actual future events. There will be a training stage during which the leaderboard is based on historical data, followed by a stage where participants are scored on real future events.
You have complete freedom in how to produce your forecasts: e.g. use of univariate vs multi-variate models, use of metadata (article identifier), hierarchical time series modeling (for different types of traffic), data augmentation (e.g. using Google Trends data to extend the dataset), anomaly and outlier detection and cleaning, different strategies for missing value imputation, and many more types of approaches.
We thank Google Inc. and Voleon for sponsorship of this competition, and Oren Anava and Vitaly Kuznetsov for organizing it.

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.
Submission File
For each article and day combination (see key.csv), you must predict the web traffic. The file should contain a header and have the following format:
Id,Visits
bf4edcf969af,0
929ed2bf52b9,0
ff29d0f51d5c0
etc.
Due to the large file size and number of rows, submissions may take a few minutes to score. Thank you for your patience.","The training dataset consists of approximately 145k time series. Each of these time series represent a number of daily views of a different Wikipedia article, starting from July, 1st, 2015 up until December 31st, 2016. The leaderboard during the training stage is based on traffic from January, 1st, 2017 up until March 1st, 2017.
The second stage will use training data up until September 1st, 2017. The final ranking of the competition will be based on predictions of daily views between September 13th, 2017 and November 13th, 2017 for each article in the dataset. You will submit your forecasts for these dates by September 12th.
For each time series, you are provided the name of the article as well as the type of traffic that this time series represent (all, mobile, desktop, spider). You may use this metadata and any other publicly available data to make predictions. Unfortunately, the data source for this dataset does not distinguish between traffic values of zero and missing values. A missing value may mean the traffic was zero or that the data is not available for that day.
To reduce the submission file size, each page and date combination has been given a shorter Id. The mapping between page names and the submission Id is given in the key files.
File descriptions
Files used for the first stage will end in '_1'. Files used for the second stage will end in '_2'. Both will have identical formats. The complete training data for the second stage will be made available prior to the second stage.
train_*.csv - contains traffic data. This a csv file where each row corresponds to a particular article and each column correspond to a particular date. Some entries are missing data. The page names contain the Wikipedia project (e.g. en.wikipedia.org), type of access (e.g. desktop) and type of agent (e.g. spider). In other words, each article name has the following format: 'name_project_access_agent' (e.g. 'AKB48_zh.wikipedia.org_all-access_spider').",kaggle competitions download -c web-traffic-time-series-forecasting,"['https://www.kaggle.com/code/headsortails/wiki-traffic-forecast-exploration-wtf-eda', 'https://www.kaggle.com/code/muonneutrino/wikipedia-traffic-data-exploration', 'https://www.kaggle.com/code/zoupet/predictive-analysis-with-different-approaches', 'https://www.kaggle.com/code/cpmpml/smape-weirdness', 'https://www.kaggle.com/code/ymlai87416/web-traffic-time-series-forecast-with-4-model']"
331,"Zillow’s Zestimate home valuation has shaken up the U.S. real estate industry since first released 11 years ago.
A home is often the largest and most expensive purchase a person makes in his or her lifetime. Ensuring homeowners have a trusted way to monitor this asset is incredibly important. The Zestimate was created to give consumers as much information as possible about homes and the housing market, marking the first time consumers had access to this type of home value information at no cost.
“Zestimates” are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property. And, by continually improving the median margin of error (from 14% at the onset to 5% today), Zillow has since become established as one of the largest, most trusted marketplaces for real estate information in the U.S. and a leading example of impactful machine learning.
Zillow Prize, a competition with a one million dollar grand prize, is challenging the data science community to help push the accuracy of the Zestimate even further. Winning algorithms stand to impact the home values of 110M homes across the U.S.
In this million-dollar competition, participants will develop an algorithm that makes predictions about the future sale prices of homes. The contest is structured into two rounds, the qualifying round which opens May 24, 2017 and the private round for the 100 top qualifying teams that opens on Feb 1st, 2018. In the qualifying round, you’ll be building a model to improve the Zestimate residual error. In the final round, you’ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition.
Because real estate transaction data is public information, there will be a three-month sales tracking period after each competition round closes where your predictions will be evaluated against the actual sale prices of the homes. The final leaderboard won’t be revealed until the close of the sales tracking period.","Submissions are evaluated on Mean Absolute Error between the predicted log error and the actual log error. The log error is defined as $$logerror=log(Zestimate)-log(SalePrice)$$ and it is recorded in the transactions training data. If a transaction didn't happen for a property during that period of time, that row is ignored and not counted in the calculation of MAE.
Submission File
For each property (unique parcelid), you must predict a log error for each time point. You should be predicting 6 timepoints: October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711), and December 2017 (201712). The file should contain a header and have the following format:
ParcelId,201610,201611,201612,201710,201711,201712
10754147,0.1234,1.2234,-1.3012,1.4012,0.8642-3.1412
10759547,0,0,0,0,0,0
etc.
Note that the actual log errors are accurate the 4th decimal places, so you can adjust your decimal formats to limit the size of your submission file.","In this competition, Zillow is asking you to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. The log error is defined as
$$logerror=log(Zestimate)-log(SalePrice)$$
and it is recorded in the transactions file train.csv. In this competition, you are going to predict the logerror for the months in Fall 2017. Since all the real estate transactions in the U.S. are publicly available, we will close the competition (no longer accepting submissions) before the evaluation period begins.
Train/Test split
You are provided with a full list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016.
The train data has all the transactions before October 15, 2016, plus some of the transactions after October 15, 2016.
The test data in the public leaderboard has the rest of the transactions between October 15 and December 31, 2016.
The rest of the test data, which is used for calculating the private leaderboard, is all the properties in October 15, 2017, to December 15, 2017. This period is called the ""sales tracking period"", during which we will not be taking any submissions.
You are asked to predict 6 time points for all properties: October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711), and December 2017 (201712).
Not all the properties are sold in each time period. If a property was not sold in a certain time period, that particular row will be ignored when calculating your score.",kaggle competitions download -c zillow-prize-1,"['https://www.kaggle.com/code/philippsp/exploratory-analysis-zillow', 'https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize', 'https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65', 'https://www.kaggle.com/code/anokas/simple-xgboost-starter-0-0655', 'https://www.kaggle.com/code/viveksrinivasan/zillow-eda-on-missing-values-multicollinearity']"
332,"This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details.
Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.
Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.
To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track.
The competition on Adversarial Attacks and Defenses consist of three sub-competitions:
Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.
In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.","Evaluation metric
Each defense classifier will be run on all adversarial images generated by all adversarial attacks (both targeted and non-targeted). For each correctly classified image defense classifier will get one point. In other words score for each defense will be computed using the following formula:
$$
score_{defense} = \sum_{attack \in A} \sum_{k=1}^{N} [defense(attack(Image_k)) = TrueLabel_k]
$$
where:
\(A\) is the set of all attacks, including targeted and non-targeted attacks
\(N\) - number of images in the dataset
\(Image_{k}\) - k-th image from the dataset
\(TrueLabel{k}\) - ground truth label for image k
\([ ]\) - indicator function which equals to 1 when Predicate is true, otherwise equals to 0
Higher score means better defense.","Two helpful datasets are available through Kaggle Datasets (link these datasets if you're using kernels):
The 1000 development images in the DEV set
A trained Inception-v3 model
dev_toolkit.zip is a development toolkit for the competition, which includes:
Script to download DEV dataset
Samples of attacks and defenses
Script to run all attacks against all defenses
Instructions to the provided examples
Development toolkit is also available on GitHub as a part of CleverHans library
for adversarial training.
sample_defense_submission.zip is an example of a valid submission.",kaggle competitions download -c nips-2017-defense-against-adversarial-attack,"['https://www.kaggle.com/code/benhamner/adversarial-learning-challenges-getting-started', 'https://www.kaggle.com/code/benhamner/running-inception-v3', 'https://www.kaggle.com/code/cjansen/generative-adversarial-network-in-tensorflow', 'https://www.kaggle.com/code/tw0518/adversarial-learning-challenges-getting-s-51b2c7', 'https://www.kaggle.com/code/jihyeseo/adv-attack-image-classify-robust']"
333,"This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details.
Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.
Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.
To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track.
The competition on Adversarial Attacks and Defenses consist of three sub-competitions:
Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.
In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.","Evaluation metric
Targeted adversarial attack will be provided set of images and one target class for each image.
All submitted targeted attacks will be provided the same set of images and target classes.
For each source image targeted attack is expected to produce adversarial image which is likely to be classified as desired target class by image classifier. All adversarial images generated by the targeted attack will be run through all submitted defense classifiers. Attack will get one point each time defense classifier outputs label which is equal to target label. In other words score for each targeted attack will be computed using the following formula:
$$
score_{TargetedAttack} = \sum_{defense \in D} \sum_{k=1}^{N} [defense(TargetedAttack(Image_k)) = TargetLabel_k]
$$
where:
\(D\) is the set of all defenses
\(N\) - number of images in the dataset
\(Image_{k}\) - k-th image from the dataset
\(TargetLabel_{k}\) - target label for image k
\([ ]\) - indicator function which equals to 1 when Predicate is true, otherwise equals to 0
Higher score means better targeted attack.","Two helpful datasets are available through Kaggle Datasets (link these datasets if you're using kernels):
The 1000 development images in the DEV set
A trained Inception-v3 model
dev_toolkit.zip is a development toolkit for the competition, which includes:
Script to download DEV dataset
Samples of attacks and defenses
Script to run all attacks against all defenses
Instructions to the provided examples
Development toolkit is also available on GitHub as a part of CleverHans library
for adversarial training.
sample_targeted_attack_submission.zip is an example of a valid submission.",kaggle competitions download -c nips-2017-targeted-adversarial-attack,"['https://www.kaggle.com/code/allunia/how-to-attack-a-machine-learning-model', 'https://www.kaggle.com/code/benhamner/adversarial-learning-challenges-getting-started', 'https://www.kaggle.com/code/cpruce/fast-pytorch-attack-kernel', 'https://www.kaggle.com/code/utkunal/how-to-attack-a-machine-learning-model', 'https://www.kaggle.com/code/benhamner/targeted-attack-example']"
334,"This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details.
Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.
Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.
To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track.
The competition on Adversarial Attacks and Defenses consist of three sub-competitions:
Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.
In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.","Evaluation metric
Adversarial attack will be run on a given dataset (same for all attacks) and is expected to produce adversarial image for each image from the dataset. All adversarial images generated by the attack will be run through all submitted defense classifiers. Attack will get one point for each incorrect classification, i.e. for each pair of adversarial image and defense when adversarial image was misclassified by the defense. In other words score for each attack will be computed using the following formula:
$$
score_{attack} = \sum_{defense \in D} \sum_{k=1}^{N} [defense(attack(Image_k)) \ne TrueLabel_k]
$$
where:
\(D\) is the set of all defenses
\(N\) - number of images in the dataset
\(Image_{k}\) - k-th image from the dataset
\(TargetLabel_{k}\) - target label for image k
\([ ]\) - indicator function which equals to 1 when Predicate is true, otherwise equals to 0
Higher score means better attack.","Two helpful datasets are available through Kaggle Datasets (link these datasets if you're using kernels):
The 1000 development images in the DEV set
A trained Inception-v3 model
dev_toolkit.zip is a development toolkit for the competition, which includes:
Script to download DEV dataset
Samples of attacks and defenses
Script to run all attacks against all defenses
Instructions to the provided examples
Development toolkit is also available on GitHub as a part of CleverHans library
for adversarial training.
sample_targeted_attack_submission.zip is an example of a valid submission.",kaggle competitions download -c nips-2017-non-targeted-adversarial-attack,"['https://www.kaggle.com/code/allunia/how-to-attack-a-machine-learning-model', 'https://www.kaggle.com/code/benhamner/adversarial-learning-challenges-getting-started', 'https://www.kaggle.com/code/benhamner/fgsm-attack-example', 'https://www.kaggle.com/code/bguberfain/visualizing-max-perturbation', 'https://www.kaggle.com/code/cpruce/fast-pytorch-attack-kernel']"
335,"Tangles of kudzu overwhelm trees in Georgia while cane toads threaten habitats in over a dozen countries worldwide. These are just two invasive species of many which can have damaging effects on the environment, the economy, and even human health. Despite widespread impact, efforts to track the location and spread of invasive species are so costly that they’re difficult to undertake at scale.
Currently, ecosystem and plant distribution monitoring depends on expert knowledge. Trained scientists visit designated areas and take note of the species inhabiting them. Using such a highly qualified workforce is expensive, time inefficient, and insufficient since humans cannot cover large areas when sampling.
Because scientists cannot sample a large quantity of areas, some machine learning algorithms are used in order to predict the presence or absence of invasive species in areas that have not been sampled. The accuracy of this approach is far from optimal, but still contributes to approaches to solving ecological problems.
In this playground competition, Kagglers are challenged to develop algorithms to more accurately identify whether images of forests and foliage contain invasive hydrangea or not. Techniques from computer vision alongside other current technologies like aerial imaging can make invasive species monitoring cheaper, faster, and more reliable.
Acknowledgments
Data providers: Christian Requena Mesa, Thore Engel, Amrita Menon, Emma Bradley.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each image in the test set, you must predict a probability for the target variable on whether the image contains invasive species or not. The file should contain a header and have the following format:
name,invasive
2,0.5
5,0
6,0.2
etc.","The data set contains pictures taken in a Brazilian national forest. In some of the pictures there is Hydrangea, a beautiful invasive species original of Asia. Based on the training pictures and the labels provided, the participant should predict the presence of the invasive species in the testing set of pictures.
File descriptions
train.7z - the training set (contains 2295 images).
train_labels.csv - the correct labels for the training set.
test.7z - the testing set (contains 1531 images), ready to be labeled by your algorithm.
sample_submission.csv - a sample submission file in the correct format.
Data fields
name - name of the sample picture file (numbers)
invasive - probability of the picture containing an invasive species. A probability of 1 means the species is present.",kaggle competitions download -c invasive-species-monitoring,"['https://www.kaggle.com/code/fujisan/use-keras-pre-trained-vgg16-acc-98', 'https://www.kaggle.com/code/jamesrequa/keras-k-fold-inception-v3-1st-place-lb-0-99770', 'https://www.kaggle.com/code/ogurtsov/0-99-with-r-and-keras-inception-v3-fine-tune', 'https://www.kaggle.com/code/ievgenvp/keras-flow-from-directory-on-python', 'https://www.kaggle.com/code/crequena/starter-s-pack-for-invasives-detection']"
336,"Whether you shop from meticulously planned grocery lists or let whimsy guide your grazing, our unique food rituals define who we are. Instacart, a grocery ordering and delivery app, aims to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you.
Instacart’s data science team plays a big part in providing this delightful shopping experience. Currently they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open sourced this data - see their blog post on 3 Million Instacart Orders, Open Sourced.
In this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user’s next order. They’re not only looking for the best model, Instacart’s also looking for machine learning engineers to grow their team.
Winners of this competition will receive both a cash prize and a fast track through the recruiting process. For more information about exciting opportunities at Instacart, check out their careers page here or e-mail their recruiting team directly at ml.jobs@instacart.com.","Submissions will be evaluated based on their mean F1 score.
Submission File
For each order_id in the test set, you should predict a space-delimited list of product_ids for that order. If you wish to predict an empty order, you should submit an explicit 'None' value. You may combine 'None' with product_ids. The spelling of 'None' is case sensitive in the scoring metric. The file should have a header and look like the following:
order_id,products  
17,1 2  
34,None  
137,1 2 3  
etc.","The dataset for this competition is a relational set of files describing customers' orders over time. The goal of the competition is to predict which products will be in a user's next order. The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users. For each user, we provide between 4 and 100 of their orders, with the sequence of products purchased in each order. We also provide the week and hour of day the order was placed, and a relative measure of time between orders. For more information, see the blog post accompanying its public release.
File descriptions
Each entity (customer, product, order, aisle, etc.) has an associated unique id. Most of the files and variable names should be self-explanatory.
aisles.csv
aisle_id,aisle  
1,prepared soups salads  
2,specialty cheeses  
3,energy granola bars  
...
departments.csv",kaggle competitions download -c instacart-market-basket-analysis,"['https://www.kaggle.com/code/philippsp/exploratory-analysis-instacart', 'https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-instacart', 'https://www.kaggle.com/code/datatheque/association-rules-mining-market-basket-analysis', 'https://www.kaggle.com/code/asindico/customer-segments-with-pca', 'https://www.kaggle.com/code/fabienvs/instacart-xgboost-starter-lb-0-3791']"
337,"A lot has been said during the past several years about how precision medicine and, more concretely, how genetic testing is going to disrupt the way diseases like cancer are treated.
But this is only partially happening due to the huge amount of manual work still required. Memorial Sloan Kettering Cancer Center (MSKCC) launched this competition, accepted by the NIPS 2017 Competition Track,  because we need your help to take personalized medicine to its full potential.
Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers). 
Currently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature.
For this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists have manually annotated thousands of mutations.
We need your help to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations.

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","Submissions are evaluated on Multi Class Log Loss between the predicted probability and the observed target.
Submission File
For each ID in the test set, you must predict a probability for each of the different classes a genetic mutation can be classified on. The file should contain a header and have the following format:
ID,class1,class2,class3,class4,class5,class6,class7,class8,class9
0,0.1,0.7,0.05,0.05,0.1,0,0,0,0
1,0.7,0.1,0.05,0.05,0.1,0,0,0,0
2,0.05,0.05,0.1,0.7,0.1,0,0,0,0
3,0,0,0,0,0.05,0.05,0.1,0,7,0,1
etc.","In this competition you will develop algorithms to classify genetic mutations based on clinical evidence (text).
There are nine different classes a genetic mutation can be classified on.
This is not a trivial task since interpreting clinical evidence is very challenging even for human specialists. Therefore, modeling the clinical evidence (text) will be critical for the success of your approach.
Both, training and test, data sets are provided via two different files. One (training/test_variants) provides the information about the genetic mutations, whereas the other (training/test_text) provides the clinical evidence (text) that our human experts used to classify the genetic mutations. Both are linked via the ID field.
Therefore the genetic mutation (row) with ID=15 in the file training_variants, was classified using the clinical evidence (text) from the row with ID=15 in the file training_text
Finally, to make it more exciting!! Some of the test data is machine-generated to prevent hand labeling. You will submit all the results of your classification algorithm, and we will ignore the machine-generated samples. 
File descriptions
training_variants - a comma separated file containing the description of the genetic mutations used for training. Fields are ID (the id of the row used to link the mutation to the clinical evidence), Gene (the gene where this genetic mutation is located), Variation (the aminoacid change for this mutations), Class (1-9 the class this genetic mutation has been classified on)",kaggle competitions download -c msk-redefining-cancer-treatment,"['https://www.kaggle.com/code/headsortails/personalised-medicine-eda-with-tidy-r', 'https://www.kaggle.com/code/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm', 'https://www.kaggle.com/code/dextrousjinx/brief-insight-on-genetic-variations', 'https://www.kaggle.com/code/miracl16/basic-xgboost-tfidf-russian-version', 'https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-personalized-medicine']"
338,"While long lines and frantically shuffling luggage into plastic bins isn’t a fun experience, airport security is a critical and necessary requirement for safe travel.
No one understands the need for both thorough security screenings and short wait times more than U.S. Transportation Security Administration (TSA). They’re responsible for all U.S. airport security, screening more than two million passengers daily.
As part of their Apex Screening at Speed Program, DHS has identified high false alarm rates as creating significant bottlenecks at the airport checkpoints. Whenever TSA’s sensors and algorithms predict a potential threat, TSA staff needs to engage in a secondary, manual screening process that slows everything down. And as the number of travelers increase every year and new threats develop, their prediction algorithms need to continually improve to meet the increased demand.
Currently, TSA purchases updated algorithms exclusively from the manufacturers of the scanning equipment used. These algorithms are proprietary, expensive, and often released in long cycles. In this competition, TSA is stepping outside their established procurement process and is challenging the broader data science community to help improve the accuracy of their threat prediction algorithms. Using a dataset of images collected on the latest generation of scanners, participants are challenged to identify the presence of simulated threats under a variety of object types, clothing types, and body types. Even a modest decrease in false alarms will help TSA significantly improve the passenger experience while maintaining high levels of security.
This is a two-stage competition. Please read our two-stage FAQs to understand more about what this means.
All persons contained in the dataset are volunteers who have agreed to have their images used for this competition. The images may contain sensitive content. We kindly request that you conduct yourself with professionalism, respect, and maturity when working with this data.","For every scan in the dataset, you will be predicting the probability that a threat is present in each of 17 body zones. A diagram of the body zone locations is available in the competition files section.
The description of each zone is as follows:
If there are N images, you will be making 17N predictions. Submissions are scored on the log loss:
$$ - \frac{1}{N} \sum_{i=1}^N \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],$$
where:
N is the 17 * the number of scans in the test set
\( \hat{y}_i \) is the predicted probability of the scan having a threat in the given body zone
\( y_i \) is 1 if a threat is present, 0 otherwise
\( log() \) is the natural (base e) logarithm
Note: the actual submitted predicted probabilities are replaced with \(max(min(p,1-10^{-15}),10^{-15})\). A smaller log loss is better.
Submission File
You must predict a probability for each Id and body zone. The Id used for the submission is created by concatenating the image Id with the zone for which you are predicting ('_Zone1' through '_Zone17'). The file should have a header and be in the following format:
Id,Probability  
0397026df63bbc8fd88f9860c6e35b4a_Zone1,0.002  
0397026df63bbc8fd88f9860c6e35b4a_Zone2,0.32  
0397026df63bbc8fd88f9860c6e35b4a_Zone3,0.88  
etc...","This dataset contains a large number of body scans acquired by a new generation of millimeter wave scanner called the High Definition-Advanced Imaging Technology (HD-AIT) system. The competition task is to predict the probability that a given body zone (out of 17 total body zones) has a threat present.
The images in the dataset are designed to capture real scanning conditions. They are comprised of volunteers wearing different clothing types (from light summer clothes to heavy winter clothes), different body mass indices, different genders, different numbers of threats, and different types of threats. Due to restrictions on revealing the types of threats for which the TSA screens, the threats in the competition images are ""inert"" objects with varying material properties. These materials were carefully chosen to simulate real threats.
The volunteers used in the first and second stage of the competition will be different (i.e. your algorithm should generalize to unseen people). In addition, you should not make assumptions about the number, distribution, or location of threats in the second stage.
All volunteers have agreed to have their images used for this competition. The images may contain sensitive content. We kindly request that you conduct yourself with professionalism, respect, and maturity when working with this data.
Data size and access
The data for stage one is more than three terabytes in size. Stage two will have a similar size. Since most internet connections can not reliably download this much data, we are making the full dataset available on multi-regional Google Cloud Storage.",kaggle competitions download -c passenger-screening-algorithm-challenge,"['https://www.kaggle.com/code/jbfarrar/exploratory-data-analysis-and-example-generation', 'https://www.kaggle.com/code/wcukierski/reading-images', 'https://www.kaggle.com/code/jbfarrar/preprocessing-pipeline-and-convnet-trainer', 'https://www.kaggle.com/code/philippsp/reading-images-in-r', 'https://www.kaggle.com/code/zusmani/predictions-based-on-zone-means-0-29089']"
339,"Housing costs demand a significant investment from both consumers and developers. And when it comes to planning a budget—whether personal or corporate—the last thing anyone needs is uncertainty about one of their biggets expenses. Sberbank, Russia’s oldest and largest bank, helps their customers by making predictions about realty prices so renters, developers, and lenders are more confident when they sign a lease or purchase a building.
Although the housing market is relatively stable in Russia, the country’s volatile economy makes forecasting prices as a function of apartment characteristics a unique challenge. Complex interactions between housing features such as number of bedrooms and location are enough to make pricing predictions complicated. Adding an unstable economy to the mix means Sberbank and their customers need more than simple regression models in their arsenal.
In this competition, Sberbank is challenging Kagglers to develop algorithms which use a broad spectrum of features to predict realty prices. Competitors will rely on a rich dataset that includes housing data and macroeconomic patterns. An accurate forecasting model will allow Sberbank to provide more certainty to their customers in an uncertain economy.","Submissions are evaluated on the RMSLE between their predicted prices and the actual data. The target variable, called price_doc in the training set, is the sale price of each property.
Submission File
For each id in the test set, you must predict the price that the property sold for. The file should contain a header and have the following format:
id,price_doc
30474,7118500.44
30475,7118500.44
30476,7118500.44
etc.","The aim of this competition is to predict the sale price of each property. The target variable is called price_doc in train.csv.
The training data is from August 2011 to June 2015, and the test set is from July 2015 to May 2016. The dataset also includes information about overall conditions in Russia's economy and finance sector, so you can focus on generating accurate price forecasts for individual properties, without needing to second-guess what the business cycle will do.
Data Files
train.csv, test.csv: information about individual transactions. The rows are indexed by the ""id"" field, which refers to individual transactions (particular properties might appear more than once, in separate transactions). These files also include supplementary information about the local area of each property.
macro.csv: data on Russia's macroeconomy and financial sector (could be joined to the train and test sets on the ""timestamp"" column)
sample_submission.csv: an example submission file in the correct format
data_dictionary.txt: explanations of the fields available in the other data files
Update: please see the pinned discussion thread for some optional extra data, resolving an issue with some GIS features.",kaggle competitions download -c sberbank-russian-housing-market,"['https://www.kaggle.com/code/captcalculator/a-very-extensive-sberbank-exploratory-analysis', 'https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-sberbank', 'https://www.kaggle.com/code/jtremoureux/map-visualizations-with-external-shapefile', 'https://www.kaggle.com/code/konradb/adversarial-validation-and-other-scary-terms', 'https://www.kaggle.com/code/robertoruiz/dealing-with-multicollinearity']"
340,"Every minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively.
Planet, designer and builder of the world’s largest constellation of Earth-imaging satellites, will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter resolution. While considerable research has been devoted to tracking changes in forests, it typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250 meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest degradation dominate.
Furthermore, these existing methods generally cannot differentiate between human causes of forest loss and natural causes. Higher resolution imagery has already been shown to be exceptionally good at this, but robust methods have not yet been developed for Planet imagery.
In this competition, Planet and its Brazilian partner SCCON are challenging Kagglers to label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond.
To dig into/explore more Planet data, sign up for a free account.
And if you're interested in building applications on Planet data, check out our Application Developer Program.
Getting Started
Review the data page, which includes detailed information about the labels and the labeling process.
Download a subsample of the data to get familiar with how it looks.
Explore the subsample on Kernels. We’ve created a notebook for you to get started.","Submissions will be evaluated based on their mean (F_{2}) score. The F score, commonly used in information retrieval, measures accuracy using the precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The (F_{2}) score is given by
$$
(1 + \beta^2) \frac{pr}{\beta^2 p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn},\ \beta = 2.
$$
Note that the (F_{2}) score weights recall higher than precision. The mean (F_{2}) score is formed by averaging the individual (F_{2}) scores for each row in the test set.
Submission File
For each image listed in the test set, predict a space-delimited list of tags which you believe are associated with the image. There are 17 possible tags: agriculture, artisinal_mine, bare_ground, blooming, blow_down, clear, cloudy, conventional_mine, cultivation, habitation, haze, partly_cloudy, primary, road, selective_logging, slash_burn, water. The file should contain a header and have the following format:
image_name,tags
test_0,agriculture road water
test_1,primary clear
test_2,haze primary
etc.","File formats
train.csv - a list of training file names and their labels, the labels are space-delimited
sample_submission.csv - correct format of submission, contains all the files in the test set. For more information about the submission file, please go to the Evaluation page
[train/test]-tif-v2.tar.7z - tif files for the training/test set (updated: May 5th, 2017)
[train/test]-jpg[-additional].tar.7z - jpg files for the trainin/test set (updated: May 5th, 2017)
Kaggle-planet-[train/test]-tif.torrent - a BitTorrent file for downloading [train/test]-tif-v2.tar.7z (updated: May 5th, 2017)
For all the tar.7z files, you can extract them with:
7za x train-jpg.tar.7z
tar xf train-jpg.tar
Chip (Image) Data Format
The chips for this competition were derived from Planet's full-frame analytic scene products using our 4-band satellites in sun-synchronous orbit (SSO) and International Space Station (ISS) orbit. The set of chips for this competition use the format and each contain four bands of data: red, green, blue, and near infrared. The specific spectral response of the satellites can be found in the Planet documentation. Each of these channels is in 16-bit digital number format, and meets the specification of the Planet .",kaggle competitions download -c planet-understanding-the-amazon-from-space,"['https://www.kaggle.com/code/anokas/data-exploration-analysis', 'https://www.kaggle.com/code/ekami66/0-92837-on-private-lb-solution-with-keras', 'https://www.kaggle.com/code/philschmidt/multilabel-classification-rainforest-eda', 'https://www.kaggle.com/code/robinkraft/getting-started-with-the-data-now-with-docs', 'https://www.kaggle.com/code/mratsim/starting-kit-for-pytorch-deep-learning']"
341,"Steller sea lions in the western Aleutian Islands have declined 94 percent in the last 30 years. The endangered western population, found in the North Pacific, are the focus of conservation efforts which require annual population counts. Specially trained scientists at NOAA Fisheries Alaska Fisheries Science Center conduct these surveys using airplanes and unoccupied aircraft systems to collect aerial images. Having accurate population estimates enables us to better understand factors that may be contributing to lack of recovery of Stellers in this area.
Currently, it takes biologists up to four months to count sea lions from the thousands of images NOAA Fisheries collects each year. Once individual counts are conducted, the tallies must be reconciled to confirm their reliability. The results of these counts are time-sensitive.
In this competition, Kagglers are invited to develop algorithms which accurately count the number of sea lions in aerial photographs. Automating the annual population count will free up critical resources allowing NOAA Fisheries to focus on ensuring we hear the sea lion’s roar for many years to come. Plus, advancements in computer vision applied to aerial population counts may also greatly benefit other endangered species.
Resources
Learn more about research being done to better understand what's going on with the endangered Steller sea lion populations by joining scientists on a research vessel to the western Aleutian Islands in the video below.","The aim of the competition is to count each type of steller sea lion in each photo. See the Data tab for more details.
Your submission file should have the following format:
test_id,adult_males,subadult_males,adult_females,juveniles,pups
0,1,1,1,1,1
1,1,1,1,1,1
2,1,1,1,1,1
etc

Your submissions will be evaluated by their RMSE from the human-labelled ground truth, averaged over the columns.","The goal of this competition is to estimate the number of each type of sea lion in each one of the test images. The different types of sea lion are: adult males (also known as bulls), subadult males, adult females, juveniles, and pups.
Your submission file should have the following format:
test_id,adult_males,subadult_males,adult_females,juveniles,pups
0,1,1,1,1,1
1,1,1,1,1,1
2,1,1,1,1,1
etc
Your submissions will be evaluated by their RMSE from the human-labelled ground truth, averaged over the columns.
The file KaggleNOAASeaLions.7z is an encrypted 7-zip archive. An identical file is available via BitTorrent. Please do not share the archive password.
The archive contains the following items:
sample_submission.csv: an example valid submission file
Train/*.jpg: a set of training images, with each filename corresponding to a train_id
Train/train.csv: a list of ground-truth counts for each train_id",kaggle competitions download -c noaa-fisheries-steller-sea-lion-population-count,"['https://www.kaggle.com/code/threeplusone/sea-lion-coordinates', 'https://www.kaggle.com/code/outrunner/use-keras-to-count-sea-lions', 'https://www.kaggle.com/code/philschmidt/sea-lion-correlations-cv2-template-matching', 'https://www.kaggle.com/code/radustoicescu/use-keras-to-classify-sea-lions-0-91-accuracy', 'https://www.kaggle.com/code/radustoicescu/get-coordinates-using-blob-detection']"
342,"Where else but Quora can a physicist help a chef with a math problem and get cooking tips in return? Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.
Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.
Currently, Quora uses a Random Forest model to identify duplicate questions. In this competition, Kagglers are challenged to tackle this natural language processing problem by applying advanced techniques to classify whether question pairs are duplicates or not. Doing so will make it easier to find high quality answers to questions resulting in an improved experience for Quora writers, seekers, and readers.","Submissions are evaluated on the log loss between the predicted values and the ground truth.
Submission File
For each ID in the test set, you must predict the probability that the questions are duplicates (a number between 0 and 1). The file should contain a header and have the following format:
test_id,is_duplicate
0,0.5
1,0.4
2,0.9
etc.","The goal of this competition is to predict which of the provided pairs of questions contain two questions with the same meaning. The ground truth is the set of labels that have been supplied by human experts. The ground truth labels are inherently subjective, as the true meaning of sentences can never be known with certainty. Human labeling is also a 'noisy' process, and reasonable people will disagree. As a result, the ground truth labels on this dataset should be taken to be 'informed' but not 100% accurate, and may include incorrect labeling. We believe the labels, on the whole, to represent a reasonable consensus, but this may often not be true on a case by case basis for individual items in the dataset.
Please note: as an anti-cheating measure, Kaggle has supplemented the test set with computer-generated question pairs. Those rows do not come from Quora, and are not counted in the scoring. All of the questions in the training set are genuine examples from Quora.
Data fields
id - the id of a training set question pair
qid1, qid2 - unique ids of each question (only available in train.csv)
question1, question2 - the full text of each question
is_duplicate - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise.",kaggle competitions download -c quora-question-pairs,"['https://www.kaggle.com/code/anokas/data-analysis-xgboost-starter-0-35460-lb', 'https://www.kaggle.com/code/rajmehra03/a-detailed-explanation-of-keras-embedding-layer', 'https://www.kaggle.com/code/lystdo/lstm-with-word2vec-embeddings', 'https://www.kaggle.com/code/cpmpml/spell-checker-using-word2vec', 'https://www.kaggle.com/code/philschmidt/quora-eda-model-selection-roc-pr-plots']"
343,"As shoppers move online, it’d be a dream come true to have product attributes in photos detected automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine grained attribute labels may look very similar, for example, royal blue vs turquoise in color. Many of today’s general-purpose recognition machines simply can’t perceive such subtle differences between photos.
Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC4 workshop. As part of this workshop, CVPR is partnering with Google to challenge the data science community to help push the state of the art in automatic image classification.
In this competition, FGVC workshop organizers and Google challenge you to develop algorithms that will help with the an important step towards automatic product detection–accurately assigning attribute labels for product images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC4 workshop.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","Submissions are evaluated on top 1 error rate of the predictions. For each image \( i \) and task \( t\), there might be multiple labels \(g_{it,k}\) and the algorithm will produce a most confident label \(l_{it}\). As long as the label produced is the same as one of the ground truth labels, then the prediction is considered to be correct. Mathematically, the error rate for a single image and task is:
$$e_{it} = \min_{k}d(l_{it},g_{it,k})$$
Where
$$d(x,y) = \begin{cases} 0, & \text{if}\ x=y \\ 1, & \text{otherwise} \end{cases}$$
The overall error score is the average error over all test images and tasks
$$score = \frac{1}{N} \sum_{i,t}(e_{it})$$ where \( N \) is the total number of valid ground truth labels
Submission File
For each image_id and task_id in the test set, you must produce the most confident label. The file should contain a header and have the format below. Besides the header, each row has two columns. First column specifies the image and task ids, in the format of ""imageid_taskid"". Second column is the label id.
id,expected
1_1,0
1_2,1
2_1,10
etc.
Please note that, for each test image, participants need to produce labels for all the possible tasks specified in the task dictionary. So the submission file should have N rows, where:
N = number_test_image * number_tasks","All the data described below are txt files in JSON format.
Training Data
The training dataset includes images from four apparel classes (outerwear, dresses, pants, shoes), with a varied number of attribute label for each image. It includes a total of 50,461 images and 381 attribute labels.
The training data are splitted into train and validation sets. There are 8432 images in the validation set. Both sets have the same format as shown below:
{
  ""images"" : [image],
  ""annotations"" : [annotation],
}
image{
  ""image_id"" : int,
   ""url"": [string]
}
annotation{
  ""image_id"" : int,
  ""task_id"" : int,
  ""label_id"" : int
}",kaggle competitions download -c imaterialist-challenge-FGVC2017,[]
344,"With so much diversity, accurately classifying animals and plants is a tough challenge. Check out the photos below. Alpaca or Llama? Donkey or mule? Roses or kale?
It’s estimated that our planet contains several million species of plants and animals–many that look really similar to each other. Because of this, a lot of species in the natural world are too hard to classify without an expert.
As part of the FGVC4 workshop at CVPR 2017 we are conducting the iNat Challenge 2017 large scale species classification competition, sponsored by Google. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features fine-grained categories, big class imbalances, and large numbers of classes.

The iNat Challenge 2017 dataset contains 5,089 species, with a combined training and validation set of 675,000 images that have been collected and verified by multiple users from inaturalist.org. The dataset features many visually similar species, captured in a wide variety of situations, from all over the world.  Example images, along with their unique GBIF ID numbers (where available), can be viewed here.
Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC4 workshop. 

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.","We follow a similar metric to the classification tasks of the ILSVRC. For each image, an algorithm will produce 5 labels. We allow 5 labels because some categories are disambiguated with additional data provided by the observer, such as latitude, longitude and date. It might also be the case that multiple categories occur in an image (e.g. a photo of a bee on a flower). For this competition each image has one ground truth label. For a given image, if the ground truth label is found among the 5 predicted labels, then the error for that image is 0, otherwise it is 1. The final score is the error averaged across all images. See here for additional information. 
Submission File
For each image in the test set, you must predict 5 class labels. The csv file should contain a header and have the following format:
id,predicted
12345,0 78 23 3 42
67890,83 13 42 0 21","Images and Annotations available here 
File descriptions
train_val_images.tar.gz - Contains the training and validation images in a directory structure following //.jpg . 
train_val2017.zip - Contains the training and validation annotations.
test2017.tar.gz - Contains a single directory of test images.
test2017.zip - Contains test image information. 
kaggle_sample_submission.csv - a sample submission file in the correct format
train_val_images_mini.tar.gz - a subset of the category images that you can download for easy viewing. Contains 3 sample categories for each of the 13 super categories.
These data files can also be accessed via links here.
Image Format
All images have been saved in the JPEG format and have been resized to have a maximum dimension of 800 pixels. 
Annotation Format",kaggle competitions download -c inaturalist-challenge-at-fgvc-2017,['https://www.kaggle.com/code/jihyeseo/image-jpeg']
345,"Cervical cancer is so easy to prevent if caught in its pre-cancerous stage that every woman should have access to effective, life-saving treatment no matter where they live. Today, women worldwide in low-resource settings are benefiting from programs where cancer is identified and treated in a single visit. However, due in part to lacking expertise in the field, one of the greatest challenges of these cervical cancer screen and treat programs is determining the appropriate method of treatment which can vary depending on patients’ physiological differences.
Especially in rural parts of the world, many women at high risk for cervical cancer are receiving treatment that will not work for them due to the position of their cervix. This is a tragedy: health providers are able to identify high risk patients, but may not have the skills to reliably discern which treatment which will prevent cancer in these women. Even worse, applying the wrong treatment has a high cost. A treatment which works effectively for one woman may obscure future cancerous growth in another woman, greatly increasing health risks.
Currently, MobileODT offers a Quality Assurance workflow to support remote supervision which helps healthcare providers make better treatment decisions in rural settings. However, their workflow would be greatly improved given the ability to make real-time determinations about patients’ treatment eligibility based on cervix type.
In this competition, Intel is partnering with MobileODT to challenge Kagglers to develop an algorithm which accurately identifies a woman’s cervix type based on images. Doing so will prevent ineffectual treatments and allow healthcare providers to give proper referral for cases that require more advanced treatment.
Competition Partner
MobileODT has developed and sells the Enhanced Visual Assessment (EVA) System, a digital toolkit for health care workers of every level to provide expert services to patients, anchored at the point-of-care by an FDA-approved, intelligent, mobile-phone based medical device. Combining the algorithmic power of biomedical optics with the computational capabilities and connectivity of mobile phones, MobileODT's connected, intelligent medical systems can be used everywhere, under nearly any conditions. MobileODT's first product, the FDA approved EVA System for colposcopy, is in use by health providers in 31 hospital systems across the US, and in 22 countries, to better screen and treat women for cervical cancer and to conduct forensic colposcopy.","Submissions are evaluated using the multi-class logarithmic loss. Each image has been labeled with one type. For each image, you must submit a set of predicted probabilities (one for every category). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of images in the test set, M is the number of categories,  \\(log\\) is the natural logarithm, (y_{ij}) is 1 if observation (i) belongs to class (j) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given image are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission File
You must submit a csv file with the image id, and a probability for each class.
The order of the rows does not matter. The file must have a header and should look like the following:
image_name,Type_1,Type_2,Type_3
0.jpg,0.1,0.3,0.6
1.jpg,0,0,1
...","Warning: This data contains graphic contents that some may find disturbing.
In this competition, you will develop algorithms to correctly classify cervix types based on cervical images. These different types of cervix in our data set are all considered normal (not cancerous), but since the transformation zones aren't always visible, some of the patients require further testing while some don't. This decision is very important for the healthcare provider and critical for the patient. Identifying the transformation zones is not an easy task for the healthcare providers, therefore, an algorithm-aided decision will significantly improve the quality and efficiency of cervical cancer screening for these patients. 
To understand more about the background of how these cervix types are defined, please refer to this document. ",kaggle competitions download -c intel-mobileodt-cervical-cancer-screening,"['https://www.kaggle.com/code/philschmidt/cervix-eda-model-selection', 'https://www.kaggle.com/code/grfiv4/plot-a-confusion-matrix', 'https://www.kaggle.com/code/chattob/cervix-segmentation-gmm', 'https://www.kaggle.com/code/kambarakun/how-to-start-with-python-on-colfax-cluster', 'https://www.kaggle.com/code/aamaia/leeak']"
346,"Video captures a cross-section of our society. And major advances in analyzing and understanding video have the potential to touch all aspects of life from learning and communication to entertainment and play. In this competition, Google is inviting the Kaggle community to join efforts to accelerate research in large-scale video understanding, while giving participants access to the Google Cloud Machine Learning Engine.
Today, one of the greatest obstacles to rapid improvements in video understanding research has been the lack of large-scale, labeled datasets open to the public. For example, the availability of large, labeled datasets such as ImageNet has enabled continued breakthroughs in machine learning and machine perception. To that end, Google’s recent release of the YouTube-8M (YT-8M) dataset represents a significant step in this direction. Making this resource open to everyone from students and industry professionals is expected to kickstart innovation in areas such as representation learning and video modeling architectures.
In this competition, you are challenged to develop classification algorithms which accurately assign video-level labels using the new and improved YT-8M V2 dataset. The dataset was created from over 7 million YouTube videos (450,000 hours of video) and includes video labels from a vocabulary of 4716 classes (3.4 labels/video on average). It also comes with pre-extracted audio & visual features from every second of video (3.2B feature vectors in total). By taking part, Kagglers will not only play a pivotal role in setting state-of-the-art benchmarks, but also improve search and organization of video archives.
Getting Started
Review the data page for special instructions on how to access the competition's data. It will be hosted on Google Cloud. Participants have the option to download the data to work locally or work within the Google Cloud ML beta Platform.
Review the tutorial on Getting Started with Google Cloud, and try the starter code.
Sign up for a Google Cloud ML Platform free trial account. The free trial account includes $300 in credits!
We've also provided a subsample of the data to explore on Kernels. Take a look at this Python notebook and create your own.
Don't forget to review the prize eligibility details, which includes requirements for code open-sourcing and a paper submission.
Because Cloud ML is currently a beta product, Google welcomes the opportunity to hear your feedback about using the tool. Please share your questions and thoughts on the competition's forums. Additional resources specific to the YT-8M dataset and Google Cloud ML can be found here.
Acknowledgements
Google Cloud Machine Learning, Competition Sponsor
Google Cloud Machine Learning is a managed service that enables you to easily build machine learning models, that work on any type of data, of any size. Create your model with the powerful TensorFlow framework that powers many Google products, from GooglePhotos to Google Cloud Speech. Build models of any size with our managed scalable infrastructure. Your trained model is immediately available for use with our global prediction platform that can support thousands of users and TBs of data. The service is integrated with Google Cloud Dataflow for pre-processing, allowing you to access data from Google Cloud Storage, Google BigQuery, and others.","Submissions are evaluated the Global Average Precision (GAP) at k, where k=20. For each video, you will submit a list of predicted labels and their corresponding confidence scores. The evaluation takes the predicted labels that have the highest k confidence scores for each video, then treats each prediction and the confidence score as an individual data point in a long list of global predictions, to compute the Average Precision across all of the predictions and all the videos. 
If a submission has N predictions (label/confidence pairs) sorted by its confidence score, then the Global Average Precision is computed as:
$$GAP = \sum_{i=1}^N p(i) \Delta r(i)$$
where N is the number of final predictions (if there are 20 predictions for each video, then N = 20 * #Videos ), p(i) is the precision, and r(i) is the recall.  
A python implementation of GAP can be found at youtube-8m's github. For other definitions of Average Precision, please refer to Kaggle's metric page.
Submission File
For each VideoId in the test set, you must predict a list of Labels and their corresponding confidence scores. The file should contain a header and have the following format:
VideoId,LabelConfidencePairs
100000001,1 0.5 2 0.3 3 0.1 4 0.05 5 0.05
etc.","In this competition, you will predict the labels of a YouTube video. We provide you extracted frame-level and video-level features. The feature data and detailed feature information can be found on the YouTube-8M dataset webpage. 
The training dataset in this competition contain videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition. 
We have a step-by-step tutorial to get you started with Google Cloud. 
File descriptions
video-level data
Available on Google Cloud at gs://us.data.yt8m.org/1/video_level/train , gs://us.data.yt8m.org/1/video_level/validate , and gs://us.data.yt8m.org/1/video_level/test
You can access the data on Google Cloud, or download to your local computer with instructions here
Total size of 31GB 
Each video has
""video_id"": unique id for the video, in train set it is a Youtube video id, and in test/validation they are anonymized
""labels"": list of labels of that video",kaggle competitions download -c youtube8m,"['https://www.kaggle.com/code/wendykan/starter-explore-youtube8m-sample-data', 'https://www.kaggle.com/code/philschmidt/youtube8m-eda', 'https://www.kaggle.com/code/drn01z3/keras-baseline-on-video-features-0-7941-lb', 'https://www.kaggle.com/code/vlarine/most-popular-labels', 'https://www.kaggle.com/code/amunategui/download-tfrecords-for-case-insensitive-systems']"
347,"Finding the perfect place to call your new home should be more than browsing through endless listings. RentHop makes apartment search smarter by using data to sort rental listings by quality. But while looking for the perfect apartment is difficult enough, structuring and making sense of all available real estate data programmatically is even harder. Two Sigma and RentHop, a portfolio company of Two Sigma Ventures, invite Kagglers to unleash their creative engines to uncover business value in this unique recruiting competition.

Two Sigma invites you to apply your talents in this recruiting competition featuring rental listing data from RentHop. Kagglers will predict the number of inquiries a new listing receives based on the listing’s creation date and other features. Doing so will help RentHop better handle fraud control, identify potential listing quality issues, and allow owners and agents to better understand renters’ needs and preferences.

Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. This challenge is an opportunity for competitors to gain a sneak peek into Two Sigma's data science work outside of finance.
Acknowledgments
This competition is co-hosted by Two Sigma and RentHop (a portfolio company of Two Sigma Ventures, which is a division of Two Sigma Investments) to encourage creativity in using real world data to solve everyday problems.","Submissions are evaluated using the multi-class logarithmic loss. Each listing has one true class. For each listing, you must submit a set of predicted probabilities (one for every listing). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of listings in the test set, M is the number of class labels (3 classes),  \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given listing are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission File
You must submit a csv file with the listing_id, and a probability for each class.
The order of the rows does not matter. The file must have a header and should look like the following:
listing_id,high,medium,low
7065104,0.07743170693194379,0.2300252644876046,0.6925430285804516
7089035,0.0, 1.0, 0.0
...","In this competition, you will predict how popular an apartment rental listing is based on the listing content like text description, photos, number of bedrooms, price, etc. The data comes from renthop.com, an apartment listing website. These apartments are located in New York City.
The target variable, interest_level, is defined by the number of inquiries a listing has in the duration that the listing was live on the site. 
File descriptions
train.json - the training set
test.json - the test set
sample_submission.csv - a sample submission file in the correct format
images_sample.zip - listing images organized by listing_id (a sample of 100 listings)
Kaggle-renthop.7z - (optional) listing images organized by listing_id. Total size: 78.5GB compressed. Distributed by BitTorrent (Kaggle-renthop.torrent). 
Data fields
bathrooms: number of bathrooms",kaggle competitions download -c two-sigma-connect-rental-listing-inquiries,
348,"Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our fourth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2017 tournament. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2017 results.","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of games played
\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2
\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins
\\( log() \\) is the natural (base e) logarithm
A smaller log loss is better. Games which are not played are ignored in the scoring. Play-in games are also ignored (only the games among the final 64 teams are scored). The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2017 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2  = 2278 matchups. 
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_1104_1129"" indicates team 1104 played team 1129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id.
The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:
id,pred
2013_1103_1107,0.5
2013_1103_1112,0.5
2013_1103_1125,0.5
...",,,
349,,,,,
350,,,,,
351,,,,,
352,"How can we use the world’s tools and intelligence to forecast economic outcomes that can never be entirely predictable? This question is at the core of countless economic activities around the world – including at Two Sigma Investments, who has been applying technology and systematic strategies to financial trading since 2001.
For over 15 years, Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. Through this exclusive partnership, Two Sigma is excited to explore what untapped value Kaggle's diverse data science community can discover in the financial markets.
Economic opportunity depends on the ability to deliver singularly accurate forecasts in a world of uncertainty. By accurately predicting financial movements, Kagglers will learn about scientifically-driven approaches to unlocking significant predictive capability. Two Sigma is excited to find predictive value and gain a better understanding of the skills offered by the global data science crowd.
What is a Code Competition?
Welcome to Kaggle's very first Code Competition! In contrast to our traditional competitions, where competitors submit only prediction outputs, participants in Code Competitions will submit their code via Kaggle Kernels. All kernels are private by default in Code Competitions. You can build your models in Kernels by running them on a training set and, once you're ready to submit your code, your model's performance will be evaluated against the test set and your score and public leaderboard position revealed. As with our traditional competitions, we still maintain a private leaderboard test set, which your code is also evaluated against for final scoring, but is not revealed until the competition closes.
Since Code Competitions are brand new, we ask for your patience if you encounter bugs or frustrating platform quirks. Please report any issues you find in the forums and we'll do our best to respond.
Who owns my code?
You do. Even though you are submitting code, the intellectual property exchange here works similarly to a standard prediction competition, whereby prize winners have the option to grant a non-exclusive license in exchange for a prize. There is a new addition to the terms for Code Competitions: Kaggle and the competition host reserve a right to review submissions ""for purposes related to evaluation and scoring in this Competition, including but not limited to the assessment of potential cheating behavior."" Please refer to the official competition rules for full details.
Getting Started
Review the data page for details about the data and the evaluation metric. You may download the train set for local training.
Take a look at the tutorial covering the new code submission process under the submission instructions tab. You'll find step-by-step instructions, some helpful pointers, plus details on environment constraints.
Get feedback on your benchmark code and share exploratory analyses with the community by making any of your kernels public.
Improve your score!
Note: there is no cost of entry for participation.","Submissions will be evaluated on the R value between the predicted and actual values. The R value similar to the R squared value, also called the coefficient of determination. R squared can be calculated as:
$$R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \mu)^2}.$$
To calculate R, we then use:
$$R = sign\left( R^2 \right) \sqrt{\left|R^2\right|},$$
where \\(y\\) is the actual value, \\(\mu\\) is the mean of the actual values, and \\(\hat{y}\\) is the predicted value. Do not be discouraged by low R values; in finance, given the high ratio of signal-to-noise, even a small R can deliver meaningful value!
Negative R values are clipped at -1, i.e. the score you see will be \\(max(-1,R)\\). Additionally, if a submission errors for any reason, you will receive a simple ""Error"" status.","This dataset contains anonymized features pertaining to a time-varying value for a financial instrument. Each instrument has an id. Time is represented by the 'timestamp' feature and the variable to predict is 'y'. No further information will be provided on the meaning of the features, the transformations that were applied to them, the timescale, or the type of instruments that are included in the data. Moreover, in accordance with competition rules, participants must not use data other than the data linked from the competition website for the purpose of use in this competition to develop and test their models and submissions.
Data is saved and accessed as a .h5 file in the Kernels environment. We have used the .h5 file format instead of the standard .csv format to achieve faster read speeds. The training set file is available for download and offline modeling outside of Kernels. The test set is not available for download.
In addition to the data, you will need to familiarize yourself with the Kernels environment and the competition data API. The API is designed to prevent accessing data beyond the timestamp for which you are predicting and informs you which ids require predictions at which timestamps. The API also provides a ""reward"" for each timestamp, in the form of an average R value over the predicted values for the previous day. You may choose to use this reward to do reinforcement-style learning. Your code should expect and handle missing values.
We have setup the kernels environment such that the code structure you use for training on the test set (clicking ""Run"") will ideally work for submissions on the test set (clicking ""Submit""). To achieve this, we have partitioned the training set such that the first half is provided as a training set at the start of a run, and the latter half is streamed through the API, as though it is a holdout set. In other words:",kaggle competitions download -c two-sigma-financial-modeling,"['https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-5', 'https://www.kaggle.com/code/chenjx1005/physical-meanings-of-technical-20-30', 'https://www.kaggle.com/code/anokas/two-sigma-time-travel-eda', 'https://www.kaggle.com/code/sudalairajkumar/am-i-over-fitting', 'https://www.kaggle.com/code/slothouber/kagglegym-emulation']"
353,"Nearly half of the world depends on seafood for their main source of protein. In the Western and Central Pacific, where 60% of the world’s tuna is caught, illegal, unreported, and unregulated fishing practices are threatening marine ecosystems, global seafood supplies and local livelihoods. The Nature Conservancy is working with local, regional and global partners to preserve this fishery for the future.
Currently, the Conservancy is looking to the future by using cameras to dramatically scale the monitoring of fishing activities to fill critical science and compliance monitoring data gaps. Although these electronic monitoring systems work well and are ready for wider deployment, the amount of raw data produced is cumbersome and expensive to process manually.
The Conservancy is inviting the Kaggle community to develop algorithms to automatically detect and classify species of tunas, sharks and more that fishing boats catch, which will accelerate the video review process. Faster review and more reliable data will enable countries to reallocate human capital to management and enforcement activities which will have a positive impact on conservation and our planet.
Machine learning has the ability to transform what we know about our oceans and how we manage them. You can be part of the solution.
Resources
You can learn more about this competition and The Nature Conservancy in the video below.
Your browser does not support the video tag.","Submissions are evaluated using the multi-class logarithmic loss. Each image has been labeled with one true class. For each image, you must submit a set of predicted probabilities (one for every image). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of images in the test set, M is the number of image class labels,  \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given image are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission File
You must submit a csv file with the image file name, and a probability for each class.
The 8 classes to predict are: 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK','YFT'
The order of the rows does not matter. The file must have a header and should look like the following:
image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT
img_00001.jpg,1,0,0,0,0,...,0
img_00002.jpg,0.3,0.1,0.6,0,...,0
...","Warning: This data contains graphic contents that some may find disturbing. Please note that this competition is governed by the Competition Rules and this NDA. 
In this competition, The Nature Conservancy asks you to help them detect which species of fish appears on a fishing boat, based on images captured from boat cameras of various angles.  
Your goal is to predict the likelihood of fish species in each picture.
Eight target categories are available in this dataset: Albacore tuna, Bigeye tuna, Yellowfin tuna, Mahi Mahi, Opah, Sharks, Other (meaning that there are fish present but not in the above categories), and No Fish (meaning that no fish is in the picture). Each image has only one fish category, except that there are sometimes very small fish in the pictures that are used as bait. 
The dataset was compiled by The Nature Conservancy in partnership with Satlink, Archipelago Marine Research, the Pacific Community, the Solomon Islands Ministry of Fisheries and Marine Resources, the Australia Fisheries Management Authority, and the governments of New Caledonia and Palau.",kaggle competitions download -c the-nature-conservancy-fisheries-monitoring,"['https://www.kaggle.com/code/anokas/finding-boatids', 'https://www.kaggle.com/code/zfturbo/fishy-keras-lb-1-25267', 'https://www.kaggle.com/code/narae78/fish-detection', 'https://www.kaggle.com/code/drn01z3/mxnet-xgboost-simple-solution', 'https://www.kaggle.com/code/sbrugman/tricks-for-the-kaggle-leaderboard']"
354,"Get out your dowsing rods, electromagnetic sensors, … and gradient boosting machines. Kaggle is haunted and we need your help. After a month of making scientific observations and taking careful measurements, we’ve determined that 900 ghouls, ghosts, and goblins are infesting our halls and frightening our data scientists. When trying garlic, asking politely, and using reverse psychology didn't work, it became clear that machine learning is the only answer to banishing our unwanted guests.
So now the hour has come to put the data we’ve collected in your hands. We’ve managed to identify 371 of the ghastly creatures, but need your help to vanquish the rest. And only an accurate classification algorithm can thwart them. Use bone length measurements, severity of rot, extent of soullessness, and other characteristics to distinguish (and extinguish) the intruders. Are you ghost-busters up for the challenge?","Submissions are evaluated on the categorization accuracy (the percent of creatures that you correctly classify).
Submission File
Your submission file should predict the type for each creature in the test set. The file should contain a header and have the following format:
id,type
0,Ghost
1,Goblin
2,Ghoul
etc.","File descriptions
train.csv - the training set
test.csv - the test set
sample_submission.csv - a sample submission file in the correct format
Data fields
id - id of the creature
bone_length - average length of bone in the creature, normalized between 0 and 1
rotting_flesh - percentage of rotting flesh in the creature
hair_length - average hair length, normalized between 0 and 1
has_soul - percentage of soul in the creature
color - dominant color of the creature: 'white','black','clear','blue','green','blood'
type - target variable: 'Ghost', 'Goblin', and 'Ghoul'",kaggle competitions download -c ghouls-goblins-and-ghosts-boo,"['https://www.kaggle.com/code/amberthomas/ghosts-goblins-and-ghouls-oh-my', 'https://www.kaggle.com/code/mrtgocer/from-zero-to-hero-lightgbm-classifier', 'https://www.kaggle.com/code/daavoo/tensorflow-1vs1', 'https://www.kaggle.com/code/samratp/machine-learning-with-ghouls-goblins-and-ghosts', 'https://www.kaggle.com/code/oysteijo/ghosts-n-goblins-n-neural-networks-lb-0-74858']"
355,"What does physics have in common with biology, cooking, cryptography, diy, robotics, and travel? If you answered ""all pursuits are governed by the immutable laws of physics"" we'll begrudgingly give you partial credit. If you answered ""all were chosen randomly by a scheming Kaggle employee for a twisted transfer learning competition"", congratulations, we accept your answer and mark the question as solved.
In this competition, we provide the titles, text, and tags of Stack Exchange questions from six different sites. We then ask for tag predictions on unseen physics questions. Solving this problem via a standard machine approach might involve training an algorithm on a corpus of related text. Here, you are challenged to train on material from outside the field. Can an algorithm learn appropriate physics tags from ""extreme-tourism Antarctica""? Let's find out.
Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from the Stack Exchange data dump.","The evaluation metric for this competition is Mean F1-Score. The F1 score measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F1 score is given by:
\[ F1 = 2\frac{p \cdot r}{p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn} \]
The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.
Submission File
For every question in the dataset, submission files should contain two columns: id and tags. The predicted tags should be a space-delimited list. The file must have a header and should look like the following:
id,tags
1,physics poetry
2,physics poetry chemistry
3,physics electrons
etc.","In this dataset, you are provided with question titles, content, and tags for Stack Exchange sites on a variety of topics (biology, cooking, cryptography, diy, robotics, and travel). The content of each question is given as HTML. The tags are words or phrases that describe the topic of the question.
The test set is comprised of questions from the physics.stackexchange.com. For each question in the test set, you should use the title and question content in order to generate potential tags.",kaggle competitions download -c transfer-learning-on-stack-exchange-tags,"['https://www.kaggle.com/code/rajat700/tag-prediction-tf-idf', 'https://www.kaggle.com/code/sowm123/nlp-final-combined', 'https://www.kaggle.com/code/kaledata/text-classification-stack-exchange-tags', 'https://www.kaggle.com/code/nived89/useful-text-preprocessing-on-the-datasets', 'https://www.kaggle.com/code/kunwarbisht/text-data-clustering']"
356,"Ready to make a downpayment on your first house? Or looking to leverage the equity in the home you have? To support needs for a range of financial decisions, Santander Bank offers a lending hand to their customers through personalized product recommendations.
Under their current system, a small number of Santander’s customers receive many recommendations while many others rarely see any resulting in an uneven customer experience. In their second competition, Santander is challenging Kagglers to predict which products their existing customers will use in the next month based on their past behavior and that of similar customers.
With a more effective recommendation system in place, Santander can better meet the individual needs of all customers and ensure their satisfaction no matter where they are in life.
Disclaimer: This data set does not include any real Santander Spain's customer, and thus it is not representative of Spain's customer base. ","Submissions are evaluated according to the Mean Average Precision @ 7 (MAP@7):
$$MAP@7 = \frac{1}{|U|} \sum_{u=1}^{|U|} \frac{1}{min(m, 7)} \sum_{k=1}^{min(n,7)} P(k)$$
where |U| is the number of rows (users in two time points), P(k) is the precision at cutoff k, n is the number of predicted products, and m is the number of added products for the given user at that time point. If m = 0, the precision is defined to be 0.
Submission File
For every user at each time point, you must predict a space-delimited list of the products they added. The file should contain a header and have the following format:
ncodpers,added_products
15889,ind_tjcr_fin_ult1
15890,ind_tjcr_fin_ult1 ind_recibo_ult1
15892,ind_nomina_ult1
15893,
etc.","In this competition, you are provided with 1.5 years of customers behavior data from Santander bank to predict what new products customers will purchase. The data starts at 2015-01-28 and has monthly records of products a customer has, such as ""credit card"", ""savings account"", etc. You will predict what additional products a customer will get in the last month, 2016-06-28, in addition to what they already have at 2016-05-28. These products are the columns named: ind_(xyz)_ult1, which are the columns #25 - #48 in the training data. You will predict what a customer will buy in addition to what they already had at 2016-05-28. 
The test and train sets are split by time, and public and private leaderboard sets are split randomly.
Please note: This sample does not include any real Santander Spain customers, and thus it is not representative of Spain's customer base. 
File descriptions
train.csv - the training set
test.csv - the test set
sample_submission.csv - a sample submission file in the correct format
Data fields
Column Name Description",kaggle competitions download -c santander-product-recommendation,"['https://www.kaggle.com/code/apryor6/detailed-cleaning-visualization-python', 'https://www.kaggle.com/code/apryor6/detailed-cleaning-visualization', 'https://www.kaggle.com/code/sudalairajkumar/when-less-is-more', 'https://www.kaggle.com/code/anokas/collaborative-filtering-btb-lb-0-01691', 'https://www.kaggle.com/code/donyoe/santander-quick-first-view']"
357,"The internet is a stimulating treasure trove of possibility. Every day we stumble on news stories relevant to our communities or experience the serendipity of finding an article covering our next travel destination. Outbrain, the web’s leading content discovery platform, delivers these moments while we surf our favorite sites.
Currently, Outbrain pairs relevant content with curious readers in about 250 billion personalized recommendations every month across many thousands of sites. In this competition, Kagglers are challenged to predict which pieces of content its global base of users are likely to click on. Improving Outbrain’s recommendation algorithm will mean more users uncover stories that satisfy their individual tastes.","Submissions are evaluated according to the Mean Average Precision @12  (MAP@12):
$$MAP@12 = \frac{1}{|U|} \sum_{u=1}^{|U|} \sum_{k=1}^{min(12, n)} P(k)$$ 
where |U| is the number of display_ids, P(k) is the precision at cutoff k, n is the number of predicted ad_ids.
Submission File
For each display_id in the test set, you must predict a space-delimited list of ad_ids, ordered by decreasing likelihood of being clicked. The candidate ad_ids for each display_id are provided in clicks_test.csv. Note that each display_id can have a different number of associated ads. The file should contain a header and have the following format:
display_id,ad_id
16874594,66758 150083 162754 170392 172888 180797
16874595,8846 30609 143982
16874596,11430 57197 132820 153260 173005 288385 289122 289915
etc.","Data Use Update: The Competition Sponsor has updated the permitted use of this competition's dataset. You may access and use the Competition Data for any purpose, whether commercial or non-commercial, including for participating in the Competition and on Kaggle.com forums, and for academic research and education.

The dataset for this challenge contains a sample of users’ page views and clicks, as observed on multiple publisher sites in the United States between 14-June-2016 and 28-June-2016. Each viewed page or clicked recommendation is further accompanied by some semantic attributes of those documents. For full details, see data specifications below.
The dataset contains numerous sets of content recommendations served to a specific user in a specific context. Each context (i.e. a set of recommendations) is given a display_id. In each such set, the user has clicked on at least one recommendation. The identities of the clicked recommendations in the test set are not revealed. Your task is to rank the recommendations in each group by decreasing predicted likelihood of being clicked.
As a warning, this is a very large relational dataset. While most of the tables are small enough to fit in memory, the page views log (page_views.csv) is over 2 billion rows and 100GB uncompressed. We have also uploaded a sample version of this file with the first 10,000,000 rows. The MD5 checksum of page_views.csv.zip is 3742c116bab4030e0a7ea1c0be623bd9.
Data Fields
Each user in the dataset is represented by a unique id (uuid). A person can view a document (document_id), which is simply a web page with content (e.g.  a news article). On each document, a set of ads (ad_id) are displayed. Each ad belongs to a campaign (campaign_id) run by an advertiser (advertiser_id). You are also provided metadata about the document, such as which entities are mentioned, a taxonomy of categories, the topics mentioned, and the publisher.",kaggle competitions download -c outbrain-click-prediction,"['https://www.kaggle.com/code/anokas/outbrain-eda', 'https://www.kaggle.com/code/gspmoreira/unveiling-page-views-csv-with-pyspark', 'https://www.kaggle.com/code/andreyg/explore-user-base-by-geo', 'https://www.kaggle.com/code/sudalairajkumar/ftrl-starter-with-leakage-vars', 'https://www.kaggle.com/code/clustifier/btb-0-63523-evaluation']"
358,"In 2013, we hosted one of our favorite for-fun competitions:  Dogs vs. Cats. Much has since changed in the machine learning landscape, particularly in deep learning and image analysis. Back then, a tensor flow was the diffusion of the creamer in a bored mathematician's cup of coffee. Now, even the cucumber farmers are neural netting their way to a bounty.
Much has changed at Kaggle as well. Our online coding environment Kernels didn't exist in 2013, and so it was that we approached sharing by scratching primitive glpyhs on cave walls with sticks and sharp objects. No more. Now, Kernels have taken over as the way to share code on Kaggle. IPython is out and Jupyter Notebook is in. We even have TensorFlow. What more could a data scientist ask for? But seriously, what more? Pull requests welcome.
We are excited to bring back the infamous Dogs vs. Cats classification problem as a playground competition with kernels enabled. Although modern techniques may make light of this once-difficult problem, it is through practice of new techniques on old datasets that we will make light of machine learning's future challenges.","Submissions are scored on the log loss:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of images in the test set
\\( \hat{y}_i \\) is the predicted probability of the image being a dog
\\( y_i \\) is 1 if the image is a dog, 0 if cat
\\( log() \\) is the natural (base e) logarithm
A smaller log loss is better.
Submission File
For each image in the test set, you must submit a probability that image is a dog. The file should have a header and be in the following format:
id,label
1,0.5
2,0.5
3,0.5
...","The train folder contains 25,000 images of dogs and cats. Each image in this folder has the label as part of the filename. The test folder contains 12,500 images, named according to a numeric id. For each image in the test set, you should predict a probability that the image is a dog (1 = dog, 0 = cat).",kaggle competitions download -c dogs-vs-cats-redux-kernels-edition,"['https://www.kaggle.com/code/shivamb/cnn-architectures-vgg-resnet-inception-tl', 'https://www.kaggle.com/code/jeffd23/catdognet-keras-convnet-starter', 'https://www.kaggle.com/code/sarvajna/dogs-vs-cats-keras-solution', 'https://www.kaggle.com/code/rajmehra03/a-comprehensive-guide-to-transfer-learning', 'https://www.kaggle.com/code/hortonhearsafoo/fast-ai-lesson-1']"
359,"Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective. Even after surgical removal of epilepsy, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring.
Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for electrical brain activity (EEG) based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.
The Competition
Transitioning from the Kaggle contests held on seizure detection and seizure prediction in 2014 that primarily involved long-term electrical brain activity recordings from dogs, the current contest focuses on seizure prediction using long-term electrical brain activity recordings from humans obtained from the world-first clinical trial of the implantable NeuroVista Seizure Advisory System.
Human brain activity was recorded in the form of intracranial EEG (iEEG), which involves electrodes positioned on the surface of the cerebral cortex and the recording of electrical signals with an ambulatory monitoring system. These are long duration recordings, spanning multiple months up to multiple years and recording large numbers of seizures in some humans. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity. 
Acknowledgments 
This competition is sponsored by MathWorks, the National Institutes of Health (NINDS), the American Epilepsy Society and the University of Melbourne, and organised in partnership with the Alliance for Epilepsy Research, the University of Pennsylvania and the Mayo Clinic.
      References
Cook MJ, O'Brien TJ, Berkovic SF, Murphy M, Morokoff A, Fabinyi G, D'Souza W, Yerra R, Archer J, Litewka L, Hosking S, Lightfoot P, Ruedebusch V, Sheffield WD, Snyder D, Leyde K, Himes D (2013) Prediction of seizure likelihood with a long-term, implanted seizure advisory system in patients with drug-resistant epilepsy: a first-in-man study. LANCET NEUROL 12:563-571.
Brinkmann, B. H., Wagenaar, J., Abbot, D., Adkins, P., Bosshard, S. C., Chen, M., ... & Pardo, J. (2016). Crowdsourcing reproducible seizure forecasting in human and canine epilepsy. Brain, 139(6), 1713-1722.
Gadhoumi, K., Lina, J. M., Mormann, F., & Gotman, J. (2016). Seizure prediction for therapeutic devices: A review. Journal of neuroscience methods, 260, 270-282.
Karoly, P. J., Freestone, D. R., Boston, R., Grayden, D. B., Himes, D., Leyde, K., ... & Cook, M. J. (2016). Interictal spikes and epileptic seizures: their relationship and underlying rhythmicity. Brain, aww019.
Andrzejak RG, Chicharro D, Elger CE, Mormann F (2009) Seizure prediction: Any better than chance? Clin Neurophysiol.
Snyder DE, Echauz J, Grimes DB, Litt B (2008) The statistics of a practical seizure warning system. J Neural Eng 5: 392–401.
Mormann F, Andrzejak RG, Elger CE, Lehnertz K (2007) Seizure prediction: the long and winding road. Brain 130: 314–333.
Haut S, Shinnar S, Moshe SL, O'Dell C, Legatt AD. (1999) The association between seizure clustering and status epilepticus in patients with intractable complex partial seizures. Epilepsia 40:1832–1834.","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each File in the test set, you must predict a probability for the Class variable. A probability of 1 indicates a prediction of the preictal class. The file should contain a header and have the following format:
File,Class
1_1.mat,0.1
1_2.mat,0.00
1_3.mat,1
etc.",,,"['https://www.kaggle.com/code/profcarlosdelfino/minhas-anota-es-de-estudos-no-kaggle', 'https://www.kaggle.com/code/anandaraja/feature-extractor-for-eeg-with-classifier', 'https://www.kaggle.com/code/lucaszw/notebook787946f732', 'https://www.kaggle.com/code/solomonk/bayesian-logistic-regression-and-pymc3', 'https://www.kaggle.com/code/biopedro/notebook1d240684cd']"
360,"Start here if...
You have some experience with R or Python and machine learning basics. This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition. 
💡Getting Started Notebook
To get started quickly, feel free to take advantage of this starter notebook.
Competition Description
Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.
With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.
Practice Skills
Creative feature engineering 
Advanced regression techniques like random forest and gradient boosting
Acknowledgments
The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. 
Photo by Tom Thain on Unsplash.","Goal
It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. 
Metric
Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)
Submission File Format
The file should contain a header and have the following format:
Id,SalePrice
1461,169000.1
1462,187724.1233
1463,175221
etc.
You can download an example submission file (sample_submission.csv) on the Data page.","File descriptions
train.csv - the training set
test.csv - the test set
data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here
sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms
Data fields
Here's a brief version of what you'll find in the data description file.
SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.
MSSubClass: The building class
MSZoning: The general zoning classification
LotFrontage: Linear feet of street connected to property
LotArea: Lot size in square feet
Street: Type of road access",kaggle competitions download -c house-prices-advanced-regression-techniques,"['https://www.kaggle.com/code/gusthema/house-prices-prediction-using-tfdf', 'https://www.kaggle.com/code/pmarcelino/comprehensive-data-exploration-with-python', 'https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard', 'https://www.kaggle.com/code/apapiu/regularized-linear-models', 'https://www.kaggle.com/code/dansbecker/submitting-from-a-kernel']"
361,"There are estimated to be nearly half a million species of plant in the world. Classification of species has been historically problematic and often results in duplicate identifications.
The objective of this playground competition is to use binary leaf images and extracted features, including shape, margin & texture, to accurately identify 99 species of plants. Leaves, due to their volume, prevalence, and unique characteristics, are an effective means of differentiating plant species. They also provide a fun introduction to applying techniques that involve image-based features.
As a first step, try building a classifier that uses the provided pre-extracted features. Next, try creating a set of your own features. Finally, examine the errors you're making and see what you can do to improve.
Acknowledgments
Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from leaf images collected by  
James Cope, Thibaut Beghin, Paolo Remagnino, & Sarah Barman of the Royal Botanic Gardens, Kew, UK.
Charles Mallah, James Cope, James Orwell. Plant Leaf Classification Using Probabilistic Integration of Shape, Texture and Margin Features. Signal Processing, Pattern Recognition and Applications, in press. 2013.
We thank the UCI machine learning repository for hosting the dataset.","Submissions are evaluated using the multi-class logarithmic loss. Each image has been labeled with one true species. For each image, you must submit a set of predicted probabilities (one for every species). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of images in the test set, M is the number of species labels, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given device are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum), but they need to be in the range of [0, 1]. In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission File
You must submit a csv file with the image id, all candidate species names, and a probability for each species. The order of the rows does not matter. The file must have a header and should look like the following:
id,Acer_Capillipes,Acer_Circinatum,Acer_Mono,...
2,0.1,0.5,0,0.2,...
5,0,0.3,0,0.4,...
6,0,0,0,0.7,...
etc.","The dataset consists approximately 1,584 images of leaf specimens (16 samples each of 99 species) which have been converted to binary black leaves against white backgrounds. Three sets of features are also provided per image: a shape contiguous descriptor, an interior texture histogram, and a ﬁne-scale margin histogram. For each feature, a 64-attribute vector is given per leaf sample.
Note that of the original 100 species, we have eliminated one on account of incomplete associated data in the original dataset.
File descriptions
train.csv - the training set
test.csv - the test set
sample_submission.csv - a sample submission file in the correct format
images - the image files (each image is named with its corresponding id)
Data fields
id - an anonymous id unique to an image
margin_1, margin_2, margin_3, ..., margin_64 - each of the 64 attribute vectors for the margin feature",kaggle competitions download -c leaf-classification,"['https://www.kaggle.com/code/jeffd23/10-classifier-showdown-in-scikit-learn', 'https://www.kaggle.com/code/lorinc/feature-extraction-from-images', 'https://www.kaggle.com/code/abhmul/keras-convnet-lb-0-0052-w-visualization', 'https://www.kaggle.com/code/selfishgene/visualizing-pca-with-leaf-dataset', 'https://www.kaggle.com/code/selfishgene/visualizing-k-means-with-leaf-dataset']"
362,"A good chocolate soufflé is decadent, delicious, and delicate. But, it's a challenge to prepare. When you pull a disappointingly deflated dessert out of the oven, you instinctively retrace your steps to identify at what point you went wrong. Bosch, one of the world's leading manufacturing companies, has an imperative to ensure that the recipes for the production of its advanced mechanical components are of the highest quality and safety standards. Part of doing so is closely monitoring its parts as they progress through the manufacturing processes.
Because Bosch records data at every step along its assembly lines, they have the ability to apply advanced analytics to improve these manufacturing processes. However, the intricacies of the data and complexities of the production line pose problems for current methods.
In this competition, Bosch is challenging Kagglers to predict internal failures using thousands of measurements and tests made for each component along the assembly line. This would enable Bosch to bring quality products at lower costs to the end user.","Submissions are evaluated on the Matthews correlation coefficient (MCC) between the predicted and the observed response. The MCC is given by:
$$ MCC = \frac{(TP*TN) - (FP * FN)}{\sqrt{(TP+FP)(TP+FN)(TN + FP)(TN+FN)}}, $$
where TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives.
Submission File
For each Id in the test set, you must predict a binary prediction for the Response variable. The file should contain a header and have the following format:
Id,Response
1,0
2,1
3,0
etc.","The data for this competition represents measurements of parts as they move through Bosch's production lines. Each part has a unique Id. The goal is to predict which parts will fail quality control (represented by a 'Response' = 1).
The dataset contains an extremely large number of anonymized features. Features are named according to a convention that tells you the production line, the station on the line, and a feature number. E.g. L3_S36_F3939 is a feature measured on line 3, station 36, and is feature number 3939.
On account of the large size of the dataset, we have separated the files by the type of feature they contain: numerical, categorical, and finally, a file with date features. The date features provide a timestamp for when each measurement was taken. Each date column ends in a number that corresponds to the previous feature number. E.g. the value of L0_S0_D1 is the time at which L0_S0_F0 was taken.
In addition to being one of the largest datasets (in terms of number of features) ever hosted on Kaggle, the ground truth for this competition is highly imbalanced. Together, these two attributes are expected to make this a challenging problem.
File descriptions
train_numeric.csv - the training set numeric features (this file contains the 'Response' variable)
test_numeric.csv - the test set numeric features (you must predict the 'Response' for these Ids)
train_categorical.csv - the training set categorical features
test_categorical.csv - the test set categorical features",kaggle competitions download -c bosch-production-line-performance,"['https://www.kaggle.com/code/emphymachine/76-78-failure-prediction', 'https://www.kaggle.com/code/emphymachine/failure-prediction-pjt', 'https://www.kaggle.com/code/emphymachine/kdt-03-ai-bosch-production-line-performance-pjt', 'https://www.kaggle.com/code/emphymachine/lges-bosch-production-line-eda-practice', 'https://www.kaggle.com/code/lgjinukeom/bosch-production-line-eda-practice-ju']"
363,"Like most companies, Red Hat is able to gather a great deal of information over time about the behavior of individuals who interact with them. They’re in search of better methods of using this behavioral data to predict which individuals they should approach—and even when and how to approach them.
In this competition, Kagglers are challenged to create a classification algorithm that accurately identifies which customers have the most potential business value for Red Hat based on their characteristics and activities.
With an improved prediction model in place, Red Hat will be able to more efficiently prioritize resources to generate more business and better serve their customers.","Submissions are evaluated on area under the ROC curve between the predicted and the observed outcome.
Submission File
For each activity_id in the test set, you must predict a probability for the 'outcome' variable, represented by a number between 0 and 1. The file should contain a header and have the following format:
activity_id,outcome
act1_1,0
act1_100006,0
act1_100050,0
etc.","This competition uses two separate data files that may be joined together to create a single, unified data table: a people file and an activity file.
The people file contains all of the unique people (and the corresponding characteristics) that have performed activities over time. Each row in the people file represents a unique person. Each person has a unique people_id.
The activity file contains all of the unique activities (and the corresponding activity characteristics) that each person has performed over time. Each row in the activity file represents a unique activity performed by a person on a certain date. Each activity has a unique activity_id.
The challenge of this competition is to predict the potential business value of a person who has performed a specific activity. The business value outcome is defined by a yes/no field attached to each unique activity in the activity file. The outcome field indicates whether or not each person has completed the outcome within a fixed window of time after each unique activity was performed.
The activity file contains several different categories of activities. Type 1 activities are different from type 2-7 activities because there are more known characteristics associated with type 1 activities (nine in total) than type 2-7 activities (which have only one associated characteristic).
To develop a predictive model with this data, you will likely need to join the files together into a single data set. The two files can be joined together using person_id as the common key. All variables are categorical, with the exception of 'char_38' in the people file, which is a continuous numerical variable.",kaggle competitions download -c predicting-red-hat-business-value,"['https://www.kaggle.com/code/mohammedelhamraoui/dnn-red-hat-business-value', 'https://www.kaggle.com/code/deepakkumargunjetti/classify-customer-potential', 'https://www.kaggle.com/code/dataengel/predicting-red-hat-business-value', 'https://www.kaggle.com/code/marinovik/predicting-business-value-with-lightgbm-red-hat', 'https://www.kaggle.com/code/wafadje/predicting-red-hat-business-value']"
364,"Nothing is more comforting than being greeted by your favorite drink just as you walk through the door of the corner café. While a thoughtful barista knows you take a macchiato every Wednesday morning at 8:15, it’s much more difficult in a digital space for your preferred brands to personalize your experience.
TalkingData, China’s largest third-party mobile data platform, understands that everyday choices and behaviors paint a picture of who we are and what we value. Currently, TalkingData is seeking to leverage behavioral data from more than 70% of the 500 million mobile devices active daily in China to help its clients better understand and interact with their audiences.
In this competition, Kagglers are challenged to build a model predicting users’ demographic characteristics based on their app usage, geolocation, and mobile device properties. Doing so will help millions of developers and brand advertisers around the world pursue data-driven marketing efforts which are relevant to their users and catered to their preferences.
Acknowledgements","Submissions are evaluated using the multi-class logarithmic loss. Each device has been labeled with one true class. For each device, you must submit a set of predicted probabilities (one for each class). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of devices in the test set, M is the number of class labels,  \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if device \\(i\\) belongs to class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given device are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum), but they need to be in the range of [0, 1]. In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission File
You must submit a csv file with the device id, and a probability for each class.
The 12 classes to predict are:
'F23-', 'F24-26','F27-28','F29-32', 'F33-42', 'F43+',
'M22-', 'M23-26', 'M27-28', 'M29-31', 'M32-38', 'M39+'
The order of the rows does not matter. The file must have a header and should look like the following:
device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+
1234,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833
5678,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833
...","In this competition, you are going to predict the demographics of a user (gender and age) based on their app download and usage behaviors. 
The Data is collected from TalkingData SDK integrated within mobile apps TalkingData serves under the service term between TalkingData and mobile app developers. Full recognition and consent from individual user of those apps have been obtained, and appropriate anonymization have been performed to protect privacy. Due to confidentiality, we won't provide details on how the gender and age data was obtained. Please treat them as accurate ground truth for prediction. 
The data schema can be represented in the following chart:
File descriptions
gender_age_train.csv, gender_age_test.csv - the training and test set
group: this is the target variable you are going to predict
events.csv, app_events.csv - when a user uses TalkingData SDK, the event gets logged in this data. Each event has an event id, location (lat/long), and the event corresponds to a list of apps in app_events.
timestamp: when the user is using an app with TalkingData SDK",kaggle competitions download -c talkingdata-mobile-user-demographics,"['https://www.kaggle.com/code/beyondbeneath/geolocation-visualisations', 'https://www.kaggle.com/code/dvasyukova/a-linear-model-on-apps-and-labels', 'https://www.kaggle.com/code/dvasyukova/brand-and-model-based-benchmarks', 'https://www.kaggle.com/code/tonyliu/the-battle-of-smart-phones', 'https://www.kaggle.com/code/zfturbo/xgboost-simple-starter']"
365,"Planning a celebration is a balancing act of preparing just enough food to go around without being stuck eating the same leftovers for the next week. The key is anticipating how many guests will come. Grupo Bimbo must weigh similar considerations as it strives to meet daily consumer demand for fresh bakery products on the shelves of over 1 million stores along its 45,000 routes across Mexico.
Currently, daily inventory calculations are performed by direct delivery sales employees who must single-handedly predict the forces of supply, demand, and hunger based on their personal experiences with each store. With some breads carrying a one week shelf life, the acceptable margin for error is small.
In this competition, Grupo Bimbo invites Kagglers to develop a model to accurately forecast inventory demand based on historical sales data. Doing so will make sure consumers of its over 100 bakery products aren’t staring at empty shelves, while also reducing the amount spent on refunds to store owners with surplus product unfit for sale.","The evaluation metric for this competition is Root Mean Squared Logarithmic Error.
The RMSLE is calculated as
$$
\epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }
$$
Where:
\\(\epsilon\\) is the RMSLE value (score)
\\(n\\) is the total number of observations in the (public/private) data set,
\\(p_i\\) is your prediction of demand, and
\\(a_i\\) is the actual demand for \\(i\\).
\\(\log(x)\\) is the natural logarithm of \\(x\\)
Submission File
For every row in the dataset, submission files should contain two columns: id and Demanda_uni_equi.  The id corresponds to the column of that id in the test.csv. The file should contain a header and have the following format:
id,Demanda_uni_equil
0,1
1,0
2,500
3,100
etc.","In this competition, you will forecast the demand of a product for a given week, at a particular store. The dataset you are given consists of 9 weeks of sales transactions in Mexico. Every week, there are delivery trucks that deliver products to the vendors. Each transaction consists of sales and returns. Returns are the products that are unsold and expired. The demand for a product in a certain week is defined as the sales this week subtracted by the return next week.

The train and test dataset are split based on time, as well as the public and private leaderboard dataset split.

Things to note:
There may be products in the test set that don't exist in the train set. This is the expected behavior of inventory data, since there are new products being sold all the time. Your model should be able to accommodate this.
There are duplicate Cliente_ID's in cliente_tabla, which means one Cliente_ID may have multiple NombreCliente that are very similar. This is due to the NombreCliente being noisy and not standardized in the raw data, so it is up to you to decide how to clean up and use this information. 
The adjusted demand (Demanda_uni_equil) is always >= 0 since demand should be either 0 or a positive value. The reason that Venta_uni_hoy - Dev_uni_proxima sometimes has negative values is that the returns records sometimes carry over a few weeks.
File descriptions",kaggle competitions download -c grupo-bimbo-inventory-demand,"['https://www.kaggle.com/code/vykhand/exploring-products', 'https://www.kaggle.com/code/fabienvs/grupo-bimbo-data-analysis', 'https://www.kaggle.com/code/bpavlyshenko/bimbo-xgboost-r-script-lb-0-457', 'https://www.kaggle.com/code/anokas/exploratory-data-analysis', 'https://www.kaggle.com/code/apapiu/mean-vs-medians-a-mathy-approach']"
366,"7. You read that correctly. That's the start to a real integer sequence, the powers of primes. Want something easier? How about the next number in 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55? If you answered 89, you may enjoy this challenge. Your computer may find it considerably less enjoyable.
The On-Line Encyclopedia of Integer Sequences is a 50+ year effort by mathematicians the world over to catalog sequences of integers. If it has a pattern, it's probably in the OEIS, and probably described with amazing detail. This competition challenges you create a machine learning algorithm capable of guessing the next number in an integer sequence. While this sounds like pattern recognition in its most basic form, a quick look at the data will convince you this is anything but basic!
Acknowledgments
Kaggle is hosting this competition for the data science community to use for fun and education. We thank the OEIS and its contributors for cataloging this data.","This competition is evaluated on accuracy of your predictions (the percentage of sequences where you predict the next number correctly).
Submission File
For every sequence Id in the test set, you should predict the next integer. The file should contain a header and have the following format:
Id,Last
1,0
2,22
4,9378
etc.","This dataset contains the majority of the integer sequences from the OEIS. It is split into a training set, where you are given the full sequence, and a test set, where we have removed the last number from the sequence. The task is to predict this removed integer.
Note that some sequences may have identical beginnings (or even be identical altogether). We have not removed these from the dataset.
File descriptions
train.csv - the training set, contains full sequences
test.csv - the test set, missing the last number in each sequence
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c integer-sequence-learning,"['https://www.kaggle.com/code/bcantt/sequence-learning', 'https://www.kaggle.com/code/andreysolvyov/integer-sequence-learning-homework', 'https://www.kaggle.com/code/jihyeseo/integer-sequence-learning-eda', 'https://www.kaggle.com/code/ferden/summation', 'https://www.kaggle.com/code/wikkefly/notebook0ff0f029a1']"
367,"Even the bravest patient cringes at the mention of a surgical procedure. Surgery inevitably brings discomfort, and oftentimes involves significant post-surgical pain. Currently, patient pain is frequently managed through the use of narcotics that bring a bevy of unwanted side effects.
This competition's sponsor is working to improve pain management through the use of indwelling catheters that block or mitigate pain at the source. Pain management catheters reduce dependence on narcotics and speed up patient recovery.
Accurately identifying nerve structures in ultrasound images is a critical step in effectively inserting a patient’s pain management catheter. In this competition, Kagglers are challenged to build a model that can identify nerve structures in a dataset of ultrasound images of the neck. Doing so would improve catheter placement and contribute to a more pain free future. ","This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:
$$ \frac{2 * |X \cap Y|}{|X| + |Y|},$$
where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.
Submission File
In order to reduce the submission file size, our metric uses run-length encoding on the pixel values.  Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).
The competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.
The file should contain a header and have the following format:
img,pixels
1,1 1 5 1
2,1 1
3,1 1
etc.","The task in this competition is to segment a collection of nerves called the Brachial Plexus (BP) in ultrasound images. You are provided with a large training set of images where the nerve has been manually annotated by humans. Annotators were trained by experts and instructed to annotate images where they felt confident about the existence of the BP landmark.
Please note these important points:
The dataset contains images where the BP is not present. Your algorithm should predict no pixel values in these cases.
As with all human-labeled data, you should expect to find noise, artifacts, and potential mistakes in the ground truth. Any individual mistakes (not affecting the broader integrity of the competition) will be left as is.
Due to the way the acquisition machine generates image frames, you may find identical images or very similar images.
In order to reduce the submission file sizes, this competition uses run-length encoding (RLE) on the pixel values. The details of how to use RLE are described on the 'Evaluation' page.
File descriptions
/train/ contains the training set images, named according to subject_imageNum.tif. Every image with the same subject number comes from the same person. This folder also includes binary mask images showing the BP segmentations.
/test/ contains the test set images, named according to imageNum.tif. You must predict the BP segmentation for these images and are not provided a subject number. There is no overlap between the subjects in the training and test sets.
gives the training image masks in run-length encoded format. This is provided as a convenience to demonstrate how to turn image masks into encoded text values for submission.",kaggle competitions download -c ultrasound-nerve-segmentation,"['https://www.kaggle.com/code/somersettuska/mynet', 'https://www.kaggle.com/code/aditya9790/nerve-segmentation', 'https://www.kaggle.com/code/ghanatej/ultrasoundnotebook2cbea42066', 'https://www.kaggle.com/code/vidushibhatia/2-ultrasound-nerve-seg-unet-from-scratch', 'https://www.kaggle.com/code/coregang/computer-statistical-analysis-of-data-5th-lab']"
368,"Ever wonder what it's like to work at Facebook? Facebook and Kaggle are launching a machine learning engineering competition for 2016. Trail blaze your way to the top of the leaderboard to earn an opportunity at interviewing for one of the 10+ open roles as a software engineer, working on world class machine learning problems.
The goal of this competition is to predict which place a person would like to check in to. For the purposes of this competition, Facebook created an artificial world consisting of more than 100,000 places located in a 10 km by 10 km square. For a given set of coordinates, your task is to return a ranked list of the most likely places. Data was fabricated to resemble location signals coming from mobile devices, giving you a flavor of what it takes to work with real data complicated by inaccurate and noisy values. Inconsistent and erroneous location data can disrupt experience for services like Facebook Check In.
We highly encourage competitors to be active on Kaggle Scripts. Your work there will be thoughtfully included in the decision making process.
Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions.","Submissions are evaluated according to the Mean Average Precision @3  (MAP@3):
$$MAP@3 = \frac{1}{|U|} \sum_{u=1}^{|U|} \sum_{k=1}^{min(3, n)} P(k)$$ 
where |U| is the number of check in events, P(k) is the precision at cutoff k, n is the number of predicted businesses. 
Submission File
For every user check in, you must predict a space-delimited list of the businesses they check into. You may submit up to 3 predictions for each check in. The file should contain a header and have the following format:
row_id,place_id
0,2083653582 1476539553 1000015801
1,6147316961 6147316961 8999137193
etc...","In this competition, you are going to predict which business a user is checking into based on their location, accuracy, and timestamp. ",kaggle competitions download -c facebook-v-predicting-check-ins,"['https://www.kaggle.com/code/apapiu/random-forest-on-a-few-blocks', 'https://www.kaggle.com/code/msjgriffiths/exploratory-data-analysis', 'https://www.kaggle.com/code/beyondbeneath/data-exploration-and-visualisations', 'https://www.kaggle.com/code/rsakata/3rd-place-solution-simple-version', 'https://www.kaggle.com/code/leonlu/another-way-to-know-the-time-definition']"
369,"Online marketplaces make it a breeze for users to both find and buy unique treasures or unload their dusty record collections in the spirit of spring cleaning. As one of the world's largest and fastest growing online classifieds, Avito hosts high volumes of listings and competitive sellers often go to great lengths to get their wares noticed. 
For some sellers, this means posting the same ad several times with slightly altered text or photos taken from different angles. To ensure that buyers can easily find what they're looking for without sifting through dozens of deceptively identical ads, Avito is asking Kagglers to develop a model that can automatically spot duplicate ads. With more accurate duplicate ad detection, Avito will make it much easier for buyers to find and make their next purchase with an honest seller.","The goal of this competition is to predict if a pair of ads are duplicates. The evaluation metric for this competition is the AUC (area under the curve).
Submission File
Each line of your submission should contain an id and a probability prediction. The prediction column should have values between 0 and 1, representing the probability of the pair of ads being duplicates. 
Your submission file must have a header row and should have the following format:
id,probability
0,0.543404941791
1,0.278369385094
2,1.0
3,0.5
etc.","In this competition, you will predict whether pairs of ads are duplicates. The data is captured in the following schema:

To ensure that winning models are robust enough to generalize to new duplicate cases, the train and test datasets are sampled from different time intervals. Hence, you may see different distributions of duplicates in train/test datasets. ",kaggle competitions download -c avito-duplicate-ads-detection,"['https://www.kaggle.com/code/nandhinimuthusamy/notebooke709e390cc', 'https://www.kaggle.com/code/yuriyanansckih/lab2-u', 'https://www.kaggle.com/code/odinnoneodin/notebook88d5a91af8', 'https://www.kaggle.com/code/emilyweilingu/avito-duplicate-ad-detection', 'https://www.kaggle.com/code/jihyeseo/detect-duplicate-ads-pair-nlp']"
370,"With an original Picasso carrying a 106 million dollar price tag, identifying an authentic work of art from a forgery is a high-stakes industry. While algorithms have gotten good at telling us if a still life is of a basket of apples or a sunflower bouquet, they aren't yet able to tell us with certainty if both paintings are by van Gogh.  
In this playground competition, we're challenging Kagglers to examine pairs of paintings and determine if they are by the same artist. This is an excellent opportunity to improve your computer vision skills and engage with a unique dataset of art. From the movement of brushstrokes to the use of light and dark, successful algorithms will likely incorporate many aspects of a painter's unique style. 
Resources
A neural algorithm of artistic style
How Do We See Art: An Eye-Tracker Study
Acknowledgments
Many of the images in this dataset were obtained from wikiart.org. Additional paintings were provided by artists whose contributions will be acknowledged at the close of the competition.
This playground competition and its datasets were prepared by Small Yellow Duck (Kiri Nichol). This includes the design of the pairwise-evaluation scheme.","The goal of this competition is to predict if a pair of images are artworks made by the same artist (or not). The evaluation metric for this competition is the AUC (area under the curve).
Submission File
The submission_info.csv file contains three columns: index, image1 and image2. For each row in the submission_info file, you will need to predict the probability that image1 and image2 are by the same artist.
Every row in the submission file should contain two columns: index and sameArtist. index is the row number from submission_info.csv and sameArtist is the probability that image1 and image2 are by the same artist.
The file should contain a header and have the following format:
index,sameArtist
1,0.5
2,0.55
3,1.0
4,0.0
etc.","Most of the images in this competition are from WikiArt.org. Please assume that all images are protected by copyright and utilize the images only for the purposes of data mining, which constitutes a form of fair use.
File descriptions
train.zip -zip file containing the images in the training set (.jpg)
train_{1,2,..9}.zip - subsets of train.zip since the data size is large. You don't need any of these if you can download train.zip. 
test.zip - zip file containing the images in the test set (.jpg)
train_info.csv - file listing the image filename, artistID, genre, style, date and title for images in the training set.
submission_info.csv - each row lists an index and the filenames of the two images for which the algorithm needs to make a prediction about whether they were created by the same artist.
sampleSubmission.csv - a sample submission file with a column for comparison number and the predicted probability
replacements_for_corrupted_files.zip - 10 files in train and test sets that were corrupted images, these are the correct images
Some artists have work in both the training and test set, while others have work in just the training set or just the test set.
Submission format",kaggle competitions download -c painter-by-numbers,"['https://www.kaggle.com/code/spyrosrigas/20-painters-classification-with-cnns-and-svms', 'https://www.kaggle.com/code/bonovandoo/deep-dreaming-dali-keras-deepdream', 'https://www.kaggle.com/code/johnaji/notebookea493c1202', 'https://www.kaggle.com/code/mfekadu/picasso-not-picasso', 'https://www.kaggle.com/code/d22ski/the-painting-art-colors-timeline']"
371,"Imagine a world where we can use satellite images to help find better access to clean water, prevent poaching of wildlife, predict storms more efficiently, optimize traffic patterns more readily, and inform human behaviors to mitigate the spread of disease.
Thanks to a marked increase of satellites in orbit, we will be able to capture images – and the information contained within – of nearly every place on Earth, every day by 2017. However, our ability to analyze datasets of these images has not advanced as quickly. Changes from day to day in images of the same location are subtle, can be hard to detect, and are difficult to understand in terms of their significance.
In this competition, Draper provides a unique dataset of images taken at the same locations over 5 days. Kagglers are challenged to predict the chronological order of the photos taken at each location. Accurately doing so could uncover approaches that have a global impact on commerce, science, and humanitarian works.","Submissions are evaluated on the mean Spearman's correlation coefficient. To calculate this, the Spearman's rank correlation is computed for each set of 5 images. Identical predicted values are assigned fractional ranks equal to the average of their positions in the sorted values. These correlations are then averaged to create the score for the leaderboard.
Submission File
For each set of 5 images, you should predict a space-delimited list of the correct day for each respective image. For example, ""1,3 1 2 5 4"" implies that for setId = 1, the image set1_1 occurred third, set1_2 occurred first, set1_3 occurred second, set1_4 occurred fifth, and set1_5 occurred fourth.
The file should contain a header and have the following format:
setId,day
1,1 2 3 4 5
2,1 2 3 4 5
3,1 2 3 4 5
etc.","This dataset contains over one thousand high-resolution images of aerial photographs taken in southern California. The photographs were taken from a plane and are meant as a reasonable facsimile for satellite images.
Images are grouped into sets of five, each of which have the same setId. Each image in a set was taken on a different day (but not necessarily at the same time each day). The images for each set cover approximately the same area but are not exactly aligned. Images are named according to the convention setId_day. In the training set, the images have the correct day ordering. In the test set, the images are scrambled.
This competition asks you order the scrambled images in the test set. This is anticipated to be a challenging task. Some locations will provide very little evidence of changes from day to day. Other locations will have subtle changes that hint at an ordering, such as parked or moving vehicles, differences in bodies of water, or changes in shadows. On account of the difficulty, you may use both manual and computer-based analysis in this competition.
File/Folder descriptions
For convenience, we have provided both lossless, high-quality images (train/test) as well as the same images in a compressed jpeg format (train_sm/test_sm).
train - 8-bit tif images of the training set
train_sm - compressed (lossy) jpegs of the training set
test - 8-bit tif images of the test set",kaggle competitions download -c draper-satellite-image-chronology,"['https://www.kaggle.com/code/kartml/draper-cnn-gru-ii', 'https://www.kaggle.com/code/kartml/draper-cnn-lstm', 'https://www.kaggle.com/code/kartml/draper-cnn-gru', 'https://www.kaggle.com/code/kartml/transformers-forecasting-draper-satellites', 'https://www.kaggle.com/code/snehalsandri007/notebook67bccd6b30']"
372,"Planning your dream vacation, or even a weekend escape, can be an overwhelming affair. With hundreds, even thousands, of hotels to choose from at every destination, it's difficult to know which will suit your personal preferences. Should you go with an old standby with those pillow mints you like, or risk a new hotel with a trendy pool bar? 
Expedia wants to take the proverbial rabbit hole out of hotel search by providing personalized hotel recommendations to their users. This is no small task for a site with hundreds of millions of visitors every month!
Currently, Expedia uses search parameters to adjust their hotel recommendations, but there aren't enough customer specific data to personalize them for each user. In this competition, Expedia is challenging Kagglers to contextualize customer data and predict the likelihood a user will stay at 100 different hotel groups.
The data in this competition is a random selection from Expedia and is not representative of the overall statistics. ","Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):
$$MAP@5 = \frac{1}{|U|} \sum_{u=1}^{|U|} \sum_{k=1}^{min(5, n)} P(k)$$ 
where |U| is the number of user events, P(k) is the precision at cutoff k, n is the number of predicted hotel clusters. 
Submission File
For every user event, you must predict a space-delimited list of the hotel clusters they booked. You may submit up to 5 predictions for each user event. The file should contain a header and have the following format:
id,hotel_cluster
0,99 3 1 75 20
1,2 50 30 23 9
etc...","Expedia has provided you logs of customer behavior. These include what customers searched for, how they interacted with search results (click/book), whether or not the search result was a travel package. The data in this competition is a random selection from Expedia and is not representative of the overall statistics.
Expedia is interested in predicting which hotel group a user is going to book. Expedia has in-house algorithms to form hotel clusters, where similar hotels for a search (based on historical price, customer star ratings, geographical locations relative to city center, etc) are grouped together. These hotel clusters serve as good identifiers to which types of hotels people are going to book, while avoiding outliers such as new hotels that don't have historical data.
Your goal of this competition is to predict the booking outcome (hotel cluster) for a user event, based on their search and other attributes associated with that user event.
The train and test datasets are split based on time: training data from 2013 and 2014, while test data are from 2015. The public/private leaderboard data are split base on time as well. Training data includes all the users in the logs, including both click events and booking events. Test data only includes booking events. 
destinations.csv data consists of features extracted from hotel reviews text. 
Note that some srch_destination_id's in the train/test files don't exist in the destinations.csv file. This is because some hotels are new and don't have enough features in the latent space. Your algorithm should be able to handle this missing information.
File descriptions",kaggle competitions download -c expedia-hotel-recommendations,
373,"Kobe Bryant marked his retirement from the NBA by scoring 60 points in his final game as a Los Angeles Laker on Wednesday, April 12, 2016. Drafted into the NBA at the age of 17, Kobe earned the sport’s highest accolades throughout his long career.
Using 20 years of data on Kobe's swishes and misses, can you predict which shots will find the bottom of the net? This competition is well suited for practicing classification basics, feature engineering, and time series analysis. Practice got Kobe an eight-figure contract and 5 championship rings. What will it get you?
Acknowledgements
Kaggle is hosting this competition for the data science community to use for fun and education. For more data on Kobe and other NBA greats, visit stats.nba.com.","Submissions are evaluated on the log loss.
Submission File
For each missing shot_made_flag in the data set, you should predict a probability that Kobe made the field goal. The file should have a header and the following format:
shot_id,shot_made_flag
1,0.5
8,0.5
17,0.5
etc.",,,
374,,,,,
375,,,,,
376,,,,,
377,,,,,
378,,,,,
379,"Does your favorite Ethiopian restaurant take reservations? Will a first date at that authentic looking bistro break your wallet? Is the diner down the street a good call for breakfast? Restaurant labels help Yelp users quickly answer questions like these, narrowing down their results to only restaurants that fit their nuanced needs.
In this competition, Yelp is challenging Kagglers to build a model that automatically tags restaurants with multiple labels using a dataset of user-submitted photos. Currently, restaurant labels are manually selected by Yelp users when they submit a review. Selecting the labels is optional, leaving some restaurants un- or only partially-categorized. 
In an age of food selfies and photo-centric social storytelling, it may be no surprise to hear that Yelp's users upload an enormous amount of photos every day alongside their written reviews. Can you turn their pictures into (less than a thousand) words?
Yelp isn’t only looking for your best model; we’re looking for data mining engineers that can help us use our data in novel ways while pushing code to production. The prize for this competition is a fast track through the recruiting process and an opportunity to show our data mining teams just what you’ve got! For more information about exciting opportunities at Yelp, check out the Jobs at Yelp competition page and Yelp's own careers page.","The evaluation metric for this competition is Mean F1-Score also known as example-based F-measure in the multi-label learning literature. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F1 score is given by:
\[ F1 = 2\frac{p \cdot r}{p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn} \]
The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.
Submission File
For every business in the dataset, submission files should contain two columns: business_id and labels. Labels should be a space-delimited list. The file must have a header and should look like the following:
business_id,labels
003sg,1 2 3
00er5,0 7
00kad,1 5 8
etc.","At Yelp, there are lots of photos and lots of users uploading photos. These photos provide rich local business information across categories. Teaching a computer to understand the context of these photos is not an easy task. Yelp engineers work on deep learning image classification projects in-house, and you can read about them here. 
In this competition, you are given photos that belong to a business and asked to predict the business attributes. There are 9 different attributes in this problem:
0: good_for_lunch
1: good_for_dinner
2: takes_reservations
3: outdoor_seating
4: restaurant_is_expensive
5: has_alcohol
6: has_table_service
7: ambience_is_classy
8: good_for_kids",kaggle competitions download -c yelp-restaurant-photo-classification,"['https://www.kaggle.com/code/wendykan/expensive-restaurants-look-like-this', 'https://www.kaggle.com/code/valueq/util-for-data-exploration', 'https://www.kaggle.com/code/enerrio/data-exploration-yelp-classification', 'https://www.kaggle.com/code/wongjingping/naive-benchmark-0-61', 'https://www.kaggle.com/code/innerproduct/trying-to-process-images']"
380,"We all have a heart. Although we often take it for granted, it's our heart that gives us the moments in life to imagine, create, and discover. Yet cardiovascular disease threatens to take away these moments. Each day, 1,500 people in the U.S. alone are diagnosed with heart failure—but together, we can help. We can use data science to transform how we diagnose heart disease. By putting data science to work in the cardiology field, we can empower doctors to help more people live longer lives and spend more time with those that they love.
Declining cardiac function is a key indicator of heart disease. Doctors determine cardiac function by measuring end-systolic and end-diastolic volumes (i.e., the size of one chamber of the heart at the beginning and middle of each heartbeat), which are then used to derive the ejection fraction (EF). EF is the percentage of blood ejected from the left ventricle with each heartbeat. Both the volumes and the ejection fraction are predictive of heart disease. While a number of technologies can measure volumes or EF, Magnetic Resonance Imaging (MRI) is considered the gold standard test to accurately assess the heart's squeezing ability.
The challenge with using MRI to measure cardiac volumes and derive ejection fraction, however, is that the process is manual and slow. A skilled cardiologist must analyze MRI scans to determine EF. The process can take up to 20 minutes to complete—time the cardiologist could be spending with his or her patients. Making this measurement process more efficient will enhance doctors' ability to diagnose heart conditions early, and carries broad implications for advancing the science of heart disease treatment.
The 2015 Data Science Bowl challenges you to create an algorithm to automatically measure end-systolic and end-diastolic volumes in cardiac MRIs. You will examine MRI images from more than 1,000 patients. This data set was compiled by the National Institutes of Health and Children's National Medical Center and is an order of magnitude larger than any cardiac MRI data set released previously. With it comes the opportunity for the data science community to take action to transform how we diagnose heart disease.
This is not an easy task, but together we can push the limits of what's possible. We can give people the opportunity to spend more time with the ones they love, for longer than ever before.
Acknowledgments
The Data Science Bowl is presented by:
The National Heart, Lung, and Blood Institute (NHLBI) provided the MRI images for this competition. Special thanks to NHLBI Intramural Investigators Dr. Michael Hansen and Dr. Andrew Arai.
Additional support for the Data Science Bowl was provided by NVIDIA:","Submissions will be evaluated on the Continuous Ranked Probability Score (CRPS). For each MRI, you must predict a cumulative probability distribution for both the systolic and diastolic volumes (two separate distributions per case). The CRPS is computed as follows:
\[ C = \frac{1}{600N} \sum_{m=1}^{N} \sum_{n=0}^{599} (P(y \le n) -H(n - V_m))^2, \] 
where P is the predicted distribution, N is the number of rows in the test set (equal to twice the number of cases), V is the actual volume (in mL) and H(x) is the Heaviside step function (\\(H(x) = 1\\) for \\(x \ge 0\\) and zero otherwise). While it is not simple to visualize the CRPS, the shaded area on the figure below may be a helpful guide for understanding the error term between the predicted distribution and actual volume:
The entry will not score if any of the predicted values has \[P(y \le k) > P(y \le k+1)\] for any k (i.e., the CDF must be non-decreasing).
Submission File
For each Id, you must predict 600 values that represent its cumulative distribution from 0 to 599 mL. P0 represents the probability the volume is less than or equal to 0 mL, P1 represents the probability the volume is less than or equal to 1 mL, etc. The file must have a header and contain all 600 values in the following format:
Id,P0,P1,P2,P3,...,P599
1_systolic,0.1,0.3,0.33,0.4,...,1.0
1_diastolic,0.1,0.24,0.25,0.35,...,1.0
...
etc","In this dataset, you are given hundreds of cardiac MRI images in DICOM format. These are 2D cine images that contain approximately 30 images across the cardiac cycle. Each slice is acquired on a separate breath hold. This is important since the registration from slice to slice is expected to be imperfect.
The competition task is to create an automated method capable of determining the left ventricle volume at two points in time: after systole, when the heart is contracted and the ventricles are at their minimum volume, and after diastole, when the heart is at its largest volume.
The volumes at systole, \\(V_S\\), and diastole, \\(V_D\\), form the basis of an important clinical measurement known as the ejection fraction:
$$ 100 * \frac{V_D - V_S}{V_D}. $$
This quantity represents the fraction of outbound blood pumped from the heart with each heartbeat. An ejection fraction that is too low can signify a wide range of cardiac problems.
Variations in anatomy, function, image quality, and acquisition make automated quantification of left ventricle size a challenging problem. You will encounter this variation in the competition dataset, which aims to provide a diverse representation of cases. It contains patients from young to old, images from numerous hospitals, and hearts from normal to abnormal cardiac function. A computational method which is robust to these variations could both validate and automate the cardiologists' manual measurement of ejection fraction.
This is a two-stage competition. In the first stage, you are building models based on the training dataset, and testing your models by submitting predictions on the validation set. Two weeks before the final deadline, you will submit your model to Kaggle. At this point, the second stage of the competition starts. Kaggle will release the final test dataset, on which you will run your models. The final standings are based on this final test set.",kaggle competitions download -c second-annual-data-science-bowl,"['https://www.kaggle.com/code/deepak207/kare-final-year-project', 'https://www.kaggle.com/code/mucharlarvt/mri-heart-processing', 'https://www.kaggle.com/code/mehdinait/pfa-v2-0', 'https://www.kaggle.com/code/tnilsson/qbi-preprocessing', 'https://www.kaggle.com/code/jkronander/bigimg-images']"
381,"This is a Masters competition. You must be a Kaggle Master to participate.
Cervical cancer is the third most common cancer in women worldwide, affecting over 500,000 women and resulting in approximately 275,000 deaths every year. After reading these statistics, you may be surprised to he
ar that cervical cancer is potentially preventable and curable.
Cervical cancer can be prevented through early administration of the HPV vaccine and regular pap smear screenings, which indicate the presence of precancerous cells. It is also sometimes curable by the removal of the early-stage cancerous tissue that is identified through pap smears. Screening and early treatment can lead to potential cures in about 95% of women at risk for cervical cancer.
Most women in the US have access to cervical cancer screening, yet 4,000 women die every year from cervical cancer in the US and it is estimated that 30% of US women do not receive regular pap screenings. We know little about who these women are and why they are not getting screened. Prior research suggests that lower screening rates are associated with low income, low education, lack of interaction with the healthcare system, and lack of health insurance. But research also shows that even in women with access to healthcare fail to get this preventive test, indicating that barriers like lack of education and not being comfortable with the procedure are influencing their behavior (Patient Survey). 
There are many patient advocacy programs on the importance of pap smears in cervical cancer prevention. However, these widespread programs may not be reaching or effectively speaking to the most vulnerable populations. If one could better identify these women, education campaigns could target them with content that speaks directly to their unique risk factors. Identifying predictors of not receiving pap smears will provide important information to stakeholders in cervical cancer prevention who run awareness programs.
With this Masters competition, Genentech is asking you to join their mission to help prevent cervical cancer. Given a dataset of de-identified health records, your challenge is to predict which women will not be screened for cervical cancer on the recommended schedule. Identifying at-risk populations will make education and other intervention efforts more effective, ideally ultimately reducing the number of women who die from this disease.
About Genentech
Founded more than 35 years ago, Genentech is a leading biotechnology company that discovers, develops, manufactures and commercializes medicines to treat patients with serious or life-threatening medical conditions. The company, a member of the Roche Group, has headquarters in South San Francisco, California. For additional information about the company, please visit http://www.gene.com.
Acknowledgements
The dataset for this competition is provided by Symphony Health Solutions. 
@Jotform.Show(35)","Submissions are judged on area under the ROC curve.
Submission File
Each line of your submission should contain a patient_id and a predict_screener prediction. The prediction column should have values between 0 and 1, representing the probability of the patient being a cervical cancer screener. 
Your submission file must have a header row and should have the following format:
patient_id,predict_screener
1,1
2,0.2
3,0.6
4,0.9
etc.",,,"['https://www.kaggle.com/code/paweljankiewicz/pawel-maks-insights', 'https://www.kaggle.com/code/abhishek/hmmmmm', 'https://www.kaggle.com/code/hurlburt/plottest5', 'https://www.kaggle.com/code/hurlburt/plot-test3', 'https://www.kaggle.com/code/hurlburt/diagnosiscodesetcv2']"
382,"Fork this script and get started on the problem
The North Pole is in an uproar over news that Santa's magic sleigh has been stolen. Able to carry all the world's presents in one trip, it was considered crucial to successfully delivering holiday goodies across the globe in one night.
Unwilling to cancel Christmas, Santa is determined to deliver toys to all the good girls and boys using his day-to-day, magic-less sleigh. With so little time to pull off this plan, Santa is once again counting on Kagglers to help.
Given the sleigh's antiquated, weight-limited specifications, your challenge is to optimize the routes and loads Santa will take to and from the North Pole. And don't forget about Dasher, Dancer, Prancer, and Vixen; Santa is adamant that the best solutions will minimize the toll of this hectic night on his reindeer friends.
Acknowledgements
This competition is brought to you by FICO.","Your goal is to minimize total weighted reindeer weariness:
Weighted Reindeer Weariness = (distance traveled) * (weights carried for that segment)
All sleighs start from north pole (Lat=90, Long=0), then head to each gift in the order that a user gives, and then head back to north pole
Sleighs have a base weight = 10
Each sleigh has a weight limit = 1000 (excluding the sleigh base weight)
Mathematically, weighted reindeer weariness is calculated by:
$$WRW = \sum\limits_{j=1}^{m} \sum\limits_{i=1}^{n} \Big[ \big( \sum\limits_{k=1}^{n} w_{kj} - \sum\limits_{k=1}^{i} w_{kj} \big) \cdot Dist(Loc_i, Loc_{i-1}) \Big]_j ,$$
where m is the number of trips, n is the number of gifts for each trip \\(j\\), \\(w_{ij}\\) is the weight of the \\(i^{th}\\) gift at trip \\(j\\), \\(Dist()\\) is calculated with Haversine Distance between two locations, and \\(Loc_i\\) is the location of gift \\(i\\). \\(Loc_0\\) and \\(Loc_n\\) are North Pole, and \\(w_{nj}\\), a.k.a. the last leg of each trip, is always the base weight of the sleigh.
For example, if you have 2 gifts A, B to deliver in the trip, then the WRW is calculated as:
dist( North pole to A ) * ( weight A + weight B + base_weight ) +
dist( A to B ) * ( weight B + base_weight ) +
dist( B to North pole ) * ( base_weight )
Submission File
Submission files should contain two columns: GiftId and TripId. GiftId should be ordered by the order of delivery, and different trips should have different TripIds. 
The file should contain a header and have the following format:
GiftId,TripId
1,1
5,1
3,2
4,2
2,1
etc.","See on user's exploration of the data on Kaggle Scripts
For this competition, you are asked to optimize the total weighted distance traveled (weighted reindeer weariness). You are given a list of gifts with their destinations and their weights. You will plan sleigh trips to deliver all the gifts to their destinations while optimizing the routes. 
All sleighs start from north pole, then head to each gift in the order that you assign, and then head back to north pole. 
You may have an unlimited number of sleigh trips.
All the gifts must be traveling with the sleigh at all times until the sleigh delivers it to the destination. A gift may not be dropped off anywhere before it's delivered. 
Sleighs have a base weight of 10, and a maximum weight capacity of 1000 (excluding the sleigh). 
All trips are flying trips, which means you don't need to travel via land. Haversine is used in calculating distance.  ",kaggle competitions download -c santas-stolen-sleigh,"['https://www.kaggle.com/code/nigelcarpenter/santa-exploration', 'https://www.kaggle.com/code/wendykan/computing-weighted-reindeer-weariness', 'https://www.kaggle.com/code/gaborfodor/you-ll-shoot-your-eye-out-kid', 'https://www.kaggle.com/code/davidshinn/where-are-all-the-gifts', 'https://www.kaggle.com/code/thakurrajanand/beat-the-benchmark-14358424217']"
383,"Instead of waking to overlooked ""Do not disturb"" signs, Airbnb travelers find themselves rising with the birds in a whimsical treehouse, having their morning coffee on the deck of a houseboat, or cooking a shared regional breakfast with their hosts.
New users on Airbnb can book a place to stay in 34,000+ cities across 190+ countries. By accurately predicting where a new user will book their first travel experience, Airbnb can share more personalized content with their community, decrease the average time to first booking, and better forecast demand.
In this recruiting competition, Airbnb challenges you to predict in which country a new user will make his or her first booking. Kagglers who impress with their answer (and an explanation of how they got there) will be considered for an interview for the opportunity to join Airbnb's Data Science and Analytics team.
Wondering if you're a good fit? Check out this article on how Airbnb scaled data science to all sides of their organization, and visit their careers page for more on Airbnb's mission to create a world that inspires human connection.","The evaluation metric for this competition is NDCG (Normalized discounted cumulative gain) @k where k=5. NDCG is calculated as:
\[ DCG_k=\sum_{i=1}^k\frac{2^{rel_i}-1}{\log_2{\left(i+1\right)}}, \]
\[ nDCG_k=\frac{DCG_k}{IDCG_k}, \]
where \\(rel_i\\) is the relevance of the result at position \\(i\\).
\\(IDCG_k\\) is the maximum possible (ideal) \\(DCG\\) for a given set of queries. All NDCG calculations are relative values on the interval 0.0 to 1.0.
For each new user, you are to make a maximum of 5 predictions on the country of the first booking. The ground truth country is marked with relevance = 1, while the rest have relevance = 0.
For example, if for a particular user the destination is FR, then the predictions become:
[ FR ]  gives a \\(NDCG=\frac{2^{1}-1}{log_{2}(1+1)}=1.0\\)
[ US, FR ] gives a \\(DCG=\frac{2^{0}-1}{log_{2}(1+1)}+\frac{2^{1}-1}{log_{2}(2+1)}=\frac{1}{1.58496}=0.6309\\) 
Submission File
For every user in the dataset, submission files should contain two columns: id and country. The destination country predictions must be ordered such that the most probable destination country goes first.
The file should contain a header and have the following format:
id,country
000am9932b,NDF
000am9932b,US
000am9932b,IT
01wi37r0hw,FR
etc.","In this challenge, you are given a list of users along with their demographics, web session records, and some summary statistics. You are asked to predict which country a new user's first booking destination will be. All the users in this dataset are from the USA.
There are 12 possible outcomes of the destination country: 'US', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU', 'NDF' (no destination found), and 'other'. Please note that 'NDF' is different from 'other' because 'other' means there was a booking, but is to a country not included in the list, while 'NDF' means there wasn't a booking.
The training and test sets are split by dates. In the test set, you will predict all the new users with first activities after 7/1/2014 (note: this is updated on 12/5/15 when the competition restarted). In the sessions dataset, the data only dates back to 1/1/2014, while the users dataset dates back to 2010. 
File descriptions
train_users.csv - the training set of users
test_users.csv - the test set of users
id: user id
date_account_created: the date of account creation
timestamp_first_active: timestamp of the first activity, note that it can be earlier than date_account_created or date_first_booking because a user can search before signing up",kaggle competitions download -c airbnb-recruiting-new-user-bookings,"['https://www.kaggle.com/code/ashrakatsaeed/air-bnb-case-study', 'https://www.kaggle.com/code/tunahantas/t-airbnb-new-user-bookings', 'https://www.kaggle.com/code/hopperchen/hopper-airbnb', 'https://www.kaggle.com/code/saityazici/airbnb-new-user-project-script', 'https://www.kaggle.com/code/yinshe/extreme-hyperparameter-tuning']"
384,"In their first recruiting competition, Telstra is
 challenging Kagglers to predict the severity of service disruptions on their network. Using a dataset of features from their service logs, you're tasked with predicting if a disruption is a momentary glitch or a total interruption of connectivity.
Telstra is on a journey to enhance the customer experience - ensuring everyone in the company is putting customers first. In terms of its expansive network, this means continuously advancing how it predicts the scope and timing of service disruptions. Telstra wants to see how you would help it drive customer advocacy by developing a more advanced predictive model for service disruptions and to help it better serve its customers.
This challenge was crafted as a simulation of the type of problem you might tackle as a member of the team at Telstra.
Kagglers who stand out will be considered for data science roles in Telstra's Big Data team in Telstra’s absolute discretion. Highly-ranked participants will combine technical expertise and intuition in data science problems with a keen business sense and an effortless ability to work with technical and non-technical staff to turn data into real changes that impact customers. Highly-ranked participants will be considered by Telstra for interviews for employment, based on their work in the Competition and ability to meet the selection criteria for any suitable open job vacancy in Melbourne and Sydney, Australia. Participation in this Competition is not a recruitment process and Kaggle does not provide Telstra with recruitment services.","Submissions are evaluated using the multi-class logarithmic loss. Each data row has been labeled with one true class. For each row, you must submit a set of predicted probabilities (one for every fault severity). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of rows in the test set, M is the number of fault severity classes,  \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given row are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission File
You must submit a csv file with the row id, all 3 classes (0,1,2), and a probability for each fault severity. The order of the rows does not matter. The file must have a header and should look like the following:
id,predict_0,predict_1,predict_2
11066,0,1,0
18000,0,1,0
etc.","The goal of the problem is to predict Telstra network's fault severity at a time at a particular location based on the log data available. Each row in the main dataset (train.csv, test.csv) represents a location and a time point. They are identified by the ""id"" column, which is the key ""id"" used in other data files. 
Fault severity has 3 categories: 0,1,2 (0 meaning no fault, 1 meaning only a few, and 2 meaning many). 
Different types of features are extracted from log files and other sources: event_type.csv, log_feature.csv, resource_type.csv, severity_type.csv. 
Note: “severity_type” is a feature extracted from the log files (in severity_type.csv). Often this is a severity type of a warning message coming from the log. ""severity_type"" is categorical. It does not have an ordering. “fault_severity” is a measurement of actual reported faults from users of the network and is the target variable (in train.csv).
File descriptions
train.csv - the training set for fault severity
test.csv - the test set for fault severity
sample_submission.csv - a sample submission file in the correct format
event_type.csv - event type related to the main dataset
log_feature.csv - features extracted from log files",kaggle competitions download -c telstra-recruiting-network,"['https://www.kaggle.com/code/acauveri/network-severity-prediction-automatic-model', 'https://www.kaggle.com/code/divyanshtrivedi/telstra-network-disruptions', 'https://www.kaggle.com/code/akseersafdar/telstra-nw-disruptions-using-catboost', 'https://www.kaggle.com/code/acauveri/prediction-model-selection', 'https://www.kaggle.com/code/acauveri/network-fault-prediction']"
385,"Picture this. You are a data scientist in a start-up culture with the potential to have a very large impact on the business. Oh, and you are backed up by a company with 140 years' business experience.
Curious? Great! You are the kind of person we are looking for.
Prudential, one of the largest issuers of life insurance in the USA, is hiring passionate data scientists to join a newly-formed Data Science group solving complex challenges and identifying opportunities. The results have been impressive so far but we want more. 
The Challenge
In a one-click shopping world with on-demand
everything, the life insurance application process is antiquated. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams, a process that takes an average of 30 days.
The result? People are turned off. That’s why only 40% of U.S. households own individual life insurance. Prudential wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries.
By developing a predictive model that accurately classifies risk using a more automated approach, you can greatly impact public perception of the industry.
The results will help Prudential better understand the predictive power of the data points in the existing assessment, enabling us to significantly streamline the process.","Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement between the raters than expected by chance, this metric may go below 0.
The response variable has 8 possible ratings.  Each application is characterized by a tuple (ea,eb), which corresponds to its scores by Rater A (actual risk) and Rater B (predicted risk).  The quadratic weighted kappa is calculated as follows.
First, an N x N histogram matrix O is constructed, such that Oi,j corresponds to the number of applications that received a rating i by A and a rating j by B. An N-by-N matrix of weights, w, is calculated based on the difference between raters' scores:
$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$
An N-by-N histogram matrix of expected ratings, E, is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that E and O have the same sum.
From these three matrices, the quadratic weighted kappa is calculated as: 
$$\kappa=1-\frac{\sum_{i,j}w_{i,j}O_{i,j}}{\sum_{i,j}w_{i,j}E_{i,j}}.$$
Submission File
For each Id in the test set, you must predict the Response variable. The file should contain a header and have the following format:
Id,Response
1,4
3,8
4,3
etc.","In this dataset, you are provided over a hundred variables describing attributes of life insurance applicants. The task is to predict the ""Response"" variable for each Id in the test set. ""Response"" is an ordinal measure of risk that has 8 levels.
File descriptions
train.csv - the training set, contains the Response values
test.csv - the test set, you must predict the Response variable for all rows in this file
sample_submission.csv - a sample submission file in the correct format
Data fields
Variable Description
Id A unique identifier associated with an application.
Product_Info_1-7 A set of normalized variables relating to the product applied for
Ins_Age Normalized age of applicant",kaggle competitions download -c prudential-life-insurance-assessment,"['https://www.kaggle.com/code/fahadmehfoooz/classification-with-model-interpretation', 'https://www.kaggle.com/code/casalicchio/use-the-mlr-package-scores-0-649', 'https://www.kaggle.com/code/zeroblue/xgboost-with-optimized-offsets', 'https://www.kaggle.com/code/wittmaan/exploring-the-data', 'https://www.kaggle.com/code/pruadmin/starter-script']"
386,"Before asking someone on a date or skydiving, it's
important to know your likelihood of success. The same goes for quoting home insurance prices to a potential customer. Homesite, a leading provider of homeowners insurance, does not currently have a dynamic conversion rate model that can give them confidence a quoted price will lead to a purchase. 
Using an anonymized database of information on customer and sales activity, including property and coverage information, Homesite is challenging you to predict which customers will purchase a given quote. Accurately predicting conversion would help Homesite better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments. ","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
Submission File
For each QuoteNumber in the test set, you must predict a probability for QuoteConversion_Flag. The file should contain a header and have the following format:
QuoteNumber,QuoteConversion_Flag
3,0
5,0.3
7,0
etc.","This dataset represents the activity of a large number of customers who are interested in buying policies from Homesite. Each QuoteNumber corresponds to a potential customer and the QuoteConversion_Flag indicates whether the customer purchased a policy.
The provided features are anonymized and provide a rich representation of the prospective customer and policy. They include specific coverage information, sales information, personal information, property information, and geographic information. Your task is to predict QuoteConversion_Flag for each QuoteNumber in the test set.
File descriptions
train.csv - the training set, contains QuoteConversion_Flag
test.csv - the test set, does not contain QuoteConversion_Flag
sample_submission.csv - a sample submission file in the correct format",kaggle competitions download -c homesite-quote-conversion,"['https://www.kaggle.com/code/phunter/xgboost-with-gridsearchcv', 'https://www.kaggle.com/code/skylord/digging-deeper-and-deeper', 'https://www.kaggle.com/code/mpearmain/xgboost-benchmark', 'https://www.kaggle.com/code/yangnanhai/keras-around-0-9633', 'https://www.kaggle.com/code/brandenkmurray/beat-the-benchmark-in-7-lines']"
387,"Walmart uses both art and science to continually make progress on their core mission of better understanding and serving their customers. One way Walmart is able to improve customers' shopping experiences is by segmenting their store visits into different trip types. 
Whether they're on a last minute run for new puppy supplies or leisurely making their way through a weekly grocery list, classifying trip types enables Walmart to create the best shopping experience for every customer.
Currently, Walmart's trip types are created from a combination of existing customer insights (""art"") and purchase history data (""science""). In their third recruiting competition, Walmart is challenging Kagglers to focus on the (data) science and classify customer trips using only a transactional dataset of the items they've purchased. Improving the science behind trip type classification will help Walmart refine their segmentation process.
Walmart is hosting this competition to connect with data scientists who break the mold.","Submissions are evaluated using the multi-class logarithmic loss. For each visit, you must submit a set of predicted probabilities (one for every TripType). The formula is:
$$-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of visits in the test set, M is the number of trip types, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is of class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given visit are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission Format
You must submit a csv file with the VisitNumber, all candidate TripTypes, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:
""VisitNumber"",""TripType_3"",""TripType_4"",...
1,0,0.1,...
2,1,0,...
etc.","For this competition, you are tasked with categorizing shopping trip types based on the items that customers purchased. To give a few hypothetical examples of trip types: a customer may make a small daily dinner trip, a weekly large grocery trip, a trip to buy gifts for an upcoming holiday, or a seasonal trip to buy clothes.
Walmart has categorized the trips contained in this data into 38 distinct types using a proprietary method applied to an extended set of data. You are challenged to recreate this categorization/clustering with a more limited set of features. This could provide new and more robust ways to categorize trips.
The training set (train.csv) contains a large number of customer visits with the TripType included. You must predict the TripType for each customer visit in the test set (test.csv). Each visit may only have one TripType. You will not be provided with more information than what is given in the data (e.g. what the TripTypes represent or more product information).
The test set file is encrypted. You must complete this brief survey to receive the password.
Data fields
TripType - a categorical id representing the type of shopping trip the customer made. This is the ground truth that you are predicting. TripType_999 is an ""other"" category.
VisitNumber - an id corresponding to a single trip by a single customer
Weekday - the weekday of the trip
Upc - the UPC number of the product purchased",kaggle competitions download -c walmart-recruiting-trip-type-classification,"['https://www.kaggle.com/code/aniketvishnu/walmart-trip-type-classification-complete-model', 'https://www.kaggle.com/code/naksungp/quick-eda-plus-model', 'https://www.kaggle.com/code/thitchen/walmart', 'https://www.kaggle.com/code/bonesliu10318/visualization-walmart-trip-type-classification', 'https://www.kaggle.com/code/ortegayubro/walmart-clasificaci-n-tipo-de-viaje']"
388,"Do you laugh (and then get down to work) in the face of terabytes of noisy, non-stationary data? Winton Capital is looking for data scientists who excel at finding the hidden signal in the proverbial haystack, and who are excited by creating novel statistical modelling and data mining techniques. 
In this recruiting competition, Winton challenges you to take on the very difficult task of predicting the future (stock returns). Given historical stock performance and a host of masked features, can you predict intra and end of day returns without being deceived by all the noise? 
Research scientists at Winton have crafted this competition to be challenging and fun for the community while providing a taste of the types of problems they work on everyday. They're excited to connect with Kagglers who bring a unique background and creative approach to the competition.
Winton is offering cash prizes to winning teams as a reward for their work, but the intent of the competition is not commercial. The intellectual property you create remains your own and will be evaluated in the context of suitability for employment. 
For more on the culture at Winton, check out the About Winton page or their careers page.","Submissions are evaluated using the Weighted Mean Absolute Error. Each return you predicted is compared with the actual return. The formula is then
$$WMAE = \frac{1}{n}\sum\limits_{i=1}^{n} w_i \cdot \left|y_i - \hat{y_i}\right| ,$$
where \\(w_i\\) is the weight associated with the return, Weight_Intraday, Weight_Daily for intraday and daily returns, \\(i\\), \\(y_i\\) is the predicted return, \\(\hat{y_i}\\) is the actual return, \\(n\\) is the number of predictions. 
The weights for the training set are given in the training data. The weights for the test set are unknown.
Submission File
The submission file should contain two columns: Id and Predicted. For each 5-day window, you need to predict 62 returns. For example, for the first time window, you will predict 1_1, 1_2, to 1_62. 1_1 to 1_60 are predicting Ret_121 through Ret_180, 1_61 the prediction for Ret_PlusOne, and 1_62 the prediction for Ret_PlusTwo.
The file should contain a header and have the following format:
Id,Predicted
1_1,0
1_2,0
1_3,0
1_4,0
...
1_60,0
1_61,0
1_62,0
2_1,0
2_2,0
etc.","Updated 2015-12-21: Winton have added new data into the test set. If you downloaded the test set before 2015-12-21 please re-download the data set and submit predictions on this instead. 
In this competition the challenge is to predict the return of a stock, given the history of the past few days. 
We provide 5-day windows of time, days D-2, D-1, D, D+1, and D+2. You are given returns in days D-2, D-1, and part of day D, and you are asked to predict the returns in the rest of day D, and in days D+1 and D+2.
During day D, there is intraday return data, which are the returns at different points in the day. We provide 180 minutes of data, from t=1 to t=180. In the training set you are given the full 180 minutes, in the test set just the first 120 minutes are provided.
For each 5-day window, we also provide 25 features, Feature_1 to Feature_25. These may or may not be useful in your prediction.
Each row in the dataset is an arbitrary stock at an arbitrary 5 day time window.",kaggle competitions download -c the-winton-stock-market-challenge,"['https://www.kaggle.com/code/neomatrix369/everything-you-can-do-with-a-time-series-stocks', 'https://www.kaggle.com/code/neomatrix369/everything-you-can-do-with-a-time-series-cryptos', 'https://www.kaggle.com/code/neomatrix369/everything-you-can-do-with-a-time-series-metals', 'https://www.kaggle.com/code/zonghao/predicting-stock-returns-by-xgboost', 'https://www.kaggle.com/code/prompath/eda-and-initial-modeling-for-stock-returns']"
389,"Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.
In their first Kaggle competition, Rossmann is challenging you to predict 6 weeks of daily sales for 1,115 stores located across Germany. Reliable sales forecasts enable store managers to create effective staff schedules that increase productivity and motivation. By helping Rossmann create a robust prediction model, you will help store managers stay focused on what’s most important to them: their customers and their teams! ","Submissions are evaluated on the Root Mean Square Percentage Error (RMSPE). The RMSPE is calculated as
$$
\textrm{RMSPE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left(\frac{y_i - \hat{y}_i}{y_i}\right)^2},
$$
where y_i denotes the sales of a single store on a single day and yhat_i denotes the corresponding prediction. Any day and store with 0 sales is ignored in scoring.
Submission File
The file should contain a header and have the following format:
Id,Sales
1,0
2,0
3,0
etc.","You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the ""Sales"" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment.
Files
train.csv - historical data including Sales
test.csv - historical data excluding Sales
sample_submission.csv - a sample submission file in the correct format
store.csv - supplemental information about the stores
Data fields
Most of the fields are self-explanatory. The following are descriptions for those that aren't.
Id - an Id that represents a (Store, Date) duple within the test set
Store - a unique Id for each store
Sales - the turnover for any given day (this is what you are predicting)
Customers - the number of customers on a given day
Open - an indicator for whether the store was open: 0 = closed, 1 = open",kaggle competitions download -c rossmann-store-sales,"['https://www.kaggle.com/code/elenapetrova/time-series-analysis-and-forecasts-with-prophet', 'https://www.kaggle.com/code/thie1e/exploratory-analysis-rossmann', 'https://www.kaggle.com/code/shearerp/interactive-sales-visualization', 'https://www.kaggle.com/code/bhatnagardaksh/gradient-descent-from-scratch', 'https://www.kaggle.com/code/omarelgabry/a-journey-through-rossmann-stores']"
390,"After incorporating feedback from the Kaggle community, as well as scientific and educational partners, the Artificial Intelligence Committee of the American Meteorological Society is excited to be running a second iteration of the How Much Did It Rain? competition.
How Much Did It Rain? II is focused on solving the same core rain measurement prediction problem, but approaches it with a new and improved dataset and evaluation metric. This competition will go even further towards building a useful educational tool for universities, as well as making a meaningful contribution to continued meteorological research.
Competition Description
Rainfall is highly variable across space and time, making it notoriously tricky to measure. Rain gauges can be an effective measurement tool for a specific location, but it is impossible to have them everywhere. In order to have widespread coverage, data from weather radars is used to estimate rainfall nationwide. Unfortunately, these predictions never exactly match the measurements taken using rain gauges.
Recently, in an effort to improve their rainfall predictors, the U.S. National Weather Service upgraded their radar network to be polarimetric. These polarimetric radars are able to provide higher quality data than conventional Doppler radars because they transmit radio wave pulses with both horizontal and vertical orientations. 
Dual pulses make it easier to infer the size and type of precipitation because rain drops become flatter as they increase in size, whereas ice crystals tend to be elongated vertically.
In this competition, you are given snapshots of polarimetric radar values and asked to predict the hourly rain gauge total. A word of caution: many of the gauge values in the training dataset are implausible (gauges may get clogged, for example). More details are on the data page.
Acknowledgements
This competition is sponsored by the Artificial Intelligence Committee of the American Meteorological Society. Climate Corporation is providing the prize pool.","The evaluation metric for this competition is Mean Absolute Error (MAE). Please refer to this paper about the choice of MAE over RMSE.  
Submission File
For every gauge-hour in the dataset, submission files should contain two columns: Id and Expected.  The Id corresponds to the column of that name in the test.csv. The Expected is the predicted gauge value. The file should contain a header and have the following format:
Id,Expected
1,0.73
2,1.23
3,0.49
etc.","The training data consists of NEXRAD and MADIS data collected on 20 days between Apr and Aug 2014 over midwestern corn-growing states. Time and location information have been censored, and the data have been shuffled so that they are not ordered by time or place. The test data consists of data from the same radars and gauges over the remaining days in that month. Please see this page to understand more about polarimetric radar measurements.
File descriptions
train.zip - the training set.  This consists of radar observations at gauges in the Midwestern US over 20 days each month during the corn growing season. You are also provided the gauge observation at the end of each hour.
test.zip - the test set.  This consists of radar observations at gauges in the Midwestern US over the remaining 10/11 days each month of the same year(s) as the training set.  You are required to predict the gauge observation at the end of each hour.
sample_solution.zip - a sample submission file in the correct format
sample_dask.py - Example program in Python that will produce the sample submission file.  This program applies the Marshall-Palmer relationship to the radar observations to predict the gauge observation.
Data columns
To understand the data, you have to realize that there are multiple radar observations over the course of an hour, and only one gauge observation (the 'Expected'). That is why there are multiple rows with the same 'Id'.",kaggle competitions download -c how-much-did-it-rain-ii,"['https://www.kaggle.com/code/sudalairajkumar/rainfall-test', 'https://www.kaggle.com/code/ilya16/lstm-models', 'https://www.kaggle.com/code/tunguz/gbm-inches-only', 'https://www.kaggle.com/code/captcalculator/marshall-palmer-in-r', 'https://www.kaggle.com/code/andkul/deep-lstm-to-predict-rainfall']"
391,"Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight?
If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see.
Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients. 
Acknowledgements
We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.","Submissions are evaluated on the categorization accuracy (the percent of dishes that you correctly classify).
Submission File
Your submission file should predict the cuisine for each recipe in the test set. The file should contain a header and have the following format:
id,cuisine
35203,italian
17600,italian
35200,italian
17602,italian
...
etc.","In the dataset, we include the recipe id, the type of cuisine, and the list of ingredients of each recipe (of variable length). The data is stored in JSON format. 
An example of a recipe node in train.json:
 {
 ""id"": 24717,
 ""cuisine"": ""indian"",
 ""ingredients"": [
     ""tumeric"",
     ""vegetable stock"",
     ""tomatoes"",
     ""garam masala"",
     ""naan"",
     ""red lentils"",
     ""red chili peppers"",
     ""onions"",
     ""spinach"",
     ""sweet potatoes""
 ]
 },",kaggle competitions download -c whats-cooking,"['https://www.kaggle.com/code/rajmehra03/a-detailed-explanation-of-keras-embedding-layer', 'https://www.kaggle.com/code/alonalevy/cultural-diffusion-by-recipes', 'https://www.kaggle.com/code/ccorbi/word2vec-with-ingredients', 'https://www.kaggle.com/code/dipayan/whatscooking-python', 'https://www.kaggle.com/code/amhchiu/bag-of-ingredients-in-r']"
392,"The Allen Institute for Artificial Intelligence (AI2) is working to improve humanity through fundamental advances in artificial intelligence. One critical but challenging problem in AI is to demonstrate the ability to consistently understand and correctly answer general questions about the world. 
The Aristo project at AI2 is focused on building such a system. One way Aristo ""learns"" is by extracting facts from various sources and processing them into a structured knowledge base. When taking an exam, questions are parsed and processed along with any accompanying diagrams to determine a strategy for answering. Aristo then uses entailment, statistical analysis, and inference methods to select a final answer.
While Aristo's abilities have improved significantly in the last two years, it still doesn't have perfect, reliable methods of gathering knowledge, understanding questions, or reasoning through answers.
Using a dataset of multiple choice question and answers from a standardized 8th grade science exam, AI2 is challenging you to create a model that gets to the head of the class.","Submissions are evaluated by categorization accuracy, i.e., the fraction of multiple choice questions answered correctly. Random guessing should produce an evaluation score of around 0.25.
Submission file
For every question in the dataset, submission files should contain two columns: id and correctAnswer, where the correctAnswer is one of A, B, C or D just like in the training set and the id is the question id from the dataset.
The file should contain a header and have the following format:
id,correctAnswer
102501,A
102502,B
102503,B
102504,D
etc.
Model submission and final test set
During the last two weeks of the model training period, you will be able to upload your models to Kaggle. This model submission must contain all data, code, and parameter settings necessary to evaluate your models on new questions, and include a README file with instructions on how to do so. The purpose of this is to ensure a fair competition and that no manual answering of the test set questions has been done. You must also make at least one submission on the validation set with this model before the model submission deadline.
If you would like, you may submit your model as an encrypted archive, and you will only be asked to provide the decryption key if you are one of the preliminary winners. The model submission is required to be eligible to win prize money.
When the final test set is released, the same model should be used to submit answers to the test set, and the prize winning models will be verified manually as to fulfilling these requirements. To avoid random discrepancies, make sure to seed any random number generators used in the model.","The training data consists of 2,500 multiple choice questions from a typical US 8th grade science curriculum. Each question has four possible answers, of which exactly one is correct.
Note that the questions in these datasets are private intellectual property, and by acknowledging the competition rules, you agree to not sharing or publishing any questions to parties other than yourself at any point in time, both during and after the competition.
This is a two-stage competition. In the first stage, you are building models based on the training dataset, and testing your models by submitting predictions on the validation set. One week before the final deadline, you will submit your model to Kaggle. At this point, the second stage of the competition starts. Kaggle will release the final test dataset, on which you will run your predictive models. The final scores will be calculated based on this final test set.
The validation set contains 8,132 questions of the same type without providing the correct answer. This set should only be used to submit automatically generated answers and should not be used for training purposes. To discourage inappropriate use, only a small proportion of these questions are real competition questions that will count for scoring. All the valid questions are used for public leaderboard, and none for private leaderboard.
The final test set, to be released at stage 2 of the competition, will contain 21,298 questions of the same type (including the 8,132 from the validation set). Again, only a small proportion will be used in the scoring. All the validation set questions will be used for the public leaderboard, and all the new test set questions used for private leaderboard.
EDIT: Train/Validation/Test datasets are removed as of 2/13/2016 at the end of the competition ",kaggle competitions download -c the-allen-ai-science-challenge,['https://www.kaggle.com/code/adrian1acoran/1-learn-pi-3-1415926535897932-b144dd']
393,"With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction.
Currently, only a handful of very experienced researchers can identify individual whales on sight while out on the water. For the majority of researchers, identifying individual whales takes time, making it difficult to effectively target whales for biological samples, acoustic recordings, and necessary health assessments.
To track and monitor the population, right whales are photographed during aerial surveys and then manually matched to an online photo-identification catalog. Customized software has been developed to aid in this process (DIGITS), but this still relies on a manual inspection of the potential comparisons, and there is a lag time for those images to be incorporated into the database. The current identification process is extremely time consuming and requires special training. This constrains marine biologists, who work under tight deadlines with limited budgets.
This competition challenges you to automate the right whale recognition process using a dataset of aerial photographs of individual whales. Automating the identification of right whales would allow researchers to better focus on their conservation efforts. Recognizing a whale in real-time would also give researchers on the water access to potentially life-saving historical health and entanglement records as they struggle to free a whale that has been accidentally caught up in fishing gear.
Acknowledgements
MathWorks is sponsoring the competition prize pool. If your team is participating in this competition MathWorks is also providing complimentary software. Click here for more details on how to request your copy.
Thanks to Christin Khan and Leah Crowe from NOAA for hand labeling the images to create this one of a kind dataset and to the right whale research team at New England Aquarium for maintaining the photo-identification catalog. Without their continued efforts, none of this would be possible. ","Submissions are evaluated using the multi-class logarithmic loss. Each image has been labeled with one true class. For each image, you must submit a set of predicted probabilities (one for every whale). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of images in the test set, M is the number of whale labels,  \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) belongs to whale \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to whale \\(j\\).
The submitted probabilities for a given image are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission File
You must submit a csv file with the image file name, all candidate whale ids, and a probability for each whale id. The order of the rows does not matter. The file must have a header and should look like the following:
Image,whale_00195,whale_00442,whale_02411,whale_02608,whale_02839,...,whale_99573

w_1947.jpg,1,0,0,0,0,...,0
...","In this competition you are given aerial images, each containing a single Right whale. These images were taken over the course of 10 years and hundreds of helicopter trips, and have been selected and labeled by NOAA scientists with their whale IDs.
To ensure that this is a computer vision problem, we have removed metadata such as creation dates and geo tags from the photos. To discourage hand labeling, we have supplemented the test dataset with some images that are resized, cropped, or flipped. These processed images are ignored and don't count towards your score.  
Your goal is to build a ""face recognition system"" for whales. 
File descriptions
imgs.zip - zipped folder of all (train/test) images
train.csv - the training set with image file name and whale id. 
sample_submission.csv - a sample submission file in the correct format
imgs_subset.zip - zipped folder of a subset (first 500 images) of the full dataset",kaggle competitions download -c noaa-right-whale-recognition,"['https://www.kaggle.com/code/lucasgvazquez/road-to-the-top-part-1-a-dive-to-the-bottom', 'https://www.kaggle.com/code/lucasgvazquez/road-to-the-top-part-2-less-is-more', 'https://www.kaggle.com/code/kristinapoberezhna/right-whale-part-1', 'https://www.kaggle.com/code/kristinapoberezhna/right-whale-part-2', 'https://www.kaggle.com/code/zhiyuanzhang15/resnet-34']"
394,"With over 1,200 quick service restaurants across the globe, TFI is the company behind some of the world's most well-known brands: Burger King, Sbarro, Popeyes, Usta Donerci, and Arby’s. They employ over 20,000 people in Europe and Asia and make significant daily investments in developing new restaurant sites.
Right now, deciding when and where to open new restaurants is largely a subjective process based on the personal judgement and experience of development teams. This subjective data is difficult to accurately extrapolate across geographies and cultures. 
New restaurant sites take large investments of time and capital to get up and running. When the wrong location for a restaurant brand is chosen, the site closes within 18 months and operating losses are incurred. 
Finding a mathematical model to increase the effectiveness of investments in new restaurant sites would allow TFI to invest more in other important business areas, like sustainability, innovation, and training for new employees. Using demographic, real estate, and commercial data, this competition challenges you to predict the annual restaurant sales of 100,000 regional locations.
TFI would love to hire an expert Kaggler like you to head up their growing data science team in Istanbul or Shanghai. You'd be tackling problems like the one featured in this competition on a global scale. See the job description here >>","Root Mean Squared Error (RMSE)
Submissions are scored on the root mean squared error. RMSE is very common and is a suitable general-purpose error metric. Compared to the Mean Absolute Error, RMSE punishes large errors:
\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]
where y hat is the predicted value and y is the original value.
Submission File
For every restaurant in the dataset, submission files should contain two columns: Id and Prediction. 
The file should contain a header and have the following format:
Id,Prediction
0,1.0
1,1.0
2,1.0
etc.","TFI has provided a dataset with 137 restaurants in the training set, and a test set of 100000 restaurants. The data columns include the open date, location, city type, and three categories of obfuscated data: Demographic data, Real estate data, and Commercial data. The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. 
File descriptions
train.csv - the training set. Use this dataset for training your model. 
test.csv - the test set. To deter manual ""guess"" predictions, Kaggle has supplemented the test set with additional ""ignored"" data. These are not counted in the scoring.
sampleSubmission.csv - a sample submission file in the correct format
Data fields
Id : Restaurant id. 
Open Date : opening date for a restaurant
City : City that the restaurant is in. Note that there are unicode in the names. 
City Group: Type of the city. Big cities, or Other. 
Type: Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile",kaggle competitions download -c restaurant-revenue-prediction,"['https://www.kaggle.com/code/ani310/restaurant-revenue', 'https://www.kaggle.com/code/ahayek84/restaurant-revenue-predict', 'https://www.kaggle.com/code/subbhashit/resturant-revenue-predicition', 'https://www.kaggle.com/code/ayushikaushik/eda-regression-analysis', 'https://www.kaggle.com/code/husnakhan/restaurant-prediction']"
395,"This is a Masters competition. You must be a Kaggle Master to participate.
Property rental prices are a key economic indicator, often signaling significant changes in things like unemployment rate or income. Accurately predicting rental prices would help organizations offering public and commercial services with the ability to better plan for and price these services.
Weekly rental values for properties vary due to a broad mix of factors. Some measures are objective, like proximity to hospitals, schools, transport, and coastline. Others are more subjective, like the aesthetic value of your backyard garden.
The rental market in Western Australia is unusually diverse and difficult to predict due to the region's varied landscape and small, widely spread population.
Currently, automated valuation models are used for over 90% of residential property estimates in Western Australia. Using data on location, property, zoning, past sales, and more, the goal of this competition is to improve on existing models by accurately estimating the weekly market rental value for residential properties across Western Australia.
@JotForm.Show(34)","Submissions are evaluated on the Root Mean Squared Logarithmic Error (RMSLE). The RMSLE is calculated as
\[ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } \]
Where:
n is the number of rentals in the test set
p is your predicted price
a is the actual price
log is the natural logarithm
Submission Format
Your submission file must have a header and should be structured in the following format:
REN_ID,REN_BASE_RENT
12686,100
122383,200
147691,300
...",,,
396,"Get started on this competition through Kaggle Scripts
The Otto Group is one of the world’s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate & Barrel (USA), Otto.de (Germany) and 3 Suisses (France). We are selling millions of products worldwide every day, with several thousand products being added to our product line.
A consistent analysis of the performance of our products is crucial. However, due to our diverse global infrastructure, many identical products get classified differently. Therefore, the quality of our product analysis depends heavily on the ability to accurately cluster similar products. The better the classification, the more insights we can generate about our product range.
For this competition, we have provided a dataset with 93 features for more than 200,000 products. The objective is to build a predictive model which is able to distinguish between our main product categories. The winning models will be open sourced.","Submissions are evaluated using the multi-class logarithmic loss. Each product has been labeled with one true category. For each product, you must submit a set of predicted probabilities (one for every category). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of products in the test set, M is the number of class labels, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given product are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission Format
You must submit a csv file with the product id, all candidate class names, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:
id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9
1,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
2,0.0,0.2,0.3,0.3,0.0,0.0,0.1,0.1,0.0
...
etc.","See, fork, and run a random forest benchmark model through Kaggle Scripts
Each row corresponds to a single product. There are a total of 93 numerical features, which represent counts of different events. All features have been obfuscated and will not be defined any further.
There are nine categories for all products. Each target category represents one of our most important product categories (like fashion, electronics, etc.). The products for the training and testing sets are selected randomly.
File descriptions
trainData.csv - the training set
testData.csv - the test set
sampleSubmission.csv - a sample submission file in the correct format
Data fields
id - an anonymous id unique to a product
feat_1, feat_2, ..., feat_93 - the various features of a product
target - the class of a product",kaggle competitions download -c otto-group-product-classification-challenge,"['https://www.kaggle.com/code/tqchen/understanding-xgboost-model-on-otto-data', 'https://www.kaggle.com/code/hsperr/finding-ensamble-weights', 'https://www.kaggle.com/code/cbourguignat/why-calibration-works', 'https://www.kaggle.com/code/threecourse/gbdt-implementation-kaggle-days-tokyo', 'https://www.kaggle.com/code/yus002/are-tps-june-data-related-to-otto-data']"
397,"Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people.
The US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment.
Currently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment.
Clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient.
The need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible – ideally resulting in models with realistic clinical potential. The winning models will be open sourced to maximize the impact such a model can have on improving DR detection.
Acknowledgements
This competition is sponsored by the California Healthcare Foundation.
Retinal images were provided by EyePACS, a free platform for retinopathy screening.","Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, this metric may go below 0. The quadratic weighted kappa is calculated between the scores assigned by the human rater and the predicted scores.
Images have five possible ratings, 0,1,2,3,4.  Each image is characterized by a tuple (e,e), which corresponds to its scores by Rater A (human) and Rater B (predicted).  The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that O corresponds to the number of images that received a rating i by A and a rating j by B. An N-by-N matrix of weights, w, is calculated based on the difference between raters' scores:
An N-by-N histogram matrix of expected ratings, E, is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that E and O have the same sum.
From these three matrices, the quadratic weighted kappa is calculated as: ","You are provided with a large set of high-resolution retina images taken under a variety of imaging conditions. A left and right field is provided for every subject. Images are labeled with a subject id as well as either left or right (e.g. 1_left.jpeg is the left eye of patient id 1).
A clinician has rated the presence of diabetic retinopathy in each image on a scale of 0 to 4, according to the following scale:
0 - No DR
1 - Mild
2 - Moderate
3 - Severe
4 - Proliferative DR
Your task is to create an automated analysis system capable of assigning a score based on this scale.
The images in the dataset come from different models and types of cameras, which can affect the visual appearance of left vs. right. Some images are shown as one would see the retina anatomically (macula on the left, optic nerve on the right for the right eye). Others are shown as one would see through a microscope condensing lens (i.e. inverted, as one sees in a typical live eye exam). There are generally two ways to tell if an image is inverted:",kaggle competitions download -c diabetic-retinopathy-detection,"['https://www.kaggle.com/code/kmader/inceptionv3-for-retinopathy-gpu-hr', 'https://www.kaggle.com/code/meenavyas/diabetic-retinopathy-detection', 'https://www.kaggle.com/code/manifoldix/inceptionv3-for-retinopathy-gpu-hr', 'https://www.kaggle.com/code/tanlikesmath/diabetic-retinopathy-with-resnet50-oversampling', 'https://www.kaggle.com/code/kmader/tf-data-tutorial-with-retina-and-keras']"
398,"Like last year's Higgs Boson Machine Learning Challenge, this competition deals with the  physics at the Large Hadron Collider (LHC). However, the subject of last year's challenge, the Higgs Boson, was already known to exist. The aim of this year's challenge is to find a phenomenon that is not already known to exist – charged lepton flavour violation – thereby helping to establish ""new physics"". 
Flavours of Physics 101
The laws of nature ensure that some physical quantities, such as energy or momentum, are conserved. From Noether’s theorem, we know that each conservation law is associated with a fundamental symmetry. For example, conservation of energy is due to the time-invariance (the outcome of an experiment would be the same today or tomorrow) of physical systems. The fact that physical systems behave the same, regardless of where they are located or how they are oriented, gives rise to the conservation of linear and angular momentum.
Symmetries are also crucial to the structure of the Standard Model of particle physics, our present theory of interactions at microscopic scales. Some are built into the model, while others appear accidentally from it. In the Standard Model, lepton flavour, the number of electrons and electron-neutrinos, muons and muon-neutrinos, and tau and tau-neutrinos, is one such conserved quantity.
Interestingly, in many proposed extensions to the Standard Model, this symmetry doesn’t exist, implying decays that do not conserve lepton flavour are possible. One decay searched for at the LHC is τ- → μ+μ-μ- (or τ → 3μ). Observation of this decay would be a clear indication of the violation of lepton flavour and a sign of long-sought new physics.
Competition Design
You will be working with real data from the LHCb experiment at the LHC, mixed with simulated datasets of the decay. The metric used in this challenge includes checks that physicists do in their analysis to make sure the results are unbiased. These checks have been built into the competition design to help ensure that the results will be useful for physicists in future studies. 
To get started, review the Data Page, and be sure to download the Starter Kit. The Starter Kit will help you to get used to the unique submission procedure for this competition.
Competition Video Tutorial
You've got lots of questions. Researchers at CERN & LCHb have the answers.
- What is the goal of this competition? (1:56)
- Why is finding τ → μμμ exciting? (2:18)
- What are flavours? (4:10)
- Why use machine learning to find τ → μμμ? (4:57)
- How did you decide on the size of the dataset? (5:31)
- Why is weighted AUC the evaluation metric? (6:09)
- Why use Ds → φπ data for the Agreement Test? (7:53)
- Why do we need a Correlation Check? (8:44)
- How will the competition results impact what you do? (11:38)
- How will the competition results be used at CERN? (12:17)
Resources
Flavour of Physics, Research Documentation
Roel Aaij et al., Search for the lepton flavour violating decay τ → µµµ, 2015, JHEP, 1502:121, 2015
New approaches for boosting to uniformity
Acknowledgements
This competition is brought to you by: 
                          Co-sponsored by:
Additional support from: 
                       ","The evaluation metric for this competition is Weighted Area Under the ROC Curve. The ROC curve is divided into sections based on the True Positive Rate (TPR). To calculate the total area, multiply the area with TPR in [0., 0.2] by weight 2.0, the area with TPR in [0.2, 0.4] by 1.5, the area with TPR [0.4, 0.6] with weight 1.0, and the area with TPR [0.6, 0.8] with weight 0.5. Anything above a TPR of 0.8 has weight 0.
These weights were chosen to match the evaluation methodology used by CERN scientists. Note that the weighted AUC is calculated only for events (simulated signal events for tau->µµµ and real background events for tau->µµµ) with min_ANNmuon > 0.4 (see details in section 2.2 Physics background).
Before your predictions are scored with weighted AUC, they also must pass two addition checks: first an agreement test and then the correlation test. Please refer to their respective pages to learn about these tests and what is needed to pass them.
Submission File
For every event in the dataset, submission files should contain two columns: id and prediction. The prediction should be a floating point value between 0 and 1.0, indicating the probability that this event is τ → 3μ decay. 
The file should contain a header and have the following format:
id,prediction
14711831,0.3
16316387,0.3
6771382,0.3
686045,0.3
8755882,0.3
10247299,0.3
etc.","In this competition, you are given a list of collision events and their properties. You will then predict whether a τ → 3μ decay happened in this collision. This τ → 3μ is currently assumed by scientists not to happen, and the goal of this competition is to discover τ → 3μ happening more frequently than scientists currently can understand.
It is challenging to design a machine learning problem for something you have never observed before. Scientists at CERN developed the following designs to achieve the goal.
training.csv
This is a labelled dataset (the label ‘signal’ being ‘1’ for signal events, ‘0’ for background events) to train the classifier. Signal events have been simulated, while background events are real data.
This real data is collected by the LHCb detectors observing collisions of accelerated particles with a specific mass range in which τ → 3μ can’t happen. We call these events “background” and label them 0.
FlightDistance - Distance between τ and PV (primary vertex, the original protons collision point).
FlightDistanceError - Error on FlightDistance.
mass - reconstructed τ candidate invariant mass, which is absent in the test samples.
LifeTime - Life time of tau candidate.
IP - Impact Parameter of tau candidate.
IPSig - Significance of Impact Parameter.",kaggle competitions download -c flavours-of-physics,"['https://www.kaggle.com/code/triskelion/testing-python-3', 'https://www.kaggle.com/code/benhamner/rf-xgboost-example', 'https://www.kaggle.com/code/phunter/gridsearchcv-with-feature-in-xgboost', 'https://www.kaggle.com/code/fchollet/keras-starter-code-deep-pyramidal-mlp', 'https://www.kaggle.com/code/abhishek/rf-xgboost-example-0-982253']"
399,"Recruit Ponpare is Japan's leading joint coupon site, offering huge discounts on everything from hot yoga, to gourmet sushi, to a summer concert bonanza. Ponpare's coupons open doors for customers they've only dreamed of stepping through. They can learn difficult to acquire skills, go on unheard of adventures, and dine like (and with) the stars.
Investing in a new experience is not cheap. We fear wasting our time and money on a product or service that we may not enjoy or fully understand. Ponpare takes the high price out of this equation, making it easier for you to take the leap towards your first sky-dive or diamond engagement ring.
Using past purchase and browsing behavior, this competition asks you to predict which coupons a customer will buy in a given period of time. The resulting models will be used to improve Ponpare's recommendation system, so they can make sure their customers don't miss out on their next favorite thing.","Submissions are evaluated according to the Mean Average Precision @ 10 (MAP@10):
$$MAP@10 = \frac{1}{|U|} \sum_{u=1}^{|U|} \frac{1}{min(m, 10)} \sum_{k=1}^{min(n,10)} P(k)$$ 
where |U| is the number of users, P(k) is the precision at cutoff k, n is the number of predicted coupons, and m is the number of purchased coupons for the given user. If m = 0, the precision is defined to be 0.
Submission File
For every user, you must predict a space-delimited list of the coupons they purchased. The file should contain a header and have the following format (we have substituted the coupon hashes with dummy values to fit below, but in your prediction file you should use the real hash values):
USER_ID_hash,PURCHASED_COUPONS
0004901ba699a49fd93a3c6bb1768b8f,hash4
0006d6ac7c6ef3fc0ab0dc40deb3c960,hash1 hash2
00078d03b4dda619293c1793c251f783,
etc...","You are provided with a year of transactional data for 22,873 users on the site ponpare.jp. The training set spans the dates 2011-07-01 to 2012-06-23. The test set spans the week after the end of the training set, 2012-06-24 to 2012-06-30. The goal of the competition is to recommend a ranked list of coupons for each user in the dataset (found in user_list.csv). Your predictions are scored against the actual coupon purchases, made during the test set week, of the 310 possible test set coupons.
File descriptions
The dataset has relational format, with hashed ID columns for each entity.
user_list.csv - the master list of users in the dataset
Column Name Description Type Length Decimal Note
USER_ID_hash User ID VARCHAR2 32    
REG_DATE Registered date DATE     Sign up date",kaggle competitions download -c coupon-purchase-prediction,"['https://www.kaggle.com/code/anguyen/translate-everything-to-english-using-r', 'https://www.kaggle.com/code/mandalsubhajit/modified-cosine-similarity-0-00556', 'https://www.kaggle.com/code/mandalsubhajit/match-by-ken-name', 'https://www.kaggle.com/code/fredhseymour/furthermodifiedcosine-007600', 'https://www.kaggle.com/code/tobycheese/translate-capsule-text-and-genre-name']"
400,,,,,
401,,,,,
402,,,,,
403,,,,,
404,"Think back to this morning: turning off the alarm, getting dressed, brushing your teeth, making coffee, drinking coffee, and locking the door as you left for work. Now imagine doing all those things again, without the use of your hands. 
Patients who have lost hand function due to amputation or neurological disabilities wake up to this reality everyday. Restoring a patient's ability to perform these basic activities of daily life with a brain-computer interface (BCI) prosthetic device would greatly increase their independence and quality of life. Currently, there are no realistic, affordable, or low-risk options for neurologically disabled patients to directly control external prosthetics with their brain activity.
Recorded from the human scalp, EEG signals are evoked by brain activity. The relationship between brain activity and EEG signals is complex and poorly understood outside of specific laboratory tests. Providing affordable, low-risk, non-invasive BCI devices is dependent on further advancements in interpreting EEG signals. 
This competition challenges you to identify when a hand is grasping, lifting, and replacing an object using EEG data that was taken from healthy subjects as they performed these activities. Better understanding the relationship between EEG signals and hand movements is critical to developing a BCI device that would give patients with neurological disabilities the ability to move through the world with greater autonomy. 
Acknowledgements
This competition is sponsored by the WAY Consortium (Wearable interfaces for hAnd function recoverY; FP7-ICT-288551).","Submissions are evaluated on the mean column-wise AUC. That is, the mean of the individual areas under the ROC curve for each predicted column.
Since the columns span multiple subjects and series, you should submit calibrated probabilities that fall on the same scale.
Submission File
You must predict six probabilities for each id in the test set, where each id corresponds to a single time frame. The id is formed by concatenating (subject_series_frame).
The file should contain a header and have the following format:
id,HandStart,FirstDigitTouch,BothStartLoadPhase,LiftOff,Replace,BothReleased
subj1_series9_0,0,0,0,0,0,0
subj1_series9_1,0,0,0,0,0,0
subj1_series9_2,0,0,0,0,0,0
etc.","This data contains EEG recordings of subjects performing grasp-and-lift (GAL) trials. The following video shows an example of a trial:
A detailed account of the data can be found in 
Luciw MD, Jarocka E, Edin BB (2014) Multi-channel EEG recordings during 3,936 grasp and lift trials with varying weight and friction.  1:140047. ",kaggle competitions download -c grasp-and-lift-eeg-detection,"['https://www.kaggle.com/code/alexandrebarachant/beat-the-benchmark-0-67', 'https://www.kaggle.com/code/alexandrebarachant/common-spatial-pattern-with-mne', 'https://www.kaggle.com/code/elenacuoco/simple-grasp-with-sklearn', 'https://www.kaggle.com/code/alexandrebarachant/visual-evoked-potential-vep', 'https://www.kaggle.com/code/bitsofbits/naive-nnet']"
405,"For agriculture, it is extremely important to know how much it rained on a particular field. However, rainfall is variable in space and time and it is impossible to have rain gauges everywhere. Therefore, remote sensing instruments such as radar are used to provide wide spatial coverage. Rainfall estimates drawn from remotely sensed observations will never exactly match the measurements that are carried out using rain gauges, due to the inherent characteristics of both sensors. Currently, radar observations are ""corrected"" using nearby gauges and a single estimate of rainfall is provided to users who need to know how much it rained. This competition will explore how to address this problem in a probabilistic manner.  Knowing the full probabilistic spread of rainfall amounts can be very useful to drive hydrological and agronomic models -- much more than a single estimate of rainfall.
Image courtesy of NOAA
Unlike a conventional Doppler radar, a polarimetric radar transmits radio wave pulses that have both horizontal and vertical orientations. Because rain drops become flatter as they increase in size and because ice crystals tend to be elongated vertically, whereas liquid droplets tend to be flattened, it is possible to infer the size of rain drops and the type of hydrometeor from the differential reflectivity of the two orientations.
In this competition, you are given polarimetric radar values and derived quantities at a location over the period of one hour. You will need to produce a probabilistic distribution of the hourly rain gauge total. More details are on the data page.
This competition is sponsored by the Artificial Intelligence Committee of the American Meteorological Society. The Climate Corporation has kindly agreed to sponsor the prizes.","The winning entry will be the one that minimizes the Continuous Ranked Probability Score:
\[ C = \frac{1}{70N} \sum_{N} \sum_{n=0}^{69} (P(y \le n) -H(n -z))^2 \]
over the testing dataset (of size N) where z is the actually recorded gauge value (in mm) and H(x) is the Heavyside step function i.e. H(x) = 1 for \[x \ge 0\] and zero otherwise. The entry will be discarded if any of the answers has \[P(y \le k) > P(y \le k+1)\] for any k, i.e., the CDF has to be non-decreasing.
Submission Instructions
The submission file specifies for each location (with Id), the predicted cumulative probabilities between 0 and 69 mm (both inclusive). There are 70 columns of probabilities for each Id. 
Id,Predicted0,Predicted1,Predicted2,Predicted3,...,Predicted69
1,0.493069074336,0.725572625942,0.87785520942,0.951305748155,...,1.0
2,0.5,0.73105857863,0.880797077978,0.952574126822,...,1.0
...
etc","File descriptions
train_2013.csv.zip - the training set
test_2014.csv.zip - the test set
sampleSubmission.csv - a sample submission file in the correct format
sample_solution.py -- a Python program that is capable of taking test_2014.csv and producing sampleSubmission.csv
Predicting probabilities
In this competition, you are given polarimetric radar values and derived quantities at a location over the period of one hour. You will need to produce a probabilistic distribution of the hourly rain gauge total, i.e., produce \[P(y \le Y)\] where y is the rain accumulation and Y lies between 0 and 69 mm (both inclusive) in increments of 1 mm. For every row in the dataset, submission files should contain 71 columns: Id and 70 numbers. 
Understanding the data
The training data consists of NEXRAD and MADIS data collected the first 8 days of Apr to Nov 2013 over midwestern corn-growing states. Time and location information have been censored, and the data have been shuffled so that they are not ordered by time or place. The test data consists of data from the same radars and gauges over the same months but in 2014. Please see this page to understand more about polarimetric radar measurements.",kaggle competitions download -c how-much-did-it-rain,"['https://www.kaggle.com/code/sudalairajkumar/rainfall-of-rr1-percentile-bins', 'https://www.kaggle.com/code/sudalairajkumar/rainfall-histogram-plot', 'https://www.kaggle.com/code/devinanzelmo/kde-and-scatter-plot', 'https://www.kaggle.com/code/devinanzelmo/component-cdf-s-and-sample-predictions', 'https://www.kaggle.com/code/devinanzelmo/fiddling-with-xgb']"
406,"From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz.
Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay.
From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given time and location, you must predict the category of crime that occurred.
We're also encouraging you to explore the dataset visually. What can we learn about the city through visualizations like this Top Crimes Map? The top most up-voted scripts from this competition will receive official Kaggle swag as prizes. 
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset is brought to you by SF OpenData, the central clearinghouse for data published by the City and County of San Francisco.","Submissions are evaluated using the multi-class logarithmic loss. Each incident has been labeled with one true class. For each incident, you must submit a set of predicted probabilities (one for every class). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of cases in the test set, M is the number of class labels, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given incident are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission Format
You must submit a csv file with the incident id, all candidate class names, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:
Id,ARSON,ASSAULT,BAD CHECKS,BRIBERY,BURGLARY,DISORDERLY CONDUCT,DRIVING UNDER THE INFLUENCE,DRUG/NARCOTIC,DRUNKENNESS,EMBEZZLEMENT,EXTORTION,FAMILY OFFENSES,FORGERY/COUNTERFEITING,FRAUD,GAMBLING,KIDNAPPING,LARCENY/THEFT,LIQUOR LAWS,LOITERING,MISSING PERSON,NON-CRIMINAL,OTHER OFFENSES,PORNOGRAPHY/OBSCENE MAT,PROSTITUTION,RECOVERED VEHICLE,ROBBERY,RUNAWAY,SECONDARY CODES,SEX OFFENSES FORCIBLE,SEX OFFENSES NON FORCIBLE,STOLEN PROPERTY,SUICIDE,SUSPICIOUS OCC,TREA,TRESPASS,VANDALISM,VEHICLE THEFT,WARRANTS,WEAPON LAWS
0,0.9,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
...
etc.","This dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set. ",kaggle competitions download -c sf-crime,"['https://www.kaggle.com/code/yannisp/sf-crime-analysis-prediction', 'https://www.kaggle.com/code/mircat/violent-crime-mapping', 'https://www.kaggle.com/code/benhamner/san-francisco-top-crimes-map', 'https://www.kaggle.com/code/swbevan/a-history-of-crime-python', 'https://www.kaggle.com/code/dbennett/test-map']"
407,"At Kaggle HQ and in offices across the country, March is a month when bracketology is in bloom. Back by popular demand, our second annual March Machine Learning Mania competition pits you against the millions of sports fans and office-pool bandwagoners who are hoping to win big by correctly predicting the outcome of the men's NCAA basketball tournament. 
While the odds of forecasting a perfect bracket are astronomical, these odds are improved by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media.
How well can machine learning and statistical techniques improve the forecast? Presented by HP Software's industry leading Big Data group and the HP Haven Big Data platform, this competition will test how well predictions based on data stack up against a (jump) shot in the dark.
This competition allows you to get creative with the datasets you use to create your model. We provide data covering three decades of historical games, but you're highly encouraged to pull in data from external sources. 
The 50+ REST APIs from HP IDOL OnDemand are a great way to get started augmenting the dataset. Developer accounts are free and includes free monthly quota! Begin by extracting trending topics and identifying entities from the IDOL OnDemand news dataset (accessed via the Query Text Index API) or by analyzing public sentiment about players and teams using data from your social media feed. 
In stage one of this two-stage competition, participants will build and test their models against the previous four tournaments. In the second stage, participants will predict the outcome of the 2015 tournament. You don’t need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2015 results, for which you’ll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket. HP is sponsoring $15,000 in cash prizes for the winners.
Please visit the FAQs for more information.
Acknowledgements
March Machine Learning Mania 2015 is presented by HP. Please see About the sponsor to read more.","Submissions are scored on the log loss, also called the predictive binomial deviance:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of games played
\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2
\\( y_i \\) is 1 if team 1 wins, 0 if team 2 wins
\\( log() \\) is the natural (base e) logarithm
A smaller log loss is better. Games which are not played are ignored in the scoring. Play-in games are also ignored (only the games among the final 64 teams are scored). The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2015 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2  = 2278 matchups. 
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""2013_1104_1129"" indicates team 1104 played team 1129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id.
The resulting submission format looks like the following, where ""pred"" represents the predicted probability that the first team will win:
id,pred
2011_1103_1106,0.5
2011_1103_1112,0.5
2011_1103_1114,0.5
...
...","If you are unfamiliar with the format and intricacies of the NCAA tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears.
As a reminder, you are encouraged to incorporate your own sources of data. We have provided team-level historical data to jump-start the modeling process, but there is also player-level and game-level data that may be useful.
If you're looking for some powerful technology to help get the most from all your data, you could load your data into the Vertica Analytics Platform for data preparation and use Distributed R, a scaleable and high-performance platform for R with out-of-the-box parallel algorithms. After you build and evaluate predictive model in Distributed R you can even deploy model(s) back to Vertica Analytics Platform for in-database prediction scoring using simple SQL and can combine prediction results with other insights that you may have derived from the data. Note that Vertica Analytics Platform offers wide array of SQL analytic functions such as in-database sentiment analysis functions, geospatial functions that can help you derive new and interesting attributes.
We extend our gratitude to Kenneth Massey for his work gathering and providing much of the historical data.
What to predict
Stage 1 - You should submit predicted probabilities for every possible matchup in the past 4 NCAA tournaments (2011-2014).
Stage 2 - You should submit predicted probabilities for every possible matchup before the 2015 tournament begins.",kaggle competitions download -c march-machine-learning-mania-2015,['https://www.kaggle.com/code/jtrotman/beautiful-mania-2015']
408,"Plankton are critically important to our ecosystem, accounting for more than half the primary productivity on earth and nearly half the total carbon fixed in the global carbon cycle. They form the foundation of aquatic food webs including those of large, important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.
Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of an underwater imagery sensor. This towed, underwater camera system captures microscopic, high-resolution images over large study areas. The images can then be analyzed to assess species populations and distributions.
Manual analysis of the imagery is infeasible – it would take a year or more to manually analyze the imagery volume captured in a single day. Automated image classification using machine learning tools is an alternative to the manual approach. Analytics will allow analysis at speeds and scales previously thought impossible. The automated system will have broad applications for assessment of ocean and ecosystem health.
The National Data Science Bowl challenges you to build an algorithm to automate the image identification process. Scientists at the Hatfield Marine Science Center and beyond will use the algorithms you create to study marine food webs, fisheries, ocean conservation, and more. This is your chance to contribute to the health of the world’s oceans, one plankton at a time.
Acknowledgements
The National Data Science Bowl is presented by
with data provided by the Hatfield Marine Science Center at Oregon State University.","Submissions are evaluated using the multi-class logarithmic loss. Each image has been labeled with one true class. For each image, you must submit a set of predicted probabilities (one for every class). The formula is then,
$$log loss = -\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{ij}\log(p_{ij}),$$
where N is the number of images in the test set, M is the number of class labels, \\(log\\) is the natural logarithm, \\(y_{ij}\\) is 1 if observation \\(i\\) is in class \\(j\\) and 0 otherwise, and \\(p_{ij}\\) is the predicted probability that observation \\(i\\) belongs to class \\(j\\).
The submitted probabilities for a given image are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\(max(min(p,1-10^{-15}),10^{-15})\\).
Submission Format
You must submit a csv file with the image name, all candidate class names, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:
image,acantharia_protist_big_center,...,unknown_unclassified
1.jpg,0.00826446,...,0.00826446
10.jpg,0.00826446,...,0.00826446
...
etc.","In total, Oregon State University’s Hatfield Marine Science Center has captured nearly 50 million plankton images over an 18-day period. This is more than 80 terabytes of data! They need your help creating an automated classification process to better understand the image contents.
For this competition, Hatfield scientists have prepared a large collection of labeled images, approximately 30k of which are provided as a training set. Each raw image was run through an automatic process to extract regions of interest, resulting in smaller images that contain a single organism/entity. You must create an algorithm that assigns class probabilities to a given image. Several characteristics of this problem make this classification difficult:
There are many different species, ranging from the smallest single-celled protists to copepods, larval fish, and larger jellies.
Representatives from each taxon can have any orientation within 3-D space.
The ocean is replete with detritus (often decomposing plant or animal matter that scientists like to call “whale snot”) and fecal pellets that have no taxonomic identification but are important in other marine processes.
Some images are so noisy or ambiguous that experts have a difficult time labeling them. Some amount of noise in the ground truth is thus inevitable.
The presence of ""unknown"" classes require models to handle the special cases of unidentifiable objects.",kaggle competitions download -c datasciencebowl,"['https://www.kaggle.com/code/sagnik1511/plankton-classification-pytorch', 'https://www.kaggle.com/code/esa2018/access-plankton-data-in-zip-files', 'https://www.kaggle.com/code/kimberlytyz/plankthonnn', 'https://www.kaggle.com/code/dawool1218/bowl-dl-p']"
409,"In Russia, if you're looking to sell a tractor, a designer dress, a vintage lunchbox, or even a house, your first stop will likely be Avito.ru. As the largest general classified website in Russia, Avito connects buyers and sellers across the world's biggest country.
Sellers are highly motivated to place ads on Avito, hoping to gain attention from the site's 70 million unique monthly visitors. There are three different types of ads available to sellers on Avito: regular, highlighted, and context. 
Context ads are seen as the best way to target users with goods and services. Currently, Avito uses general statistics on ad performance to drive the placement of context ads. Their existing model ignores individual user behavior, making it difficult to predict which ad will be the most relevant for (and earn the most clicks from) each potential buyer. 
In this competition, Avito is challenging you to improve on their model by predicting if individual users will click a given context ad. To create the most robust model and fun competition possible, Avito has provided eight comprehensive relational datasets for you to explore. This competition will help Avito more accurately predict click-through rates for their ads, creating a world where both buyers and sellers win.","Submissions are scored on the log loss, also called the predictive binomial deviance:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of rows in sampleSubmissions, each row represents one impression, which is a unique combination of an ad and a search 
\\( \hat{y}_i \\) is the predicted probability of this ad being clicked
\\( y_i \\) is 1 if the actual ad's IsClick = 1, 0 if IsClick = 0
\\( log() \\) is the natural (base e) logarithm
A smaller log loss is better.
The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value of \\(1.0 * 10 ^ {-15} \\).
Submission File
The file should contain a header and have the following format:
TestId,IsClick
0,0
1,0
5,0
7,0
9,0
etc.",This competition has 8 relational datasets. All these files were encoded in UTF-8 and stored into tab separated format (.tsv). There is also an sqlite database (database.sqlite) alternative with all data available. Relationships between the datasets are captured in the following schema:,kaggle competitions download -c avito-context-ad-clicks,"['https://www.kaggle.com/code/rahulpatel11315/read-data-from-tsv-file-using-pandas-dataframe', 'https://www.kaggle.com/code/jihyeseo/avito-tsv-csv-sqlite', 'https://www.kaggle.com/code/wangwei123/notebook1cf274395a', 'https://www.kaggle.com/code/gugululu1122/hehehe', 'https://www.kaggle.com/code/rinalin/notebookfb9fc8c339']"
410,"Optical Character Recognition (OCR) is the process of getting type or handwritten documents into a digitized format. If you've read a classic novel on a digital reading device or had your doctor pull up old healthcare records via the hospital computer system, you've probably benefited from OCR.
OCR makes previously static content editable, searchable, and much easier to share. But, a lot of documents eager for digitization are being held back. Coffee stains, faded sun spots, dog-eared pages, and lot of wrinkles are keeping some printed documents offline and in the past. 
This competition challenges you to give these documents a machine learning makeover. Given a dataset of images of scanned text that has seen better days, you're challenged to remove the noise. Improving the ease of document enhancement will help us get that rare mathematics book on our e-reader before the next beach vacation.
We've kicked off the fun with a few handy scripts to get you started on the dataset.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was created by RM.J. Castro-Bleda, S. España-Boquera, J. Pastor-Pellicer, F. Zamora-Martinez. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:
Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science","Submissions are evaluated on the root mean squared error between the cleaned pixel intensities and the actual grayscale pixel intensities.
Submission File
Form the submission file by melting each images into a set of pixels, assigning each pixel an id of image_row_col (e.g. 1_2_1 is image 1, row 2, column 1). Intensity values range from 0 (black) to 1 (white). The file should contain a header and have the following format:
id,value
1_1_1,1
1_2_1,1
1_3_1,1
etc.","You are provided two sets of images, train and test. These images contain various styles of text, to which synthetic noise has been added to simulate real-world, messy artifacts. The training set includes the test without the noise (train_cleaned). You must create an algorithm to clean the images in the test set.",kaggle competitions download -c denoising-dirty-documents,"['https://www.kaggle.com/code/michalbrezk/denoise-images-using-autoencoders-tf-keras', 'https://www.kaggle.com/code/aakashnain/denoising-autoencoders-to-the-rescue', 'https://www.kaggle.com/code/rdokov/background-removal', 'https://www.kaggle.com/code/palaksood97/image-denoising', 'https://www.kaggle.com/code/anmour/convolutional-autoencoder-with-keras']"
411,"In this tutorial competition, we dig a little ""deeper"" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis.
Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another Kaggle competition for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem.
Deep learning has been in the news a lot over the past few years, even making it to the front page of the New York Times. These machine learning techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a drug discovery task, and cat and dog image recognition.
Tutorial Overview
This tutorial will help you get started with Word2Vec for natural language processing. It has two goals: 
Basic Natural Language Processing: Part 1 of this tutorial is intended for beginners and covers basic natural language processing techniques, which are needed for later parts of the tutorial.
Deep Learning for Text Understanding: In Parts 2 and 3, we delve into how to train a model using Word2Vec and how to use the resulting word vectors for sentiment analysis.
Since deep learning is a rapidly evolving field, large amounts of the work has not yet been published, or exists only as academic papers. Part 3 of the tutorial is more exploratory than prescriptive -- we experiment with several ways of using Word2Vec rather than giving you a recipe for using the output.
To achieve these goals, we rely on an IMDB sentiment analysis data set, which has 100,000 multi-paragraph movie reviews, both positive and negative. 
Acknowledgements
This dataset was collected in association with the following publication:
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). ""Learning Word Vectors for Sentiment Analysis."" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011). (link)
Please email the author of that paper if you use the data for any research applications. The tutorial was developed by Angela Chapman during her summer 2014 internship at Kaggle.","Metric
Submissions are judged on area under the ROC curve. 
Submission Instructions
You should submit a comma-separated file with 25,000 row plus a header row. There should be 2 columns: ""id"" and ""sentiment"", which contain your binary predictions: 1 for positive reviews, 0 for negative reviews. For an example, see ""sampleSubmission.csv"" on the Data page. 
id,sentiment
123_45,0 
678_90,1
12_34,0
...","Data Set
The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.
File descriptions
labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  
testData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. 
unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. 
sampleSubmission - A comma-delimited sample submission file in the correct format.
Data fields
id - Unique ID of each review",kaggle competitions download -c word2vec-nlp-tutorial,"['https://www.kaggle.com/code/rajmehra03/a-detailed-explanation-of-keras-embedding-layer', 'https://www.kaggle.com/code/nilanml/imdb-review-deep-model-94-89-accuracy', 'https://www.kaggle.com/code/alexcherniuk/imdb-review-word2vec-bilstm-99-acc', 'https://www.kaggle.com/code/rohandx1996/google-movie-reviews-sentiment-deep-stack-models', 'https://www.kaggle.com/code/gazu468/all-about-bert-you-need-to-know']"
412,"Imagine you're planning a summer holiday to Iceland: you read a travel blog on your smartphone on the subway to work, search for hotels on your laptop during lunch, browse Reykjavik restaurants on a tablet while half-watching TV after dinner, and then download a travel book to your e-reader to skim before bed. 
As consumers move across devices to complete online tasks, their identity becomes fragmented. Marketers, hoping to target them with meaningful messages, recommendations, and customized experiences, aren't always able to discern when activity on different devices is tied to one user vs. many users.
Given usage data and a set of fabricated non-personally-identifiable IDs, this competition tasks you with making individual user connections across a variety of digital devices. Improving marketers' ability to identify individual users as they switch between devices means you'll see relevant messages wherever you go, making it easy for you to plan the best, most fjord-filled trip ever. 
Acknowledgements
The competition dataset and prize pool have been generously provided by Drawbridge in sponsorship of the ICDM 2015 conference. ","Submissions will be evaluated based on their mean \\(F_{0.5}\\) score. The F score, commonly used in information retrieval, measures accuracy using the precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The \\(F_{0.5}\\) score is given by
$$
(1 + \beta^2) \frac{pr}{\beta^2 p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn},\ \beta = 0.5.
$$
Note that the \\(F_{0.5}\\) score weights precision higher than recall. The mean \\(F_{0.5}\\) score is formed by averaging the individual \\(F_{0.5}\\) scores for each row in the test set.
Submission File
For each device listed in the test set, predict a space-delimited list of cookie_ids which you believe are associated with the device. The file should contain a header and have the following format:
device_id,cookie_id
id_1,id_10
id_100002,id_10 id_20 id_30
id_1000035,id_10 id_20
etc.","Update 8/31/15 - this data set has been removed at the request of the host. Thank you for your participation!
See this script for a quick exploration of the data
This competition asks you to determine which cookies belong to an individual using a device. You are provided with relational information about users (represented by the id column drawbridge_handle), devices (device_id), cookies (cookie_id), as well as other information on IP addresses and behavior. For each device in the test set (dev_test_basic.csv), you must provide a list of cookie ids (from cookie_all_basic.csv) which you believe belong to the person using the given device_id. As you will see, the drawbridge_handle column is missing, denoted by the value -1, in the test set.
Training set (semi-)supervised learning methods
If you want to construct the training set and apply supervised learning, you can take the training data (dev_train_basic.csv), and find those cookies in the file cookie_all_basic.csv with the same drawbridge_handle. You could use device and cookie pairs with different drawbridge_handles as negative training data. Please note that some of the cookies have drawbridge_handle = -1, which means the drawbridge_handle for that cookie is unknown.
The same set of cookies that will be used for both training and testing purposes.
Data types
There are four different types of data attributes: Index, Categorical, Boolean and Int. Index and Categorical are both enumerated type. Index has bigger set of elements (e.g. device_id or cookie_id), while Categorical has smaller set of elements (e.g. all the device types, or all the desktop browser version). Boolean applies to those attributes with only 2 possible values, and Int describe the count of the attribute in a continuous way.",kaggle competitions download -c icdm-2015-drawbridge-cross-device-connections,"['https://www.kaggle.com/code/jihyeseo/unzip-eda-dataset-removed', 'https://www.kaggle.com/code/benhamner/test1', 'https://www.kaggle.com/code/liweilin/notebook-3502a5286da328b8a938', 'https://www.kaggle.com/code/chennnn/notebook-ca82c981d79a375e29ad', 'https://www.kaggle.com/code/srwomg/test1']"
413,"For automobile insurers, telematics represents a growing and valuable way to quantify driver risk. Instead of pricing decisions on vehicle and driver characteristics, telematics gives the opportunity to measure the quantity and quality of a driver's behavior. This can lead to savings for safe or infrequent drivers, and transition the burden to policies that represent increased liability.
AXA has provided a dataset of over 50,000 anonymized driver trips. The intent of this competition is to develop an algorithmic signature of driving type. Does a driver drive long trips? Short trips? Highway trips? Back roads? Do they accelerate hard from stops? Do they take turns at high speed? The answers to these questions combine to form an aggregate profile that potentially makes each driver unique.
For this competition, Kaggle participants must come up with a ""telematic fingerprint"" capable of distinguishing when a trip was driven by a given driver. The features of this driver fingerprint could help assess risk and form a crucial piece of a larger telematics puzzle.","Submissions are judged on area under the ROC curve. The ROC area is calculated in a global manner (all predictions together). You should therefore aim to submit calibrated probabilities between the drivers.
Submission File
You must submit a predicted probability for all possible driver_trip pairs. The resulting submission format looks like like the following, where ""prob"" represents the predicted probability that the trip belongs to the associated driver (1 = the trip belongs to the driver of interest, 0 = the trip does not belong):
driver_trip,prob
1_1,1
1_2,1
1_3,1
...",,,[]
414,"Your friend bailed last minute on poker night? Before giving up on a much-needed evening of bad bluffs and quarter buy ins, light a cigar and get familiar with the rules of the game. Each record in this competition consists of five playing cards and an attribute representing the poker hand. You are asked to predict the best hand you can play based on the cards you've been dealt. 
The order of cards is important, which means there are 480 possible Royal Flush hands instead of just four. Identify those, and the other 311,875,200 possible hands correctly, and you’re in the money!
""Isn't this easy? I know two-of-a-kind when I see it"", you might rightfully wonder.
And you'd be right.  The intent of this challenge is automatic rules induction, i.e. to learn the rules using machine learning, without hand coding heuristics. Pretend you are in a foreign land, have never played the game before, are given a history of thousands of games, and are asked to come up with the rules. It is potentially difficult to discover rules that can correctly classify poker hands, yet it is trivial for a human to validate the rules objectively. Remember, your algorithm will need to find rules that are general enough to be broadly useful, without being so broad that they end up being occasionally wrong. We suggest reading the paper by Cattral et al. for more background on the topic.
Playground competitions are an opportunity to build and stretch your machine learning muscles. Pull up a chair to the data science poker table and ante up.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was created by Robert Cattral and Franz Oppacher. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:
Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science","Submissions are evaluated on the categorization accuracy (the percent of hands that you correctly classify).
Submission File
Your submission file should predict the hand for each id in the test set. The file should contain a header and have the following format:
id,hand
1,0
2,0
3,9
...
etc.","You are provided with 25,010 poker hands in train.csv and 1,000,000 in test.csv. Each hand consists of five cards with a given suit and rank, drawn from a standard deck of 52. Suits and ranks are represented as ordinal categories:
S1 “Suit of card #1”
Ordinal (1-4) representing {Hearts, Spades, Diamonds, Clubs}
C1 “Rank of card #1”
Numerical (1-13) representing (Ace, 2, 3, ... , Queen, King)

...
S5 “Suit of card #5”
C5 “Rank of card #5”
Each row in the training set has the accompanying class label for the poker hand it comprises. The hands are omitted from the test set and must be predicted by participants. Hands are classified into the following ordinal categories:
0: Nothing in hand; not a recognized poker hand 
1: One pair; one pair of equal ranks within five cards
2: Two pairs; two pairs of equal ranks within five cards
3: Three of a kind; three equal ranks within five cards
4: Straight; five cards, sequentially ranked with no gaps
5: Flush; five cards with the same suit
6: Full house; pair + different rank three of a kind
7: Four of a kind; four equal ranks within five cards
8: Straight flush; straight + flush
9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush",kaggle competitions download -c poker-rule-induction,"['https://www.kaggle.com/code/prakharrathi25/iterative-proker-hand-prediction', 'https://www.kaggle.com/code/valeriisielikhov/100-pokerface', 'https://www.kaggle.com/code/kenkpixdev/poker-comb-without-ml-model-accuracy-1-00']"
415,"So many of our favorite daily activities are mediated by proprietary search algorithms. Whether you're trying to find a stream of that reality TV show on cat herding or shopping an eCommerce site for a new set of Japanese sushi knives, the relevance of search results is often responsible for your (un)happiness. Currently, small online businesses have no good way of evaluating the performance of their search algorithms, making it difficult for them to provide an exceptional customer experience.
The goal of this competition is to create an open-source model that can be used to measure the relevance of search results. In doing so, you'll be helping enable small business owners to match the experience provided by more resource rich competitors. It will also provide more established businesses a model to test against. Given the queries and resulting product descriptions from leading eCommerce sites, this competition asks you to evaluate the accuracy of their search algorithms.
Make a first submission with this Python benchmark on Kaggle scripts. 
The dataset for this competition was created using query-result pairings enriched on the CrowdFlower platform. They are sponsoring this competition as an investment in the open-source data science community. A dataset collected, cleaned, and labeled by CrowdFlower can make your supervised machine learning dreams come true.","Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, the metric may go below 0. The quadratic weighted kappa is calculated between the scores assigned by the human rater and the predicted scores.
Results have 4 possible ratings, 1,2,3,4.  Each search record is characterized by a tuple (ea,eb), which corresponds to its scores by Rater A (human) and Rater B (predicted).  The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that Oi,j corresponds to the number of search records that received a rating i by A and a rating j by B. An N-by-N matrix of weights, w, is calculated based on the difference between raters' scores:
$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$
An N-by-N histogram matrix of expected ratings, E, is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that E and O have the same sum.
From these three matrices, the quadratic weighted kappa is calculated as: 
$$\kappa=1-\frac{\sum_{i,j}w_{i,j}O_{i,j}}{\sum_{i,j}w_{i,j}E_{i,j}}.$$
Submission Format
You must submit a csv file with the product id and a predicted search relevance for each search record. The order of the rows does not matter. The file must have a header and should look like the following:
id,prediction
1,3
3,2
4,1
5,4
etc..","See this script for a quick exploration of the data
To evaluate search relevancy, CrowdFlower has had their crowd evaluate searches from a handful of eCommerce websites. A total of 261 search terms were generated, and CrowdFlower put together a list of products and their corresponding search terms. Each rater in the crowd was asked to give a product search term a score of 1, 2, 3, 4, with 4 indicating the item completely satisfies the search query, and 1 indicating the item doesn't match the search term.",kaggle competitions download -c crowdflower-search-relevance,"['https://www.kaggle.com/code/abhishek/beating-the-benchmark', 'https://www.kaggle.com/code/triskelion/normalized-kaggle-distance', 'https://www.kaggle.com/code/benhamner/exploring-the-crowdflower-data', 'https://www.kaggle.com/code/solution/lda-visualization', 'https://www.kaggle.com/code/triskelion/kappa-intuition']"
416,"Ever wonder what it's like to work at Facebook? Facebook and Kaggle are launching an Engineering competition for 2015. Trail blaze your way to the top of the leader board to earn an opportunity at interviewing for a role as a software engineer, working on world class Machine Learning problems. 
In this competition, you'll be chasing down robots for an online auction site. Human bidders on the site are becoming increasingly frustrated with their inability to win auctions vs. their software-controlled counterparts. As a result, usage from the site's core customer base is plummeting.
In order to rebuild customer happiness, the site owners need to eliminate computer generated bidding from their auctions. Their attempt at building a model to identify these bids using behavioral data, including bid frequency over short periods of time, has proven insufficient. 
The goal of this competition is to identify online auction bids that are placed by ""robots"", helping the site owners easily flag these users for removal from their site to prevent unfair auction activity. 
The data in this competition comes from an online platform, not from Facebook.
Please note: You must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions. ","Submissions are judged on area under the ROC curve.
Submission File
Each line of your submission should contain an Id and a prediction of the probability that this bidder is a robot. Your submission file must have a header row. The file should have the following format:
bidder_id,prediction
38d9e2e83f25229bd75bfcdc39d776bajysie,0.3
9744d8ea513490911a671959c4a530d8mp2qm,0.0
dda14384d59bf0b3cb883a7065311dac3toxe,0.9
...
etc","There are two datasets in this competition. One is a bidder dataset that includes a list of bidder information, including their id, payment account, and address. The other is a bid dataset that includes 7.6 million bids on different auctions. The bids in this dataset are all made by mobile devices.
The online auction platform has a fixed increment of dollar amount for each bid, so it doesn't include an amount for each bid. You are welcome to learn the bidding behavior from the time of the bids, the auction, or the device. 
The data in this competition comes from an online platform, not from Facebook.
File descriptions
train.csv - the training set from the bidder dataset
test.csv - the test set from the bidder dataset
sampleSubmission.csv - a sample submission file in the correct format
bids.csv - the bid dataset
Data fields
For the bidder dataset
bidder_id – Unique identifier of a bidder.",kaggle competitions download -c facebook-recruiting-iv-human-or-bot,"['https://www.kaggle.com/code/chongzhenjie/human-or-robot-random-forest', 'https://www.kaggle.com/code/meln1337/facebook-notebook', 'https://www.kaggle.com/code/lingjie0/model-fitting-fb', 'https://www.kaggle.com/code/lingjie0/eda-fb-data', 'https://www.kaggle.com/code/dilipkumar2k6/facebook-recruiting-iv-human-or-robot']"
417,"This is the second of two data science challenges that share the same dataset. The Taxi Service Trajectory competition predicts the final destination of taxi trips. 
To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict how long a driver will have his taxi occupied. If a dispatcher knew approximately when a taxi driver would be ending their current ride, they would be better able to identify which driver to assign to each pickup request. 
In this challenge, we ask you to build a predictive framework that is able to infer the trip time of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the travel time of a particular taxi trip.
This competition is affiliated with the organization of ECML/PKDD 2015.","Submissions are evaluated one the Root Mean Squared Logarithmic Error (RMSLE). The RMSLE is calculated as
\[ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } \]
Where:
\\(n\\) is the number of hours in the test set
\\(p_i\\) is your predicted count
\\(a_i\\) is the actual count
\\(\log(x)\\) is the natural logarithm
Submission Format
For every trip in the dataset, submission files should contain two columns: TRIP_ID and TRAVEL_TIME. TRIP_ID represents the ID of the trip for which you are predicting the total travel time (i.e. a string), while the TRAVEL_TIME column contains your prediction (i.e. a positive integer value containing the travel time in seconds).
The file should contain a header and have the following format:
TRIP_ID,TRAVEL_TIME
T1,60
T2,90
T3,122
etc.","I. Training Dataset

We have provided an accurate dataset describing a complete year (from 01/07/2013 to 30/06/2014) of the trajectories for all the 442 taxis running in the city of Porto, in Portugal (i.e. one CSV file named ""train.csv""). These taxis operate through a taxi dispatch central, using mobile data terminals installed in the vehicles. We categorize each ride into three categories: A) taxi central based, B) stand-based or C) non-taxi central based. For the first, we provide an anonymized id, when such information is available from the telephone call. The last two categories refer to services that were demanded directly to the taxi drivers on a B) taxi stand or on a C) random street.
Each data sample corresponds to one completed trip. It contains a total of
9 (nine) features, described as follows:
TRIP_ID: (String) It contains an unique identifier for each trip;
CALL_TYPE: (char) It identifies the way used to demand this service. It may contain one of three possible values:
‘A’ if this trip was dispatched from the central;
‘B’ if this trip was demanded directly to a taxi driver on a specific stand;
‘C’ otherwise (i.e. a trip demanded on a random street).
ORIGIN_CALL: (integer) It contains an unique identifier for each phone number which was used to demand, at least, one service. It identifies the trip’s customer if CALL_TYPE=’A’. Otherwise, it assumes a NULL value;",kaggle competitions download -c pkdd-15-taxi-trip-time-prediction-ii,"['https://www.kaggle.com/code/wikunia/speed-visualization', 'https://www.kaggle.com/code/willieliao/beat-the-benchmark', 'https://www.kaggle.com/code/benhamner/max-time-elapsed-mean-time-benchmark', 'https://www.kaggle.com/code/benhamner/speed-visualization', 'https://www.kaggle.com/code/dataista/0-60-on-lb-w-max-mean-time-plus-trick']"
418,"♫ Us what to do all day.
O what fun it is to build
Toys all kids love to play! ♫ 
Every year Santa has to satisfy a grueling toy-production schedule. Toy orders arrive all year long and the Elfin Workers Union is stronger than ever. Let's help Santa develop a job scheduling algorithm that meets his toy target while keeping his elves healthy and happy.
Problem Description
In this job scheduling problem, you will assign which elves work on which toys, at what time, and for how long. The goal is to complete all of the toys as early as possible, scaled by the natural log of the number of elves that work. Thus the objective S is
\[S = t_f * \log(1 +n_e) \]
where
\\(t_f\\) is the last minute the final toy is complete, from reference date Jan 1, 2014 at 12:00 AM
\\(n_e\\) is the number of unique elves that were needed to build the toys
Toys
There are 10 million toys that will need to be built by the elves. Each toy is described by an id, the time the order for the toy arrives in Santa's workshop, and the amount of time it takes to build the toy. 
Work on toys cannot start before the order comes in but can start any time after it comes in. Once work on a toy starts, it must continue until the toy is complete, and it must be performed by only one elf. As a result, an elf cannot start work one day, stop, and then resume the next morning, or have a different elf resume the work.
All toys must be completely built for the submission to be valid. Submissions with incomplete toys or where work starts too soon or too late will result in an invalid scoring exception.
Working Conditions
Santa's Workshop opens for the year on January 1, 2014 at 9:00 am North Pole Time. Sanctioned elf working hours are every day, 7 days a week, from 9:00 to 19:00 (10 hours per day). Work outside of these hours are unsanctioned and penalized.
Every minute worked during unsanctioned hours must be compensated with a rest period of equivalent time during sanctioned hours. If an elf works from 14:00-19:33, the next time he can work is the following day at 9:33. Thus 33 minutes overtime results in 33 minutes rest time. Submissions that have elves returning to work before the appropriate amount of rest time has passed will result in an invalid scoring exception. An elf with no accrued resting period may start work at any time.
Elves
There are 900 elves in Santa's Workshop. Each elf is described by
id: an integer from 1 to 900
productivity rating: a double ranging from 0.25 to 4.0, with starting value 1.0
An elf's productivity rating determines how efficiently he builds a toy. A productivity rating of 1.0 means a 120-min toy takes 120 minutes to build. A 1.25-rating means a 120-min toy takes him only 120-min/1.25 = 96 minutes to build. Minimum and maximum values for the productivity rating are 0.25 and 4.0, respectively. All elves start the year with a productivity rating of 1.0.
An elf’s productivity rating changes as he completes toys. Ratings are held constant during the building of a toy and updated once the toy is complete. The rating is calculated per the required time for a toy, not per the time he spends on a toy. The time used in the productivity calculation will be toy_duration/elf_starting_productivity, e.g.: a 0.5-elf working on a 120-min toy uses 240 minutes in his productivity calculation. If a 1.0-rated elf is assigned to work on a 120-min toy for 180 minutes, his productivity rating will only take into account the 120 minutes of needed work. For each hour worked outside of the sanctioned hours, the rest period will apply (see Working Conditions described above). 
For every hour worked on actively building a toy during sanctioned work hours, an elf's productivity increases as
\[p = p' * (1.02)^n\]
where p is the updated productivity, p' is the previous productivity, and n is the number of hours (not minutes, can be a decimal value) worked that contributed to the building of the toy.
Work performed during unsanctioned hours decrease an elf's productivity:
\[p = p' * (0.9)^m\]
where m is the number of hours (not minutes, can be a decimal value) worked that contributed to the building of the toy.
In practice, the productivity is updated in a single step once work is over as
\[p = p' * (1.02)^n * (0.9)^m\]
Acknowledgements
This competition is brought to you by ","The best submissions will minimize the value of the objective function, \\(S\\), which is given by
\[S = t_f * \log(1 + n_e) \]
where
\\(t_f\\) is the last minute the final toy is complete, from reference date January 1, 2014 12:00 AM
\\(n_e\\) is the number of unique elves that were needed to build the toys
As with all optimization problems, scoring will ensure that all of the constraints have been met. In particular:
All presents must be complete. Be careful in matching the productivity rating of a elf with the toy he/she is building and the time you give him/her to build it. Incomplete toys result in an invalid submission.
Work building a toy cannot begin before the order arrives.
Job schedules must honor the resting period for all time spent working during non-sanctioned hours. An elf starting work on the next two before the resting period is complete results in an invalid submission.
Submission Instructions
The submission file specifies, for each toy, which elf started work, when the work started, and how long the work lasted (in minutes). Start_time is formatted as YYYY MM DD HH MM (e.g., 2014 02 24 15 30 is February 24th, 2014 at 15h30, or 3:30 PM). Note that leading zeros are not necessary (both '05' and '5' work for May).
Note: The submission file is read sequentially, from the top down, processing the lines in order. List the rows in your file to match the chronology of when toys are built. There is no way to go ""backwards"" in time with this scoring scheme.
ToyId,ElfId,StartTime,Duration
1,1,2014 1 1 9 45,120
2,2,2014 1 10 13 47,43
3,3,2014 2 28 16 35,125
4,1,2014 3 8 12 10,173
...","Data Files
toys_rev2: List of 10 million toys to be built by Santa's elves, described in more detail below
sampleSubmission_rev2: an example submission file in the correct format.
Sample code for the evaluation metric and sample submission
Data Fields
ToyId: integers from 1, ..., 10000000
Arrival_time: Date and time a toy order arrives at the Workshop, formatted as 'YYYY MM DD HH MM'. For example '2014 3 29 16 52' is March 29, 2014 at 4:52 PM North Pole time. Leading zeros (e.g., 05 for May) are not needed.
Duration: time needed to build a toy, in units of minutes
ToyId,Arrival_time,Duration
1,2014 1 1 0 30,803
2,2014 1 1 9 0,10
3,2014 1 1 12 0,34
4,2014 1 2 3 30,7384
5,2014 1 3 12 45,55",kaggle competitions download -c helping-santas-helpers,[]
419,"In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.
For this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.","Submissions are evaluated using the Logarithmic Loss (smaller is better).
Submission Format
The submissions should contain the predicted probability of click for each ad impression in the test set using the following format:
id,click
60000000,0.384
63895816,0.5919
759281658,0.1934
895936184,0.9572
...","File descriptions
train - Training set. 10 days of click-through data, ordered chronologically. Non-clicks and clicks are subsampled according to different strategies.
test - Test set. 1 day of ads to for testing your model predictions. 
sampleSubmission.csv - Sample submission file in the correct format, corresponds to the All-0.5 Benchmark.
Data fields
id: ad identifier
click: 0/1 for non-click/click
hour: format is YYMMDDHH, so 14091123 means 23:00 on Sept. 11, 2014 UTC.
C1 -- anonymized categorical variable
banner_pos
site_id
site_domain
site_category",kaggle competitions download -c avazu-ctr-prediction,"['https://www.kaggle.com/code/akishen74/ctr-practice', 'https://www.kaggle.com/code/sharanharsoor/ctr-analysis-of-different-ml-models', 'https://www.kaggle.com/code/leejunseok97/deepfm-deepctr-torch', 'https://www.kaggle.com/code/warazubairkhan/mobile-advertisement-ctr', 'https://www.kaggle.com/code/gauravduttakiit/data-sampling']"
420,"West Nile virus is most commonly spread to humans through infected mosquitos. Around 20% of people who become infected with the virus develop symptoms ranging from a persistent fever, to serious neurological illnesses that can result in death.
In 2002, the first human cases of West Nile virus were reported in Chicago. By 2004 the City of Chicago and the Chicago Department of Public Health (CDPH) had established a comprehensive surveillance and control program that is still in effect today.
Every week from late spring through the fall, mosquitos in traps across the city are tested for the virus. The results of these tests influence when and where the city will spray airborne pesticides to control adult mosquito populations.
Given weather, location, testing, and spraying data, this competition asks you to predict when and where different species of mosquitos will test positive for West Nile virus. A more accurate method of predicting outbreaks of West Nile virus in mosquitos will help the City of Chicago and CPHD more efficiently and effectively allocate resources towards preventing transmission of this potentially deadly virus. 
We've jump-started your analysis with some visualizations and starter code in R and Python on Kaggle Scripts. No data download or local environment setup needed!
Acknowledgements 
This competition is sponsored by the Robert Wood Johnson Foundation. Data is provided by the Chicago Department of Public Health.","Submissions are evaluated on area under the ROC curve between the predicted probability that West Nile Virus is present and the observed outcomes.
Submission File
For each record in the test set, you should predict a real-valued probability that WNV is present. The file should contain a header and have the following format:
Id,WnvPresent
1,0
2,1
3,0.9
4,0.2
etc.","Ready to explore the data? Kaggle Scripts is the most frictionless way to get familiar with the competition dataset! No data download needed to start publishing and forking code in R and Python. It's already pre-loaded with our favorite packages and ready for you to start competing!
Data Description
In this competition, you will be analyzing weather data and GIS data and predicting whether or not West Nile virus is present, for a given time, location, and species. 
Every year from late-May to early-October, public health workers in Chicago setup mosquito traps scattered across the city. Every week from Monday through Wednesday, these traps collect mosquitos, and the mosquitos are tested for the presence of West Nile virus before the end of the week. The test results include the number of mosquitos, the mosquitos species, and whether or not West Nile virus is present in the cohort. 
Main dataset
These test results are organized in such a way that when the number of mosquitos exceed 50, they are split into another record (another row in the dataset), such that the number of mosquitos are capped at 50. 
The location of the traps are described by the block number and street name. For your convenience, we have mapped these attributes into Longitude and Latitude in the dataset. Please note that these are derived locations. For example, Block=79, and Street= ""W FOSTER AVE"" gives us an approximate address of ""7900 W FOSTER AVE, Chicago, IL"", which translates to (41.974089,-87.824812) on the map.",kaggle competitions download -c predict-west-nile-virus,"['https://www.kaggle.com/code/abhishek/vote-me-up', 'https://www.kaggle.com/code/mennotaanman/motion', 'https://www.kaggle.com/code/vascovv/west-nile-heatmap', 'https://www.kaggle.com/code/fchollet/keras-deep-net-starter-code', 'https://www.kaggle.com/code/mlandry/h2o-starter']"
421,"This BCI Challenge is being proposed as part of the IEEE Neural Engineering Conference (NER2015). Participation is open without restriction, and the winners will be selected among participants who have submitted an abstract to the conference (see rules).
Problem Description
As humans think, we produce brain waves. These brain waves can be mapped to actual intentions. In this competition, you are given the brain wave data of people with the goal of spelling a word by only paying attention to visual stimuli. The goal of the competition is to detect errors during the spelling task, given the subject's brain waves. 
The Setup
The “P300-Speller” is a well-known brain-computer interface (BCI) paradigm which uses Electroencephalography (EEG) and the so-called P300 response evoked by rare and attended stimuli in order to select items displayed on a computer screen. In this experiment, each subject was presented with letters and numbers (36 possible items displayed on a matrix) to spell words. Each item of a word is selected one at a time, by flashing screen items in group and in random order. The selected item is the one for which the online algorithm could most likely recognize the typical target response.
The goal of this challenge is to determine when the selected item is not the correct one by analyzing the brain signals after the subject received feedback.
Experimental Design
For each participant, a prototypical target response was learned from a short calibration session prior to the test sessions. In test sessions, the spelling performance is highly dependent upon the subject’s attentional effort towards the target item and his/her simultaneous effort to ignore the flashes of the irrelevant items. Since subjects' attention might fluctuate, performance does too (e.g. over time, with fatigue). Two copy-spelling conditions were used, corresponding to short and long trials, respectively:
A fast mode, more error-prone condition (each item was flashed 4 times);
A slower, less error-prone one (each item was flashed 8 times).
At each trial, after the last flash, the subject was instructed to keep looking at the screen and wait for the feedback. The feedback consisted in the selected item, displayed in the middle of the screen in large font. Even if the feedback was incorrect, the subject was asked to then look at the next target.
Download sample video

Twenty-six healthy subjects took part in this study (13 male, mean age = 28.8±5.4 (SD), range 20-37). All subjects reported normal or corrected-to-normal vision and had no previous experience with the P300-Speller paradigm or any other BCI application. Subject’s brain activity was recorded with 56 passive Ag/AgCl EEG sensors (VSM-CTF compatible system) whose placement followed the extended 10-20 system. Their signals were sampled at 600 Hz and were all referenced to the nose. The ground electrode was placed on the shoulder and impedances were kept below 10 kΩ.
The subjects had to go through five copy spelling sessions. Each session consisted of twelve 5-letter words, except the fifth which consisted of twenty 5-letter words.
Objective
In this paradigm and BCI in general, at least in situations where a discrete feedback can be presented to the user, the EEG evoked response to the feedback can be recorded and processed online in order to evaluate whether the item selection was correct or not. This decision, if reliable, could then be used to improve the BCI performance by implementing some error correction strategy. One possible strategy for online error detection and correction has been proposed in Perrin et al. 2012. Most of the data for this competition come from this study and this paper should be cited whenever the competition data will be used and results reported.
In this competition, participants are asked to submit an Error Potential detection algorithm, capable of detecting the erroneous feedbacks online and to generalize across subjects (transfer learning).
Perrin, M., Maby, E., Daligault, S., Bertrand, O., & Mattout, J. Objective and subjective evaluation of online error correction during P300-based spelling. Advances in Human-Computer Interaction, 2012, 4. (link)
Acknowledgements
This competition is brought to you by","Submissions are judged on area under the ROC curve. 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
Submission Format
Each line of your submission should contain an Id and a Class prediction. Your submission file must have a header row.  The format looks like this:
IdFeedBack,Prediction
S01_Sess01_FB001,0.481413
S01_Sess01_FB002,0.95924
S01_Sess01_FB003,0.461558
S01_Sess01_FB004,0.0054562
etc ...","Data description
Data are downsampled at 200 Hz.
Subject’s brain activity was recorded with 56 passive EEG sensors whose placement followed the extended 10-20 system.
Eye movements are detected by EOG derivation.
File descriptions
ChannelsLocation.csv: information for each channel to a topographical representation of multichannel EEG.
train.zip: Training set made of 16 subjects who had gone through 5 sessions, for a total of 80 Data_S*_Sess*.csv files. 60 feedbacks were provided in each session except the fifth one for which 100 feedbacks were provided.
TrainLabels.csv: the expected labels  for the training set.
test.zip: Test set made of 10 other subjects who had gone through 5 sessions, for a total of 50 Data_S*_Sess*.csv files
SampleSubmission.csv: Sample submission file in the correct format
Data fields",kaggle competitions download -c inria-bci-challenge,[]
422,"The taxi industry is evolving rapidly. New competitors and technologies are changing the way traditional taxi services do business. While this evolution has created new efficiencies, it has also created new problems. 
One major shift is the widespread adoption of electronic dispatch systems that have replaced the VHF-radio dispatch systems of times past. These mobile data terminals are installed in each vehicle and typically provide information on GPS localization and taximeter state. Electronic dispatch systems make it easy to see where a taxi has been, but not necessarily where it is going. In most cases, taxi drivers operating with an electronic dispatch system do not indicate the final destination of their current ride.
Another recent change is the switch from broadcast-based (one to many) radio messages for service dispatching to unicast-based (one to one) messages. With unicast-messages, the dispatcher needs to correctly identify which taxi they should dispatch to a pick up location. Since taxis using electronic dispatch systems do not usually enter their drop off location, it is extremely difficult for dispatchers to know which taxi to contact. 
To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict the final destination of a taxi while it is in service. Particularly during periods of high demand, there is often a taxi whose current ride will end near or exactly at a requested pick up location from a new rider. If a dispatcher knew approximately where their taxi drivers would be ending their current rides, they would be able to identify which taxi to assign to each pickup request.
The spatial trajectory of an occupied taxi could provide some hints as to where it is going. Similarly, given the taxi id, it might be possible to predict its final destination based on the regularity of pre-hired services. In a significant number of taxi rides (approximately 25%), the taxi has been called through the taxi call-center, and the passenger’s telephone id can be used to narrow the destination prediction based on historical ride data connected to their telephone id.
In this challenge, we ask you to build a predictive framework that is able to infer the final destination of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the final trip's destination (WGS84 coordinates).
This is the first of two data science challenges that share the same dataset. The Taxi Service Trip Time competition predicts the total time of taxi rides.
This competition is affiliated with the organization of ECML/PKDD 2015.","The evaluation metric for this competition is the Mean Haversine Distance. The Haversine Distance is commonly used in navigation. It measures distances between two points on a sphere based on their latitude and longitude.
The Harvesine Distance between the two locations can be computed as follows
\[a=sin^2 \left( \frac{\phi_2-\phi_1}{2} \right)+cos \left( \phi_1 \right)cos \left(\phi_2 \right)sin^2 \left(\frac{\lambda_2-\lambda_1}{2} \right)\]
\[d=2 \cdot r \cdot atan \left( \sqrt{ \frac{a}{1-a}} \right) \]
where \\( \phi \\) is the latitude, \\( \lambda \\) is the longitude, 
\\( d\\) is the distance between two points, and \\( r \\) is the sphere's radius, 
In our case, it should be replaced by the Earth's radius in the desired metric (e.g., 6371 kilometers).
Submission Format
For every trip in the dataset, submission files should contain three columns: TRIP_ID, LATITUDE, and LONGITUDE. TRIP_ID represents the ID of the trip for which you are predicting the destination (i.e. a string). The LATITUDE/LONGITUDE represent the location's coordinates (using WGS84 format) of your predicted destination
The file should contain a header and have the following format:
TRIP_ID, LATITUDE, LONGITUDE
T1, 41.146504,-8.611317
T2, 42.230000,-8.629454
T10, 42.110000,-8.721111
etc.","I. Training Dataset

We have provided an accurate dataset describing a complete year (from 01/07/2013 to 30/06/2014) of the trajectories for all the 442 taxis running in the city of Porto, in Portugal (i.e. one CSV file named ""train.csv""). These taxis operate through a taxi dispatch central, using mobile data terminals installed in the vehicles. We categorize each ride into three categories: A) taxi central based, B) stand-based or C) non-taxi central based. For the first, we provide an anonymized id, when such information is available from the telephone call. The last two categories refer to services that were demanded directly to the taxi drivers on a B) taxi stand or on a C) random street.
Each data sample corresponds to one completed trip. It contains a total of
9 (nine) features, described as follows:
TRIP_ID: (String) It contains an unique identifier for each trip;
CALL_TYPE: (char) It identifies the way used to demand this service. It may contain one of three possible values:
‘A’ if this trip was dispatched from the central;
‘B’ if this trip was demanded directly to a taxi driver on a specific stand;
‘C’ otherwise (i.e. a trip demanded on a random street).
ORIGIN_CALL: (integer) It contains an unique identifier for each phone number which was used to demand, at least, one service. It identifies the trip’s customer if CALL_TYPE=’A’. Otherwise, it assumes a NULL value;",kaggle competitions download -c pkdd-15-predict-taxi-service-trajectory-i,"['https://www.kaggle.com/code/hochthom/visualization-of-taxi-trip-end-points', 'https://www.kaggle.com/code/hunter0007/trajectory-prediction-ensemble-aws-solution', 'https://www.kaggle.com/code/benhamner/test-trips-map', 'https://www.kaggle.com/code/dahouda/taxi-trajectory-prediction-i', 'https://www.kaggle.com/code/mcwitt/heatmap']"
423,"IMPORTANT NOTE: This competition is only open to students of the MITx free, online course 15.071x - The Analytics Edge.
What makes online news articles popular?
Newspapers and online news aggregators like Google News need to understand which news articles will be the most popular, so that they can prioritize the order in which stories appear. In this competition, you will predict the popularity of a set of New York Times blog articles from the time period September 2014-December 2014.
The following screenshot shows an example of the New York Times technology blog ""Bits"" homepage:
Many blog articles are published each day, and the New York Times has to decide which articles should be featured. In this competition, we challenge you to develop an analytics model that will help the New York Times understand the features of a blog post that make it popular.
To download the data and learn how this competition works, please be sure to read the ""Data"" page, as well as the ""Evaluation"" page, which can both be found in the panel on the left.
Acknowledgements
This competition is brought to you by MITx and edX.","The evaluation metric for this competition is AUC. The AUC, which we described in Unit 3 when we taught logistic regression, is a commonly used evaluation metric for binary classification problems like this one. The interpretation is that given a random positive observation and negative observation, the AUC gives the proportion of the time you guess which is which correctly. It is less affected by sample balance than accuracy. A perfect model will score an AUC of 1, while random guessing will score an AUC of around 0.5.
Submission File
For every observation in the test set, submission files should contain two columns: UniqueID and Probability1. The submission should be a csv file. The UniqueID should just be the corresponding UniqueID column from the test dataset. The Probability1 column should be the predicted probability of the outcome 1 according to your model, for that UniqueID. We have provided an example of a submission file, SampleSubmission.csv, which can be found in the Data section on Kaggle.
As an example of how to generate a submission file in R, suppose that your test set probability predictions are called ""testPred"" and your test data set is called ""test"". Then you can generate a submission file called ""submission.csv"" by running the following two lines of code in R (if you copy and paste these lines of code into R, the quotes around submission.csv might not read properly - please delete and re-type the quotes if you get an error):
submission = data.frame(UniqueID = test$UniqueID, Probability1 = testPred)
write.csv(submission, “submission.csv”, row.names=FALSE)
You should then submit the file ""submission.csv"" by clicking on ""Make a Submission"" on the Kaggle website.
If you take a look at the file ""submission.csv"", you should see that the file contains a header and has the following format:
UniqueID,Probability1
6533,0.279672578 
6534,0.695794648 
6535,0.695794648 
6536,0.279672578 
6537,0.554216867 
6538,0.640816327 
6539,0.695794648
etc.","File Descriptions
The data provided for this competition is split into two files:
NYTimesBlogTrain.csv = the training data set. It consists of 6532 articles.
NYTimesBlogTest.csv = the testing data set. It consists of 1870 articles.  
We have also provided a sample submission file, SampleSubmission.csv. This file gives an example of the format of submission files (see the Evaluation page for more information). The data for this competition comes from the New York Times website.
Variable Descriptions
The dependent variable in this problem is the variable Popular, which labels if an article had 25 or more comments in its online comment section (equal to 1 if it did, and 0 if it did not). The dependent variable is provided in the training data set, but not the testing dataset. This is an important difference from what you are used to - you will not be able to see how well your model does on the test set until you make a submission on Kaggle.
The independent variables consist of 8 pieces of article data available at the time of publication, and a unique identifier:
NewsDesk = the New York Times desk that produced the story (Business, Culture, Foreign, etc.)",kaggle competitions download -c 15-071x-the-analytics-edge-competition-spring-2015,
424,"Elite chess players are rated, ranked, analyzed, and compared in many ways. Classical methods of ranking chess players have focused on game histories, paying particular attention to the relative strength of the players involved. This includes the popular FIDE Elo score, which was the focus of one of Kaggle's first ever competitions - Elo vs. the Rest of the World.
Recent work on chess analysis has focused on intrinsic performance ratings, where one assesses skill based on the quality of decisions rather than the outcomes of games. For an example of this kind of approach, see this draft by Kenneth Regan. Two advantages of an intrinsic approach are an increased sample size (there are many more moves than games) and the ability to approach new challenges, such as determining whether a player is cheating by performing moves above their skill level.
This competition challenges Kagglers to determine players' FIDE Elo ratings at the time a game is played, based solely on the moves in one game. Do a player's moves reflect their absolute skill? Does the opponent matter? How closely does one game reflect intrinsic ability? How well can an algorithm do? Does computational horsepower increase accuracy? Let's find out!
You do not need to be a chess expert -- or even know how to play chess -- to attempt this competition. You do need patience and a computer that doesn't mind some heat. The dataset includes 50,000 games between elite, ranked players. As a getting-started computational bonus, Kaggle has run these games through a chess engine to score each move.
Good luck finding Elo!","Submissions are evaluated using the mean absolute error.
Submission File
For each game in the test set, you should predict the Elo rating of both the black and white player. The file should contain a header and have the following format:
Event,WhiteElo,BlackElo
25001,1000,2000
25002,2000,1500
25003,1746,2200
etc...","A total of 50,000 games are provided in portable game notation (pgn) format. Each game in the training set (the first 25,000 games) has both the white and black Elo rating. Each game in the test set (the latter 25,000 games) omits the Elo ratings, which you must predict. Player ids were scrubbed from the data. The goal is to predict the Elo rating based on only a single game.
Each game has the following format, with the move text in Standard Algebraic Notation (SAN):
[Event ""1""]
[Site ""kaggle.com""]
[Date ""??""]
[Round ""??""]
[White ""??""]
[Black ""??""]
[Result ""1/2-1/2""]
[WhiteElo ""2354""]
[BlackElo ""2411""]
1. Nf3 Nf6 2. c4 c5 3. b3 g6 4. Bb2 Bg7 5. e3 O-O 6. Be2 b6 7. O-O Bb7 8. Nc3 Nc6 9. Qc2 Rc8 10. Rac1 d5 11. Nxd5 Nxd5 12. Bxg7 Nf4 13. exf4 Kxg7 14.Qc3+ Kg8 15. Rcd1 Qd6 16. d4 cxd4 17. Nxd4 Qxf4 18. Bf3 Qf6 19. Nb5 Qxc3
1/2-1/2",kaggle competitions download -c finding-elo,['https://www.kaggle.com/code/stephenfenel/chess-elo-predictor']
425,"In the late 90's, Yann LeCun's team pioneered the successful application of machine learning to optical character recognition. 25 years later, machine learning continues to be an invaluable tool for text processing downstream from the OCR process.
Tradeshift has created a dataset with thousands of documents, representing millions of words. In each document, several bounding boxes containing text are selected. For each piece of text, many features are extracted and certain labels are assigned.
In this competition, participants are asked to create and open source an algorithm that correctly predicts the probability that a piece of text belongs to a given class.","Note: due to the size of the submission file for this competition, submission scoring may take up to 5 minutes. This may cause some browsers to hang as they wait for a response. Be patient! You do not need to resubmit. The file will eventually score and be visible on the leaderboard and your submissions page.
Scoring function
The prediction model \\(f\\) for a given set of parameters \\(\theta\\) generates the predicted probabilities \\(\hat{y}_i = \left \langle \hat{y}_{ij} \right \rangle = f_{\theta}(x_i) \in {[0,1]}^K\\) where each element \\(\hat{y}_{ij}\\) is the probability that the jth-label is true for the ith-sample. The goal of the prediction model is that the expected \\(y_{ij}\\) and predicted \\(\hat{y}_{ij}\\) probabilities have similar values. The metric used to score the performance of the prediction model is the negative logarithm of the likelihood function averaged over Nt test samples and K labels.
$$
\textrm{LogLoss} = \frac{1}{N_{t} \cdot K} \sum_{idx=1}^{N_{t} \cdot K} \textrm{LogLoss}_{idx} \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; 
\; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \;  \\  = \frac{1}{N_{t} \cdot K} \sum_{idx=1}^{N_{t} \cdot K} \left[ - y_{idx} \log(\hat{y}_{idx}) - (1 - y_{idx}) \log(1 - \hat{y}_{idx})\right] \\
 = \frac{1}{N_{t} \cdot K} \sum_{i=1}^{N_{t}} \sum_{j=1}^K \left[ - y_{ij} \log(\hat{y}_{ij}) - (1 - y_{ij}) \log(1 - \hat{y}_{ij})\right]  \; \;  
$$
where \\(log()\\) represents the natural logarithm and \\(idx = (i-1) \cdot K + j\\). The inclusion of the logarithm in the metric function highly penalizes predicted probabilities that are confident and wrong. In the worst case, a prediction of true (1) for an expected false (0) sample adds infinity to the \\(\textrm{LogLoss}\\), \\(-log(0)=\infty\\), which makes a total score of infinity regardless the score for the other samples.
This metric is also symmetric in the sense than predicting 0.1 for a false (0) sample has the same penalty as predicting 0.9 for a positive sample (1). The value is bounded between zero and infinity, i.e. \\(\textrm{LogLoss} \in [0, \infty)\\). The competition corresponds to a minimization problem where smaller metric values, \\(\textrm{LogLoss} \sim 0\\), implie better prediction models. 
More information about the ranking of participants can be found in the Rules Page. In order to avoid infinite values and resolution problems, the predicted probabilities \\(\hat{y}_{ij}\\) are bounded within the range \\([10^{-15},1-10^{-15}]\\).
Submission File
The submitted file must contain only one predicted probability per row. For example, the number in the second line is the predicted probability \\(\hat{y}_{1,2}\\) for the 1st-sample and 2nd-label (1_y2), which is calculated from the features \\(x_1\\) and must be similar to the value of \\(y_{1,2}\\).
In Matlab, the transformation corresponds to:
testLabelsPredicted = [0.9, 0.1, 0.0, 0.3; 0.03, 0.7, 0.2, 0.85; 0.19, 0.0, 1.0, 0.27];
testLabelsSubmitted = reshape(testLabelsPredicted.',[],1);
Besides, the submitted file must contain column and row headers following the format of the sampleSubmission.csv (25MB).
Example
If the testLabels.csv file is (this file is not public):
id_label,pred
1_y1,1.0000
1_y2,0.0000
1_y3,0.0000
1_y4,0.0000
2_y1,0.0000
2_y2,1.0000
2_y3,0.0000
2_y4,1.0000
3_y1,0.0000
3_y2,0.0000
3_y3,1.0000
3_y4,0.0000
and the testLabelsSubmitted.csv file is (this file is submitted by each participant):
id_label,pred
1_y1,0.9000
1_y2,0.1000
1_y3,0.0000
1_y4,0.3000
2_y1,0.0300
2_y2,0.7000
2_y3,0.2000
2_y4,0.8500
3_y1,0.1900
3_y2,0.0000
3_y3,1.0000
3_y4,0.2700
the score is 0.1555
$$
L = - \frac{1}{12} \left [ log(0.9) + log(1-0.1) + log(1-0.0) +log(1-0.3) \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \\ + log(1-0.03) + log(0.7) + log(1-0.2) + log(0.85) \; \; \; \; \; \\ \; \; \; \; \; \; \; \; \; \; \; \; \; \; + log(1-0.19)  + log(1-0.0) + log(1.0) +log(1-0.27) \right ] = 0.1555
$$
In Matlab, the metric calculation corresponds to:
testLabels = [1 0 0 0; 0 1 0 1; 0 0 1 0];
testLabelsPredicted = [0.9, 0.1, 0.0, 0.3; 0.03, 0.7, 0.2, 0.85; 0.19, 0.0, 1.0, 0.27];

testLabels = max(min(reshape(testLabels.',[],1),1-1e-15),1e-15);
testLabelsPredicted = max(min(reshape(testLabelsPredicted.',[],1),1-1e-15),1e-15);

score = -mean((testLabels .* log(testLabelsPredicted) + (1-testLabels) .* log(1-testLabelsPredicted)));","Data extraction
For all the documents, words are detected and combined to form text blocks that may overlap to each other. Each text block is enclosed within a spatial box, which is depicted by a red line in the sketch below. The text blocks from all documents are aggregated in a data set where each text block corresponds to one sample (row).
For example, if we have 3 documents with 34, 62 and 53 text blocks, respectively, the data set will have 149 samples.
Features
For each sample, several features are extracted that are stored in the train.csv and test.csv. The features include content, parsing, spatial and relational information.
Content: The cryptographic hash of the raw text.
Parsing: Indicates if the text parses as number, text, alphanumeric, etc.",kaggle competitions download -c tradeshift-text-classification,"['https://www.kaggle.com/code/jamesmcguigan/tradeshift-profilereport-eda', 'https://www.kaggle.com/code/rohanrao/datatableton-capstone-projects-solutions', 'https://www.kaggle.com/code/tarunaryyan/imputation-for-missing-values-in-features']"
426,"This competition is now complete. Congratulations to the winners!
Millions of programmers use Stack Overflow to get high quality answers to their programming questions every day.  We take quality very seriously, and have evolved an effective culture of moderation to safe-guard it.
With more than six thousand new questions asked on Stack Overflow every weekday we're looking to add more sophisticated software solutions to our moderation toolbox.
Closing Questions
Currently about 6% of all new questions end up ""closed"".  Questions can be closed as off topic, not constructive, not a real question, or too localized.  More in depth descriptions of each reason can be found in the Stack Overflow FAQ.  The exact duplicate close reason has been excluded from this contest, since it depends on previous questions.
Your goal is to build a classifier that predicts whether or not a question will be closed given the question as submitted, along with the reason that the question was closed.  Additional data about the user at question creation time is also available.","Your solution should produce a probability (between 0 and 1) of a given question not being closed and the probabilities of being closed as each of the four close reasons.  The evaluation metric is Multiclass Logarithmic Loss.
Refer to Submission Instructions for instructions on how to format your submissions file.
While live data is available from Stack Overflow, be aware that the final evaluation will use data gathered after all solutions are submitted.","The code for the benchmarks is on Github.
The training data contains data through July 31st UTC, and the public leaderboard data goes from August 1 UTC to August 14 UTC.
The train.csv file contains post text and associated metadata at the time of post creation which will serve as inputs to your solution.  The state of the post as of July 31st is also included. It contains the following fields (not in this order):
Input
PostCreationDate
OwnerUserId
OwnerCreationDate
ReputationAtPostCreation
OwnerUndeletedAnswerCountAtPostTime
Title
BodyMarkdown
Tag1
Tag2",kaggle competitions download -c pycon-2015-tutorial-predict-closed-questions-on-stack-overflow,
427,"Advances in rapid, low cost analysis of soil samples using infrared spectroscopy, georeferencing of soil samples, and greater availability of earth remote sensing data provide new opportunities for predicting soil functional properties at unsampled locations. Soil functional properties are those properties related to a soil’s capacity to support essential ecosystem services such as primary productivity, nutrient and water retention, and resistance to soil erosion. Digital mapping of soil functional properties, especially in data sparse regions such as Africa, is important for planning sustainable agricultural intensification and natural resources management.
Diffuse reflectance infrared spectroscopy has shown potential in numerous studies to provide a highly repeatable, rapid and low cost measurement of many soil functional properties. The amount of light absorbed by a soil sample is measured, with minimal sample preparation, at hundreds of specific wavebands across a range of wavelengths to provide an infrared spectrum (Fig. 1). The measurement can be typically performed in about 30 seconds, in contrast to conventional reference tests, which are slow and expensive and use chemicals.
Conventional reference soil tests are calibrated to the infrared spectra on a subset of samples selected to span the diversity in soils in a given target geographical area. The calibration models are then used to predict the soil test values for the whole sample set. The predicted soil test values from georeferenced soil samples can in turn be calibrated to remote sensing covariates, which are recorded for every pixel at a fixed spatial resolution in an area, and the calibration model is then used to predict the soil test values for each pixel. The result is a digital map of the soil properties.
This competition asks you to predict 5 target soil functional properties from diffuse reflectance infrared spectroscopy measurements.
Acknowledgements
This competition is sponsored by the Africa Soil Information Service.","Submissions are scored on MCRMSE (mean columnwise root mean squared error):
$$
\textrm{MCRMSE} = \frac{1}{5}\sum_{j=1}^{5}\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_{ij} - \hat{y}_{ij})^2},
$$
where \\(y\\) and \\(\hat{y}\\) are the actual and predicted values, respectively.
Submission File
For each row in the dataset, the submission file should contain an identifier column (PIDN) and 5 prediction columns: Ca, P, pH, SOC, and Sand. PIDN, the sample identifier, should be copied from the first column of test data file. Ca, P, pH, SOC, and Sand are soil properties whose values you must predict.
The file should contain a header and have the following format:
PIDN,Ca,P,pH,SOC,Sand
XNhoFZW5,1.3,3.1,2.0,1.5,4.8","File descriptions
train.csv - the training set has 1158 rows.
test.csv - the test set has 728 rows.
sample_submission.csv - all zeros prediction, serving as a sample submission file in the correct format.
Data fields
SOC, pH, Ca, P, Sand are the five target variables for predictions. The data have been monotonously transformed from the original measurements and thus include negative values. 
PIDN: unique soil sample identifier
SOC: Soil organic carbon
pH: pH values
Ca: Mehlich-3 extractable Calcium
P: Mehlich-3 extractable Phosphorus
Sand: Sand content 
m7497.96 - m599.76: There are 3,578 mid-infrared absorbance measurements. For example, the ""m7497.96"" column is the absorbance at wavenumber 7497.96 cm-1. We suggest you to remove spectra CO2 bands which are in the region m2379.76 to m2352.76, but you do not have to.",kaggle competitions download -c afsis-soil-properties,"['https://www.kaggle.com/code/chachiawacef/africa-soil-property-prediction-0-6-mse', 'https://www.kaggle.com/code/datubadiver/notebookd328f54f81']"
428,"Seizure forecasting systems hold promise for improving the quality of life for patients with epilepsy.
Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective -- and even after surgical removal of epilepsy-causing brain tissue, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring.
Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for EEG-based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.
There is emerging evidence that the temporal dynamics of brain activity can be classified into 4 states: Interictal (between seizures, or baseline), Preictal (prior to seizure), Ictal (seizure), and Post-ictal (after seizures). Seizure forecasting requires the ability to reliably identify a preictal state that can be differentiated from the interictal, ictal, and postictal state. The primary challenge in seizure forecasting is differentiating between the preictal and interictal states. The goal of the competition is to demonstrate the existence and accurate classification of the preictal brain state in dogs and humans with naturally occurring epilepsy.
The Competition
Intracranial EEG was recorded from dogs with naturally occurring epilepsy using an ambulatory monitoring system. EEG was sampled from 16 electrodes at 400 Hz, and recorded voltages were referenced to the group average. These are long duration recordings, spanning multiple months up to a year and recording up to a hundred seizures in some dogs.
In addition, datasets from patients with epilepsy undergoing intracranial EEG monitoring to identify a region of brain that can be resected to prevent future seizures are included in the contest. These datasets have varying numbers of electrodes and are sampled at 5000 Hz, with recorded voltages referenced to an electrode outside the brain. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity. Seizures are known to cluster, or occur in groups. Patients who typically have seizure clusters receive little benefit from forecasting follow-on seizures. For this contest only lead seizures, defined here as seizures occurring four hours or more after another seizure, are included in the training and testing data sets. In order to avoid any potential contamination between interictal, preictal, and post-ictal EEG signals interictal segments in the canine training and test data were restricted to be at least one week before or after any seizure. In the human data, where the entire monitoring session may last less than one week, interictal data segments were restricted to be at least four hours before or after any seizure. Interictal data segments were chosen at random within these restrictions for both canine and human subjects.
Participants are invited to visit the NIH-sponsored International Epilepsy Electrophysiology portal (http://ieeg.org) to review and download annotated interictal and preictal data from other patients and animal subjects. Using ieeg.org data for additional algorithm training is permitted.
Acknowledgements
This competition is sponsored by the National Institutes of Health (NINDS), the Epilepsy Foundation, and the American Epilepsy Society.
  References
Howbert JJ, Patterson EE, Stead SM, Brinkmann B, Vasoli V, Crepeau D, Vite CH, Sturges B, Ruedebusch V, Mavoori J, Leyde K, Sheffield WD, Litt B, Worrell GA (2014) Forecasting seizures in dogs with naturally occurring epilepsy. PLoS One 9(1):e81920.
Cook MJ, O'Brien TJ, Berkovic SF, Murphy M, Morokoff A, Fabinyi G, D'Souza W, Yerra R, Archer J, Litewka L, Hosking S, Lightfoot P, Ruedebusch V, Sheffield WD, Snyder D, Leyde K, Himes D (2013) Prediction of seizure likelihood with a long-term, implanted seizure advisory system in patients with drug-resistant epilepsy: a first-in-man study. LANCET NEUROL 12:563-571.
Park Y, Luo L, Parhi KK, Netoff T (2011) Seizure prediction with spectral power of EEG using cost-sensitive support vector machines. Epilepsia 52:1761-1770.
Davis KA, Sturges BK, Vite CH, Ruedebusch V, Worrell G, Gardner AB, Leyde K, Sheffield WD, Litt B (2011) A novel implanted device to wirelessly record and analyze continuous intracranial canine EEG. Epilepsy Res 96:116-122.
Andrzejak RG, Chicharro D, Elger CE, Mormann F (2009) Seizure prediction: Any better than chance? Clin Neurophysiol.
Snyder DE, Echauz J, Grimes DB, Litt B (2008) The statistics of a practical seizure warning system. J Neural Eng 5: 392–401.
Mormann F, Andrzejak RG, Elger CE, Lehnertz K (2007) Seizure prediction: the long and winding road. Brain 130: 314–333.
Haut S, Shinnar S, Moshe SL, O'Dell C, Legatt AD. (1999) The association between seizure clustering and status epilepticus in patients with intractable complex partial seizures. Epilepsia 40:1832–1834.","Submissions are judged on the area under the ROC curve (AUC).
Submission File
For each clip in the test set, you should predict a real-valued probability that a given clip is preictal. The file should contain a header and have the following format:
clip,preictal
Dog_1_test_segment_0001.mat,0
Dog_1_test_segment_0002.mat,0
Dog_1_test_segment_0003.mat,0
etc.","Intracranial EEG (iEEG) data clips are organized in folders containing training and testing data for each human or canine subject. The training data is organized into ten minute EEG clips labeled ""Preictal"" for pre-seizure data segments, or ""Interictal"" for non-seizure data segments. Training data segments are numbered sequentially, while testing data are in random order. Within folders data segments are stored in .mat files as follow:
preictal_segment_N.mat - the Nth preictal training data segment
interictal_segment_N.mat - the Nth non-seizure training data segment
test_segment_N.mat - the Nth testing data segment
Each .mat file contains a data structure with fields as follow:
data: a matrix of EEG sample values arranged row x column as electrode x time.
data_length_sec: the time duration of each data row
sampling_frequency: the number of data samples representing 1 second of EEG data.
channels: a list of electrode names corresponding to the rows in the data field
sequence: the index of the data segment within the one hour series of clips. For example, preictal_segment_6.mat has a sequence number of 6, and represents the iEEG data from 50 to 60 minutes into the preictal data.
Preictal training and testing data segments are provided covering one hour prior to seizure with a five minute seizure horizon. (i.e. from 1:05 to 0:05 before seizure onset.) This pre-seizure horizon ensures that 1) seizures could be predicted with enough warning to allow administration of fast-acting medications, and 2) any seizure activity before the annotated onset that may have been missed by the epileptologist will not affect the outcome of the competition.",kaggle competitions download -c seizure-prediction,"['https://www.kaggle.com/code/ahernandez1/american-epilepsy-society-seizure-a-playground', 'https://www.kaggle.com/code/mpwolke/epilepsy-mat-file', 'https://www.kaggle.com/code/nizarislah/american-epilepsy-society-seizure-prediction', 'https://www.kaggle.com/code/kiran3799/epilepsy-prediction-notebook', 'https://www.kaggle.com/code/m0nika/seizurecnn']"
429,"IMPORTANT NOTE: This competition is only open to students of 15.071x - The Analytics Edge (https://www.edx.org/course/analytics-edge-mitx-15-071x)
What makes online news articles popular?
Newspapers and online news aggregators like Google News need to understand which news articles will be the most popular, so that they can prioritize the order in which stories appear. In this competition, you will predict the popularity of a set of New York Times blog articles from the time period September 2014-December 2014. 
The following screenshot shows an example of the New York Times technology blog ""Bits"" homepage:
Many blog articles are published each day, and the New York Times has to decide which articles should be featured. In this competition, we challenge you to develop an analytics model that will help the New York Times understand the features of a blog post that make it popular. 
To download the data and learn how this competition works, please be sure to read the ""Data"" page, as well as the ""Evaluation"" page, which can both be found in the panel on the left.
Acknowledgements
This competition is brought to you by MITx and edX.","The evaluation metric for this competition is AUC. The AUC, which we described in Unit 3 when we taught logistic regression, is a commonly used evaluation metric for binary classification problems like this one. The interpretation is that given a random positive observation and negative observation, the AUC gives the proportion of the time you guess which is which correctly. It is less affected by sample balance than accuracy. A perfect model will score an AUC of 1, while random guessing will score an AUC of around 0.5.
Submission File
For every observation in the test set, submission files should contain two columns: UniqueID and Probability1. The submission should be a csv file. The UniqueID should just be the corresponding UniqueID column from the test dataset. The Probability1 column should be the predicted probability of the outcome 1 according to your model, for that UniqueID. We have provided an example of a submission file, SampleSubmission.csv, which can be found in the Data section on Kaggle.
As an example of how to generate a submission file in R, suppose that your test set probability predictions are called ""testPred"" and your test data set is called ""test"". Then you can generate a submission file called ""submission.csv"" by running the following two lines of code in R (if you copy and paste these lines of code into R, the quotes around submission.csv might not read properly - please delete and re-type the quotes if you get an error):
submission = data.frame(UniqueID = test$UniqueID, Probability1 = testPred)
write.csv(submission, “submission.csv”, row.names=FALSE)
You should then submit the file ""submission.csv"" by clicking on ""Make a Submission"" on the Kaggle website.
If you take a look at the file ""submission.csv"", you should see that the file contains a header and has the following format:
UniqueID,Probability1
6533,0.279672578 
6534,0.695794648 
6535,0.695794648 
6536,0.279672578 
6537,0.554216867 
6538,0.640816327 
6539,0.695794648
etc.","File Descriptions
The data provided for this competition is split into two files:
NYTimesBlogTrain.csv = the training data set. It consists of 6532 articles.
NYTimesBlogTest.csv = the testing data set. It consists of 1870 articles.  
We have also provided a sample submission file, SampleSubmission.csv. This file gives an example of the format of submission files (see the Evaluation page for more information). The data for this competition comes from the New York Times website.
Variable Descriptions
The dependent variable in this problem is the variable Popular, which labels if an article had 25 or more comments in its online comment section (equal to 1 if it did, and 0 if it did not). The dependent variable is provided in the training data set, but not the testing dataset. This is an important difference from what you are used to - you will not be able to see how well your model does on the test set until you make a submission on Kaggle.
The independent variables consist of 8 pieces of article data available at the time of publication, and a unique identifier:
NewsDesk = the New York Times desk that produced the story (Business, Culture, Foreign, etc.)",kaggle competitions download -c 15-071x-the-analytics-edge-spring-20152,
430,"This competition is designed to help you get started with Julia. If you are looking for a good programming language for data science, or if you are already accustomed to one language, we encourage you to also try Julia. Julia is a relatively new language for technical computing that attempts to combine the strengths of other popular programming languages. 
Here we introduce two tutorials to highlight some of Julia's features. The first is focused on the basics of the language. In the second, a complete implementation of the K Nearest Neighbor algorithm is presented, highlighting features such as parallelization and speed. 
Both tutorials show that it is easy to write code in Julia, due to its intuitive syntax and design. The tutorials also describe some basics of image processing and some concepts of machine learning such as cross validation. After reviewing them, we hope you will be motivated to write your own machine learning algorithms in Julia.
This tutorial focuses on the task of identifying characters from Google Street View images. It differs from traditional character recognition because the data set contains different character fonts and the background is not the same for all images. 
Acknowledgements
The data was taken from the Chars74K dataset, which consists of images of characters selected from Google Street View images. We ask that you cite the following reference in any publication resulting from your work:
T. E. de Campos, B. R. Babu and M. Varma, Character recognition in natural images, Proceedings of the International Conference on Computer Vision Theory and Applications (VISAPP), Lisbon, Portugal, February 2009.
This tutorial was developed by Luis Tandalla during his summer 2014 internship at Kaggle.","Your model should identify the character in each image in the test set. The possible characters are 'A-Z', 'a-z', and '0-9'. 
The predictions will be evaluated using Classification Accuracy.
\[  \textrm{Accuracy} =\frac{  \sum_{i=1}^N \textrm{true}_i = \textrm{prediction}_i  }{N} \]
Submission File
For every image in the dataset, submission files should contain two columns: ImageId and Class (character predicted) . 
The file should contain a header and have the following format:
ImageId,Class
6284,A
6285,b
6286,0
...","train.zip and test.zip - Bmp images of characters taken from Google Street View pictures. 
trainResized.zip and testResized.zip - Same images appearing in train.zip and test.zip, but resized to 20x20 pixels.
trainLabels.csv - Class label for each picture in the training data.
sampleSubmission.csv - IDs of the images in the test data, and an example submission file in the correct format.
resizeData.py - Code used to resize the images.
source-code-files.zip - Contains all the source code described in the tutorials. There is also a ipython-iJulia version for each file.",kaggle competitions download -c street-view-getting-started-with-julia,"['https://www.kaggle.com/code/jazivxt/and-the-winner-is-spacy-render', 'https://www.kaggle.com/code/francescoliveras/julia-en-es-prediction', 'https://www.kaggle.com/code/vandan20/ocr-using-different-methods-in-python', 'https://www.kaggle.com/code/andreibakulin/second-attempt-2', 'https://www.kaggle.com/code/martinhaha/fastai-cv']"
431,"Avito.ru is the largest general classified website in Russia that helps connect buyers with sellers across all Russian territories. There are more than 22 million active ads on Avito and each day a huge number of ads are added or modified. The efficiency of Avito depends heavily on the content quality -- when buyers can quickly find relevant content, sellers can sell their items in hours.
The larger and more popular Avito becomes the more attractive it becomes to sell illicit items or services. Some items that people try to sell are completely illegal while others might seem allowable but are still prohibited by our rules. This is why all new or modified ads are thoroughly moderated by our team of human moderators. The moderators can remove the ad if it conflicts with the Russian legislation or with the internal rules of AVITO.ru. However, with our growth it becomes more and more challenging to thoroughly moderate all ads. This is where machine learning comes into play.
The objective of this challenge is to create a predictive model that will learn from moderators' answers how to classify if an ad contains illicit content or not.","This competition uses Average Precision at k (AP@k) as the evaluation metric. It is calculated as:
\[AP@K = \frac{\sum_{i=1}^K P@i}{K} \]
where
\[ P@i = \frac{ \text{#relevant - forbidden ads retrieved in the first ith places} }{i} \]
Submission File
The submission file should contain a header row and be a single column of ad id's, giving 1,351,243 rows in total (including the header row). Ad ids are sorted with those ads most likely to contain illicit content at the top and those most likely to not contain illicit content at the bottom.
Id
385 <-- most likely to have illicit content
924691
85893
2719
93752
...
83543
3481
4838 <- least likely to have illicit content","Data for this competition consists mainly of Russian text. All files are encoded in UTF-8 and are in tab separated format (.tsv). To help you transform Russian text into a set of features we have prepared intoductory code, where we recommend which modules in Python to use.
Also note that uncompressed training and test data together take ~4GB of space.
Training and Test data sets consist of individual ads that have either been blocked for illicit content or that have never been blocked. All ads that participate in this competition have already been closed.
Using External Data
External data is allowed in this competition with approval. To gain approval for a data set/source, please post your request on this forum thread.
File descriptions
avio_train.zip - training dataset. Contains both ads descriptions and labels. Ads for training has been sampled from Dec 2013 - Mar 2014.
avio_test.zip - testing dataset. Contains only ads description. Ads for testing has been sampled from Apr 2014.
sampleSubmission.tsv- a sample submission file in the correct format produced by our introductory code.",kaggle competitions download -c avito-prohibited-content,"['https://www.kaggle.com/code/mpwolke/prohibited-content-zip', 'https://www.kaggle.com/code/mpwolke/hunting-prohibited-content']"
432,"A Fortune 100 company, Liberty Mutual Insurance has provided a wide range of insurance products and services designed to meet our customers' ever-changing needs for over 100 years.
Within the business insurance industry, fire losses account for a significant portion of total property losses. High severity and low frequency, fire losses are inherently volatile, which makes modeling them difficult. In this challenge, your task is to predict the target, a transformed ratio of loss to total insured value, using the provided information. This will enable more accurate identification of each policyholder’s risk exposure and the ability to tailor the insurance coverage for their specific operation.
Because we seek to tap innovation both inside and outside the company, certain eligible Liberty Mutual employees are encouraged to participate in this challenge for development purposes. Refer to the competition rules for the full details.","Submissions are evaluated on the normalized, weighted Gini coefficient. The weights used in the calculation are represented by var11 in the dataset.
To calculate the normalized weighted Gini, your predictions are sorted from largest to smallest. This is the only step where the explicit prediction values are used (i.e. only the order of your predictions matters).  We then move from largest to smallest, asking ""In the leftmost x% of the data, how much of the actual observed, weighted loss (the target multiplied by the provided weight) have you accumulated?"" With no model, you expect to accumulate 10% of the loss in 10% of the predictions, so no model (or a ""null"" model) achieves a straight line. The area between your curve and this straight line the Gini coefficient.
There is a maximum achievable area for a perfect model. The normalized Gini is obtained by dividing the weighted Gini coefficient of your model by the weighted Gini coefficient of a perfect model.
Submission File
For each id in the test set, predict the value of the target variable. Your submission file must have a header and should look like the following:
id,target
10,0
11,1.2
14,0.3
...","This data represents almost a million insurance records and the task is to predict a transformed ratio of loss to total insured value (called ""target"" within the data set). The provided features contain policy characteristics, information on crime rate, geodemographics, and weather.
The train and test sets are split randomly. For each id in the test set, you must predict the target using the provided features.
Data Fields
id : A unique identifier of the data set
target : The transformed ratio of loss to total insured value
dummy : Nuisance variable used to control the model, but not working as a predictor
var1 – var17 : A set of normalized variables representing policy characteristics (note: var11 is the weight used in the weighted gini score calculation)
crimeVar1 – crimeVar9: A set of normalized Crime Rate variables
geodemVar1 – geodemVar37 : A set of normalized geodemographic variables
weatherVar1 – weatherVar236 : A set of normalized weather station variables",kaggle competitions download -c liberty-mutual-fire-peril,['https://www.kaggle.com/code/sivavedantam/liberty-mutual-project']
433,"Schizophrenia is a severe and disabling mental illnesses which has no well-established, non-invasive diagnosis biomarker. Currently, due to its symptom overlap with other mental illnesses (like bipolar disorder) it can only be diagnosed subjectively, by process of elimination.
This competition invites you to automatically diagnose subjects with schizophrenia based on multimodal features derived from their brain magnetic resonance imaging (MRI) scans.
The features made available in this competition are a result from current state-of-the art developments in neuroimaging and MRI data processing. Two modalities of MRI scans are used to obtain these features: functional and structural MRI. One challenge in this competition is how to optimally combine this type of multimodal information and select features that enhance diagnosis. Optional additional information is provided that could be helpful with this particular aspect of the task.
This is an official competition of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2014)
Acknowledgements
Collection of this dataset was made at the Mind Research Network under an NIH NIGMS Centers of Biomedical Research Excellence (COBRE) grant 5P20RR021938/P20GM103472 to Vince Calhoun (PI).","Submissions are judged on area under the ROC curve. 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
Submission Format
Each line of your submission should contain an Id and a Class prediction. Your submission file must have a header row.  The format looks like this:
Id,Probability
1,0.481413
2,0.95924
3,0.461558
4,0.0054562
etc ...","Competition Files
Train.zip:
train_labels.csv - Labels for the training set. The labels are indicated in the ""Class"" column. 0 = 'Healthy Control', 1 = 'Schizophrenic Patient'
train_FNC.csv - FNC features for the training set. These are correlation values. They describe the connection level between pairs of brain maps over time.
train_SBM.csv - SBM features for the training set. These are standardized weights. They describe the expression level of ICA brain maps derived from gray-matter concentration.
Test.zip:
test_FNC.csv - FNC features for the test set. Test subject labels have been removed. Your task is to predict these unknown labels from the provided features.
test_SBM.csv - SBM features for the test set. Test subject labels have been removed.
To discourage certain forms of cheating (such as hand labeling) we have inflated the number of rows in the test set to create a much larger data sample. The extra rows will be ignored in the scoring and their presence will not affect the scoring. Your submission file must provide a prediction for each subject in the test set.",kaggle competitions download -c mlsp-2014-mri,[]
434,"Get started on this competition through Kaggle Scripts
In machine learning, it is often said there are no free lunches. How wrong we were.
This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.
""I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,"" says one hopeful poster. What about making an algorithm?
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This data was collected and graciously shared by Althoff et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work:
Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014.
Pizza icons designed by Matthew Dera from the Noun Project","Submissions are evaluated on area under the ROC curve between the predicted probability that a request will get pizza and the observed outcomes.
Submission File
For each request in the test set, you should predict a real-valued probability that it resulted in a pizza. The file should contain a header and have the following format:
request_id,requester_received_pizza
t3_i8iy4,0
t3_1mfqi0,0
t3_lclka,0
...
...","See, fork, and run a random forest benchmark model through Kaggle Scripts
This dataset includes 5671 requests collected from the Reddit community Random Acts of Pizza between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.
Each JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.
Data fields
""giver_username_if_known"": Reddit username of giver if known, i.e. the person satisfying the request (""N/A"" otherwise).
""number_of_downvotes_of_request_at_retrieval"": Number of downvotes at the time the request was collected.
""number_of_upvotes_of_request_at_retrieval"": Number of upvotes at the time the request was collected.
""post_was_edited"": Boolean indicating whether this post was edited (from Reddit).
""request_id"": Identifier of the post on Reddit, e.g. ""t3_w5491"".
""request_number_of_comments_at_retrieval"": Number of comments for the request at time of retrieval.",kaggle competitions download -c random-acts-of-pizza,"['https://www.kaggle.com/code/alvations/basic-nlp-with-nltk', 'https://www.kaggle.com/code/jatinraina/random-acts-of-pizza-xgboost', 'https://www.kaggle.com/code/benhamner/simple-julia-benchmark', 'https://www.kaggle.com/code/ynue21/random-act-of-pizza', 'https://www.kaggle.com/code/benhamner/rmarkdown-default-text']"
435,"Get started on this competition through Kaggle Scripts
Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.
The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Hadi Fanaee Tork using data from Capital Bikeshare. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:
Fanaee-T, Hadi, and Gama, Joao, Event labeling combining ensemble detectors and background knowledge, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.","Submissions are evaluated one the Root Mean Squared Logarithmic Error (RMSLE). The RMSLE is calculated as
\[ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } \]
Where:
\\(n\\) is the number of hours in the test set
\\(p_i\\) is your predicted count
\\(a_i\\) is the actual count
\\(\log(x)\\) is the natural logarithm
Submission Format
Your submission file must have a header and should be structured in the following format:
datetime,count
2011-01-20 00:00:00,0
2011-01-20 01:00:00,0
2011-01-20 02:00:00,0
...
...","See, fork, and run a random forest benchmark model through Kaggle Scripts
You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.
Data Fields
datetime - hourly date + timestamp  
season -  1 = spring, 2 = summer, 3 = fall, 4 = winter 
holiday - whether the day is considered a holiday
workingday - whether the day is neither a weekend nor holiday
weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy
2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 
- temperature in Celsius
- ""feels like"" temperature in Celsius
- relative humidity
- wind speed
- number of non-registered user rentals initiated
- number of registered user rentals initiated
- number of total rentals",kaggle competitions download -c bike-sharing-demand,"['https://www.kaggle.com/code/viveksrinivasan/eda-ensemble-model-top-10-percentile', 'https://www.kaggle.com/code/rajmehra03/bike-sharing-demand-rmsle-0-3194', 'https://www.kaggle.com/code/kwonyoung234/for-beginner', 'https://www.kaggle.com/code/benhamner/bike-rentals-by-time-and-temperature', 'https://www.kaggle.com/code/kimtaehun/ml-deployment-evaluate-and-monitor-data-drift']"
436,"In the past, gathering information was paramount only for top-tier companies. In the information age, mining and categorization of relevant information is necessary for all companies. Media monitoring - the activity of monitoring the output of the print, online and broadcast media - allows every company to search a wide range of media, from printed media to internet publications, and be informed on their area of expertise and remain competitive.
This is a multi-label classification competition for articles coming from Greek printed media. Raw data comes from the scanning of print media, article segmentation, and optical character segmentation, and therefore is quite noisy. Each article is examined by a human annotator and categorized to one or more of the topics being monitored. Topics range from specific persons, products, and companies that can be easily categorized based on keywords, to more general semantic concepts, such as environment or economy. Building multi-label classifiers for the automated annotation of articles into topics can support the work of human annotators by suggesting a list of all topics by order of relevance, or even automate the annotation process for media and/or categories that are easier to predict. This saves valuable time and allows a media monitoring company to expand the portfolio of media being monitored.  
Organizers
The competition is organized by media monitoring solutions company DataScouting, media monitoring services company ENIMEROSI and the Deparment of Informatics of the Aristotle University of Thessaloniki. It is the challenge accompanying the 15th International Conference on Web Information System Engineering (WISE 2014) that will be held in Thessaloniki, Greece on 12-14 October 2014.
   ARISTOTLE
   UNIVERSITY OF
   THESSALONIKI ","The evaluation metric for this competition is Mean F1-Score also known as example-based F-measure in the multi-label learning literature. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F1 score is given by:
\[ F1 = 2\frac{p \cdot r}{p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn} \]
The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.
Submission File
For every article in the dataset, submission files should contain two columns: ArticleId and Topics. ArticleId coincides with the row number of each article inside the test file. Note that ArticleId is a string field so the exact Id in the test file should be used. Topics should be a space-delimited list. The output of each system should be in plain text format. 
ArticleId,Topics
1,12 2
2,30 4 5
3,123 5
4,210 9
etc.","Data was collected by scanning a number of Greek print media from May 2013 to September 2013. Articles were manually segmented and their text extracted throuch OCR (optical character recognition) software. The text of the articles is represented using the bag-of-words model and for each token encountered inside the text of all articles, the tf-idf statistic is computed and unit normalization is applied to the tf-idf values of each article. There are therefore 301561 numerical attributes corresponding to the tokens encountered inside the text of the collected articles. Articles were manually annotated with one or more out of 203 labels. 99780 articles were collected. The chronologically first 64857 form the training set, and the following 34923 form the test set. The goal is to predict the relevant labels in the test set, where the labels of the articles are withheld. 
Data are provided in two different formats:
In sparse ARFF format. The first attribute is the Id of the article. Then follow the 301561 numerical attributes. Then follow 203 binary attributes corresponding to the different labels.  
In LIBSVM format. Each row starts with comma separated integers corresponding to the labels, followed by the id and value of each token of an article. There is no Id attribute in this representation. Ids are equivalent to the row number, ranging from 1 to 64857 in the train set and from 64858 to 99780 in the test set. 
Working with the data
The ARFF format is mainly supported by Weka. You can work directly with this format by using the open-source Weka libraries Mulan and Meka. LIBSVM supports multi-label classification through LIBSVM tools. Matlab software for multi-label classification can also be found .   ",kaggle competitions download -c wise-2014,[]
437,"Get started on this competition with Kaggle Scripts. No data download or local environment needed!
Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing.
In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.
This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:
Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science","Submissions are evaluated on multi-class classification accuracy.
Submission File
Your submission file should have the observation Id and a predicted cover type (an integer between 1 and 7, inclusive). The file should contain a header and have the following format:
Id,Cover_Type
15121,1
15122,1
15123,1
...","The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:
1 - Spruce/Fir
2 - Lodgepole Pine
3 - Ponderosa Pine
4 - Cottonwood/Willow
5 - Aspen
6 - Douglas-fir
7 - Krummholz
The training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations).
Data Fields
Elevation - Elevation in meters
- Aspect in degrees azimuth
- Slope in degrees
- Horz Dist to nearest surface water features
- Vert Dist to nearest surface water features
- Horz Dist to nearest roadway
(0 to 255 index) - Hillshade index at 9am, summer solstice
(0 to 255 index) - Hillshade index at noon, summer solstice
(0 to 255 index) - Hillshade index at 3pm, summer solstice
- Horz Dist to nearest wildfire ignition points
(4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation
(40 binary columns, 0 = absence or 1 = presence) - Soil Type designation
(7 types, integers 1 to 7) - Forest Cover Type designation",kaggle competitions download -c forest-cover-type-prediction,"['https://www.kaggle.com/code/sharmasanthosh/exploratory-study-on-feature-selection', 'https://www.kaggle.com/code/kashnitsky/topic-10-practice-with-logit-rf-and-lightgbm', 'https://www.kaggle.com/code/sharmasanthosh/exploratory-study-of-ml-algorithms', 'https://www.kaggle.com/code/siddheshpujari/forest-cover-eda', 'https://www.kaggle.com/code/thariqnugrohotomo/comparing-original-competition-dataset']"
438,"The evaluation metric is the approximate median significance (AMS):
\[ \text{AMS} = \sqrt{2\left((s+b+b_r) \log \left(1 + \frac{s}{b + b_r}\right)-s\right)}\]
where
s, b : unnormalized true positive and false positive rates, respectively,
b_r =10 is the constant regularization term,
\\(\log\\) is the natural log.
More precisely, let \\((y_1, \ldots, y_n) \in \{\text{b},\text{s}\}^n\\) be the vector of true test labels, let \\((\hat{y}_1, \ldots, \hat{y}_n) \in \{\text{b},\text{s}\}^n\\) be the vector of predicted (submitted) test labels, and let \\((w_1, \ldots, w_n) \in {\mathbb{R}^+}^n\\) be the vector of weights. Then
\[ s = \sum_{i=1}^n w_i\mathbb{1}\{y_i = \text{s}\} \mathbb{1}\{\hat{y}_i = \text{s}\} \]
and
\[ b = \sum_{i=1}^n w_i\mathbb{1}\{y_i = \text{b}\} \mathbb{1}\{\hat{y}_i = \text{s}\}, \]
where the indicator function \\(\mathbb{1}\{A\}\\) is 1 if its argument \\(A\\) is true and 0 otherwise.
For more information on the statistical model and the derivation of the metric, see the technical documentation. We have provided python code for the metric is available from the Data page and a Python starting kit.
Submission Instructions
The submission file format is 
EventId,RankOrder,Class
1,2,b
2,541234,s
3,5,b
4,1,b
5,542456,s
...
Your submission file should have a header row and three columns
EventId is a unique identifier for each event. The list of EventIds must correspond to the exact list of EventIds in test.csv, but the ordering can be arbitrary.
RankOrder is a permutation of the integer list [1,550000] . The higher the rank, the more signal-like is the event. Most predictors output a real-valued score for each event in the test set, in which case RankOrder is just the ordering of the test points according to the score. The RankOrder is not used for computing the AMS, but it allows the organizers to compute other metrics (e.g., ROC) related to the classification task, which is not captured entirely by the classification alone.
Class is either ""b"" or ""s"", and it indicates if your prediction (yi above in the formal definition) for the event is background or signal. The AMS will be calculated based on the (hidden) weights of events that you mark ""s""","The evaluation metric is the approximate median significance (AMS):
\[ \text{AMS} = \sqrt{2\left((s+b+b_r) \log \left(1 + \frac{s}{b + b_r}\right)-s\right)}\]
where
\\(s, b\\): unnormalized true positive and false positive rates, respectively,
\\(b_r =10\\) is the constant regularization term,
\\(\log\\) is the natural log.
More precisely, let \\((y_1, \ldots, y_n) \in \{\text{b},\text{s}\}^n\\) be the vector of true test labels, let \\((\hat{y}_1, \ldots, \hat{y}_n) \in \{\text{b},\text{s}\}^n\\) be the vector of predicted (submitted) test labels, and let \\((w_1, \ldots, w_n) \in {\mathbb{R}^+}^n\\) be the vector of weights. Then
\[ s = \sum_{i=1}^n w_i\mathbb{1}\{y_i = \text{s}\} \mathbb{1}\{\hat{y}_i = \text{s}\} \]
and
\[ b = \sum_{i=1}^n w_i\mathbb{1}\{y_i = \text{b}\} \mathbb{1}\{\hat{y}_i = \text{s}\}, \]
where the indicator function \\(\mathbb{1}\{A\}\\) is 1 if its argument \\(A\\) is true and 0 otherwise.
For more information on the statistical model and the derivation of the metric, see the technical documentation. We have provided python code for the metric is available from the Data page and a Python starting kit.
Submission Instructions
The submission file format is 
EventId,RankOrder,Class
1,2,b
2,541234,s
3,5,b
4,1,b
5,542456,s
...
Your submission file should have a header row and three columns
EventId is a unique identifier for each event. The list of EventIds must correspond to the exact list of EventIds in test.csv, but the ordering can be arbitrary.
RankOrder is a permutation of the integer list [1,550000]. The higher the rank, the more signal-like is the event. Most predictors output a real-valued score for each event in the test set, in which case RankOrder is just the ordering of the test points according to the score. The RankOrder is not used for computing the AMS, but it allows the organizers to compute other metrics (e.g., ROC) related to the classification task, which is not captured entirely by the classification alone.
Class is either ""b"" or ""s"", and it indicates if your prediction (yi above in the formal definition) for the event is background or signal. The AMS will be calculated based on the (hidden) weights of events that you mark ""s""","File descriptions
training.csv - Training set of 250000 events, with an ID column, 30 feature columns, a weight column and a label column.
test.csv - Test set of 550000 events with an ID column and 30 feature columns.
random_submission - Sample submission file in the correct format. File format is described on the Evaluation page.
HiggsBosonCompetition_AMSMetric - Python script to calculate the competition evaluation metric.
For detailed information on the semantics of the features, labels, and weights, see the technical documentation from the LAL website on the task.
Some details to get started:
all variables are floating point, except PRI_jet_num which is integer
variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.
variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by  the physicists of ATLAS
it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables",kaggle competitions download -c higgs-boson,"['https://www.kaggle.com/code/sugataghosh/implementing-logistic-regression-from-scratch', 'https://www.kaggle.com/code/sugataghosh/higgs-boson-event-detection-part-1-eda', 'https://www.kaggle.com/code/johnakwei/higgsml-in-r', 'https://www.kaggle.com/code/anuragbagadi/detecting-the-higgs-boson-using-rnn', 'https://www.kaggle.com/code/shirshmall/search-for-higgs-boson-decay-modes']"
439,"For individuals with drug-resistant epilepsy, responsive neurostimulation systems hold promise for augmenting current therapies and transforming epilepsy care.
Of the more than two million Americans who suffer from recurrent, spontaneous epileptic seizures, 500,000 continue to experience seizures despite multiple attempts to control the seizures with medication. For these patients responsive neurostimulation represents a possible therapy capable of aborting seizures before they affect a patient's normal activities. 
In order for a responsive neurostimulation device to successfully stop seizures, a seizure must be detected and electrical stimulation applied as early as possible. A seizure that builds and generalizes beyond its area of origin will be very difficult to abort via neurostimulation. Current seizure detection algorithms in commercial responsive neurostimulation devices are tuned to be hypersensitive, and their high false positive rate results in unnecessary stimulation.
In addition, physicians and researchers working in epilepsy must often review large quantities of continuous EEG data to identify seizures, which in some patients may be quite subtle. Automated algorithms to detect seizures in large EEG datasets with low false positive and false negative rates would greatly assist clinical care and basic research.
The Competition:
Intracranial EEG was recorded from dogs with naturally occurring epilepsy using an ambulatory monitoring system. EEG was sampled from 16 electrodes at 400 Hz, and recorded voltages were referenced to the group average. 
In addition, datasets from patients with epilepsy undergoing intracranial EEG monitoring to identify a region of brain that can be resected to prevent future seizures are included in the contest. These datasets have varying numbers of electrodes and are sampled at 500 Hz or 5000 Hz, with recorded voltages referenced to an electrode outside the brain.
Seizure detection algorithms are relevant to two specific applications:
1) High sensitivity & specificity applications. Potential applications of this algorithm would be for automated seizure diaries, where latency to seizure onset is not critical. Here the goal is to optimize the accuracy of detection.
2) For responsive stimulation application the latency of onset is of particular importance. The key to successful therapy is the ability to rapidly detect the onset of seizures. Often a highly sensitive detector is created, and high false positive rates are tolerated as the stimulation is below patient perception.
Acknowledgements
This competition is sponsored by the National Institues of Health (NINDS), and the American Epilepsy Society.","Submissions are judged on the mean area under the ROC curve (AUC) of two predictions. Firstly, you must predict the probability that a given clip is a seizure. Secondly, you must predict the probability that the clip is within the first 15 seconds its respective seizure (the technical term for time into the seizure is ""latency""). These early clips are double counted because early detection is critical to successful intervention with a responsive neurostimulation device.
The competition metric is the mean these two AUCs:
$$ 1/2 \left( AUC_{seizure} + AUC_{early} \right) $$
Submission File
Participants should submit answers in a CSV file with classifications for each test segment. The file should contain a header and have the following format:
clip,seizure,early
Dog_1_test_segment_1.mat,0,0
Dog_1_test_segment_2.mat,0,0
Dog_1_test_segment_3.mat,0,0
etc.","Data are organized in folders containing training and testing data for each human or canine subject. The training data is organized into 1-second EEG clips labeled ""Ictal"" for seizure data segments, or ""Interictal"" for non-seizure data segments. Training data are arranged sequentially while testing data are in random order. Ictal training and testing data segments are provided covering the entire seizure, while interictal data segments are provided covering approximately the mean seizure duration for each subject. Starting points for the interictal data segments were chosen randomly from the full data record, with the restriction that no interictal segment be less than one hour before or after a seizure.
Within folders data segments are stored in matlab .mat files, arranged in a data structure with fields as follow:
data: a matrix of EEG sample values arranged row x column as electrode x time.
data_length_sec: the time duration of each data row (1 second for all data in this case)
latency: the time in seconds between the expert-marked seizure onset and the first data point in the data segment (in ictal training segments only) 
sampling_frequency: the number of data samples representing 1 second of EEG data. (Non-integer values represent an average across the full data record and may reflect missing EEG samples)
channels: a list of electrode names corresponding to the rows in the data field
The human data are from patients with temporal and extratemporal lobe epilepsy undergoing evaluation for epilepsy surgery. The iEEG recordings are from depth electrodes implanted along anterior-posterior axis of hippocampus, and from subdural electrode grids in various locations. Data sampling rates vary from 500 Hz to 5,000 Hz.",kaggle competitions download -c seizure-detection,[]
440,"DonorsChoose.org is an online charity that makes it easy to help students in need through school donations. At any time, thousands of teachers in K-12 schools propose projects requesting materials to enhance the education of their students. When a project reaches its funding goal, they ship the materials to the school.
The 2014 KDD Cup asks participants to help DonorsChoose.org identify projects that are exceptionally exciting to the business, at the time of posting. While all projects on the site fulfill some kind of need, certain projects have a quality above and beyond what is typical. By identifying and recommending such projects early, they will improve funding outcomes, better the user experience, and help more students receive the materials they need to learn.
Successful predictions may require a broad range of analytical skills, from natural language processing on the need statements to data mining and classical supervised learning on the descriptive factors around each project.
About KDD
KDD 2014 is a premier interdisciplinary conference that brings together researchers and practitioners from all aspects of data science, data mining, knowledge discovery, large-scale data analytics, and big data. This year's KDD features 4 keynotes, 151 Research Track papers, 44 Industry & Government Track papers, 24 workshops, 12 tutorials, and more.
Acknowledgements
Data and logistical support has been graciously provided by DonorsChoose.org. DonorsChoose.org is an online charity and 501(c)(3) nonprofit organization that makes it easy for anyone to help students in need. Public school teachers from every corner of America post classroom project requests, and donors can give any amount to the project that most inspires them.","Submissions are evaluated on area under the ROC curve between the predicted probability that a project is exciting and the observed outcomes.
Submission File
For each project in the test set, you should predict a real-valued probability that the project was exciting. The file should contain a header and have the following format:
projectid,is_exciting
ffff7266778f71242675416e600b94e1,0
fffeb510ee37a0bb01079f06bf141246,0
fff979abefa35a6bdd133b4e4150b737,0
etc.","The data is provided in a relational format and split by dates. Any project posted prior to 2014-01-01 is in the training set (along with its funding outcomes). Any project posted after is in the test set. Some projects in the test set may still be live and are ignored in the scoring. We do not disclose which projects are still live to avoid leakage regarding the funding status. 
File descriptions
donations.csv - contains information about the donations to each project. This is only provided for projects in the training set.
essays.csv - contains project text posted by the teachers. This is provided for both the training and test set.
projects.csv - contains information about each project. This is provided for both the training and test set.
resources.csv - contains information about the resources requested for each project. This is provided for both the training and test set.
outcomes.csv - contains information about the outcomes of projects in the training set.
sampleSubmission.csv - contains the project ids of the test set and shows the submission format for the competition.
""Exciting"" Projects
Exciting projects meet a number of requirements specified by DonorsChoose.org. Note that the term ""exciting"" is meant as a business construct and does not imply that non-exciting projects are not compelling to teachers/students/donors! To be exciting, a project must meet of the following five criteria. The name in parentheses indicates the field containing each feature in the data set.",kaggle competitions download -c kdd-cup-2014-predicting-excitement-at-donors-choose,[]
441,"This competition uses the billion-word benchmark corpus provided by Chelba et al. for language modeling. Rather than ask participants to create a classic language model and evaluate sentence probabilities -- a task which is difficult to faithfully score in Kaggle's supervised ML setting -- we have introduced a variation on the language modeling task.
For each sentence in the test set, we have removed exactly one word. Participants must create a model capable of inserting back the correct missing word at the correct location in the sentence. Submissions are scored using an edit distance to allow for partial credit.
We extend our thanks to authors who created this corpus and shared it for the research community to use. Please cite this paper if you use this dataset in your research: Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn: One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling, CoRR, 2013.
Note: the train/test split used in this competition is different than the published version used for language modeling. If you are creating full language models and scoring perplexity, you should download the official version of the corpus from the authors' website.","Submissions are evaluated on the mean Levenshtein distance between the sentences you submit and the original sentences in the test set.
Note: due to the size and computations necessary to score submissions for this competition, scoring may take 5-10 minutes, and possibly longer if there are other submissions in front of yours. Please be patient!
Submission File
Your submission file should contain the sentence id and a predicted sentence. To prevent parsing issues, you should use double quotes to escape the sentence text and two double quotes ("""") for double quotes within a sentence. Note that test.csv is a valid submission file itself.
The file should contain a header and have the following format:
id,""sentence""
1,""Former Dodgers manager , the team 's undisputed top ambassador , is going strong at 83 and serving up one great story after another .""
2,""8 parliamentary elections meant to restore democracy in this nuclear armed nation , a key ally against Islamic .""
3,""Sales of drink are growing 37 per cent month-on-month from a small base .""
etc...","The data for this competition is a large corpus of English language sentences. You should use only the sentences in the training set to build you model.
We have removed one word from each sentence in the test set. The location of the removed word was chosen uniformly randomly and is never the first or last word of the sentence (in this dataset, the last word is always a period). You must attempt to submit the sentences in the test set with the correct missing word located in the correct location. 
Note: the train/test split used in this competition is different than the published version used for language modeling. If you are creating full language models and scoring perplexity, you should download the official version of the corpus from the authors' website.
File descriptions
train.txt - the training set, contains a large collection of English language sentences
test.txt - the test set, contains a large number of sentences where one word has been removed",kaggle competitions download -c billion-word-imputation,['https://www.kaggle.com/code/mpwolke/billion-word-imputation-zip-file']
442,"Social Circles help users organize their personal social networks.  These are implemented as ""circles"" on Google+, and as ""lists"" on Facebook and Twitter. Each circle consists of a subset of a particular user's friends. Such circles may be disjoint, overlap, or be hierarchically nested.
The goal of this competition is to automatically infer users' social circles. You are provided a set of users, each of whose circles must be inferred. To do this, participants have access to:
A list of the user's friends
Anonymized Facebook profiles of each of those friends
A network of connections between those friends (their ""ego network"")
To give you an idea of how to use this data, the problem of detecting social circles has been discussed (in an academic setting) in http://i.stanford.edu/~julian/pdfs/nips2012.pdf
Those of you who've been around Kaggle for a while may remember that we called upon you to create this data set. We extend our thanks to those of you who helped out. As a reward, you might be able to find yourself in the data and gain a one-row advantage?!","Submissions are evaluated using the edit distance between the proposed and the correct solution. Each of the following operations costs one edit:
Add a user to an existing circle
Create a circle with one user
Remove a user from a circle
Delete a circle with one user
The minimum number of such operations required to transform your solution into the correct solution is then the edit distance, which your solution should minimize.
For example, if your solution contains circles with user ID's
circle1: 3 1 2;   circle2: 2 3;   circle3: 5 4 6
And the correct solution is
circleA: 4 5;   circleB: 1 3 2 4
Then the minimum number of required edits would be:
Add user 4 to circle1
Delete user 2 from circle2
Delete user 3 from circle2
Delete user 6 from circle3
Thus the proposed solution has an error of 4 edits.
Submission Instructions
The submission file must have two columns, a UserId and a space-separated list of friends in a circle. Each row of the file corresponds to a single user. Separate circles with a semicolon. Every UserId needs a prediction. The format should look like this:
UserId,Prediction
1,3 5 9 12 15 39 24 11;4 12 98 54 384 6 22;88 78 45 34 19 27 101
2,341 9086 542 17 36;55 56 57 58 10 106 14;384 1;7 12 19 35 11;88
etc.
In this example, User 1 has their friends in 3 circles while User 2 has their friends in 5 circles.","We use the following nomenclature
User: Target individual whose network is the focus of the task
Friend: Friend of the target individual (user) that needs to be placed into a circle
We include the following files:
egonets: Each file in this directory contains the ego-network of a single Facebook user, i.e., a list of connections between their friends. Each file userId.egonet contains lines of the form
UserId: Friends
1: 4 6 12 2 208
2: 5 3 17 90 7
These are node-adjacency lists indicating that User 1 is friends with Friend 4, Friend 6, etc. Edges are undirected in Facebook. It can be assumed that user (the owner of the egonet) is friends with all ids in this file.
features: Contains features for all users. Each line is of the form
UserId feature1 feature2 feature3 ...",kaggle competitions download -c learning-social-circles,[]
443,"(Please note: this competition is only open to students of https://www.edx.org/course/mitx/mitx-15-071x-analytics-edge-1416)
What predicts happiness? In this competition, you'll be using data from Show of Hands, an informal polling platform for use on mobile devices and the web, to see what aspects and characteristics of people's lives predict happiness.
Show of Hands has been downloaded over 300,000 times across Apple and Android app stores, and users have cast more than 75 million votes. In this problem, we'll use data from thousands of users and one hundred different questions to see which responses predict happiness.
Acknowledgements
This competition is brought to you by 15.071x, edX, and Show of Hands.","The evaluation metric for this competition is AUC. The AUC, which we described in Week 3 when we taught logistic regression, is a commonly used evaluation metric for binary problems like this one. The interpretation is that given a random positive observation and negative observation, the AUC gives the proportion of the time you guess which is which correctly. It is less affected by sample balance than accuracy. A perfect model will score an AUC of 1, while random guessing will score an AUC of around of 0.5.
Submission File
For every observation in the test set, submission files should contain two columns: UserID and Probability1. The submission should be a csv file. The UserID should just be the corresponding UserID column from the dataset. The Probability1 column should be the predicted probability of the outcome 1 for that UserID.
As an example of how to generate a submission file in R, suppose that your test set probability predictions are called ""testPred"" and your test data set is called ""test"". Then you can generate a submission file called ""submission.csv"" by running the following two lines of code in R (if you copy and paste these lines of code into R, the quotes around submission.csv might not read properly - please delete and re-type the quotes if you get an error):
submission = data.frame(UserID = test$UserID, Probability1 = testPred)
write.csv(submission, “submission.csv”, row.names=FALSE) 
You should then submit the file ""submission.csv"" by clicking on ""Make a Submission"" on the Kaggle website.
The generated file should have the following format:
UserID,Probability1
3,0.279672578 
4,0.695794648 
10,0.695794648 
14,0.279672578 
16,0.554216867 
23,0.640816327 
29,0.695794648
etc.","File descriptions
Here is a description of the files you have been provided for this competition:
train.csv - the training set of data that you should use to build your models
test.csv - the test set that you will be evaluated on. It contains all of the independent variables, but not the dependent variable.
sampleSubmission.csv - a sample submission file in the correct format.
Questions.pdf - the question test corresponding to each of the question codes, as well as the possible answers.
Data fields
UserID - an anonymous id unique to a given user
YOB - the year of birth of the user
Gender - the gender of the user, either Male, Female, or not provided
Income - the household income of the user. Either not provided, or one of ""under $25,000"", ""$25,001 - $50,000"", ""$50,000 - $74,999"", ""$75,000 - $100,000"", ""$100,001 - $150,000"", or ""over $150,000"".
HouseholdStatus - the household status of the user. Either not provided, or one of ""Domestic Partners (no kids)"", ""Domestic Partners (w/kids)"", ""Married (no kids)"", ""Married (w/kids)"", ""Single (no kids)"", or ""Single (w/kids)"".",kaggle competitions download -c the-analytics-edge-mit-15-071x,
444,"Understanding how the human brain works is a primary goal in neuroscience research. Non-invasive functional neuroimaging techniques, such as magnetoencephalography (MEG), are able to capture the brain activity as multiple timeseries. When a subject is presented a stimulus and the concurrent brain activity is recorded, the relation between the pattern of recorded signal and the category of the stimulus may provide insights on the underlying mental process. Among the approaches to analyse the relation between brain activity and stimuli, the one based on predicting the stimulus from the concurrent brain recording is called brain decoding.
The goal of this competition is to predict the category of a visual stimulus presented to a subject from the concurrent brain activity. The brain activity is captured with an MEG device which records 306 timeseries at 1KHz of the magnetic field associated with the brain currents. The categories of the visual stimulus for this competition are two: face and scrambled face. A stimulus and the concurrent MEG recording is called trial and thousands of randomized trials were recorded from multiple subjects. The trials of some of the subjects, i.e. the train set, are provided to create prediction models. The remaining trials, i.e. the test set, belong to different subjects and they will be used to score the prediction models. Because of the variability across subjects in brain anatomy and in the patterns of brain activity, a certain degree of difference is expected between the data of different subjects and thus between the train set and the test set.
Bibliography
Full details of the neuroscientific experiment in which the data were collected are described in:
Henson RN, Wakeman DG, Litvak V and Friston KJ (2011), ""A parametric empirical Bayesian framework for the EEG/MEG inverse problem: generative models for multi-subject and multi-modal integration"". Front. Hum. Neurosci. 5:76. doi: 10.3389/fnhum.2011.00076
available at: http://www.frontiersin.org/Journal/10.3389/fnhum.2011.00076/abstract. 
A brief survey of the scientific literature on the problem of decoding across subjects, together with the description of the train set of this competition and a preliminary solution in terms of transfer learning, are described in:
Olivetti E, Kia SM, Avesani P (2014), ""MEG decoding across subjects"", Pattern Recognition in Neuroimaging, 2014 International Workshop on, pp.1-4, 4-6 June 2014. doi: 10.1109/PRNI.2014.6858538
available at ieeexplore.ieee.org (or freely-available preprint here: http://arxiv.org/abs/1404.4175).
Conference
This competition is associated with the the 19th International Conference on Biomagnetism, Biomag 2014. The Biomag conference will be held in Halifax, Canada, August 24-28, 2014.
Organization
This competition is organized by Emanuele Olivetti, Mostafa Kia and Paolo Avesani (NeuroInformatics Lab, Fondazione Bruno Kessler and Università di Trento, IT).
Acknowledgements
The awards of this competition are funded by Elekta Oy, MEG International Services Ltd (MISL), Fondazione Bruno Kessler, and Besa. We would also like to thank Daniel Wakeman (Martinos Center, MGH, USA), Richard Henson (MRC/CBU, Cambridge, UK), Ole Jensen (Donders Institute, NL), Nathan Weisz (University of Trento, IT) and Alexandre Gramfort (Telecom ParisTech, CNRS, CEA / Neurospin) for their contributions in preparing this competition.
                   ","The evaluation metric for this competition is prediction accuracy, i.e. the total number of correct predictions over the total size of the test set.
Notice that the (public) score on the leaderboard and the final (private) score of the competition are computed on trials from different sets of subjects. For this reason it could happen that the public score on the leaderboard may be substantially different from the final score.
Submission File
The submission file should contain two columns: Id and Prediction. The Id is a unique identifier of each trial in the test set. It is a numeric value of 5 digits where the first two are the identifier of the subject and the last three are the identifier of the trial. For example, the prediction of trial 15 for subject 22 has Id 22015. The Prediction value indicates whether the predicted value is 0 (Scramble) or 1 (Face). 
The file should contain a header and have the following format:
Id,Prediction
22001,1
22002,0
22003,0
22004,1
etc.","The training data consist of 9414 trials, i.e. MEG recordings and the class labels (Face/Scramble), from 16 subjects (subject01 to subject16). The test set comprises 4058 MEG recordings from 7 subjects (subject17 to subject23), without class labels. For each subject approximately 580-590 trials are available. Each trial consists of 1.5 seconds of MEG recording (starting 0.5sec before the stimulus starts) and the related class label, Face (class 1) or Scramble (class 0) . The data were down-sampled to 250Hz and high-pass filtered at 1Hz. 306 timeseries were recorded, one for each of the 306 channels, for each trial. All the pre-processing steps were carried out with mne-python. The trials of each subject are arranged into a 3D data matrix (trial x channel x time) of size 580 x 306 x 375. For each subject in the train set, the 3D data matrix and the associated vector of 580 class labels are saved into a file named train_subjectXX.mat. The data of the subjects in the test set are saved in files named test_subjectXX.mat. These files are in Matlab v5.0 format and can be easily read from Octave/Matlab, Python (see scipy.io.loadmat()), R (see the R.matlab package), Java (see JMatIO) and from other programming languages.
Notice that the all train and test files are conveniently grouped into 4 large zip files: train_01_06.zip, train_07_12.zip, train_13_16.zip and test_17_23.zip.
The MEG sensors are spatially arranged as described in the file Vectorview-mag.lout, which is a 2D approximation of the actual layout (see the picture in the description page). The 306 sensors are grouped, three at a time, in 102 locations. At each location two orthogonal gradiometers and one magnetometer record the magnetic field induced by the brain currents. The magnetometer measures the z (radial) component of the magnetic field, while the gradiometers measure the x and y spatial derivative of the magnetic field.
Notice that if the number in the sensor name ends with ""1"" then it is a magnetometer. If it ends with ""2"" or ""3"", then it is a gradiometer. For example, ""MEG 0113"" is a gradiometer and ""MEG 0111"" is a magnetometer.",kaggle competitions download -c decoding-the-human-brain,['https://www.kaggle.com/code/abdelrahmanyehia/bci-project']
445,"Consumer brands often offer discounts to attract new shoppers to buy their products. The most valuable customers are those who return after this initial incented purchase.  With enough purchase history, it is possible to predict which shoppers, when presented an offer, will buy a new item. However, identifying the shopper who will become a loyal buyer -- prior to the initial purchase -- is a more challenging task.
The Acquire Valued Shoppers Challenge asks participants to predict which shoppers are most likely to repeat purchase. To aid with algorithmic development, we have provided complete, basket-level, pre-offer shopping history for a large set of shoppers who were targeted for an acquisition campaign. The incentive offered to that shopper and their post-incentive behavior is also provided.
This challenge provides almost 350 million rows of completely anonymised transactional data from over 300,000 shoppers. It is one of the largest problems run on Kaggle to date.","Submissions are evaluated on area under the ROC curve between the predicted probability that a customer repeat-purchased and the observed purchase outcomes.
Submission File
For each customer (id) in testHistory.csv, predict a probability that the customer repeat-purchased the product from the promotion they received. Your submission file must have a header and should look like the following:
id,repeatProbability
12262064,0
12277270,0
12332190,0
...","Warning: this is a large data set. The decompressed files require about 22GB of space.
This data captures the process of offering incentives (a.k.a. coupons) to a large number of customers and forecasting those who will become loyal to the product. Let's say 100 customers are offered a discount to purchase two bottles of water. Of the 100 customers, 60 choose to redeem the offer. These 60 customers are the focus of this competition. You are asked to predict which of the 60 will return (during or after the promotional period) to purchase the same item again.
To create this prediction, you are given a minimum of a year of shopping history prior to each customer's incentive, as well as the purchase histories of many other shoppers (some of whom will have received the same offer). The transaction history contains all items purchased, not just items related to the offer. Only one offer per customer is included in the data. The training set is comprised of offers issued before 2013-05-01. The test set is offers issued on or after 2013-05-01.
Files
You are provided four relational files:
transactions.csv - contains transaction history for all customers for a period of at least 1 year prior to their offered incentive
trainHistory.csv - contains the incentive offered to each customer and information about the behavioral response to the offer
testHistory.csv - contains the incentive offered to each customer but does not include their response (you are predicting the repeater column for each id in this file)",kaggle competitions download -c acquire-valued-shoppers-challenge,"['https://www.kaggle.com/code/sainiknitin/nitin-saini-walking-through-customer-transaction', 'https://www.kaggle.com/code/truereddevil/kernel21fd4a7c59', 'https://www.kaggle.com/code/chanyoungs/acquire-valued-shoppers-challenge']"
446,"Improve credit risk models by predicting the probability of default on a consumer credit product in the next 18 months. More accurate credit risk evaluations allow issuers of credit to be able to responsibly extend and manage credit lines for their customers. The goal of this contest is to make the most accurate ranking of customers' credit risk given key data related to customer behavior. 
How well can you detect the early signs of credit distress?
Enter Now!
This competition is only open to Masters-level participants who meet the eligibility criteria. Visit the Enter the Competition page to view the eligibility criteria and request entrance.",Please refer to AdditionalCompetitionInformation.pdf available on the Data page.,,,
447,"""There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.""
The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of Socher et al [2]. We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper:
http://nlp.stanford.edu/sentiment/
There you will find have source code, a live demo, and even an online interface to help train the model.
[1] Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124.
[2] Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).
Image credits: Popcorn - Maura Teague, http://www.flickr.com/photos/93496438@N06/","Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly) for every parsed phrase. The sentiment labels are:
0 - negative
1 - somewhat negative
2 - neutral
3 - somewhat positive
4 - positive
Submission Format
For each phrase in the test set, predict a label for the sentiment. Your submission should have a header and look like the following:
PhraseId,Sentiment
156061,2
156062,2
156063,2
...","The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.
train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.
test.tsv contains just phrases. You must assign a sentiment label to each phrase.
The sentiment labels are:
0 - negative
1 - somewhat negative
2 - neutral
3 - somewhat positive
4 - positive",kaggle competitions download -c sentiment-analysis-on-movie-reviews,"['https://www.kaggle.com/code/sounishnath003/huggingface-pytorch-lightning-distilbert-optimized', 'https://www.kaggle.com/code/nj2684/sentiment-movie-analysis-naman', 'https://www.kaggle.com/code/vikkach/sentiment-analysis-lstm-pytorch', 'https://www.kaggle.com/code/eavprog/sentiment-analysis-of-movie-reviews', 'https://www.kaggle.com/code/gauravduttakiit/class-dataset-sentiment-analysis-on-movie-reviews']"
448,"One challenge of modeling retail data is the need to make decisions based on limited history. If Christmas comes but once a year, so does the chance to see how strategic decisions impacted the bottom line.
In this recruiting competition, job-seekers are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains many departments, and participants must project the sales for each department in each store. To add to the challenge, selected holiday markdown events are included in the dataset. These markdowns are known to affect sales, but it is challenging to predict which departments are affected and the extent of the impact.
Want to work in a great environment with some of the world's largest data sets? This is a chance to display your modeling mettle to the Walmart hiring teams.
This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Walmart, check the box ""Allow host to contact me"" when you make your first entry. 
You must compete as an individual in recruiting competitions. You may only use the provided data to make your predictions.","This competition is evaluated on the weighted mean absolute error (WMAE):
$$
\textrm{WMAE} = \frac{1}{\sum{w_i}} \sum_{i=1}^n w_i | y_i - \hat{y}_i |
$$
where
n is the number of rows
\\( \hat{y}_i \\) is the predicted sales
\\( y_i \\) is the actual sales
\\( w_i \\) are weights. w = 5 if the week is a holiday week, 1 otherwise
Submission File
For each row in the test set (store + department + date triplet), you should predict the weekly sales of that department. The Id column is formed by concatenating the Store, Dept, and Date with underscores (e.g. Store_Dept_2012-11-02).  The file should have a header and looks like the following:
Id,Weekly_Sales
1_1_2012-11-02,0
1_1_2012-11-09,0
1_1_2012-11-16,0
...","You are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains a number of departments, and you are tasked with predicting the department-wide sales for each store.
In addition, Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete/ideal historical data.
stores.csv
This file contains anonymized information about the 45 stores, indicating the type and size of store.
train.csv
This is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields:
Store - the store number
Dept - the department number
Date - the week
Weekly_Sales -  sales for the given department in the given store",kaggle competitions download -c walmart-recruiting-store-sales-forecasting,"['https://www.kaggle.com/code/andredornas/tp2-walmart-sales-forecast', 'https://www.kaggle.com/code/avelinocaio/walmart-store-sales-forecasting', 'https://www.kaggle.com/code/maxdiazbattan/wallmart-sales-top-3-eda-feature-engineering', 'https://www.kaggle.com/code/yasirhussain1987/eda-and-store-sales-predictions-using-xgb', 'https://www.kaggle.com/code/yepp2411/walmart-prediction-1-eda-with-time-and-space']"
449,"As a customer shops an insurance policy, he/she will receive a number of quotes with different coverage options before purchasing a plan. This is represented in this challenge as a series of rows that include a customer ID, information about the customer, information about the quoted policy, and the cost. Your task is to predict the purchased coverage options using a limited subset of the total interaction history. If the eventual purchase can be predicted sooner in the shopping window, the quoting process is shortened and the issuer is less likely to lose the customer's business.
Using a customer’s shopping history, can you predict what policy they will end up choosing?","Submissions are evaluated on an all-or-none accuracy basis. You must predict every coverage option correctly to receive credit for a given customer. Your score is the percent of customers for whom you predict the exact purchased policy.
Submission File
The submission format is created by concatenating each plan option (A,B,C,D,E,F,G) as a single string, in order. The file should contain a header and have the following format:
customer_ID,plan
10000001,1111111
10000002,1111111
10000003,1111111
...","Files
The training and test sets contain transaction history for customers that ended up purchasing a policy. For each customer_ID, you are given their quote history. In the training set you have the entire quote history, the last row of which contains the coverage options they purchased. In the test set, you have only a partial history of the quotes and do not have the purchased coverage options. These are truncated to certain lengths to simulate making predictions with less history (higher uncertainty) or more history (lower uncertainty).
For each customer_ID in the test set, you must predict the seven coverage options they end up purchasing.
What is a customer?
Each customer has many shopping points, where a shopping point is defined by a customer with certain characteristics viewing a product and its associated cost at a particular time.
Some customer characteristics may change over time (e.g. as the customer changes or provides new information), and the cost depends on both the product and the customer characteristics.
A customer may represent a collection of people, as policies can cover more than one person.
A customer may purchase a product that was not viewed!
Product Options
Each product has 7 customizable options selected by customers, each with 2, 3, or 4 ordinal values possible:",kaggle competitions download -c allstate-purchase-prediction-challenge,"['https://www.kaggle.com/code/caoyi41/allstate-pg1', 'https://www.kaggle.com/code/chachiawacef/allstate-purchase-prediction', 'https://www.kaggle.com/code/memememesxy/cp3403-all-state-challenge', 'https://www.kaggle.com/code/andrewebenbach/allstate-eda', 'https://www.kaggle.com/code/shwetabhujbal/alllstate']"
450,"Quality pseudorandom number generation forms the bedrock on which all of computing is built. From cryptography to financial markets to particle physics, it is our trust in random numbers that props up the modern economy and allows technology to march forth, unabated.
Machine learning is incredibly powerful at recognizing statistical patterns. For this competition, we challenge you to apply your machine learning skills to predict a set of random numbers. To date, there are no known methods for predicting this set of numbers. Some experts have said it is too random, that it can't be done within current limitations of computing power. We believe the creativity of the Kaggle community will triumph over the cynical doubts of the naysayers. If there is one thing the working data scientist can do, it is extract insights from a sea of randomness.
Before asking questions in the forums, we recommend reading the authoritative source in this field: A Million Random Digits with 100,000 Normal Deviates, RAND et. al.
We also provide this helpful widget to cross validate your submissions:
The numbers generated by this widget come from RANDOM.ORG's true random number generator.","Submissions are scored on the Mean Absolute Error
\[ \mathrm{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat y_i| \]
where \\(y_i\\) is the \\(i\\)th solution and \\(\hat y_i\\) is the \\(i\\)th prediction.
Sample Submission
Your submission file should have a header row and two columns:
Id,Prediction
1,1.0
2,2.5
3,4.29","The data is comprised of random numbers, which may or may not be integers. None of the numbers has an imaginary component.",kaggle competitions download -c random-number-grand-challenge,[]
451,"This competition asks you to determine whether a loan will default, as well as the loss incurred if it does default. Unlike traditional finance-based approaches to this problem, where one distinguishes between good or bad counterparties in a binary way, we seek to anticipate and incorporate both the default and the severity of the losses that result. In doing so, we are building a bridge between traditional banking, where we are looking at reducing the consumption of economic capital, to an asset-management perspective, where we optimize on the risk to the financial investor.
This competition is sponsored by researchers at Imperial College London.","This competition is evaluated on the mean absolute error (MAE):
$$
\textrm{MAE} = \frac{1}{n} \sum_{i=1}^n | y_i - \hat{y}_i |,
$$
where
n is the number of rows
\\( \hat{y}_i \\) is the predicted loss
\\( y_i \\) is the actual loss
Submission File
For each row in the test set, you should predict the loss. The file should have a header and looks like the following:
id,loss
118679,0
118680,0
118681,0
...","This data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. You are provided with over two hundred thousand observations and nearly 800 features.  Each observation is independent from the previous. 
For each observation, it was recorded whether a default was triggered. In case of a default, the loss was measured. This quantity lies between 0 and 100. It has been normalised, considering that the notional of each transaction at inception is 100. For example, a loss of 60 means that only 40 is reimbursed. If the loan did not default, the loss was 0. You are asked to predict the losses for each observation in the test set.
Missing feature values have been kept as is, so that the competing teams can really use the maximum data available, implementing a strategy to fill the gaps if desired. Note that some variables may be categorical (e.g. f776 and f777).
The competition sponsor has worked to remove time-dimensionality from the data. However, the observations are still listed in order from old to new in the training set. In the test set they are in random order.",kaggle competitions download -c loan-default-prediction,"['https://www.kaggle.com/code/panamby/loan-default-prediction', 'https://www.kaggle.com/code/kittiyaneerungon/loan-default-prediction', 'https://www.kaggle.com/code/ma12492002/ml-loan-default-prediction', 'https://www.kaggle.com/code/darisdzakwanhoesien2/loan-default-prediction-imperial-college-london', 'https://www.kaggle.com/code/kinsiangchia/loan-default-prediction-rf-xg-svr']"
452,"The goal of PAKDD 2014 competition is to predict future malfunctional components of ASUS notebooks from historical data. This will help estimate how many products will require maintenance or repair services.
ASUS has provided information on its laptop shipments as well as the laptops requiring maintenance or repair services. Participants will use this information to estimate how many of each module of a specific model will require maintenance or repair services.
Acknowledgements
The organizers of PAKDD would like to thank ASUS for sponsorship of this competition.","This competition is evaluated on the mean absolute error (MAE):
$$
\textrm{MAE} = \frac{1}{n} \sum_{i=1}^n | y_i - \hat{y}_i |,
$$
where
n is the number of rows
\\( \hat{y}_i \\) is the predicted target
\\( y_i \\) is the actual target
Submission File
For each row in the test set, you should predict the target. The file should have a header and looks like the following:
id,target
1,0
2,0
3,0
...","In this page, we describe the format of the historical data and the desired output prediction. Two kinds of historical information are given: sale log and repair log. The time period of the sale log is from January/2005 to February/2008; while the time period of the repair log is from February/2005 to December/2009. Details of these two files are described in the File description section.
Participants should exploit the sale and repair log to predict the the monthly repair amount for each module-component from January/2010 to July/2011. In other words, the model should output a series (nineteen elements, one element for one month) of predicted real-value (amount of repair) for each module-component. The desired output format is specified in the File description section.
All the information provided by ASUS omits customers’ personal information. The numbers in the data are transformed from the original values.
File descriptions
SaleTrain.csv - the historical sale data. It specified the number of times that each module-component is sold for each month. Each module-component may have more than one sale log in a month. This file contains four fields: module_category, component_category, year/month, and number_sale. This file should be used for training the predictive model.
RepairTrain.csv - the historical repair data. Each entry in this file is a log of repair, including which module-component to be repaired, the month/year of repair, the month/year that this part is sold, the number of module-component to be repaired. The time period is from February/2005 to December/2009. This file contains five fields: module_category, component_category, year/month(sale), year/month(repair), and number_repair. This file should be used for training the predictive model.",kaggle competitions download -c pakdd-cup-2014,[]
453,"We are pleased to announce the 4th edition of the Large Scale Hierarchical Text Classification (LSHTC) Challenge. The LSHTC Challenge is a hierarchical text classification competition, using very large datasets.
Hierarchies are becoming ever more popular for the organization of text documents, particularly on the Web. Web directories and Wikipedia are two examples of such hierarchies. Along with their widespread use comes the need for automated classification of new documents to the categories in the hierarchy. As the size of the hierarchy grows and the number of documents to be classified increases, a number of interesting machine learning problems arise. In particular, it is one of the rare situations where data sparsity remains an issue, despite the vastness of available data: as more documents become available, more classes are also added to the hierarchy, and there is a very high imbalance between the classes at different levels of the hierarchy. Additionally, the statistical dependence of the classes poses challenges and opportunities for new learning methods.
The challenge is based on a large dataset created from Wikipedia. The dataset is multi-class, multi-label and hierarchical. The number of categories is roughly 325,000 and number of the documents is 2,400,000.
This challenge builds upon a series of successful challenges on large-scale hierarchical text classification. More information can be found at http://lshtc.iit.demokritos.gr/
Very Large Scale Supervised Learning Track
This track concerns multi-label classification based on the Wikipedia dataset. The hierarchy is a graph that can have cycles.  The number of categories is roughly 325,000 and the number of documents is 2,400,000. A document can appear in multiple classes.
Organizers
Ioannis Partalas, LIG, Grenoble, France
Massih-Reza Amini, LIG, Grenoble, France
Ion Androutsopoulos, AUEB, Athens, Greece
Thierry Artières, LIP6, Paris, France
Nicolas Baskiotis, LIP6, Paris, France
Patrick Gallinari, LIP6, Paris, France
Eric Gaussier, LIG, Grenoble, France
Aris Kosmopoulos, NCSR ""Demokritos"" & AUEB, Athens, Greece
George Paliouras, NCSR ""Demokritos"", Athens, Greece
Acknowledgements
Class-Y ANR project, University of Grenoble, University of Pierre and Marie Curie, NCSR ""Demokritos"", and Athens University of Economics and Business. We would also like to thank the Kaggle team for their support.","The evaluation metric for this competition is Macro F1-score (MaF) calculated as follows:
$$MaF=\frac{2*MaP*MaR}{MaP+MaR}$$
For a set of classes \\(C=\{c_1,\ldots,c_k\}\\) macro precision and macro recall are calculated as follows:
$$MaP=\frac{\sum_{i=1}^{|C|}\frac{tp_{c_i}}{tp_{c_i}+fp_{c_i}}}{|C|}$$
$$MaR=\frac{\sum_{i=1}^{|C|}\frac{tp_{c_i}}{tp_{c_i}+fn_{c_i}}}{|C|}$$
where \\(tp_{c_i}\\), \\(fp_{c_i}\\) and \\(fn_{c_i}\\) are the true positives, false positives and false negatives respectively for class \\(c_i\\).
Submission File
The output of each system should be in plain text format. Each line of this file must contain the predicted classes (separated by white spaces) of the hierarchy chosen by the system for the corresponding vector of the test file. Note that, only the leaves of the hierarchy are valid classification answers. The submission file should contain 452,167 lines plus a header row and have this format:
Id,Predicted
1,123 4567 23333
2,542 111
3,345
4,987 9887 1223
...","File descriptions
train - Training set
test - Test set
hierarchy - Wikipedia hierarchy
AllZerosBenchmark - example submission file
knn-baseline - A simple flat kNN baseline
train-remapped, test-remapped - Training and Test sets reformatted per this forum thread
Hierarchy
The hierarchy file contains the information regarding the hierarchy of classes. Each line of this file is a relation between a parent and a child node. For example, the line:
897 67
is to be read as node 897 is parent of node 67
Data",kaggle competitions download -c lshtc,[]
454,"Flight Quest Phase 2 Winners
Final Prizes
1st
Jose Fonollosa
2nd
Sergey Kozub
3rd
Willem Mestrom
4th
Dmytro Lystopad
Read more about the winners »
Milestone Prize
Roman A. Prokopenko
  The submissions to the Final Phase leaderboard closed on Sunday 11:59 pm UTC February 23, 2014.  The teams on the leaderboard carried forward from the Main Phase of the Flight Quest 2 competition.","In this Flight Quest, you are tasked with creating algorithms to generate optimal flight plans (routes) given a flight's origin and destination airports, current air traffic conditions, and weather information.
Each route, represented as a series of points (latitude and longitude) with airspeed and altitude, will be evaluated against the Cost Function.
The Cost Function, a measure of the relative cost in dollars of the flight given a submitted flight plan, seeks to quantitatively capture the direct and indirect costs of flights. See the Simulator code for the exact Cost Function.","The code linked above is provided subject to the limited License to Code contained in the Competition Rules. See the included license.txt for more information.
Simulator Files
Files used and needed by the Flight Simulator can be found on the Flight Quest Data wiki.
Data Files for Final Evaluation Phase
Final Evaluation Set: Jan 18, 2013 - Feb 1, 2014, 15 days
Jan 18: training day, 1-day Simulator files provided for this day
Jan 19-Feb 1: test set evaluation days (leaderboard days)
Guide To Files
Naming convention: Revised files will have ""RevX"" in the name where X represents the revision number. Previous versions of the file will be removed.
FinalEval-training: compressed file containing full FlightStats information for January 18.
FinalEval-testFeatures: compressed file containing features for the test set period.",kaggle competitions download -c flight2-final,[]
455,"Understanding how and why we are here is one of the fundamental questions for the human race. Part of the answer to this question lies in the origins of galaxies, such as our own Milky Way. Yet questions remain about how the Milky Way (or any of the other ~100 billion galaxies in our Universe) was formed and has evolved. Galaxies come in all shapes, sizes and colors: from beautiful spirals to huge ellipticals. Understanding the distribution, location and types of galaxies as a function of shape, size, and color are critical pieces for solving this puzzle.
The Whirlpool Galaxy (M51). Credit: NASA and European Space Agency
With each passing day telescopes around and above the Earth capture more and more images of distant galaxies. As better and bigger telescopes continue to collect these images, the datasets begin to explode in size. In order to better understand how the different shapes (or morphologies) of galaxies relate to the physics that create them, such images need to be sorted and classified. Kaggle has teamed up with Galaxy Zoo and Winton Capital to produce the Galaxy Challenge, where participants will help classify galaxies into categories.
  Image Credit: ESA/Hubble & NASA
Galaxies in this set have already been classified once through the help of hundreds of thousands of volunteers, who collectively classified the shapes of these images by eye in a successful citizen science crowdsourcing project. However, this approach becomes less feasible as data sets grow to contain of hundreds of millions (or even billions) of galaxies. That's where you come in.
This competition asks you to analyze the JPG images of galaxies to find automated metrics that reproduce the probability distributions derived from human classifications. For each galaxy, determine the probability that it belongs in a particular class. Can you write an algorithm that behaves as well as the crowd does?
Contributors: D. Harvey, C. Lintott, T. Kitching, P. Marshall, K. Willett, Galaxy Zoo 
Acknowledgments
The Contributors and the rest of the Galaxy Zoo and Kaggle teams would like to say a big thank you to Winton Capital for helping make this happen. Without their support, we would have not been able to make this competition go ahead.","This competition uses Root Mean Squared Error as the evaluation metric.
\[ \mathrm{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^N(p_i - a_i)^2} \]
Where:
\\(N\\) is the number of galaxies times the total number of responses
\\(p_i\\) is your predicted value
\\(a_i\\) is the actual value
Submission Format
Your submission file must have a header and should be structured in the following format. The Data page provides some example solution files.
GalaxyId,Class1.1,Class1.2,Class1.3,Class2.1, ..., Class11.5, Class11.6
100002,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
100003,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
100004,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
etc...","images_training: JPG images of 61578 galaxies. Files are named according to their GalaxyId.
solutions_training: Probability distributions for the classifications for each of the training images.
images_test: JPG images of 79975 galaxies. Files are name according to their GalaxyId. You will provide probabilities for each of these images. 
all_ones_benchmark: Sample submission file corresponding to the All Ones Benchmark
all_zeros_benchmark: Sample submission file corresponding to the All Zeros Benchmark
central_pixel_benchmark: Simple benchmark that clusters training galaxies according to the color in the center of the image and then assigns the associated probability values to like-colored images in the test set.
The first column in each solution is labeled GalaxyID; this is a randomly-generated ID that only allows you to match the probability distributions with the images. The next 37 columns are all floating point numbers between 0 and 1 inclusive. These represent the morphology (or shape) of the galaxy in 37 different categories as identified by crowdsourced volunteer classifications as part of the Galaxy Zoo 2 project. These morphologies are related to probabilities for each category; a high number (close to 1) indicates that many users identified this morphology category for the galaxy with a high level of confidence. Low numbers for a category (close to 0) indicate the feature is likely not present. 
Visit the Galaxy Zoo Decision Tree page for a detailed description of the data.",kaggle competitions download -c galaxy-zoo-the-galaxy-challenge,"['https://www.kaggle.com/code/shibinjudah/galaxy-morphology-predictor', 'https://www.kaggle.com/code/helmehelmuto/keras-cnn', 'https://www.kaggle.com/code/atogni85/galaxy-convnet', 'https://www.kaggle.com/code/henriquelimanas/galaxy-zoo-classifier-galaxies', 'https://www.kaggle.com/code/hironobukawaguchi/galaxy-zoo-xception']"
456,"Seasonal influenza, commonly referred to as “the flu”, affects 5-20% of the United States population ever year, causing over 200,000 people to be hospitalized from associated complications. Influenza is a contagious respiratory illness, which can range in severity from mild cases with cold-like symptoms to death.
Flu epidemics are fast-moving and spread rapidly due to rapid viral reproduction and short generation times (time from when an infected person infects another), which makes them very difficult to control. Additionally, there are several different strains of the influenza virus and new viruses constantly evolving. All together, this poses a significant challenge when it comes to predicting when, where and at what level of severity the flu will strike during the flu season.
The objective of this competition is to build an algorithm that helps predict where the occurrence, peak and severity of influenza in a given season.
Enter Now!
This competition is only open to Masters-level participants who meet the eligibility criteria. Visit the Enter the Competition page to view the eligibility criteria and request entrance.
* Image courtesy of CDC","This competition uses the root mean square logarithmic error evaluation metric:
\[ \varepsilon = \sqrt{\frac{1}{N}\sum_{i=1}^N [\log(p_i+1) - \log(a_i+1) ]^2} \]
where p is the prediction, a is the solution, and log is the natural log.",,,
457,"Each year, millions of people fill out a bracket to predict the outcome of the popular men’s college basketball tournament that tips off in March. While the odds of creating a perfect bracket are astronomical, these odds are made better by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media. How well can machine learning and statistical techniques improve the forecast? Presented by Intel, this competition will test how well predictions based on data stack up against a (jump) shot in the dark.
We have assembled the basic elements necessary to get started with tournament prediction. The provided data covers nearly two decades of historical games, but you’re also encouraged to use data from external sources. To help turn all of that information into useful insight, Intel is making its big data technologies more affordable, available, and easier to use for everything from helping develop new scientific discoveries and business models to gaining the upper hand on good-natured predictions of sporting events.
In stage one of this two-stage competition, participants will build and test their models against the previous five tournaments. In the second stage, participants will predict the outcome of the 2014 tournament. You don’t need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2014 results, for which you’ll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket.
To sweeten the pot, Intel will present the team with the most accurate predictions a $15,000 cash prize. Get started today – predictions are due by Wednesday, March 19, 2014.
Please visit the FAQs for more information.","Submissions are scored on the log loss, also called the predictive binomial deviance:
$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
where
n is the number of games played
\\( \hat{y}_i \\) is the predicted probability of team 1 beating team 2
\\( y_i \\) is the outcome of each game
\\( log() \\) is the natural (base e) logarithm
A smaller log loss is better. Games which are not played are ignored in the scoring. The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.
Submission File
The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2014 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2  = 2278 matchups. 
Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, ""N_501_502"" indicates team 501 played team 502 in season N.
The resulting submission format looks like like the following, where ""pred"" represents the predicted probability that the first team will win:
id,pred
N_503_507,0.2
N_503_511,0.5
N_503_521,0.8
...","If you are unfamiliar with the format and intricacies of the NCAA tournament, we encourage reading its wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but it's not as complicated as it looks.
As a reminder, you are encouraged to incorporate your own sources of data. We have provided team-level historical data to jump-start the modeling process, but there is also a world of player-level and game-level data that may be useful.
We extend our gratitude to Kenneth Massey for his work gathering and providing the historical data.
What to predict
Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA tournaments.
Stage 2 - You should submit predicted probabilities for every possible matchup before the 2014 tournament begins.
Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.
File descriptions
Below we describe the format and fields of the ""essential"" data files. Optional files may be added to the data while the competition is running. You can assume that we will provide the essential files for the current season. You should not assume that we will provide optional files for the current season. To avoid confusion, we will keep the current season data (for stage 2) separate from the historical data (stage 1).",kaggle competitions download -c march-machine-learning-mania-2014,[]
458,"Boston Data Festival is hosting a Hackathon on Sunday 11/10/13 from 10 am to 6 pm. The event will take place at Hack/Reduce (275 Third Street, Cambridge, MA).
The goal of the Hackathon is to predict the directional accuracy of a stock prices. The following cash prices will be awarded!
1st place: $500
2nd place: $250
Best submission using Matlab: $250
Self-respect from doing better than a monkey throwing darts: Priceless.
During the Hackathon every participant can use a free Matlab license.
Acknowledgements
We thank MathWorks for providing free licences for competitors during the competition. ","Value to predict - probability of stock moving up from  opening of day 10 to closing of day 10.
Prediction should be in 0-1 range:
1 - ""stock surely will go up""
0- ""stock surely will go down"".
Test set is randomly sampled without overlapping from year following training data time period.
Model performance metrics - AUC.
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
Public leaderboard based on 10 segments of the test dataset with 15 segments used to create private leaderboard.
Prize award is based on private leaderboard.
Several cases when stock has the same price at the close as at the opening were excluded from score calculations.
Submission Format
Submission files should contain two columns: ID and Prediction. 
The file should contain a header and have the following format:
Id,Prediction
1,0
2,1
3,0
4,1
etc.","Data consists of two files:
training.csv - time series for 94 stocks (94 rows). First number in each row is the stock ID. Then data for 500 days. Data for each day contain - day opening price, day maximum price, day minimum price, day closing price, trading volume for the day. Price data normalised to the first day opening price.
test.csv - data to create prediction. Data provided for 25 time segments. Each segment contains data for the same 94 stocks. Each segment has opening, max, min, closing, volume data for 9 days and opening for day #10. Each line of the file starts with segment number following by stock ID and then price and volume data organized by day the same way as training set.  Price data normalised to the first day opening price.
Each line in train.csv and test.csv contains consecutive trading days. Days when market was closed were excluded. Thus day N may be Friday and day N+1 may be Monday or even Tuesday if Monday was a holiday. 
  Value to predict - probability of stock moving up from  opening of day 10 to closing of day 10. Prediction should be in 0-1 range, where 1 - ""stock surely will go up"", 0- ""stock surely will go down"".
Test set is randomly sampled without overlapping from year following training data time period.
File descriptions",kaggle competitions download -c boston-data-festival-hackathon,
459,"CIFAR-10  is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
Kaggle is hosting a CIFAR-10 leaderboard for the machine learning community to use for fun and practice. You can see how your approach compares to the latest research methods on Rodrigo Benenson's classification results page.
Please cite this technical report if you use this dataset: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.","Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly).
Submission Format
For each image in the test set, predict a label for the given id. Your labels must match the official labels exactly {airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck}. Your submission should have a header.
id,label
1,cat
2,cat
3,cat
4,cat
...","The CIFAR-10 data consists of 60,000 32x32 color images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images in the official data. We have preserved the train/test split from the original dataset.  The provided files are:
train.7z - a folder containing the training images in png format
test.7z - a folder containing the test images in png format
trainLabels.csv - the training labels
To discourage certain forms of cheating (such as hand labeling) we have added 290,000 junk images in the test set. These images are ignored in the scoring. We have also made trivial modifications to the official 10,000 test images to prevent looking them up by file hash. These modifications should not appreciably affect the scoring. You should predict labels for all 300,000 images.
The label classes in the dataset are:
airplane 
automobile 
bird 
cat ",kaggle competitions download -c cifar-10,"['https://www.kaggle.com/code/roblexnana/cifar10-with-cnn-for-beginer', 'https://www.kaggle.com/code/ayushnitb/cifar10-custom-resnet-cnn-pytorch-97-acc', 'https://www.kaggle.com/code/devsubhash/cifar-10-image-classification-using-cnn', 'https://www.kaggle.com/code/datajameson/cifar-10-object-recognition-cnn-explained', 'https://www.kaggle.com/code/kedarsai/cifar-10-88-accuracy-using-keras']"
460,"Understanding customer loyalty is an important part of any business. The ability to predict ahead of time when a customer is likely to churn can enable early intervention processes to be put in place, and ultimately a reduction in customer churn.  This competition seeks a solution for predicting which current customers of an insurance company will leave in 12 months time, and when.
This competition is now closed to new entrants.","This competition asks you to predict the probability that a customer will churn in the next 12 months. The evaluation metric will be based on the Area Under the ROC Curve.
Please download the Additional Competition Information from the Data page for further details.",,,
461,"It's that time of year again, when Santa and his helpers gear up for their big night. Last year's path recommendations were such a success that Santa is back for more. As the latest in a line of many data science converts, Santa is looking to you to help pack his sleigh.




♫ Then one foggy Christmas Eve
Santa came to say
""Rudolph with your code so bright
won't you fill my sleigh tonight?"" ♫







Problem Description
Given a list of presents, pack them in Santa's sleigh as compactly as possible and in the best order possible.
The sleigh and presents are discretized and described in units of the fundamental length unit \\(\ell\\). The sleigh is 1000 x 1000 with infinite vertical extent as needed by your highest placed present. The cells of the sleigh go from 1 to 1000 for the length and width, and 1 to infinity in height.
Presents come in random sizes and are represented by their extent in the x, y, and z dimensions. Each present has a PresentId, a number which determines the order in which it is to be delivered. Ideally, Present 1 is to be delivered first, Present 2 second, etc. 
PresentId,Dimension1,Dimension2,Dimension3
1,2,5,3
2,243,207,73
Present 1 is 2 x 5 x 3 \\(\ell^3\\) and Present 2 is 243 x 207 x 73 \\(\ell^3\\). Presents can be packed in any orientation provided they are parallel and perpendicular to the x-y-z axes, meaning they can be rotated in any direction by multiples of 90\\(^\circ\\) but not, for example, by 60\\(^\circ\\). 
Please see the evaluation page to learn how your packing configurations will be scored and for the submission file schema.
Acknowledgements
This competition is brought to you by MathWorks, creators of MATLAB® and Simulink®. Learn more about MathWorks.","The packing of Santa's sleigh will be judged by (1) the compactness of the packing and (2) the ordering of the presents. 
Compactness is measured by the maximum height of the highest present. Ordering is determined by going down each horizontal cross section of the sleigh (an xy-layer for a fixed z) and noting the order in which presents are packed. The evaluation metric M is given by 
\[ M = 2 \max_i(z_i) + \sigma(\Gamma)\]
where
\[z_i = z\mathrm{-coordinate\ of\ the\ ith\ present} \]
\[\sigma(\Gamma) = \sum_{i=1}^{N_p} |i-\Gamma_i|\]
\[\Gamma = \mathrm{order\ the\ presents\ appear}\] and 
\[\Gamma_i = \mathrm{the\ Present\ Id\ in\ the\ ith\ position}\]
Note that
\[\Gamma_{\mathrm{ideal}} = 1, 2, 3, 4, \ldots, N_p\]
\[\sigma(\Gamma_{\mathrm{ideal}}) = 0\]
where \\(N_p\\) is the total number of presents.
Example Calculation
Consider a simple 2D example (Santa's sleigh is in 3D). We discretize the sleigh using a grid of spacing \\(\ell\\) where all lengths (sleigh dimensions and present dimensions) are integer-multiples of this spacing. We are using occupied and unoccupied cells to calculate the metric, not Cartesian coordinates. Thus Present 8 has its vertices in cells (1,1), (1,3), (3,1) and (3,3); Present 3 has its vertices in cells (8, 1), (10, 1), (8, 5) and (10, 5).
The maximum z-extent of presents in this example is 8, so 
\[\max_i(z_i) = 8\]
Presents 2 and 9 are the highest packed presents in the sleigh and are found to be occupying cells at the same height.
This table describes the order-based calculation.
Layer List of Presents (repeat) Layer Configuration
8 2, 9 2, 9
7 1, (2), 5, 6, (9) 1, 5, 6
6 (1), (2), (5), (6), (9)
5 (1), (2), 4, 3 3, 4 (sorted)
4 (1), (2), (4), (3)
3 8, (4), 10, (3) 8, 10
2 (8), 7, (10), (3) 7
1 (8), (7), (10), (3)
The configurations for each layer are then concatenated to create the overall present order configuration for the entire sleigh. For each layer, the presents are sorted in numerical order to give the best possible score for that layer. In this example, the final order configuration for the sleigh is
\[\Gamma = 2, 9, 1, 5, 6, 3, 4, 8, 10, 7\]
Solving for the ordering term:
\[\sigma(\Gamma) = |1-2| + |2-9| + |3-1| + \cdots + |10-7|  = 22\]
Combining these terms gives the overall Metric for this competition:
\[ M = 2*(8) + (22) = 38\]
Submission File
For every present, submission files should contain 25 columns: a PresentId followed by the coordinates for each of the 8 vertices of a 3D box, thus 24 cell coordinates. x1,y1,z1 are for the first vertex, x2,y2,z2 are for the second vertex and so on. The vertices can be listed in any order. Coordinate values are integer multiples of \\(\ell\\).
So, for box 8 above (in the 2D example), the coordinates would be (1,1), (3,1), (1,3), (3,3). There are only 4 vertices for box 8 since this example is in 2D. Note also that the coordinates of the box are the occupied cells of the present, and that the ""origin"" is at (1,1). The ""origin"" in the 3D problem will be (1,1,1).
The submission file should contain a header and have the following format:
PresentId,x1,y1,z1,x2,y2,z2,x3, ...,x8,y8,z8
1,1,1,1,3,1,1,1,3,1,3,3,1,3,3,5,1,1,5,3,1,5,3,3,5
2,4,3,6,9,3,6,4,10,6,9,10,6,4,3,15,9,3,15,4,10,15,9,10,15
etc.
Scoring on the website takes about ~45 seconds, so please be patient. And it's a good idea to upload zipped submission files. There are a million presents after all!","Data file description
presents.csv - list of 1,000,000 presents that need to be packed (in .csv format). 
presents.mat - list of 1,000,000 presents that need to be packed (in MATLAB .mat format).
Note: Both files contain the same list of presents and present dimensions and you don't necessarily need to download both versions of the files.
Data fields
PresentId - a number that indicates the order this present should ideally be delivered
Dimension1, Dimension2, Dimension3 - integer length units for the 3-dimensional package
Additional files
Santas_Sleigh_MATLAB_Sample_Code.m - MATLAB sample code to get competitors started on the Packing Santa's Sleigh Competition. Use it to import data as a .csv file, analyze and visualize the data, implement a naive packing strategy, evaluate the competition metric, and export a submission file.
Santas_Sleigh_MATLAB_Metric_Calculation_v2.m - MATLAB function that can be called from a script or from the command line to calculate the competition metric. Update: “v2” corrects the boundaries on the present collision check.",kaggle competitions download -c packing-santas-sleigh,[]
462,"The Neural Information Processing Scaled for Bioacoustics (NIPS4B) bird song competition asks participants to identify which of 87 sound classes of birds and their ecosystem are present in 1000 continuous wild recordings from different places in Provence, France. The data is provided by the BIOTOPE society, which maintains the largest collection of wild recordings of birds in Europe. This challenge is a more complex task than the previous ICML4B challenge, in which 77 teams participated (see proceedings at sabiod.org).
For more information about the Neural Information Processing Scaled for Bioacoustics workshop, please visit the official site.
Organizers
Pr. H. Glotin - Institut Universitaire de France, CNRS LSIS and USTV, glotin@univ-tln.fr
O. Dufour - CNRS LSIS, FR Dr. Y. Bas - BIOTOPE, FR","Submissions are judged on area under the ROC curve. 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
Submission Format
We combine the name of each test file with the number of the class we consider into a single ""ID"" column. The header line must be ""ID,Probability"". The format is:
ID,Probability
nips4b_birds_testfile0001.wav_classnumber_1,0.442
nips4b_birds_testfile0001.wav_classnumber_2,0.124
nips4b_birds_testfile0001.wav_classnumber_3,0.03214324
nips4b_birds_testfile0001.wav_classnumber_4,0.65436
nips4b_birds_testfile0001.wav_classnumber_5,0.321436
nips4b_birds_testfile0001.wav_classnumber_6,0.54677
nips4b_birds_testfile0001.wav_classnumber_7,0.733
...
nips4b_birds_testfile1000.wav_classnumber_87,0.004325","The training set contains 687 files. Each species is represented by nearly 10 training files (within various context / other species). The files were recorded at a frequency sample of 44.1 kHz on an SM2 system.
The test set is composed of 1000 files. All species in the test set are in the training set. The training set matches the test set conditions. Provided here are two sample clips:
Sylvia cantillans (which is singing) and Sylvia melanocephala (which is calling)
Sylvia cantillans (which is also singing) and Petronia petronia (which is calling)
The histogram of train and test file durations is here:",kaggle competitions download -c multilabel-bird-species-classification-nips2013,"['https://www.kaggle.com/code/leirahua/bird-songs-pad-and-resize-spectrogram', 'https://www.kaggle.com/code/leirahua/classify-bird-songs-using-spectrogram', 'https://www.kaggle.com/code/yuhaeun/12-mfcc', 'https://www.kaggle.com/code/adelyagarieva/kernel45f13df162', 'https://www.kaggle.com/code/leirahua/bird-songs-generate-spectrograms']"
463,"The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state:
Any live cell with fewer than two live neighbours dies, as if by underpopulation.
Any live cell with two or three live neighbours lives on to the next generation.
Any live cell with more than three live neighbours dies, as if by overpopulation.
Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction.
These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics.  As Wikipedia tells it,
Ever since its publication, Conway's Game of Life has attracted much interest, because of the surprising ways in which the patterns can evolve. Life provides an example of emergence and self-organization. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that ""design"" and ""organization"" can spontaneously emerge in the absence of a designer. For example, philosopher and cognitive scientist Daniel Dennett has used the analogue of Conway's Life ""universe"" extensively to illustrate the possible evolution of complex philosophical constructs, such as consciousness and free will, from the relatively simple set of deterministic physical laws governing our own universe.
The emergence of order from simple rules begs an interesting question--what happens if we set time backwards?
This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse.  Is the chaotic start of Life predictable from its orderly ends?  We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board. Although some people have examined this problem, it is unknown (at least, to us...) just how difficult this will be.","You are evaluated on the mean absolute error of your predictions. In this case, this is equivalent to 1 - (classification accuracy). You may only predict 0 (dead) or 1 (alive) for each cell.
Submission File
For every game in the test set, your submission file should list the predicted starting board on a single row.  Values are listed in a column-wise order. That is, if you want to predict a matrix,
1 2
3 4
the predicted row would be (1,3,2,4). The submission file should contain a header and have the following format:
id,start.1,start.2,start.3,...,start.400
1,0,0,0,0,0,0,...
2,0,0,0,0,0,0,...
etc.","We have provided 50,000 training games and 50,000 test games, whose starting board you must predict.  Each board is 20x20, for a total of 400 cells per board. Values are listed in a column-wise order.  You are free to create more training games if you desire.
The provided variables are:
id - each game has an id
delta - the number of steps between the start and stop boards 
start.1 - row 1, column 1 of the game's starting board
start.2 - row 2, column 1 of the game's starting board
...
stop.1 - row 1, column 1 of the game's stopping board
...
Your test-set predictions should be the starting board at delta steps before the stopping board. The games were created by the following procedure:
An initial board was chosen by filling the board with a random density between 1% full (mostly zeros) and 99% full (mostly ones).",kaggle competitions download -c conway-s-reverse-game-of-life,"['https://www.kaggle.com/code/ptyshevs/cnn-for-reversing-game-of-life', 'https://www.kaggle.com/code/ruchibahl18/starting-of-an-end-game', 'https://www.kaggle.com/code/wepasabyrow/conway-s-reverse-game-of-life']"
464,"This competition is the successor to the See Click Predict Fix Hackathon. The purpose of both competitions is to quantify and predict how people will react to a specific 311 issue. What makes an issue urgent? What do citizens really care about? How much does location matter? Being able to predict the most pressing 311 topics will allow governments to focus their efforts on fixing the most important problems. The data set for the competitions contains several hundred thousand 311 issues from four cities.
For those who are more interested in using the data for visualization or ""non-predictive"" data mining, we have added a $500 visualization prize. You may submit as many entries as you wish via the Visualization page. If you're plotting issues on maps, displaying the text in some meaningful way, or making any other creative use of the data, save it and post it!
About 311
311 is a mechanism by which citizens can express their desire to solve a problem the city or government by submitting a description of what needs to be done, fixed, or changed. In effect, this provides a high degree of transparency between government and its constituents. Once an issue has been established, citizens can vote and make comments on the issue so that government officials have some degree of awareness about what is the most important issue to address.
Sponsors
The meeting space has been provided by Microsoft.  Prize money is graciously offered by our sponsors:
On the citizen side, SeeClickFix leverages crowdsourcing to help both maintain the flow of incoming requests but show the public how effective you can be. When anyone in the community can report or comment on any issue, the entire group has a better perspective on what's happening--and how to fix it effectively.
For governments, SeeClickFix acts as a completely-customizable CRM that plugs into your existing request management tools. From types of service requests to managing different watch areas, SeeClickFix helps better maintain and fulfill 311 requests in your city.
A public policy entrepreneur and open innovation expert David advises numerous governments on open government and open data and works with leading non-profits and businesses on strategy, open innovation and community management. In addition to his work, David is an affiliate with the Berkman Centre for Internet and Society at Harvard where he is looking at issues surrounding the politics of data.
You can find David's writing on open innovation, public policy, public sector renewal and open source systems at his blog, or at TechPresident. In addition to his writing, David is frequently invited to speak on open government, policy making, negotiation and strategy to executives, policymakers, and students.
You can read a background on how this challenge came to be here.","Your model should predict, for each issue in the test set, the number of views, votes, and comments. We will use the Root Mean Squared Logarithmic Error (RMSLE) to measure the accuracy.
The RMSLE is calculated as
\[ \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } \]
Where:
\\(n\\) is three times the total number of issues in the test set (summing over each of views, votes, and comments for each issue)
\\(p_i\\) is your predicted value
\\(a_i\\) is the actual value
\\(\log(x)\\) is the natural logarithm
Submission Format
Your submission file must have a header and should be structured in the following format:
id,num_views,num_votes,num_comments
343272,0,0,0
274860,0,0,0
43608,0,0,0
etc...","You are provided with 311 issues from four cities covering the time period since 2012. The goal of the contest is to predict the number of views, votes, and comments that a given issue has received to date. The training set contains the 311 data with the three target variables. The test set contains just the 311 data.
While we have done a small amount of data cleaning, this is largely raw data from SeeClickFix. It will contain noise! Expect to find repeated issues, completed descriptions, and any number of data quality hurdles. Among the unique challenges of this data set:
SeeClickFix is dynamically evolving - adding users, incorporating new input sources, and changing how it is structured. Your predictions may be affected by global influences outside the issues themselves.
Communities are dynamically evolving as well, as are the governments responding the issues.
Older 311 issues leave more time for people to view & vote & comment, but solved quickly become less relevant.
The data is coming from many different inputs, which may result in systematic differences in the response to the issues.
Data Dictionary
id - a randomly assigned id
latitude - the lattitude of the issue
longitude - the longitude of the issue
- a short text title
- a longer text explanation
- the number of user-generated votes
- the number of user-generated comments
 - the number of views
- a categorical variable indicating where the issue was created
- the time the issue originated
- a categorical variable (assigned automatically) of the type of issue",kaggle competitions download -c see-click-predict-fix,['https://www.kaggle.com/code/masdeval/seeclickfix']
465,"The Personalized Web Search Challenge provides a unique opportunity to consolidate and scrutinize the work from industrial labs on personalizing web search using user-logged search behavior context. It provides a fully anonymized dataset shared by Yandex, which has anonymized user ids, queries, query terms, urls, url domains and clicks.
This Challenge and the shared dataset will enable a whole new set of researchers to study the problem of personalizing web search experience. The Personalized Web Search Challenge is a part of series of contests organized by Yandex over many years. This year’s event is the eighth since 2004. In previous years, participants tried to learn to rank documents, predict traffic jams, find similar images, predict relevance of documents using search logs and detect search engine switchings in search sessions.
The Challenge is intended as a logical follow-up to the previous two challenges. We ask participants to re-rank URLs of each SERP returned by the search engine according to the personal preferences of the users. In other words, participants need to personalize search using the long-term (user history based) and short-term (session-based) user context. The evaluation relies on a variant of a dwell-time based model of personal relevance and is data-driven, as it is presently accepted in the state-of the-art research on personalized search.
The Challenge is a part of the Web Search Click Data workshop (WSCD 2014) and the reports of the best teams are welcome to be presented at this workshop, to be held at WSDM 2014 conference, on 28th February in New York, USA. The workshop is organized by Pavel Serdyukov (Yandex), Georges Dupret (Yahoo!) and Nick Craswell (Microsoft Research/Bing). ","Metric
The goal of this competition is to re-rank top-10 URLs returned by the search engine in response to a user query using the history of clicks on URLs for all users and the user issuing the current query in particular. 
Submissions will be evaluated using NDCG (Normalized Discounted Cumulative Gain, Kaggle Link) measure, which will be calculated using the ranking of URLs provided by participants for each query, and then averaged over queries.
The URLs are labeled using 3 grades of relevance: 0 (irrelevant), 1 (relevant), 2 (highly relevant). The labeling is done automatically, based on dwell-time and, hence, user-specific:
0 (irrelevant) grade corresponds to documents with no clicks and clicks with dwell time strictly less than 50 time units
1 (relevant) grade corresponds to documents with clicks and dwell time between 50 and 399 time units (inclusively)
2 (highly relevant) grade corresponds to the documents with clicks and dwell time not shorter than 400 time units. In addition, the relevance grade of 2 assigned to the documents associated with clicks which are the last actions in the corresponding sessions.
If a document was clicked several times then the maximum dwell time is used to label the document's relevance. Similarly, the document associated with the click which is the last action in its session is always assigned with the relevance grade of 2, independently of other possible clicks on the same document. Dwell time is the time passed between the click on the document and the next click or the next query. It is well-known that dwell time is well correlated with the probability of the user to satisfy her information need with the clicked document.  Clicks with dwell time longer than a predefined threshold are often called ""satisfied clicks"" in the state-of-the-art research of web search personalization.
We distinguish two types of satisfied clicks in this competition. We consider that the documents that are labeled using dwell time as 2 (highly relevant) are not necessarily indeed more relevant for the user than 1 (relevant) ones, but our confidence in their relevance should certainly be higher.
Submission Format
As long as only ranking of documents for a particular query in its particular session is important for calculating NDCG, we ask to submit a list of SessionID-URLID ranked from top to bottom according to their relevance (i.e. the top URLID is the most relevant in a particular session). We identify test queries by their SessionIDs, since we have no more than one test query per session. Thus, each submission should represent a CSV file with one comma-separated SessionID-URLID pair on a line:
SessionID,URLID
SessionID_1,URLID_1
SessionID_1,URLID_2
SessionID_1,URLID_3
SessionID_1,URLID_4
...
SessionID_N,URLID_N
In the above example URLID_1 is supposed to be the most relevant URLID for the test query in the SessionID_1 session. 
The first line is a header which must be included in each submission. Please also check the format of the provided baseline submissions.
All and only SessionIDs of sessions from the test period must be included into this list. Only URLIDs presented as results for the queries with TypeOfRecord=T should be re-ranked (see Log Format page).
For example, if we have a test session:
34573630 M 28 15
34573630 0 Q 0 10507991 3139706,2771252,3808573 34169548,3278460 34165793,3278348 35438447,3339074 15367590,1582976 31337693,3075260 43622876,3822427 26061675,2596986 29897513,2901859 39010230,3548763 62850010,4824984
34573630 6 C 0 34169548
34573630 250 T 1 2338342 1255686,3591321,1687414,3416146,4342041 56906042,4503913 21293423,2183949 3580938,482441 21291242,2183806 14221334,1461559 43622870,3822427 58185226,4577130 6936569,855329 5736329,747654 52480003,4295034
Then the part of the submission corresponding to this session might look like:
34573630,56906042
34573630,21293423
34573630,3580938
34573630,21291242
34573630,14221334
34573630,43622870
34573630,58185226
34573630,6936569
34573630,5736329
34573630,52480003","The dataset includes user sessions extracted from Yandex logs, with user ids, queries, query terms, URLs, their domains, URL rankings and clicks. Warning: the training set is ~16GB when uncompressed.
To allay privacy concerns the user data is fully anonymized. Only meaningless numeric IDs of users, queries, query terms, sessions, URLs and their domains are released. The queries are grouped by sessions.
Noteworthy characteristics of the dataset:
Unique queries: 21,073,569
Unique urls: 703,484,26
Unique users: 5,736,333
Training sessions: 34,573,630
Test sessions: 797,867
Clicks in the training data: 64,693,054
Total records in the log: 167,413,039
Preprocessing done with the raw dataset before release:
The logs are about two years old and represent one month of search activity",kaggle competitions download -c yandex-personalized-web-search-challenge,[]
466,"In this competition you are provided a set of tweets related to the weather. The challenge is to analyze the tweet and determine whether it has a positive, negative, or neutral sentiment, whether the weather occurred in the past, present, or future, and what sort of weather the tweet references. It's a lot to mine from so few characters, but if the going gets tough you can always blame the weather...
""Please knock out the power giant storm that is passing thru....please."" -Tweet #74096
CrowdFlower
We are excited to team up with CrowdFlower on the first of what we hope will be many fun machine learning projects. CrowdFlower is debuting a new open data library and we're always looking for an excuse to have a competition. Why is this exciting? Sweet, sweet Labels.
Data repositories sometimes have more in common with a landfill than a library. They're home to tattered piles of spreadsheets in odd formats with nary a shred of documentation to tell the GDP of Chile from the migratory patterns of North American goldfinches. If creating value from this digital exhaust is a defining theme of the big data explosion, most repositories leave you choking on the diesel fumes of data disappointment. Such data is great if you are doing a report on the GDP of Chile, but not so useful if you are doing machine learning, or its red-headed step child, data science.
Crowdflower's data sets provide the thing that makes so many repositories fall short - data paired with labels. One can decide whether two English sentences are related, make judgments about yogurt chatter, or rank emotions on tweets about nuclear energy. It's all about the (wo)manpower to label what these bytes actually mean.
The Open Data Library
CrowdFlower Open Data Library is a repository of real data set samples that developers, researchers and data scientists can download and use to test and improve algorithms. Our mission is to encourage users to explore the possibilities and power of crowdsourcing. Open Data is free, available to anyone, and ready-to-use with CrowdFlower’s Platform.
New data sets are continuously added to CrowdFlower Open Data Library as users of the CrowdFlower Platform opt-in to share their data with the crowdsourcing community. Sample data sets currently available include tweets for sentiment and topic analysis, word combinations to test similarities, sentence combinations to test related topics, and more. Learn more at www.crowdflower.com.","The Root Mean Squared Error (""RMSE"") is used to measure the accuracy:
\[ \textrm{RMSE} = \sqrt{\frac{\sum_{i=1}^n (p_i - a_i)^2}{n} } \]
Where:
\\( n \\) is 24 times the total number of tweets
\\( p_i \\) is the predicted confidence rating for a given label
\\( a_i \\) is the actual confidence rating for a given label
Submission format
For each tweet in the test set, predict the confidence score for each of the 24 possible labels.  Submission files should be in the following format and must have a header.
id,s1,s2,s3,s4,...,k12,k13,k14,k15
4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
etc.","The training set contains tweets, locations, and a confidence score for each of 24 possible labels.  The 24 labels come from three categories: sentiment, when, and kind. Human raters can choose only one label from the ""sentiment"" and ""when"" categories, but are allowed multiple choices for the ""kind"". Your goal is to predict a confidence score of all 24 labels for each tweet in the test set.
s1,""I can't tell""
s2,""Negative""
s3,""Neutral / author is just sharing information""
s4,""Positive""
s5,""Tweet not related to weather condition""
w1,""current (same day) weather""
w2,""future (forecast)""
w3,""I can't tell""
w4,""past weather""
k1,""clouds""
k2,""cold""
k3,""dry""
k4,""hot""
k5,""humid""
k6,""hurricane""
k7,""I can't tell""
k8,""ice""
k9,""other""
k10,""rain""
k11,""snow""
k12,""storms""
k13,""sun""
k14,""tornado""
k15,""wind""",kaggle competitions download -c crowdflower-weather-twitter,"['https://www.kaggle.com/code/wepasabyrow/partly-sunny-with-a-chance-of-hashtags', 'https://www.kaggle.com/code/mohitsital/0-16372-predict-the-weather', 'https://www.kaggle.com/code/swanawoo/partlysunny-tfidfdl', 'https://www.kaggle.com/code/ruchibahl18/predict-the-weather-bag-of-words', 'https://www.kaggle.com/code/ashwinikaldate/partly-sunny-with-a-chance-of-hashtags']"
467,,,"Decryption key for data.7z - 2p6yvksurc4wtw7srmf7
You are provided with 311 issues from four cities (Oakland, Richmond, New Haven, Chicago) covering the time period since 2012. The goal of the contest is to predict the number of views, votes, and comments that a given issue has received to date. The training set contains the 311 data with the three target variables. The test set contains just the 311 data.
While we have done a small amount of data cleaning, this is largely raw data from SeeClickFix. It will contain noise! Expect to find repeated issues, completed descriptions, and any number of data quality hurdles. Among the unique challenges of this data set:
SeeClickFix is dynamically evolving - adding users, incorporating new input sources, and changing how it is structured. Your predictions may be affected by global influences outside the issues themselves.
Communities are dynamically evolving as well, as are the governments responding the issues.
Older 311 issues leave more time for people to view & vote & comment, but solved issues quickly become less relevant.
The data is coming from many different inputs, which may result in systematic differences in the response to the issues.
Data Dictionary
id - a randomly assigned id
latitude - the lattitude of the issue
- the longitude of the issue
- a short text title
- a longer text explanation
- the number of user-generated votes
- the number of user-generated comments
 - the number of views
- a categorical variable indicating where the issue was created
- the time the issue originated
- a categorical variable (assigned automatically) of the type of issue",kaggle competitions download -c the-seeclickfix-311-challenge,[]
468,"In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat.  This is easy for humans, dogs, and cats. Your computer will find it a bit more difficult.
Deep Blue beat Kasparov at chess in 1997.
Watson beat the brightest trivia minds at Jeopardy in 2011.
Can you tell Fido from Mittens in 2013?
The Asirra data set
Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords.
Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface:
Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research. 
Image recognition attacks
While random guessing is the easiest form of attack, various forms of image recognition can allow an attacker to make guesses that are better than random. There is enormous diversity in the photo database (a wide variety of backgrounds, angles, poses, lighting, etc.), making accurate automatic classification difficult. In an informal poll conducted many years ago, computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art. For reference, a 60% classifier improves the guessing probability of a 12-image HIP from 1/4096 to 1/459.
State of the art
The current literature suggests machine classifiers can score above 80% accuracy on this task [1]. Therfore, Asirra is no longer considered safe from attack.  We have created this contest to benchmark the latest computer vision and deep learning approaches to this problem. Can you crack the CAPTCHA? Can you improve the state of the art? Can you create lasting peace between cats and dogs?
Okay, we'll settle for the former. 
Acknowledgements
We extend our thanks to Microsoft Research for providing the data for this competition.
Jeremy Elson, John R. Douceur, Jon Howell, Jared Saul, Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization, in Proceedings of 14th ACM Conference on Computer and Communications Security (CCS), Association for Computing Machinery, Inc., Oct. 2007","Performance is evaluated on the percentage of correctly labeled images. To determine your odds of breaking the Asirra CAPTCHA, raise your percentage to the 12th power.
""But classification accuracy is a flawed metric!"" you scream at your monitor in fury, ""my genius requires you accept the posterior probability of my predictions!""  That may be true, but sometimes simplicity is just nice. Here there are only dogs and cats... no 0.5 dog-cat hybrid guesses allowed!
Submission Format
Your submission should have a header. For each image in the test set, predict a label for its id (1 = dog, 0 = cat):
id,label
1,0
2,0
3,0
etc...","The training archive contains 25,000 images of dogs and cats. Train your algorithm on these files and predict the labels for test1.zip (1 = dog, 0 = cat).
A note on hand labeling
Per the rules and spirit of this contest, please do not manually label your submissions. We work hard to fair and fun contests, and ask for the same respect in return.
 
""I'm a good doggie who deserves a good algorithm.
And chew bones. Please send chew bones.""
However, given that there is always one willing to cheat to rise to glory, we reserve the right to wield the following anti-doping measures:
Ask to spot check your code at any point in the competition. Failure to provide proof of algorithm within a reasonable time frame may result in disqualification.
Release a new test set towards the end of the competition and require predictions for this new set (Asirra has 3 million images, lest you think we'll run out of cats).",kaggle competitions download -c dogs-vs-cats,"['https://www.kaggle.com/code/uysimty/keras-cnn-dog-or-cat-classification', 'https://www.kaggle.com/code/ruchibahl18/cats-vs-dogs-basic-cnn-tutorial', 'https://www.kaggle.com/code/bulentsiyah/dogs-vs-cats-classification-vgg16-fine-tuning', 'https://www.kaggle.com/code/bhuvanchennoju/hey-siri-is-it-a-or-class-f1-0-992', 'https://www.kaggle.com/code/serkanpeldek/keras-cnn-transfer-learnings-on-cats-dogs-dataset']"
469,"Looking for a data science position at Facebook?  After two successful prior Kaggle competitions, Facebook continues their mission to identify the best data scientists and software engineers that Kaggle has to offer. In this third installment, they seek candidates who have experience text mining large amounts of data.
This competition tests your text skills on a large dataset from the Stack Exchange sites.  The task is to predict the tags (a.k.a. keywords, topics, summaries), given only the question text and its title. The dataset contains content from disparate stack exchange sites, containing a mix of both technical and non-technical questions.
Positions are available in Menlo Park, Seattle, New York City, and London; candidates must have, or be eligible to obtain, authorization to work in the US or UK.
Please note: you must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions. Crawling stack exchange sites to look up answers is not permitted. Facebook will review the code of the top participants before deciding whether to offer an interview. 
This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Facebook, check the box ""Allow host to contact me"" when you make your first entry.
Acknowledgements
We thank Stack Exchange (and its users) for generously releasing the source dataset through its Creative Commons Data Dumps. All data is licensed under the cc-by-sa license.","The evaluation metric for this competition is Mean F1-Score.  The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F1 score is given by:
\[ F1 = 2\frac{p \cdot r}{p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn} \]
The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.
To receive credit, the tag you predict must be an exact match, regardless of whether you believe the tags are synonyms.  Why aren't synonyms counted?
Giving out a list of candidate synonyms is a potential source of leakage
Synonyms are subjective, and there are ""subjectively many"" synonyms for a given tag
Participants are equally penalized for predicting a synonym of a correct tag, so the task can be framed as not only predicting a tag, but also modeling the distribution(s) of potential synonyms
Submission Instructions
For every question in the test set, your submission file should contain two columns: Id and Tags. Id is the unique identifier of a question in test.csv. Tags should be a space-delimited list predicted tags. You should maintain the order of questions.
The file should contain a header and have the following format:
Id,Tags
1,""c++ javaScript""
2,""php python mysql""
3,""django""
etc.","All of the data is in 2 files: Train and Test.
Train.csv contains 4 columns: Id,Title,Body,Tags
Id - Unique identifier for each question
Title - The question's title
Body - The body of the question
Tags - The tags associated with the question (all lowercase, should not contain tabs '\t' or ampersands '&')
Test.csv contains the same columns but without the Tags, which you are to predict.
The questions are randomized and contains a mix of verbose text sites as well as sites related to math and programming. The number of questions from each site may vary, and no filtering has been performed on the questions (such as closed questions).
Acknowledgements
We thank Stack Exchange (and its users) for generously releasing the source dataset through its Creative Commons Data Dumps. All data is licensed under the cc-by-sa license.",kaggle competitions download -c facebook-recruiting-iii-keyword-extraction,"['https://www.kaggle.com/code/vikashrajluhaniwal/multi-label-classification-for-tag-predictions', 'https://www.kaggle.com/code/dwarika/stackoverflow-tag-predictor', 'https://www.kaggle.com/code/sumantindurkhya/so-tag-prediction-multilabel-bilstm', 'https://www.kaggle.com/code/mitishaagarwal/stackoverflow-tag-predictor', 'https://www.kaggle.com/code/elemento/stackoverflow-questiontagging']"
470,"StumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as ""ephemeral"" or ""evergreen"". The ratings we get from our community give us strong signals that a page may no longer be relevant - but what if we could make this distinction ahead of time? A high quality prediction of ""ephemeral"" or ""evergreen"" would greatly improve a recommendation system like ours.
Many people know evergreen content when they see it, but can an algorithm make the same determination without human intuition? Your mission is to build a classifier which will evaluate a large set of URLs and label them as either evergreen or ephemeral. Can you out-class(ify) StumbleUpon? As an added incentive to the prize, a strong performance in this competition may lead to a career-launching internship at one of the best places to work in San Francisco.","Submissions are judged on area under the ROC curve. 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
Submission Format
Each line of your submission should contain an urlid and a label. Note that you may submit any real-valued number as a prediction, since AUC is only sensitive to the ranking. sampleSubmission.csv shows a representative valid submission. The format looks like this:
urlid,label
5865,0
782,0
6962,0
etc...","Note: researchers who wish to use this data outside the competition should download and read the data access agreement.
There are two components to the data provided for this challenge:
The first component is two files: train.tsv and test.tsv. Each is a tab-delimited text file containing the fields outlined below for 10,566 urls total. Fields for which no data is available are indicated with a question mark.
train.tsv  is the training set and contains 7,395 urls. Binary evergreen labels (either evergreen (1) or non-evergreen (0)) are provided for this set.
test.tsv is the test/evaluation set and contains 3,171 urls.
The second component is raw_content.zip, a zip file containing the raw content for each url, as seen by StumbleUpon's crawler. Each url's raw content is stored in a tab-delimited text file, named with the urlid as indicated in train.tsv and test.tsv.
The following table includes field descriptions for train.tsv and test.tsv:
FieldName Type Description
url string Url of the webpage to be classified",kaggle competitions download -c stumbleupon,"['https://www.kaggle.com/code/mks2192/stumbleupon-eda-and-model-baseline', 'https://www.kaggle.com/code/raviista/stumbleupon-art-of-eda', 'https://www.kaggle.com/code/yahyaouiachraf/stumbleupon-classification-randomforest', 'https://www.kaggle.com/code/sumeetsawant/stumble-upon-challenge-auc-private-lb-0-85', 'https://www.kaggle.com/code/raj401/baseline-evergreen']"
471,"These pages describe the Main phase of this competition, which is now closed. Click here to visit the final phase of Flight Quest 2.
Think you can change the future of flight?
Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays.
There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with “real time business intelligence” — information available in the cockpit that would allow them to make adjustments to their flight patterns. 
Your challenge, should you decide to accept it: 
Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. For background on the competition structure, visit the Basic Structure page.
Be sure to check the Forums regularly to stay on top of the latest competition news.
Download data »
Make a submission »","In this Flight Quest, you are tasked with creating algorithms to generate optimal flight plans (routes) given a flight's origin and destination airports, current air traffic conditions, and weather information.
Each route, represented as a series of points (latitude and longitude) with airspeed and altitude, will be evaluated against the Cost Function.
The Cost Function, a measure of the relative cost in dollars of the flight given a submitted flight plan, seeks to quantitatively capture the direct and indirect costs of flights. See the Simulator code for the exact Cost Function.","The code linked above is provided subject to the limited License to Code contained in the Competition Rules. See the included license.txt for more information.
Simulator Files
Files used and needed by the Flight Simulator can be found on the Flight Quest Data wiki.
Data Files
Flight Quest Phase 2 data is temporally split into 4 datasets that will be released over the course of the competition. Flight data in each dataset is for the following time periods:
Initial Training Set: June 7 - July 4, 2013, 4 weeks
Augmented Training Set 1: Aug 14 - Sept 10, 2013, 4 weeks
Augmented Training Set 2: Nov 14-Dec 11, 2013, 4 weeks
Final Evaluation Set: Jan 19, 2013 - Feb 1, 2014, 2 weeks
The leaderboard will be based on data from the following time periods:
Milestone Leaderboard: July 5-18, 2013 (2 weeks). This leaderboard runs from the competition's launch to the Milestone Entry deadline on September 25.",kaggle competitions download -c flight2-main,[]
472,"This is a private, invitation-only competition. The relevant information is provided only to contestants. The competition is closed to new entrants.
Qualification for future private competitions is based solely on objective leaderboard performance in competitions.",,,,
473,"The Big Data Combine engineered by BattleFin are rapid fire, live tryouts for computer scientists with elite predictive analytic skills intent on monetizing their models. The first stage of the competition is a predictive modeling competition that requires participants to develop a model that predicts stock price movements using sentiment data provided by RavenPack. Traders, analysts and investors are always looking for techniques to better predict price movements.  Knowing whether a security will increase or decrease allows traders to make better investment decisions and manage risk more effectively. 
This competition is designed to identify people with the talent to create a predictive model using financial data. Competitors are given intraday trading data showing stock price movements at 5 minute intervals and asked to predict the change two hours in the future. The winners of the predictive modeling phase are invited to the ""live"" Big Data Combine tryouts in Miami, FL. Up to 12 finalists will be selected to compete in the live event in Miami. The lucky few will pitch their predictive model to expert judges and an engaged audience. They have only three minutes to present in non-technical terms three items: personal background, predictive model description, and how they would use there model to make money in finance. If their model and presentation impresses our judges, they will be eligible to work with BattleFin and Deltix to convert their predictive model into a trading strategy.
Master of Ceremonies (""MC"")
Matt Iseman - Actor, Comedian, Host of American Ninja Warrior
Mr. Iseman has hosted the game shows Scream Play on E! and Casino Night on the GSN. He currently appears as a regular cast member on the home makeover show Clean House, and its companion outtakes show, Clean House Comes Clean, both on the Style Network. Additionally, he hosted season 2 and 3 of American Ninja Warrior on the channel G4. He also has worked episodically in television shows including The Drew Carey Show, NCIS, and General Hospital. He has appeared on the syndicated MAD TV, Comedy Central’s Premium Blend, Fox’s The Best Damn Sports Show Period, and Fox News Channel’s Red Eye w/ Greg Gutfeld. Mr. Iseman was also the host of Sports Soup, a spin-off of E!'s The Soup, on Versus. Style Network and Versus are owned by Comcast. Iseman began working with American Ninja Warrior (G4) in 2010. He uses his great athleticism and work as a comedian to add his style to the show with Johnny Moseley (American Professional Freestyle Skier), and Angela Sun (Sideline Correspondent).
Judges
Ilya Gorelik, CEO of Deltix Inc. CEO, Ilya Gorelik is responsible for setting the strategic direction of the company, as well as overseeing global product development, sales and marketing.Ilya has more than 15 years of experience managing large-scale software projects and teams. He was one of the key development leaders of PTC, where he worked from 1989 to 1998, attaining the position of Senior VP of Engineering and Chief Technology Officer. From 1998 until 2000, Ilya was Senior VP of Product Strategy and Development and Chief Scientist for FirePond. From 2000 until founding Deltix in 2005, Ilya worked as Advisory CTO for HighRoads and several other software technology companies. Ilya has a Ph.D. in ComputationalMechanics from Moscow Technical University, he received an MS in Mechanical Engineering from Minsk Technical University.
Peter Hafez, Head Quantitative Research at RavenPack
Peter is an award-winning expert in the field of applied news analytics and has consulted numerous leading trading and investment firms on how to take advantage of news analytics in financial markets. Peter has more than 10 years of experience in quantitative finance with companies such as Standard & Poor's, Credit Suisse First Boston, and Saxo Bank. He is a recognized speaker at conferences on behavioral finance and algorithmic trading. Peter holds a Master's degree in Quantitative Finance from City University's Cass Business School along with an undergraduate degree in Economics from Copenhagen University.
Nabyl Charania, Managing Director at Rokk3r Labs
Nabyl is a Co-Founder and Managing Director at Rokk3r Labs.  Rokk3r Labs is Miami based Venture Capital firm with 35 employees.  Rokk3r Labs fuses entrepreneurial and professional talent to help entrepreneurs create ideas, prototypes, products and generally invests in disruptive companies designed for the modern hyper-connected world. Prior, he was a Founder and Managing Director at Decipher Labs Inc. He graduated from University of Waterloo in 2000.
Zeid Barakat, Co-Founder at Flyberry Capital LLC 
Zeid is a Co-Founder of Flyberry Capital a Boston based hedge fund that utilizes a Big Data strategy. Zeid works with ‘best-in class’ MIT & Harvard computer scientists and machine learning experts to develop proprietary trading strategies, focused on event-based arbitrage. In charge of dfining corporate growth strategy,business development, marketing, managing Board of Advisers, and fundraising. He is an Entrepreneur in high-tech companies, focused on novel approaches to biotechnology, healthcare and financial services. He earned MBA degree in General Management and Entrepreneurship from MIT in 2008.
About BattleFin
BattleFin is a tournament platform that crowdsources the world's best investment talent. The firm uses rapid fire, real capital tournaments to democratically identify up and coming investment talent. BattleFin has recently been featured in Bloomberg Business Week in an article titles ""The Hedge Fund Hunger Games"". The firm is passionate about leveling the playing field in finance so that anyone with an internet connection can participate in its tournaments. The firm specializes in finding the hedge fund managers of tomorrow. To learn more about BattleFin visit BattleFin.com.
About Deltix
Founded in 2005 and with more than 50 staff, Deltix has established itself as a leader in the growing domain of software and services for quantitative research and systematic automated trading. The Deltix Product Suite is an end-to-end platform for all phases of the alpha discovery and trading life-cycle; including data collection and aggregation, model development, back-testing, optimization, simulation, and deployment to production trading.
About RavenPack
Financial firms are overloaded with information and have turned to computers to read news and other media. What would take days for an investment professional to read and interpret takes computers only a few milliseconds. Now financial institutions can react much faster to the ever increasing amounts of news and information available for making investment decisions. Powered by a proprietary text analysis platform, RavenPack the tournament data sponsor, analyzes novel and relevant stories published by major news sources to look for key scheduled and unexpected geopolitical, macro-economic and corporate events, topics and opinions that indicate changes in market sentiment. Sampled news sources represent the most reliable and authoritative publishers of business and financial news.RavenPack continuously analyzes relevant information from Dow Jones Newswires, regional editions for the Wall Street Journal, and Barron’s to produce real time news sentiment scores and events from entities across multiple asset classes, including currencies, commodities, organizations, companies, sectors, and industries.","Submissions are evaluated by the mean absolute error between the predicted percentage change and the actual percentage change.
$$
\textrm{MAE} = \frac{1}{n} \sum_{i=1}^n | y_i - \hat{y}_i |
$$
Submission Format
Each line should have the predicted percentage change for the given FileId. A header is required. Refer to the sample submission to see the exact format. The format looks like this:
FileId,O1,O2,O3,...,O198
201,0,0,0,...
202,0,0,0,...
...
510,0,0,0...","For this competition, you are asked to predict the percentage change in a financial instrument at a time 2 hours in the future.  The data represents features of various financial securities (198 in total) recorded at 5-minute intervals throughout a trading day.  To discourage cheating, you are not provided with the features' names or the specific dates.
data.zip - contains features for 510 days worth of trading, including 200 training days and 310 testing days
trainLabels.csv - contains the targets for the 200 training days
sampleSubmission.csv - shows the submission format
Each variable named O1, O2, O3, etc. (the outputs) represents a percent change in the value of a security.  Each variable named I1, I2, I3, etc. (the inputs) represents a feature. The underlying securities and features represented by these anonymized names are the same across all files (e.g. O1 will always be the same stock).
Within each trading day, you are provided the outputs as a relative percentage compared to the previous day's closing price.  The first line of each data file represents the previous close. For example, if a security closed at $1 the previous day and opened at $2 the next day, the first output would be 0, then 100.  All output values are computed relative to the previous day's close. The timestamps within each file are as follows (ignoring the header row):
Line 1 = Outputs and inputs at previous day's close (4PM ET)
Line 2 = Outputs and inputs at current day's open (9:30AM ET)
Line 3 = Outputs and inputs at 9:35AM ET
...
Line 55 = Outputs and inputs at 1:55PM ET",kaggle competitions download -c battlefin-s-big-data-combine-forecasting-challenge,[]
474,"These pages describe the first Milestone phase of this competition, which is now closed. Click here to visit the current phase of Flight Quest 2.
Think you can change the future of flight?
Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays.
There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with “real time business intelligence” — information available in the cockpit that would allow them to make adjustments to their flight patterns. 
Your challenge, should you decide to accept it: 
Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. For background on the competition structure, visit the Basic Structure page.
Be sure to check the Forums regularly to stay on top of the latest competition news.
Download data »
Make a submission »","In this Flight Quest, you are tasked with creating algorithms to generate optimal flight plans (routes) given a flight's origin and destination airports, current air traffic conditions, and weather information.
Each route, represented as a series of points (latitude and longitude) with airspeed and altitude, will be evaluated against the Cost Function.
The Cost Function, a measure of the relative cost in dollars of the flight given a submitted flight plan, seeks to quantitatively capture the direct and indirect costs of flights, and is defined as
$$\textrm{Cost} = f \textrm{F} + t(D_\textrm{crew} + D_\textrm{other}) + D_\textrm{pass}(t) $$
where F is the quantity of fuel burned in gallons, f is the per-gallon fuel cost, t  is the time in flight in hours, and the remaining terms are hourly cost rates related to crew, passenger and overhead costs. The exact parameterization of the Cost function will be available in the Simulator documentation.","The code linked above is provided subject to the limited License to Code contained in the Competition Rules. See the included license.txt for more information.
Flight Quest Phase 2 data is temporally split into 4 datasets that will be released over the course of the competition. Flight data in each dataset is for the following time periods:
Initial Training Set: June 7 - July 4, 2013, 4 weeks
Augmented Training Set 1: Aug 14 - Sept 10, 2013, 4 weeks
Augmented Training Set 2: Nov 3-30, 2013, 4 weeks
Final Evaluation Set: Dec 19, 2013 - Jan 1, 2014, 2 weeks
The leaderboard will be based on data from the following time periods:
Milestone Leaderboard: July 5-18, 2013 (2 weeks). This leaderboard runs from the competition's launch to the Milestone Entry deadline on September 25.
Agent Training Leaderboard: September 11-24 (2 weeks). This leaderboard runs from the beginning of October to the Model (Agent) Submission deadline on December 18.
Guide To Files",kaggle competitions download -c flight2-milestone,[]
475,"Since everyone moves differently and accelerometers are fast becoming ubiquitous, this competition is designed to investigate the feasibility of using accelerometer data as a biometric for identifying users of mobile devices.
Seal has collected accelerometer data from several hundred users over a period of several months during normal device usage. To collect the data, we published an app on Googles’s Android PlayStore that samples accelerometer data in the background and posts it to a central database for analysis.
We have uploaded approximately 60 million unique samples of accelerometer data collected from 387 different devices. These are split into equal sets for training and test. Samples in the training set are labeled with the unique device from which the data was collected. The test set is demarcated into 90k sequences of consecutive samples from one device.  A file of test questions is provided in which you are asked to determine whether the accelerometer data came from the proposed device.","Submissions are judged on area under the ROC curve. 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
Submission Format
Each line of your submission should contain an QuestionId and a prediction, IsTrue. Note that you may submit any real-valued number as a prediction,  since AUC is only sensitive to the ranking. sampleSubmission.csv shows a representative valid submission.  The format looks like this:
QuestionId,IsTrue
1,0
2,0.3
3,99999
4,-0.8
etc...","Your task is to determine whether the accelerometer recordings in the test set (test.csv)  belong to the proposed devices in the question set (questions.csv). 
The following files are provided:
File name
Description
train.zip
30m samples for training, labeled with DeviceId
test.zip
30m samples for test, split into 90k sequences, labeled with SequenceId
questions.csv
90k questions that match test sequences to DeviceId
train.csv
Field name
Description",kaggle competitions download -c accelerometer-biometric-competition,[]
476,"Welcome to the American Meteorological Society 2013-2014 Solar Energy Prediction Contest! This contest is organized by the American Meteorological Society Committees on Artificial Intelligence Applications to Environmental Science, Probability and Statistics, and Earth and Energy. Prizes are sponsored by EarthRisk Technologies, Inc.
Motivation
Renewable energy sources, such as solar and wind, offer many environmental advantages over fossil fuels for electricity generation, but the energy produced by them fluctuates with changing weather conditions. Electric utility companies need accurate forecasts of energy production in order to have the right balance of renewable and fossil fuels available. Errors in the forecast could lead to large expenses for the utility from excess fuel consumption or emergency purchases of electricity from neighboring utilities. Power forecasts typically are derived from numerical weather prediction models, but statistical and machine learning techniques are increasingly being used in conjunction with the numerical models to produce more accurate forecasts.
Objective
The goal of this contest is to discover which statistical and machine learning techniques provide the best short term predictions of solar energy production. Contestants will predict the total daily incoming solar energy at 98 Oklahoma Mesonet sites, which will serve as ""solar farms"" for the contest. Input numerical weather prediction data for the contest comes from the NOAA/ESRL Global Ensemble Forecast System (GEFS) Reforecast Version 2. Data include all 11 ensemble members and the forecast timesteps 12, 15, 18, 21, and 24. Locations of the Mesonet sites relative to the GEFS data are shown in the above figure. Training data will come from 1994-2007. Public testing data will be from 2008-2009. Private testing data for a more recent period will be used for the final evaluation.
Acknowledgements
Daily solar energy data were provided by the Oklahoma Mesonet with the assistance of Dr. Jeffrey Basara. The GEFS Reforecast Version 2 data were developed and provided by Dr. Thomas Hamill. The contest is being administered by David John Gagne and Dr. Amy McGovern of the University of Oklahoma.
About Our Sponsor
EarthRisk Technologies creates a market advantage for its clients by uniquely quantifying weather data.  Our company is a research pioneer that analyzes extreme weather risk at lead times longer than one week.  Our techniques enhance competitive business decisions. TempRisk, the company’s first product suite, is a web-based platform that utilizes historical data, machine learning and predictive analytics to project risk for extreme winter cold and summer heat up to 40 days before it occurs. These patent-pending algorithms were developed in conjunction with Scripps Institution of Oceanography at the University of California San Diego.  Energy producers and commodity investment firms currently employ TempRisk in their daily operations. Our customers require a uniquely objective quantitative methods for extreme event prediction.  Our products are continuously developing thanks to ongoing support from customer-partners including large energy companies, investment firms, and reinsurance advisors.

EarthRisk's leadership team is excited to be deeply engaged with the American Meteorological Society.  In addition to our engagement with the Committee on Artificial Intelligence Applications to Environmental Science, we're also active on the AMS Energy Committee, the Board on Private Sector Meteorology, the Financial Weather/Climate Risk Management Committee, and the Weather Enterprise Economic Evaluation Team.  We have a true passion for advancing meteorological methods through the intelligent application of technology and are proud to be part of the Solar Energy Prediction Contest!","Contestants will submit predictions of the total solar daily incoming solar radiation at 98 Oklahoma Mesonet sites for each day specified in the test.csv file. The Mean Absolute Error (MAE) is the metric being used for this competition. It is commonly used in regression problems and by the renewable energy industry to compare forecast performance. The formula is given by:
\[ \text{MAE}=\frac{1}{SE}\sum_s^S\sum_e^E{\mid F_{se}-O_{se} \mid} \]
Unlike Root Mean Squared Error, MAE does not excessively punish extreme forecasts.
Submission File
Contestants should submit one csv file with the Date in the first column and each site's prediction in subsequent columns ordered alphabetically. The date should be in YYYYMMDD format. Solar energy predictions should be in units of Joules per square meter.
Date,ACME,ADAX,ALTU,APAC,...,WIST,WOOD,WYNO
20080101,0,0,0,0,...,0,0,0
20080102,0,0,0,0,...,0,0,0
...
20121130,0,0,0,0,...,0,0,0","The contest training data are separated into 3 files.
gefs_train.tar.gz and gefs_train.zip contain all of the GEFS training data. The data are in netCDF4 files with each file holding the grids for each ensemble member at every time step for a particular variable. Each netCDF file contains the latitude-longitude grid and timestep values as well as metadata listing the full names of each variable and the associated units. More infromation about the netCDF format and links to open libraries for reading the files can be found here. NetCDF libraries are known to be available for C, Java, Python, R, and MATLAB.
Each netCDF4 file contains the total data for one of the model variables and are stored in a multidimensional array. The first dimension is the date of the model run and will correspond directly with a row in either the train.csv or sampleSubmission.csv files. The second dimension is the ensemble member that the forecast comes from. The GEFS has 11 ensemble members with perturbed initial conditions. The third dimension is the forecast hour, which runs from 12 to 24 hours in 3 hour increments. All model runs start at 00 UTC, so they will always correspond to the same universal time although local solar time will vary over each year.  The fourth and fifth dimensions are the latitude and longitude uniform spatial grid. The longitudes in the file are in positive degrees from the Prime Meridian, so subtracting 360 from them will translate them to a similar range of values as in station_info.csv. A visualization of the grid can be seen on the main page.
Variable Description Units
apcp_sfc 3-Hour accumulated precipitation at the surface kg m-2",kaggle competitions download -c ams-2014-solar-energy-prediction-contest,"['https://www.kaggle.com/code/semanurkps/ams-traditional-incremental-solar-power-prediction', 'https://www.kaggle.com/code/mohitkhurana21/mk-wind-forecast', 'https://www.kaggle.com/code/amansolanke/minor-project']"
477,"It is important to gain a better understanding of bird behavior and population trends. Birds respond quickly to environmental change, and may also tell us about other organisms (e.g., insects they feed on), while being easier to detect. Traditional methods for collecting data about birds involve costly human effort. A promising alternative is acoustic monitoring. There are many advantages to recording audio of birds compared to human surveys, including increased temporal and spatial resolution and extent, applicability in remote sites, reduced observer bias, and potentially lower costs. However, it is an open problem for signal processing and machine learning to reliably identify bird sounds in real-world audio data collected in an acoustic monitoring scenario. Some of the major challenges include multiple simultaneously vocalizing birds, other sources of non-bird sound (e.g. buzzing insects), and background noise like wind, rain, and motor vehicles.
The goal in this challenge is to predict the set of bird species that are present given a ten-second audio clip. This is a multi-label supervised classification problem. The training data consists of audio recordings paired with the set of species that are present.
Background
The audio dataset for this challenge was collected in the H. J. Andrews (HJA) Long-Term Experimental Research Forest, in the Cascade mountain range of Oregon. Since 2009, members of the OSU Bioacoustics group have collected over 10TB of audio data in HJA using Songmeter audio recording devices. A Songmeter has two omnidirectional microphones, and records audio in WAV format to flash memory. A Songmeter can be left in the field for several weeks at a time before either its batteries run out, or its memory is full.
HJA has been the site of decades of experiments and data collection in ecology, geology and meteorology. This means, for example, that given an audio recording from a particular day and location in HJA, it is possible to look up the weather, vegetative composition, elevation, and much more. Such data enables unique discoveries through cross-examination, and long-term analysis.
Previous experiments on supervised classification using multi-instance and/or multi-label formulations have used audio data collected with song meters in HJA. The dataset for this competition is similar to, but perhaps more difficult than that dataset used in these prior works; in earlier work care was taken to avoid recordings with rain and loud wind, or no birds at all, and all of the recordings came from a single day. In this competition, you will consider a new dataset which includes rain and wind, and represents a sample from two years of audio recording at 13 different locations.
Conference Attendance
To participate in the conference, participants should email the following information to catherine.huang {at} intel.com no later than August 19, 2013: (1) the names of the team members (each person may belong to at most one team), (2) the name(s) of the host institutions of the researchers, (3) a 1-3 paragraph description of the approach used, (4) their submission score.  Those planning to attend the conference should additionally upload their source code to reproduce results.  Submitted models should follow the model submission best practices as closely as possible.  You do not need to submit code/models before the deadline to participate in the Kaggle competition.
Acknowledgements
Collection and preparation of this dataset was partially funded by NSF grant DGE 0333257, NSF-CDI grant 0941748, NSF grant 1055113, NSF grant CCF-1254218, and the College of Engineering, Oregon State University. We would also like to thank Sarah Hadley, Jed Irvine, and others for their contributions in data collection and labeling.","Submissions are judged on area under the ROC curve. 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
There are 19 species in the dataset. For each recording in the test set, you will predict the probability that each species is present. The test set labels are hidden from participants in the contest, and have been split into 1/3 ""public test"" and 2/3 ""private test."" When you submit your predictions (for the entire test set), Kaggle will immediately calculate your AUC score on the public test set; this is the score you will see for your submission on the Leaderboard. The final winner(s) of the competition will be determined by AUC on the private test set (participants will not be able to see their scores on this set until the competition is over).
Submission Format
Please note that a new submission parser went live after the launch of this competition, resulting in a minor change to the submission format. See here for details/questions. 
Each line of your submission should contain an Id and a prediction.  We combined ""rec_id"" and ""species"" into a single ""Id"" column by multiplying ""rec_id"" by 100 and then adding in the ""species"" number. For example a (""rec_id"",""species"") pair of ""1,2"" was mapped to a single ""Id"" of ""102"".  The format looks like this:
Id,Probability
0,0
1,0
2,0
3,0
4,0
5,0
6,0
7,0
8,0
9,0
10,0
11,0
12,0
13,0
14,0
15,0
16,0
17,0
18,0
100,0
101,0
102,0
etc...","Data Description
The dataset for this challenge consists of 645 ten-second audio recordings collected in HJA over a two-year period. In addition to the raw WAV audio files, we provide data from several stages of pre-processing, e.g. features that can be used directly for classification. The dataset is described in more detail in the included documentation, mlsp13birdchallenge_documentation.pdf and README.txt.
mlsp_contest_dataset.zip - Contains all necessary and supplemental files for the competition + additional documentation.
mlsp13birdchallenge_documentation.pdf - Main dataset documentation. Has more info than what is on the site.
Please note: rules/changes/modifications on Kaggle.com take precedence over those in the pdf documentation.
Folder contents

*** Essential Files ***
(see /essential_data)
These are the most essential files- if you want to do everything from scratch, these are the only files you need.",kaggle competitions download -c mlsp-2013-birds,"['https://www.kaggle.com/code/shreyasajal/birdclef-librosa-audio-feature-extraction', 'https://www.kaggle.com/code/shreyasajal/audio-albumentations-torchaudio-audiomentations', 'https://www.kaggle.com/code/sagniksanyal/birdclef-2022-torchaudio-audiomentations-skimpy', 'https://www.kaggle.com/code/titankinansalaatsa/mlsp-2013-bird-classification-challenge-1301180283', 'https://www.kaggle.com/code/muhammadzubairkhan92/birdclef-2021-librosa-audio-feature-extraction']"
478,"Imagine an energy feedback system that displays not only your total power consumption, but also continuously shows real-time usage, broken down by electrical appliance. Such a system could provide personalized and cost-effective energy saving recommendations. For example, it could report, ""Based on your usage patterns, you could save $215 per year by switching to a more efficient heating unit, which will pay for itself in 27 months."" The challenge in this scenario is to sense end-uses of energy to provide feedback at the fine-grained, appliance level.
There has been substantial prior research in this area [1,5,6,7,8], however most of this work has concentrated on the use of power consumption patterns and using changes in power draw as features to identify what appliance is being used and how much energy it is consuming. We recommend the reader refer to [2,3,9] for a detailed overview of machine learning features for energy disaggregation.
A more recent approach to estimate appliance usage is to examine the Electromagnetic Interference (EMI) that most consumer electronic appliances produce as identifying signatures [4]. This EMI is measured using a special sensor built at the Ubicomp Lab at the University of Washington as part of Sidhant Gupta's thesis work. The figure below shows an example of EMI captured from a home. The plot is in frequency domain and shows the signatures of various appliances.
The presence or absence of such EMI signatures would ideally tell us when a particular appliance is in use. However, due to the large numbers of appliances in a home, the solution is not straightforward. Machine learning is required not only to make an inference about the appliance class given a particular signature, but probabilistic models are needed that take into account, for example, human appliance usage patterns (think using coffee machine and toaster in morning vs. lights in evening), weather patterns (very unlikely that AC came on during winters), and appliance electrical model. The signature of an appliance can also drift or vary over time due to operating conditions and the mode in which they are used (for instance, a washing machine has many modes). We encourage participants to review [4] to better understand the use of EMI for electrical appliance use detection and classification.
Videos and Slides
Here are a few lab quality videos that may helo you grasp the big picture:
Video of the signal: http://youtu.be/o-SqO8y8XUA
Video of the technology applied to energy monitoring: http://www.youtube.com/watch?v=dcPI1Cp0VZI
Slides from conference talk for ElectriSense can be accessed here: http://homes.cs.washington.edu/~sidhant/slides/ElectriSense_PDF.pdf
References
1. Berges, M., Goldman, E., Matthews, H.S., and Soibelman, L. Training Load Monitoring Algorithms on Highly Sub-Metered Home Electricity Consumption Data. Tsinghua Science & Technology 13, Supple, 0 (2008), 406–411.
2. Carrie Armel, K., Gupta, A., Shrimali, G., and Albert, A. Is disaggregation the holy grail of energy efficiency? The case of electricity. Energy Policy 52, (2012), 213–234.
3. Froehlich, J., Larson, E., Gupta, S., Cohn, G., Reynolds, M., and Patel, S. Disaggregated End-Use Energy Sensing for the Smart Grid. IEEE Pervasive Computing 10, 1 (2011), 28–39.
4. Gupta, S., Reynolds, M., and Patel, S. ElectriSense: Single-Point Sensing Using EMI for Electrical Event Detection and Classification in the home. Ubicomp 2010, (2010).
5. Hart, G. Nonintrusive appliance load monitoring. Proceedings of the IEEE, (1992).
6. Laughman, C., Lee, K., and Cox, R. Power signature analysis. IEEE Power and Energy, april 2003 (2003).
7. Leeb, S.B., Shaw, S.R., and Kirtley, J.L. Transient Event Detection in Spectral Envelope Estimates. IEEE Transactions on Power Delivery 10, 3 (1995), 1200–1210.
8. Norford, L.K. and Leeb, S.B. Non-intrusive electrical load monitoring in commercial buildings based on steady-state and transient load-detection algorithms. Energy and Buildings 24, 1 (1996), 51–64.
9. Zeifman, M., Ph, D., and Roth, K. Non-Intrusive Appliance Load Monitoring ( NIALM ): Review and Outlook * Fraunhofer : A Leading Force in Applied R & D. Consumer Electronics, January (2011). ","Entries are evaluated on the mean Hamming Loss.  The Hamming Loss measures accuracy in a multi-label classification task. The formula is given by:
$$ \text{HammingLoss}(x_i, y_i) = \frac{1}{|D|} \sum_{i=1}^{|D|} \frac{xor(x_i, y_i)}{|L|}, $$
where
\\(|D|\\) is the number of samples (timepoints)
\\(|L|\\) is the number of labels (appliances)
\\(y_i\\) is the ground truth
\\(x_i\\)  is the prediction.

Since the number and type of appliance may vary across housholds, we take an average over all houses:
$$ score = \frac{1}{4} \sum_{j=1}^4 \text{HammingLoss}_j .$$
Submission File
Submission files should contain five columns (Id,House,Appliance,TimeStamp,Predicted) and include a header. See SampleSubmission.csv for an example of the format.
Id,House,Appliance,TimeStamp,Predicted
1,H1,30,1334300400,0
2,H1,29,1334300400,0
3,H1,15,1334300400,0
...","You do not need to download both versions of the files.
H1.zip - H4.zip = official competition data in .mat format
H1_CSV.zip - H4_CSV.zip = unofficial competition data in .csv format  
You are provided with data from 4 homes (H1-H4) consisting of both training datasets and testing datasets. The goal is to use the training datasets to learn how each appliance in each home looks and behaves from a machine-learning perspective and build a model which can be applied to the test datasets to make predictions.  Refer to SampleSubmission.csv to understand which appliances to predict at which times. 
MAT vs. CSV?
The official data for this competition is provided as MATLAB .mat files. This is the language and storage structure being being used by the prototype and its researchers.
We recognize that many people do not have access to MATLAB. As a courtesy, we have provided .csv files in addition to the .mat files. Note that the conversion results in some loss of decimal precision, contains complex numbers, and has scientific notation. The uncompressed .csv files can also be quite large. These files are provided without any guarantees regarding their correctness or usability!
Participants without MATLAB are encouraged to use the forums to discuss issues around loading/converting the data.  Note that in addition to the free language OCTAVE, there are packages in most other languages to read and write .mat files.  ",kaggle competitions download -c belkin-energy-disaggregation-competition,[]
479,"The Multi-modal gesture recognition challenge, focused on gesture recognition from 2D and 3D video data using Kinect, is organized by ChaLearn in conjunction with ICMI 2013.
Kinect is revolutionizing the field of gesture recognition given the set of input data modalities it provides, including RGB image, depth image (using an infrared sensor), and audio. Gesture recognition is genuinely important in many multi-modal interaction and computer vision applications, including image/video indexing, video surveillance, computer interfaces, and gaming. It also provides excellent benchmarks for algorithms. The recognition of continuous, natural signing is very challenging due to the multimodal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limitations such as spatial and temporal resolution and unreliable depth cues.
The Multi-modal Challenge workshop will be devoted to the presentation of most recent and challenging techniques from multi-modal gesture recognition. The committee encourages paper submissions in the following topics (but not limited to):
Multi-modal descriptors for gesture recognition
Fusion strategies for gesture recognition
Multi-modal learning for gesture recognition
Data sets and evaluation protocols for multi-modal gesture recognition
Applications of multi-modal gesture recognition
The results of the challenge will be discussed at the workshop. It features a quantitative evaluation of automatic gesture recognition from a multi-modal dataset recorded with Kinect (providing RGB images of face and body, depth images of face and body, skeleton information, joint orientation and audio sources), including about 15,000 Italian gestures from several users. The emphasis of this edition of the competition will be on multi-modal automatic learning of a vocabulary of 20 types of Italian gestures performed by several different users while explaining a history, with the aim of performing user independent continuous gesture recognition combined with audio information. 
Additionally, the challenge includes a live competition of demos/systems of applications based on multi-modal gesture recognition techniques. Demos using data from different modalities and different kind of devices are welcome. The demos will be evaluated in terms of multi-modality, technical quality, and applicability.
Best workshop papers and top three ranked participants of the quantitative evaluation will be invited to present their work at ICMI 2013 and their papers will be published in the ACM proceedings. Additionally, there will be travel grants (based on availability) and the possibility to be invited to present extended versions of their works to a special issue in a high impact factor journal. Moreover, all three top ranking participants in both, quantitative and qualitative challenges will be awarded with a ChaLearn winner certificate and an economic prize (based on availability). We will also announce a best paper and best student paper awards among the workshop contributions.","The focus of the challenge is on “multiple instance, user independent learning” of gestures, which means learning to recognize gestures from several instances for each category performed by different users, drawn from a gesture vocabulary of 20 categories. A gesture vocabulary is a set of unique gestures, generally related to a particular task. In this challenge we will focus on the recognition of a vocabulary of 20 Italian cultural/anthropological signs.

Challenge stages:
Development phase: Create a learning system capable of learning from several training examples a gesture classification problem. Practice with development data (a large database of 8,500 labeled gestures is available) and submit predictions on-line on validation data (3,500 labeled gestures) to get immediate feed-back on the leaderboard. Recommended: towards the end of the development phase, submit your code for verification purpose.
Final evaluation phase: Make predictions on the new final evaluation data (3,500 gestures) revealed at the end of the development phase. The participants will have few days to train their systems and upload their predictions.
We highly recommend that the participants take advantage of this opportunity and upload regularly updated versions of their code during the development period. Their last code submission before deadline will be used for the verification.

What do you need to predict?

Each video contains the recording of multi-modal RGB-Depth-Audio data and user mask and skeleton information of several gesture instances from a vocabulary of 20 gesture categories of Italian signs.

You need to predict the identity of those gestures, represented by a numeric label (from 1 to 20).

In the data used for training, you get several video clips with annotated gesture labels for training purposes. Multiple gesture instances from several users will be available. In the data used for evaluation (validation data) you must predict the labels of the gestures played in a set of unlabeled videos.
Prediction is expected to be performed at gesture level, using the numeric label (1-20) for each frame with recognized gesture. The equivalence between gesture labels and gesture identifiers is provided in the data description page and a script for Matlab is also provided. The result will be a csv file with the video identifier and a coma separated list of recognized gestures (see example file provided with training data).

Levenshtein Distance

For each video, you provide an ordered list of labels R corresponding to the recognized gestures. We compare this list to the corresponding list of labels T in the prescribed list of gestures that the user had to play. These are the ""true"" gesture labels (provided that the users did not make mistakes). We compute the so-called Levenshtein distance L(R, T), that is the minimum number of edit operations (substitution, insertion, or deletion) that one has to perform to go from R to T (or vice versa). The Levenhstein distance is also known as ""edit distance"".

For example:


L([1 2 4], [3 2]) = 2

L([1], [2]) = 1

L([2 2 2], [2]) = 2

Score

The overall score we compute is the sum of the Levenshtein distances for all the lines of the result file compared to the corresponding lines in the truth value file, divided by the total number of gestures in the truth value file. This score is analogous to an error rate. However, it can exceed one.
- Public score means the score that appears on the leaderboard during the development period and is based on the validation data.

- Final score means the score that will be computed on the final evaluation data released at the end of the development period, which will not be revealed until the challenge is over. The final score will be used to rank the participants and determine the prizes.

Verification procedure

To verify that the participants complied with the rule that there should be no manual labelling of the test data, the top ranking participants eligible to win prizes will be asked to cooperate with the organizers to reproduce their results.

During the development period the participants can upload executable code reproducing their results together with their submissions. The organizers will evaluate requests to support particular platforms, but do not commit to support all platforms. The sooner a version of the code is uploaded, the highest the chances that the organizers will succeed in running it on their platform. The burden of proof will rest on the participants.

The code will be kept in confidence and used only for verification purpose after the challenge is over. The code submitted will need to be standalone and in particular it will not be allowed to access the Internet. It will need to be capable of training models from the final evaluation data training examples, for each data batch, and making label predictions on the test examples of that batch.
Data split
We split the recorded data into:

- training data: fully labelled data that can be used for training and validation as desired.

- validation data: a dataset formatted in a similar way as the final evaluation data that can be used to practice making submissions on the Kaggle platform. The results on validation data will show immediately as the ""public score"" on the leaderboard.

- final evaluation data: the dataset that will be used to compute the final score (will be released shortly before the end of the challenge).

Kaggle submission format
In order to submit your results to the Kaggle platform, you should provide a csv file with the following format:
Id,Sequence
0001,2 4 5 6 1
0002,1 2 12 4 14 16 3
where the first line is the header and the rest of the lines contains the predicted sequence of gestures. Notice that the Id correspond to the last 4 digits of the SampleID, that is:
Sample00001.zip  =>  0001
A sample file corresponding to the training data is available for downloading.","The data is also available here.
The focus of the challenge is on “multiple instance, user independent learning” of gestures, which means learning to recognize gestures from several instances for each category performed by different users, drawn from a gesture vocabulary of 20 categories. A gesture vocabulary is a set of unique gestures, generally related to a particular task. In this challenge we will focus on the recognition of a vocabulary of 20 Italian cultural/anthropological signs.
Challenge stages
Development Phase: Create a learning system capable of learning from several training examples a gesture classification problem. Practice with training data (a large database of 7,754 manually labeled gestures is available) and submit predictions on-line on validation data (3,362 labelled gestures) to get immediate feed-back on the leaderboard.
Final Evaluation Phase: Make predictions on the new final evaluation data (around 3,000 gestures) revealed at the end of the development phase. The participants will have few days to train their systems and upload their predictions.
Provided Files
Both for the development and final evaluation phase, the data will have the same format. We provide several ZIP files for each dataset, each file containing all the files for one sequence. The name of the ZIP file is assumed as the sequence identifier (eg. Sample00001, Sample00002, ...), and all their related files start with this SessionID:
SessionID_audio: Audio file.",kaggle competitions download -c multi-modal-gesture-recognition,[]
480,"When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. This access may allow an employee to read/manipulate resources through various applications or web portals. It is assumed that employees fulfilling the functions of a given role will access the same or similar resources. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work (e.g. not able to log into a reporting portal). A knowledgeable supervisor then takes time to manually grant the needed access in order to overcome access obstacles. As employees move throughout a company, this access discovery/recovery cycle wastes a nontrivial amount of time and money.
There is a considerable amount of data regarding an employee’s role within an organization and the resources to which they have access. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access.
Objective
The objective of this competition is to build a model, learned using historical data, that will determine an employee's access needs, such that manual access transactions (grants and revokes) are minimized as the employee's attributes change over time. The model will take an employee's role information and a resource code and will return whether or not access should be granted.
Partners
This competition is hosted in collaboration with the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2013)","Submissions are judged on area under the ROC curve. 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
Submission File
For every line in the test set, submission files should contain two columns: id and ACTION. In the ground truth, ACTION is 1 if the resource should be allowed, 0 if the resource should not. Your predictions do not need to be binary. You may submit probabilities/predictions having any real value. The submission file should have a header.
id,ACTION
1,1
2,0.2
3,1
4,0
5,2
...","The data consists of real historical data collected from 2010 & 2011.  Employees are manually allowed or denied access to resources over time. You must create an algorithm capable of learning from this historical data to predict approval/denial for an unseen set of employees. 
File Descriptions
train.csv - The training set. Each row has the ACTION (ground truth), RESOURCE, and information about the employee's role at the time of approval
test.csv - The test set for which predictions should be made.  Each row asks whether an employee having the listed characteristics should have access to the listed resource.
Column Descriptions
Column Name Description
ACTION ACTION is 1 if the resource was approved, 0 if the resource was not
RESOURCE An ID for each resource
MGR_ID The EMPLOYEE ID of the manager of the current EMPLOYEE ID record; an employee may have only one manager at a time",kaggle competitions download -c amazon-employee-access-challenge,"['https://www.kaggle.com/code/prashant111/catboost-classifier-in-python', 'https://www.kaggle.com/code/mitribunskiy/tutorial-catboost-overview', 'https://www.kaggle.com/code/dmitrylarko/kaggledays-sf-1-amazon-baseline', 'https://www.kaggle.com/code/dmitrylarko/kaggledays-sf-4-amazon-end-to-end', 'https://www.kaggle.com/code/dmitrylarko/kaggledays-sf-3-amazon-supervised-encoding']"
481,"One motivation for representation learning is that learning algorithms can design features better and faster than humans can. To this end, we hold this challenge that does not explicitly require that entries use representation learning. Rather, we introduce an entirely new dataset and invite competitors from all related communities to solve it. The dataset for this challenge is a facial expression classification dataset that we have assembled from the internet. Because this is a newly introduced dataset, this contest will see which methods are the easiest to get quickly working on new data.
Example baseline submissions are available as part of the pylearn2 python package available at https://github.com/lisa-lab/pylearn2
The baseline submissions for this contest are in pylearn2/scripts/icml_2013_wrepl/emotions
Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set.
Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set.","Submissions are scored based on their accuracy of predicting which of the seven classes should be assigned to each image.
During the contest, the current test.csv file will be used to provide a public leaderboard. Because this task is very easy for humans to do, we will not provide the final test inputs until 72 hours before the contest closes.
After we provide the final test inputs, teams wishing to be scored for the final competition should run their classifier on the new data and upload a new .csv file before the competition closes. This procedure is intended to discourage manual labeling of test data.","The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).
train.csv contains two columns, ""emotion"" and ""pixels"". The ""emotion"" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image. The ""pixels"" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. test.csv contains only the ""pixels"" column and your task is to predict the emotion column.
The training set consists of 28,709 examples. The public test set used for the leaderboard consists of 3,589 examples. The final test set, which was used to determine the winner of the competition, consists of another 3,589 examples.
This dataset was prepared by Pierre-Luc Carrier and Aaron Courville, as part of an ongoing research project. They have graciously provided the workshop organizers with a preliminary version of their dataset to use for this contest.",kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge,"['https://www.kaggle.com/code/dima46456/notebook404d80d791', 'https://www.kaggle.com/code/miladaghalari/boosting-emotion-ai-accuracy', 'https://www.kaggle.com/code/yutaarn/datescience', 'https://www.kaggle.com/code/motoikow/notebook91e61a5713', 'https://www.kaggle.com/code/yutaarn/notebook7694590ae9']"
482,"In this contest, competitors will design systems to learn about two modalities of data: images and text. The provided training data is Louis von Ahn's Small ESP Game Dataset, containing images and word tags for these images. Competitors should train their system to associate images to sets of word tags. At test time, the system is presented with two possible sets of word tags for an image, and must determine which is the correct set of word tags.
Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manual labeling of the test set.","The test examples come in triplets: an image, and two annotations.The test images are located in public_test_images.tgz. 
The file public_test_options.tgz contains two files per test image: image_id.option_0.desc and image_id.option_1.desc. The models should predict which of the options fits the image better. If option 0 fits better, the prediction for image_id should be 0. If option 1 fits better, the prediction for image_id should be 1.
Submissions are scored based on their accuracy at predicting which of the two descriptions fits the image best.
public_test_images.tgz will be used to make the leaderboard while the contest is active. Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. After we provide the final test inputs, teams wishing to be scored for the final competition should run their classifier on the new data and upload a new .csv file before the competition closes. This procedure is intended to discourage manual labeling of test data.","The test examples come in triplets: an image, and two annotations. The test images are located in public_test_images.tgz. 
The file public_test_options.tgz contains two files per test image: image_id.option_0.desc and image_id.option_1.desc. The models should predict which of the options fits the image better. If option 0 fits better, the prediction forimage_id should be 0. If option 1 fits better, the prediction for image_id should be 1.
This test data was manually labeled by Ian Goodfellow, and designed to resemble the Small ESP Game dataset created by Luis von Ahn.
You are therefore recommended to train your system on the Small ESP Game dataset, though you are allowed to use any publicly available training set. Please download the Small ESP Game dataset from this contest website, to avoid putting undue strain on Louis von Ahn's personal site.
The ESP Game was an online game that awarded players points if they could label an image with the same word as another unknown player logged in from a different location. The fun game format encouraged a large number of people to participate in a data crowdsourcing effort. Keep in mind that the game format influences the kind of labels that were provided.
The test data has some statistical differences from the ESP Game dataset. Most notably, test images are 300 pixels long on the larger dimension, while the training data comes in a variety of sizes. It is probably important to design your learning system to handle multiple scales of images.
The ESP Game Dataset consists of 100,000 labeled images. The test data consists of 1000 images, split into public test and private test. The ESP Game Dataset has a single correct description for each image, while the test dataset provides a correct and an incorrect description for each image. The incorrect description is always the correct description of one other test image.",kaggle competitions download -c challenges-in-representation-learning-multi-modal-learning,[]
483,"Predict how successful a product will be after its launch
hack/reduce and dunnhumby announce the Product launch challenge, as part of a one day hackathon. This competition asks you to predict how successful each of a number of product launches will be 26 weeks after the launch, based only on information up to the 13th week after the launch.
The training set and question set each contain by week for each launch:
The category of the product, such as Bread, Coffee or Video Games
The number of stores selling the product
The number of units sold that week
The number of distinct customers who have bought the product (cumulative)
The number of distinct customers who have bought the product at least twice (cumulative)
Cumulative units sold to a number of different customer groups: Convenience at home, Family Focussed, Finest, Grab and Go, Shoppers On A Budget, Traditional Homes, Watching the Waistline, Least Price Sensitive, Price Sensitive, Splurge and Save, and Very Price Sensitive.
  Competition begins: Saturday, May 11, 9am EDT (1:00pm UTC)
Competition ends: Saturday, May 11, 7pm EDT (11:00pm UTC)
This competition awards 25% the ranking points of a standard competition, but does not count towards tiers. The top remote participant in the Kaggle competition will receive a ""Prize Winner"" achievement on their profile, in addition to the three local winners.","Prediction accuracy will be evaluated based on the root mean square logarithmic error:
Where:
i is a product
n is the total number of products
p is the predicted sales volume (units sold in week 26)
a is the actual sales volume (units sold in week 26)
Note: the metric is calculated using the natural log.
We have chosen a log-based metric to minimise the effects of outliers and reward accurate predictions, without overly penalising predictions that are far from the actual.",,,[]
484,"(right whale illustration courtesy of Pieter Folkens, ©2011)
This competition complements the previously held Marinexplore Whale Detection Challenge, in which Cornell University provided data from a ship monitoring application termed ""Auto Buoy"", or AB Monitoring System. In the Marinexplore challenge we received solutions that exceeded 98% accuracy and will ultimately advance the process of automatically classifying North Atlantic Right Whales using the AB Monitoring Platform.
Since the results from the previous challenge proved so successful, we decided to extend the goals and consider applications that involve running algorithms on archival data recorded using portable hydrophone assemblies, otherwise referred to as Marine Autonomous Recording Unit (or MARU’s). Since Cornell and its partners have been using the MARU for over a decade, a sizable collection of data has been accumulated. This data spans several ocean basins and covers a variety of marine mammal species.
Solutions to this challenge will be ported to a High Performance Computing (HPC) platform, being developed in part through funding provided by the Office of Naval Research (ONR grant N000141210585, Dugan, Clark, LeCun and Van Parijs). Together, Cornell will combine algorithms, HPC technologies and its data archives to explore data using highly accurate measuring tools. We encourage participants who developed prior solutions (through the collaboration with Marinexplore) to test them on this data.
The results will be presented at the Workshop on Machine Learning for Bioacoustics at ICML 2013.","Submissions are judged on area under the ROC curve. 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
Submission File
For every recording in the test set, submission files should contain two columns: clip and probability. The submission file should have a header.
clip,probability
20090404_000000_012s0ms_Test0.aif,0
20090404_000000_042s6ms_Test1.aif,0
20090404_000000_064s8ms_Test2.aif,0
20090404_000000_082s3ms_Test3.aif,0
20090404_000000_095s0ms_Test4.aif,0","You are given four days of training data and three days of testing data.  The task is to assign a probability that each recording in the test set contains a right whale call (1) or noise (0).
File descriptions
train2.zip - all of the training data as .aif clips. If the file ends in ""_1.aif"" it was labeled a whale call, if it ends in ""_0.aif"", it was labeled noise.
test2.zip -  all of the testing data as .aif clips. You should predict a probability for each of these files.
To run within hardware constraints, solutions would ideally function with limited training history, meaning the algorithm would page in no more than five previous minutes of data at a time. Since we cannot enforce this requirement in this particular competition, we provide this info for those who want to optimize on ""real world utility,"" as opposed to AUC.",kaggle competitions download -c the-icml-2013-whale-challenge-right-whale-redux,[]
485,"The objective of this task is to predict keypoint positions on face images. This can be used as a building block in several applications, such as:
tracking faces in images and video
analysing facial expressions
detecting dysmorphic facial signs for medical diagnosis
biometrics / face recognition
Detecing facial keypoints is a very challenging problem.  Facial features vary greatly from one individual to another, and even for a single individual, there is a large amount of variation due to 3D pose, size, position, viewing angle, and illumination conditions. Computer vision research has come a long way in addressing these difficulties, but there remain many opportunities for improvement.
This getting-started competition provides a benchmark data set and an R tutorial to get you going on analysing face images. Get started with R >>
Acknowledgements
The data set for this competition was graciously provided by Dr. Yoshua Bengio of the University of Montreal. The tutorial was developed by James Petterson.","Root Mean Squared Error (RMSE)
Submissions are scored on the root mean squared error. RMSE is very common and is a suitable general-purpose error metric. Compared to the Mean Absolute Error, RMSE punishes large errors:
\[\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},\]
where y hat is the predicted value and y is the original value.","Each predicted keypoint is specified by an (x,y) real-valued pair in the space of pixel indices. There are 15 keypoints, which represent the following elements of the face:
left_eye_center, right_eye_center, left_eye_inner_corner, left_eye_outer_corner, right_eye_inner_corner, right_eye_outer_corner, left_eyebrow_inner_end, left_eyebrow_outer_end, right_eyebrow_inner_end, right_eyebrow_outer_end, nose_tip, mouth_left_corner, mouth_right_corner, mouth_center_top_lip, mouth_center_bottom_lip
Left and right here refers to the point of view of the subject.
In some examples, some of the target keypoint positions are misssing (encoded as missing entries in the csv, i.e., with nothing between two commas).
The input image is given in the last field of the data files, and consists of a list of pixels (ordered by row), as integers in (0,255). The images are 96x96 pixels.
Data files
training.csv: list of training 7049 images. Each row contains the (x,y) coordinates for 15 keypoints, and image data as row-ordered list of pixels.
test.csv: list of 1783 test images. Each row contains ImageId and image data as row-ordered list of pixels
submissionFileFormat.csv: list of 27124 keypoints to predict. Each row contains a RowId, ImageId, FeatureName, Location. FeatureName are ""left_eye_center_x,"" ""right_eyebrow_outer_end_y,"" etc. Location is what you need to predict. ",kaggle competitions download -c facial-keypoints-detection,"['https://www.kaggle.com/code/nagasai524/facial-keypoint-detection-using-cnn', 'https://www.kaggle.com/code/lucasfuller96/facial-feature-recognition', 'https://www.kaggle.com/code/yakinoki/facial-keypoints-detection', 'https://www.kaggle.com/code/mahmoudeldebase/face-keypoints-detection', 'https://www.kaggle.com/code/diaaessam/face-landmarks-detection']"
486,"This competition asks participants to identify which of 35 species of birds are present in continuous recordings taken at three different locations. The data is provided by the Muséum national d'Histoire naturelle, one of the most respected bird survey institutions in the world.
We're aware the competition deadline is tight, but wanted to give Kagglers the chance to work on this interesting problem. The results will be presented at the Workshop on Machine Learning for Bioacoustics at ICML 2013.
Acknowledgements
The competition test data was graciously provided by Jérôme Sueur. The competition is coorganized by Pr. Hervé Glotin of the Institut Universitaire de France. This project is part of the Scaled Acoustic Biodiversity project - CNRS interdisciplinary mission (http://sabiod.org). Singing species were identified by
Frédéric Jiguet (Muséum national d'Histoire naturelle, France).","Submissions are judged on area under the ROC curve. 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the verification package):
auc = roc.area(true_labels, predictions)
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)
Submission File
For every line in the test set, submission files should contain three columns: clip, species, and probability. The submission file should have a header.
clip,species,probability
A_20090324_063100.wav,1,0.2
A_20090324_063100.wav,2,0.8
...
A_20090324_063100.wav,35,0
A_20090326_062700.wav,1,0.34
...","You are given recordings of 35 species of birds.  The task is to assign a probability that a given species of bird sings at any point in a continuous, 150 second recording.  This is a challenging task because of background noise, variability in the bird sounds, and the fact that the songs overlap.
File descriptions
species_numbers.csv - lists the name of the species and its id number in the data set
phylogenetic_distance.txt - describes the phylogenetic relationship between the species (use of this file is optional)
weather.txt - describes the weather during the recording times (use of this file is optional)
train_set.zip - the .wav recordings of each species
test_set.zip - Ninety 150 second recordings on which you must predict
train_set_features.zip - For your convenience, these are pre-extracted MFCC (Mel Frequency Cepstral Coefficient) features extracted on the training set (use of these files is optional). The archive contains .mat files (for those who want to use Octave/MATLAB) and the same data converted to a csv format.
test_set_features.zip -  pre-extracted MFCC features extracted on the testing set (use of these files is optional). The archive contains .mat files (for those who want to use Octave/MATLAB) and the same data converted to a csv format.
Training set info:
16bit, frequency sampling = 44.1kHz
35 recordings, one bird species per file, 30 sec by file.
Total train duration = 18 minutes.
Each of these 35 species appears at least one time in the test set.",kaggle competitions download -c the-icml-2013-bird-challenge,[]
487,"We are pleased to announce the 2013 Recommender Systems Challenge associated with ACM RecSys 2013.  Known as RecSysChallenge 2013, this is a LBS contest organized by Yelp. The theme of this year’s competition is personalized business recommendations for Yelp users. We provide a detailed snapshot of Yelp data: over 10,000 businesses, 8,000 check-in sites, 40,000 users, and 200,000 reviews from the Phoenix, AZ metropolitan area.
Contest
At the heart of any recommender system is an algorithm to predict ratings.  Contestants are asked to predict the users’ future ratings of businesses. Participants will create a model to predict the rating a user would assign to a business. Models will be graded on accuracy using the root mean squared error metric. See the Evaluation page for more details.
The competition starts on May 3, 2013 and ends on August 31, 2013. Submissions to the workshop must be submitted by September 8, 2013.
A total of $500 in prize money will be presented to the winners. Top ranking teams will also be recognized at the RecSys banquet.
Workshop
After the competition closes, we will also hold a full-day workshop, co-located with the ACM RecSys 2013 conference, to discuss interesting approaches to the competition problem as well as lessons learned. We invite all contest participants to present their approach to building business recommendations. Contributions may focus on any individual steps such as feature extraction or model building; or describe an end-to-end system to predict business ratings.  
Workshop participation is independent of the contest: researchers are welcome to participate in either (or, hopefully, both!).
Looking for last year's challenge?  Check out RecSys Challenge 2012.","The Root Mean Squared Error (""RMSE"") is used to measure the accuracy:
\[ \textrm{RMSE} = \sqrt{\frac{\sum_{i=1}^n (p_i - a_i)^2}{n} } \]
Where:
\\( n \\) is the total number of review ratings to predict
\\( p_i \\) is the predicted rating for review \\( i \\)
\\( a_i \\) is the actual rating for review \\( i \\)
Submission format
For each (user_id, business_id) pair in the testing dataset, predict the rating that user will give to that business.  Submission files should be in the following format and must have a header.
RecommendationId,Stars
1,3.67452543988905
2,3.67452543988905
3,3.67452543988905
4,3.67452543988905
etc.","The data is a detailed dump of Yelp reviews, businesses, users, and checkins for the Phoenix, AZ metropolitan area. yelp_training_set.zip and yelp_training_set_mac.zip have the same files in them. You only need to download one; both are provided for compatibility.
Summary Statistics
In the training set:
11,537 businesses
8,282 checkin sets
43,873 users
229,907 reviews
In the testing set:
1,205 businesses
734 checkin sets
5,105 users
22,956 reviews to predict",kaggle competitions download -c yelp-recsys-2013,"['https://www.kaggle.com/code/anuroopnag/yelp-ratings-review', 'https://www.kaggle.com/code/eduardomarinho44/npl-model-yelp-comp', 'https://www.kaggle.com/code/bharathkumarkathula/yelp-nlp', 'https://www.kaggle.com/code/juliastencel/natural-language-processing-yelp']"
488,"The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom.
Microsoft Academic Search is an open platform that provides a variety of metrics and experiences for the research community, in addition to literature search. It covers more than 50 million publications and over 19 million authors across a variety of domains, with updates added each week. One of the main challenges of providing this service is caused by author-name ambiguity. This KDD Cup task challenges participants to determine which authors in a given data set are duplicates.","The goal of this competition is to predict which authors are duplicates. The task is structured as a ""cold start"" problem, meaning there are no training labels provided. Participants must develop their own duplicate criteria.
The evaluation metric for this competition is Mean F1-Score. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F1 score is given by:
\[ F1 = 2\frac{p \cdot r}{p+r}\ \ \mathrm{where}\ \ p = \frac{tp}{tp+fp},\ \ r = \frac{tp}{tp+fn} \]
The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.
Since the majority of authors are not duplicates, please note that the F1 score will be close to 1 for this task. To assess competition progress, you may wish to compare your F1 score to the benchmark representing the ""null"" prediction (each author is his own duplicate). Small differences in the absolute magnitude of the F1 score can represent meaningful improvements in model performance, despite our natural inclination to assume lesser decimal places are insignificant.
Submission File
For every author in the dataset, submission files should contain two columns: AuthorId and DuplicateAuthorIds. DuplicateAuthorIds should be a space-delimited list.  Every AuthorId counts as his/her own duplicate, and every duplicate should be listed under each of its respective ids. For example, if you suspect author A, B, and C are the same, you should list (A,A B C), (B,B A C), (C,C A B).
The file should contain a header and have the following format:
AuthorId,DuplicateAuthorIds
1,1
8,8
9,9 10
10,10 9
etc.","The data for the Author-Disambiguation is identical to the data for the Author-Paper Identification Challenge. You do not need to re-download if you already have the former.
The files are available in two formats: as a zip archive containing CSV files (data.zip), and as a PostgreSQL relational database backup (data.postgres) that can be restored to an empty database. 
The dataset(s) for the challenge are provided by Microsoft Corporation and come from their Microsoft Academic Search (MAS) database. MAS is a free academic search engine that was developed by Microsoft Research, and covers more than 50 million publications and over 19 million authors across a variety of domains.
Author: is a publication author in the Academic Search dataset.
Paper: is a scholarly contribution written by one or more authors - could be of type conference or journal. Each paper also has additional metadata, such as year of publication, venue, keywords, etc.
Affiliation: the name of an organization with which an author can be affiliated. 
Dataset Descriptions:
The provided datasets are based on a snapshot taken in Jan 2013 and contain:",kaggle competitions download -c kdd-cup-2013-author-disambiguation,[]
489,"Come to our NIPS workshop (dec 9 or 10 in Tahoe).
                              The problem of attributing causes to effects is pervasive in science, medicine, economy and almost every aspects of our everyday life involving human reasoning and decision making. What affects your health? the economy? climate changes? The gold standard to establish causal relationships is to perform randomized controlled experiments. However, experiments are costly while non-experimental ""observational"" data collected routinely around the world are readily available. Unraveling potential cause-effect relationships from such observational data could save a lot of time and effort.
Consider for instance a target variable B, like occurence of ""lung cancer"" in patients. The goal would be to find whether a factor A, like ""smoking"", might cause B. The objective of the challenge is to rank pairs of variables {A, B} to prioritize experimental verifications of the conjecture that A causes B.
As is known, ""correlation does not mean causation"". More generally, observing a statistical dependency between A and B does not imply that A causes B or that B causes A;  A and B could be consequences of a common cause. But, is it possible to determine from the joint observation of samples of two variables A and B that A should be a cause of B? There are new algorithms that have appeared in the literature in the past few years that tackle this problem. This challenge is an opportunity to evaluate them and propose new techniques to improve on them.
We provide hundreds of pairs of real variables with known causal relationships from domains as diverse as chemistry, climatology, ecology, economy, engineering, epidemiology, genomics, medicine, physics. and sociology. Those are intermixed with controls (pairs of independent variables and pairs of variables that are dependent but not causally related) and semi-artificial cause-effect pairs (real variables mixed in various ways to produce a given outcome).
This challenge is limited to pairs of variables deprived of their context. Thus constraint-based methods relying on conditional independence tests and/or graphical models are not applicable. The goal is to push the state-of-the art in complementary methods, which can eventually disambiguate Markov equivalence classes. If you are skeptical that this is possible, try this quiz: Examine the plot below of values of variable B plotted as a function of values of variable A. Can you guess which one is a cause of the other? Hint: Some non-linear functions are non-invertible.
July 1: A new data release was made to address a normalization problem and the deadline was extended. Scores on the public leaderboard prior to July 1 were decreased by 0.5. Please make new submissions with the new validation set. The competition is open to new teams.","For the purpose of this challenge, two variables A and B are causally related if:
B = f (A, noise) or A = f (B, noise).
If the former case, A is cause of B and in the latter case B is a cause of A. All other factors are lumped into the ""noise"". We provide samples of joint observations of A and B, not organized in a time series. We exclude feed-back loops and consider only 4 types of relationships:
A->B      A causes B      Positive class
B->A      B causes A      Negative class
A - B      A and B are consequences of a common cause      Null class
A | B      A and B are independent      Null class
We bring the problem back to a classification problem: for each pair of variable {A, B}, you must answer the question: is A a cause of B? (or, since the problem is symmetrical in A and B, is B a cause of A?)
We expect the participants to produce a score between -Inf and +Inf, large positive values indicating that A is a cause of B with certainty, large negative values indicating that B is a cause of A with certainty. Middle range scores (near zero) indicate that neither A causes B nor B causes A. 
For each pair of variables, we have a ternary truth value indicating whether A is a cause of B (+1), B is a cause of A (-1), or neither (0). We use the scores provided by the participants as a ranking criterion and evaluate their entries with the area under the ROC curve. See details.
We make available Matlab and Python code to produce submissions from the data page. ","This is the July 1, 2013 final data release. 
The data provided on this page is in csv format, suitable to be read by:
basic python benchmark
Matlab sample code
Archived data and data in the split format (one pair per file) are also available.
The file CEfinal_basic_python_benchmark.csv provides a sample submission.
We released the final test data and an equivalent amount of training and validation data distributed similarly. The test data is encrypted, the decryption key will be revealed at the end of the development phase. The new validation set is replacing the old validation set on the leaderboard and all the scores are reset to 0.5, please re-submit results on the new validation set. The new data include pairs of variables generated in a similar way as those of SUP2data and pairs of real variables from various sources. The final data is different from the original training and validation data with respect to normalization and quantization of variables to address a problem of bias in the original data.
NEW: May-June 2013 supplementary data release:
We provide three additional training datasets artificially generated: SUP1data, SUP2data, and SUP3data. Those training datasets have normalized numerical variables and have balanced number of unique values across all classes. SUP1data includes ~6000 pairs of numerical variables. SUP2 includes ~6000 pairs of mixed variables (numerical, categorical, binary). SUP3 data includes 81 pairs of real cause-effect pairs and 81 control pairs A|B and A-B generated from the real pairs.",kaggle competitions download -c cause-effect-pairs,[]
490,"Data Science London and the UK Windows Azure Users Group in partnership with Microsoft and Peerindex, announce the Influencers in Social Networks competition as part of The Big Data Hackathon.  This competition asks you to predict human judgements about who is more influential on social media.
The dataset, provided by Peerindex, comprises a standard, pair-wise preference learning task. Each datapoint describes two individuals, A and B. For each person, 11 pre-computed, non-negative numeric features based on twitter activity (such as volume of interactions, number of followers, etc) are provided.
The binary label represents a human judgement about which one of the two individuals is more influential. A label '1' means A is more influential than B. 0 means B is more influential than A. The goal of the challenge is to train a machine learning model which, for pairs of individuals, predicts the human judgement on who is more influential with high accuracy. Labels for the dataset have been collected by PeerIndex using an application similar to the one described in this post.
A python script computing a sample benchmark solution is available here: https://gist.github.com/fhuszar/5372873
Competition begins: Saturday, Apr 13, 1pm BST (12 noon UTC)
Competition ends: Sunday, Apr 14, 1pm BST (12 noon UTC)
This competition awards 25% the ranking points of a standard competition, but does not count towards tiers. The top remote participant in the Kaggle competition will recieve a ""Prize Winner"" achievement on their profile, in addition to the three local winners.","Submissions are judged on area under the ROC curve:
http://en.wikipedia.org/wiki/Receiver_operating_characteristic 
In R (using the ROCR package):
pred = prediction(predictions, true_labels);
auc.tmp = performance(pred,""auc"");
auc = as.numeric(auc.tmp@y.values);
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)","The dataset, provided by Peerindex, comprises a standard, pair-wise preference learning task. Each datapoint describes two individuals. Pre-computed, standardised features based on twitter activity (such as volume of interactions, number of followers, etc) is provided for each individual.
The discrete label represents a human judgement about which one of the two individuals is more influential. The goal of the challenge is to train a machine learning model which, for a pair of individuals, predicts the human judgement on who is more influential with high accuracy. Labels for the dataset have been collected by PeerIndex using an application similar to the one described in this post.
UPDATE:
This competition now has an improved parser for scoring solutions, and the submission format has changed. Please check the sample_predictions.csv, generated using this, with only Id and Choice Columns. Id's are 1...n, in the order of test cases in tests.csv",kaggle competitions download -c predict-who-is-more-influential-in-a-social-network,"['https://www.kaggle.com/code/khotijahs1/influencers-in-social-networks', 'https://www.kaggle.com/code/mohitsital/random-forest-hyperparameter-tuning', 'https://www.kaggle.com/code/sdoliver/bayesian-optimized-lgbm', 'https://www.kaggle.com/code/mohitsital/bayesian-optimized-lgbm', 'https://www.kaggle.com/code/pratikg1808/influencers-part-1']"
491,"THIS COMPETITION IS COMPLETE. CONGRATULATIONS TO THE PRELIMINARY WINNERS!
The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom.
Microsoft Academic Search is an open platform that provides a variety of metrics and experiences for the research community, in addition to literature search. It covers more than 50 million publications and over 19 million authors across a variety of domains, with updates added each week. One of the main challenges of providing this service is caused by author-name ambiguity. On one hand, there are many authors who publish under several variations of their own name.  On the other hand, different authors might share a similar or even the same name.
As a result, the profile of an author with an ambiguous name tends to contain noise, resulting in papers that are incorrectly assigned to him or her. This KDD Cup task challenges participants to determine which papers in an author profile were truly written by a given author.","The goal of this competition is to predict which papers were written by the given author.
The evaluation metric for this competition is Mean Average Precision.
An author profile contains an author and a set of papers that may or may not have been written by that author. The goal is to rank the papers in a list, so that the YES instances (the papers that have been written by the author) come before the NO instances. This ranking is evaluated using average precision. This means that we calculate the precision at each point when an actual YES instance occurs in the ranking, and then take the average of those values (i.e. the MAP, or Mean Average Precision).
Example 1
Suppose that an author profile contains 7 papers P1, P2, P3, P4, P5, P6, P7. Out of these, only paper P2 has been written by the author. Your task is to provide a ranked list of these 7 papers, with the papers that have been written by the author on top. Say that your system returns the ranked list P7 P3 P2 P4 P1 P6 P5 because it thinks that papers P7 and P3 have been written by the author, and the others have not. The AP in this case is (1/3)/1 = 0.34.
Example 2
Suppose that an author profile contains 5 papers P1, P2, P3, P4 and P5. Papers P3 and P5 have been written by the author, while papers P1, P2 and P4 have not. Say that your system returns the ranked list P3 P1 P4 P5 P2. In this case the AP is (1/1 + 2/4)/2 = 0.75. Note that the relative ordering of the NO instances does not have an effect on the score. Neither does the relative ordering of the YES instances. For instance, if your system would return the ranked list P5 P2 P1 P3 P4 the AP would still be (1/1 + 2/4)/2 = 0.75.
Example 3
Suppose that an author profile contains 10 papers P1, P2, P3, P4, P5, P6, P7, P8, P9 and P10. Papers P1, P3, P5, P6, P7 and P9 have been written by the author, while papers P2, P4, P8 and P10 have not. Say that your system returns the ranked list P1 P3 P5 P6 P7 P9 P2 P4 P8 P10 then the AP is (1/1+2/2+3/3+4/4+5/5+6/6)/6 = 1, i.e. the highest score possible. If your system would return the ranked list P5 P2 P1 P10 P7 P4 P3 P8 P9 P6 then the AP would be (1/1+2/3+3/5+4/7+5/9+6/10)/6 = 0.67.
The average of these average precisions over all author profiles is calculated and displayed as the score on the public leaderboard.
Submission File
The validation and test files will have author IDs and a list of candidate paper IDs. The final submission should rank only the paper IDs for a given author that were included in the original validation and test sets.
Example submission files can be downloaded from the data page. Submission files should contain two columns, AuthorId and PaperIds, with the header:
AuthorId,PaperIds
420,360546 24220 168137 424838 
2759,5738240 50667169 4347791","Code to create the benchmarks is available on Github.
The files are available in two formats: as a zip archive containing CSV files (data.zip), and as a PostgreSQL relational database backup (data.postgres) that can be restored to an empty database. Instructions to restore the PostgreSQL datbase are on Github.
The dataset(s) for the challenge are provided by Microsoft Corporation and come from their Microsoft Academic Search (MAS) database. MAS is a free academic search engine that was developed by Microsoft Research, and covers more than 50 million publications and over 19 million authors across a variety of domains.
Author: is a publication author in the Academic Search dataset.
Paper: is a scholarly contribution written by one or more authors - could be of type conference or journal. Each paper also has additional metadata, such as year of publication, venue, keywords, etc.
Affiliation: the name of an organization with which an author can be affiliated. 
Dataset Descriptions:
The provided datasets are based on a snapshot taken in Jan 2013 and contain:",kaggle competitions download -c kdd-cup-2013-author-paper-identification-challenge,[]
492,"Here at Yelp, we really love the quality of our data. We're grateful that so many of our users take the time to write such great reviews. We track 3 community-powered metrics of review quality: Useful, Funny, Cool. Over time, a good review will accumulate lots of votes in these categories from the community. However, another extremely important quality feature is the freshness of a review. What if we didn't have to wait for the community to vote on the best reviews to know which ones are high quality?
The goal of this competition is to estimate the number of Useful votes a review will receive. Yelp isn't only looking for the answer to this question; we're looking for an engineer that can solve this problem and push their code to production. The prize is a fast track through the recruiting process -- straight to an interview and the opportunity to show Yelp Engineers just what you've got.
For more information about the exciting opportunities at Yelp, check out http://www.yelp.com/careers!

This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Yelp, check the box ""Allow host to contact me"" when you form your team.","A contestant’s model should predict, for each review from the dataset, the number of useful votes made at a specific point in time. The training data contains reviews with votes measured at time=2013-01-19. The testing data contains reviews with votes measured at time=2013-03-12. The dataset contains example reviews with the number of votes they’ve received, as well as additional information about the business and users.
We will use the Root Mean Squared Logarithmic Error (“RMSLE”) to measure the accuracy of an algorithm.
\[ \epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 } \]
Where:
\\( \epsilon \\) is the RMSLE value (score)
\\( n \\) is the total number of reviews in the data set
\\( p_i \\) is the predicted number of useful votes for review \\( i \\)
\\( a_i \\) is the actual number of useful votes for review \\( i \\)
\\( \log(x) \\) is the natural logarithm of \\( x \\)
We use the logarithm of the number of votes so that error scales properly with the total number of votes. An absolute error of 1 vote is much more significant to a review with 3 total votes than it is to a review with 100 total votes.
Although the actual votes will always be a whole number, you do not have to constrain your predictions to integer values.","Data Description
In the training set:
11,537 businesses
8,282 checkin sets
43,873 users
229,907 reviews
Each file is composed of a single object type, one JSON object per line. The training data was recorded on 2013-01-19. The testing data contains reviews, businesses, users, and checkins from the period between 2013-01-19 and 2013-03-12.
Many user and business records referenced in the test set can be found in the training data.
Training set = yelp_training_set.tgz OR yelp_training_set.zip
Testing set = yelp_test_set.tgz OR yelp_test_set.zip
Sample submission format  = sample_submission.csv 
business",kaggle competitions download -c yelp-recruiting,[]
493,"Data Science London is hosting a meetup on Scikit-learn.  This competition is a practice ground for trying, sharing, and creating examples of sklearn's classification abilities (if this turns in to something useful, we can follow it up with regression, or more complex classification problems).
We encourage participants to post code via the ""Tutorials"" link on the left.  Don't worry about accuracy or whether your code is perfect.  The aim here is to explore sklearn by using it. You do not need to use sklearn to enter the competition. If you're new, we hope you'll use this oppurtunity to practice a new tool.  If you're an expert, we hope you'll share the knowledge and document interesting ways to approach this problem.
Scikit-learn (sklearn) is an established, open-source machine learning library, written in Python with the help of NumPy, SciPy and Cython.
Scikit-learn is very user friendly, has a consistent API, and provides extensive documentation. Its implementation is high quality due to strict coding standards and high test coverage.  Behind sklearn is a very active community, which is steadily improving the library.
Meetup Information
Thursday, March 7, 2013, 6:30 PM UTC
http://www.meetup.com/Data-Science-London/events/105840372/
“Learning in Python with scikit-learn"" by Andreas Mueller
This talk will give an overview of the library and introduce general machine learning concepts such as supervised and unsupervised learning, feature extraction, cross validation for model evaluation and hyper parameter selection. We will also touch some more advanced yet practically useful concepts such as feature hashing and ensemble learning.
Andreas is a PhD student in machine learning an computer vision at Bonn University in Germany. He is one of the core developers and the maintainer of scikit-learn and the author of the blog peekaboo-vision. His interests include principles and applications of machine learning and open science.
""Parallel and large scale learning with scikit-learn"" by Olivier Grisel
This talk will give a introduce practical tools and concepts to better leverage multicore machines and small clusters to perform interactive yet scalable predictive modeling with scikit-learn and IPython.parallel. In particular we will introduce:
A short introduction to the parallel features of iPython from the notebook interface
How to perform scalable text feature extraction with the Hashing Trick
How to parallelize or distribute model evaluation (cross validation)and hyper parameters tuning
How to optimize memory usage with memory mapping
How to approximate kernel Support Vector Machines for large scale datasets
A short introduction to Ensembles with model averaging and Random Forests
Olivier is a R&D Software Engineer working in Java by day and a Python machine learning hacker by night. He is interested in applications to Natural Language Processing, Computer Vision and predictive modelling.","This is a binary classification task, You are evaluated on classification accuracy (the percentage of labels you predict correctly).  The training set has 1000 samples and the testing set has 9000.  Your prediction should be a 9000 x 1 vector of ones or zeros. You also need an Id column (1 to 9000) and should include a header. The format looks like this:
Id,Solution
1,0
2,1
3,1
...
9000,0","This is a synthetic data set of 40 features, representing objects from two classes (labeled as 0 or 1). The training set has 1000 samples and the testing set has 9000.",kaggle competitions download -c data-science-london-scikit-learn,"['https://www.kaggle.com/code/aman9d/data-science-london-scikit', 'https://www.kaggle.com/code/chahat1/data-science-london-classification', 'https://www.kaggle.com/code/ahmedgadoo/thanks-sklearn-outliers-rescaling-pca-and-more', 'https://www.kaggle.com/code/vigneshprakash/data-science-london-scikit-learn-modeling', 'https://www.kaggle.com/code/abdelrahmankhalil/data-science-london-scikit-learn']"
494,"Missed the one hour Just the Basics tutorial competition? Didn't get to implement that method you had in mind? Too many coffee breaks has your brain in Beautiful Mind mode?
This is the after-party competition. Same data. Same problem. More time! You have until the close of Strata to have fun with the problem.
Competition Starts: approximately 12:30 PM PT (3:30 PM ET), 02/26/2013
Competition Ends: 5:00 PM PT (8:00 PM ET), 02/28/2013","Submissions are judged on area under the ROC curve:
http://en.wikipedia.org/wiki/Receiver_operating_characteristic 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the ROCR package):
pred = prediction(predictions, true_labels);
auc.tmp = performance(pred,""auc"");
auc = as.numeric(auc.tmp@y.values);
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)","What's inside?
The data contains 100 features extracted from a corpus of emails. Some of the emails are spam and some are normal. Your task is to make a spam detector.
train.csv - contains 600 emails x 100 features for use training your model(s)
train_labels.csv - contains labels for the 600 training emails (1 = spam, 0 = normal)
test.csv - contains 4000 emails x 100 features. Apply your trained model(s) to these.
Participants should submit a file with each of their 4000 predictions on a separate line (in the same order as test.csv). No header is necessary. Predictions can be continuous numbers or 0/1 labels.",kaggle competitions download -c just-the-basics-the-after-party,"['https://www.kaggle.com/code/satoru90/spam-detector-classification', 'https://www.kaggle.com/code/yakinoki/xgbclassifier-optuna', 'https://www.kaggle.com/code/meln1337/just-the-basics-notebook', 'https://www.kaggle.com/code/irinana/spam-detection-strata2013after-party', 'https://www.kaggle.com/code/nikosavgeros/classification-using-machine-and-deep-learning']"
495,"There are two ways of acquiring signatures (or handwritings). The first one being the offline acquisition in which images of the signatures are acquired using an image scanner. The second one being the online acquisition in which x and y coordinates as well as the pressure are acquired with respect to time.
Further details about online acquisition can be found here.
The detection of the online trajectory (or stroke recovery) of offline handwritings has many applications including the forensic application where it can help investigators converting an offline signature into its online equivalent in order to perform the verification at the online mode. It can also be used in a similar way in handwriting recognition as online handwriting recognition reaches higher recognition rates than offline recognition.
There are several studies regarding the detection of trajectories of handwritings. A survey of such methods is given in [1].
The aim of this competition is to attract the interest of document image analysis researchers as well as data scientists to this research area and to measure the performance of recent advances in this field.
The dataset used in this study consists of 1081 signatures of 200 writers [2]. The signatures have been acquired using a Wacom Intuos4 Large digitizing tablet and a Wacom Inking pen. A blank paper has been placed on this tablet in order to acquire in a subsequent stage the offline signature using a scanner.
Offline signatures consists of jpg images scanned using an appropriate HP scanner.
Online signatures are provided in a single sequential csv file containing x and y coordinates for each time interval. Pressure is not considered in this competition.
In order to ease the comparison, each signature is normalized such that its x and y values will be in (0,1).
The online data is provided for the first 605 signatures. Participants are to predict the online signatures of the other 476 signatures.
This competition is organized in the scope of the Twelfth International Conference on Document Analysis and Recognition ICDAR2013 that will be held in Washington, DC.
[1] Nguyen, Vu, and Michael Blumenstein. Techniques for static handwriting trajectory recovery: a survey. Proceedings of the 9th IAPR International Workshop on Document Analysis Systems. ACM, 2010.
[2] S Al-Maadeed, W Ayouby, A Hassaine, A Al-Mejali, A Al-Yazeedi. Arabic Signature Verification Datasets. In: The International Arab Conference on Information Technology 2012.",Submissions are scored using the Root Mean Square Error (RMSE) criterion.,"The data consists of 1081 signatures of 200 writers.
The offline data is provided for all the signatures (images_gender.zip).
The online data is provided for the first 605 signatures in train.csv. This file has 7 olumns:
train_id: ID of the row.
signature_id: from 1 to 1081.
writer_id: from 1 to 200.
occurrence_id: id of the occurrence of signature for the current writer.
time: from 0 to n, where n varies from one signature to another according to the duration of the signing process.
x and y: corresponding signature coordinates (normalized into (0,1)).
As per some requests, we added the unnormalized training data (unnormalized_data.csv).
The same data is provided for the remaining signatures in test.csv where x and y columns are to be determined.
stroke_matlab.zip contains the code of the provided benchmarks.
Important: the predicted x and y coordinates are to be normalized into (0,1) for each signature.",kaggle competitions download -c icdar2013-stroke-recovery-from-offline-data,[]
496,"This competition was launched under the Kaggle Startup Program. If you're a startup with a predictive modelling challenge, please apply!
Adzuna wants to build a prediction engine for the salary of any UK job ad, so they can make huge improvements in the experience of users searching for jobs, and help employers and jobseekers figure out the market worth of different positions. At the moment, approximately half of the UK job ads they index have a salary publicly displayed.  They need your help to bring more transparency to this important market. 
Adzuna has a large dataset (hundreds of thousands of records), which is mostly unstructured text, with a few structured data fields. These can be in a number of different formats because of the hundreds of different sources of records.  Adzuna needs the help of the Kaggle community to figure out the best techniques to apply to this data set to build a highly accurate predictive model for new ads. You will build, train and test your salary prediction engines against a wide field of competitors.  
As an added perk, Adzuna intends to implement their chosen model on their website, both in the UK and worldwide. You will have the satisfaction of seeing your work implemented in production and change the way people search for jobs in the future.
Successful models will incorporate some analysis of the impact of including different keywords or phrases, as well as making use of the structured data fields like location, hours or company.  Some of the structured data shown (such as category) is 'inferred' by Adzuna's own processes, based on where an ad came from or its contents, and may not be ""correct"" but is representative of the real data.
You will be provided with a training data set on which to build your model, which will include all variables including salary.  A second data set will be used to provide feedback on the public leaderboard.  After approximately 6 weeks, Kaggle will release a final data set that does not include the salary field to participants, who will then be required to submit their salary predictions against each job for evaluation.","Our evaluation data set is simply a random subset of ads for which we know the salary, that were not included in the training and public testing datasets.
The evaluation metric for this competition is Mean Absolute Error
Sample submission files can be downloaded from the data page. Submission files should be formatted as follows:
Have a header: ""Id,SalaryNormalized""
Contain two columns
Id: Id for the ads in the validation set in sorted order
SalaryNormalized: Your predicted salary for the job ad
Example lines of the submission format:
Id,SalaryNormalized
13656201,36205
14663195,74570
16530664,31910.50
... ","  Code to create the benchmarks is available on Github
Job Data
The main dataset consists of a large number of rows representing individual job ads, and a series of fields about each job ad.  These fields are as follows:

Id - A unique identifier for each job ad
Title - A freetext field supplied to us by the job advertiser as the Title of the job ad.  Normally this is a summary of the job title or role.
FullDescription - The full text of the job ad as provided by the job advertiser.  Where you see ***s, we have stripped values from the description in order to ensure that no salary information appears within the descriptions.  There may be some collateral damage here where we have also removed other numerics.

LocationRaw - The freetext location as provided by the job advertiser.

LocationNormalized - Adzuna's normalised location from within our own location tree, interpreted by us based on the raw location.  Our normaliser is not perfect!",kaggle competitions download -c job-salary-prediction,"['https://www.kaggle.com/code/jillanisofttech/job-salary-prediction-by-jst', 'https://www.kaggle.com/code/mountaga/job-salary-prediction', 'https://www.kaggle.com/code/lonnieqin/job-salary-prediction-with-tensorflow', 'https://www.kaggle.com/code/uzairahmadxy/mma4-uzairahmad-individualassignment-naive', 'https://www.kaggle.com/code/maksadannamuradov/job-salary-prediction']"
497,"The prediction of gender from handwriting is a very interesting research field. It has many applications including the forensic application where it can help investigators focusing more on a certain category of suspects.
There are a few studies regarding the automatic detection of the gender of a handwritten document [1-3].
The aim of this competition is to attract the interest of the document analysis community to this research area and to measure the performance of recent advances in this field.
The dataset used in this study has been described in this paper [4].
A total of 475 writers produced 4 handwritten documents:
the first page contains an Arabic handwritten text which varies from one writer to another.
the second page contains an Arabic handwritten text which is the same for all the writers.
the third page contains an English handwritten text which varies from one writer to another.
and the fourth page contains an English handwritten text which is the same for all the writers.
The training set consists of the first 282 writers for which the genders are provided.
Participants are asked to predict the gender of the remaining 193 writers.
For participants who are not familiar with digital image-processing, a set of features extracted from all the images will be provided. Those features are similiar to that of the previous 2011 and 2012 Arabic Writer Identification Contests. Those features are described in [5].
This competition is organized in the scope of the Twelfth International Conference on Document Analysis and Recognition ICDAR2013 that will be held in Washington, DC.
Writer demographic identification using bagging and boosting
[2] Liwicki, M., Schlapbach, A., Loretan, P., Bunke, H., Automatic detection of gender and handedness from online handwriting. In: Proc. 13th Conference of the International Graphonomics Society. pp. 179–183 (2007).
[3] Liwicki, M., Schlapbach, A., Bunke, H., Automatic gender detection using on-line and off-line information. Pattern Analysis and Applications 14, 87–92 (2011).
[4] Al-Ma’adeed, S., Ayouby, W., Hassaine, A., Aljaam, J., QUWI: An Arabic and English Handwriting Dataset for Offline Writer Identification. In: Frontiers in Handwriting Recognition, International Conference on. Bari, Italy (September 2012).
[5] Hassaïne, A., Al-Maadeed, S. and Bouridane, A., A Set of Geometrical Features for Writer Identification. Neural Information Processing. Springer Berlin/Heidelberg, 2012.","Participants are asked to produce a csv file containing two columns: the first one contains the ID of the writer in the test set and the second one containing a value indicating how probable it is that the gender of questioned writer is male.
Results will be evaluated using the LogLoss evaluation metric.","images_gender.zip contains all the images in 600dpi.
We had some requests that images_gender.zip is damaged. While we look into this, please try this alternative download link which we have checked.
You could also download this 300dpi version of images (which is generally enough for such tasks).
As per some requests, we have splitted the 300dpi images (files from 1_50.zip to 451_475.zip).
images_subset.zip contains a subset of 5 writers allowing you to have an idea about the dataset before downloading it.
train_answers.csv contains two columns the first one being the ID of each writer and the second one indicating whether or not this writer is male.
train.csv and test.csv contain the following columns:
writer: the ID of the writer
page_id: from 1 to 4
language: Arabic or English
same_text: whether or not the text for this page is the same for all writers (same_text=1 for page_ids 2 and 4)
The remaining columns are features",kaggle competitions download -c icdar2013-gender-prediction-from-handwriting,[]
498,"Two of Kaggle's very own are presenting an introductory tutorial at Strata 2013. Targeted for those with basic programming experience, it will cover the end-to-end analysis of predictive data problems. The tutorial is comprised of four sections, the last of which is this, a hands-on Kaggle competition in which participants can experience firsthand the joys of creating a model and the sorrows of overfitting.
Competition ends:
open_in_newhttps://freesecure.timeanddate.com/countdown/i3icdhlm/n886/cf12/cm0/cu4/ct3/cs1/ca0/co0/cr0/ss0/cac333/cpcf00/pct/tcfff/fn3/fs200/szw320/szh135/iso2013-02-26T12:30:00
9:00am Tuesday, 02/26/2013
Location: Ballroom AB
Competition Starts: Approximately 11:15 AM PT (2:15PM ET), 02/26/2013
Competition Ends: 12:30 PM PT (3:30 PM ET), 02/26/2013
Open to the public!
Yes, you can participate in this for-fun competition without attending the tutorial.  
For fun?
You heard correctly; there's no Kaggle points or money up for grabs here. Isn't getting to lunch early a big enough motivation? With just over an hour from start to finish, this competition is going to be a sprint.
Unlimited* submissions!
Show our servers who's boss! *for small values of unlimited
Where's the data?
To prevent head starts, the data will be available at the start of the competition.
About the presenters
Ben Hamner has worked with machine learning problems in a variety of different domains, including natural language processing, computer vision, web classification, and neuroscience. Prior to joining Kaggle, he applied machine learning to improve brain-computer interfaces as a Whitaker Fellow at the École Polytechnique Fédérale de Lausanne in Lausanne, Switzerland. He graduated with BSE in Biomedical Engineering, Electrical Engineering, and Math from Duke University.
William Cukierski has a bachelor’s degree in physics from Cornell University and a Ph.D. in biomedical engineering from Rutgers University, where he studied applications of machine learning in cancer research.","Submissions are judged on area under the ROC curve:
http://en.wikipedia.org/wiki/Receiver_operating_characteristic 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the ROCR package):
pred = prediction(predictions, true_labels);
auc.tmp = performance(pred,""auc"");
auc = as.numeric(auc.tmp@y.values);
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)","Note: this data is encrypted! The password will be revealed when the competition starts.
Unencrypted data is now available for download",kaggle competitions download -c just-the-basics-strata-2013,"['https://www.kaggle.com/code/muhriddinmalik/strata', 'https://www.kaggle.com/code/ragvendrapal/model-training']"
499,"The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuaration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.
Fast Iron is creating a ""blue book for bull dozers,"" for customers to value what their heavy equipment fleet is worth at auction.
About Fast Iron
Fast Iron are a content-focused business that aids customers in creating enterprise data standards, cleansing data, and maintaining clean data. Utilizing proprietary applications and an ever growing data cleansing team, Fast Iron has normalized data for more than 2.5 million machine and customer records for the heavy equipment industry.
This competition was launched under the Kaggle Startup Program. If you're a startup with a predictive modelling challenge, please apply!
Photo credits: Antonis Lamnatos","The evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.
Sample submission files can be downloaded from the data page. Submission files should be formatted as follows:
Have a header: ""SalesID,SalePrice""
Contain two columns
SalesID: SalesID for the validation set in sorted order
SalePrice: Your predicted price of the sale
Example lines of the submission format:
SalesID,SalePrice
1222837,36205
3044012,74570
1222841,31910.50
... ","View and download the benchmark code from Github
For this competition, you are predicting the sale price of bulldozers sold at auctions.
The data for this competition is split into three parts:
Train.csv is the training set, which contains data through the end of 2011.
Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.
Test.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.
The key fields are in train.csv are:
SalesID: the uniue identifier of the sale
MachineID: the unique identifier of a machine.  A machine can be sold multiple times
saleprice: what the machine sold for at auction (only provided in train.csv)
saledate: the date of the sale
There are several fields towards the end of the file on the different options a machine can have.  The descriptions all start with ""machine configuration"" in the data dictionary.  Some product types do not have a particular option, so all the records for that option variable will be null for that product type.  Also, some sources do not provide good option and/or hours data.",kaggle competitions download -c bluebook-for-bulldozers,"['https://www.kaggle.com/code/miwojc/fast-ai-machine-learning-lesson-1', 'https://www.kaggle.com/code/therealoise/bulldozer-price-prediction-randomforestregressor', 'https://www.kaggle.com/code/afrniomelo/epv-peq-aula-1-regress-o', 'https://www.kaggle.com/code/ankursingh12/lessons-learnt-from-fast-ai-lectures-part-1', 'https://www.kaggle.com/code/rambomind/bulldozer-price-prediction']"
500,"Read the summary of the competition for a quick overview of the impact of the results.
We depend on shipping industry's uninterrupted ability to transport goods across long distances. Navigation technologies combine accurate position and environmental data to calculate optimal transport routes. Accounting for and reducing the impact of commercial shipping on the ocean’s environment, while achieving commercial sustainability, is of increasing importance, especially as it relates to the influence of cumulative noise “footprints” on the great whales.
Marinexplore is organizing the Planet's ocean data with the leading community of ocean professionals. One of the important datasets consists of acoustic recordings that can be used to detect species inhabiting the global ocean. Knowledge about animal locations can be utilized in industrial operations.
Cornell University's Bioacoustic Research Program has extensive experience in identifying endangered whale species and has deployed a 24/7 buoy network to guide ships from colliding with the world's last 400 North Atlantic right whales.
Illustration of ships navigating safely around the habitat of whales.
Right whales make a half-dozen types of sounds, but the characteristic up-call is the one identified by the auto-detection buoys. The up-call is useful because it’s distinctive and right whales give it often. A type of “contact call,” the up-call is a little like small talk--the sound of a right whale going about its day and letting others know it’s nearby. In this recording the up-call is easy to hear--a deep, rising “whoop” that lasts about a second:
Right whale up-call
Marinexplore and Cornell researchers challenge YOU to beat the existing whale detection algorithm identifying the right whale calls. This will advance ship routing decisions in the region.
[For details on the buoy network see a paper published by Acoustical Society of America.]
Read the summary of the competition for a quick overview of the impact of the results.","The challenge is to match the expert analyst labels on whether a particular 2 second sound clip does or does not contain a right whale call.
Submissions should be a real-valued column of size 54503 x 1 and in the same order of the clips in the test set (test1, test2, ...).  A row header is not necessary. Submissions are judged on area under the ROC curve:
http://en.wikipedia.org/wiki/Receiver_operating_characteristic 
In Matlab (using the stats toolbox):
[~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1);
In R (using the ROCR package):
pred = prediction(predictions, true_labels);
auc.tmp = performance(pred,""auc"");
auc = as.numeric(auc.tmp@y.values);
In python (using the metrics module of scikit-learn):
fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)
auc = metrics.auc(fpr,tpr)","Everything is packaged in the file ""whale_data.zip"". ""small_data_sample_revised.zip"" contains 10 example clips if you want to explore the data format before downloading the full data file. The data consists of 30,000 training samples and 54,503 testing samples. Each candidate is a 2-second .aiff sound clip with a sample rate of 2 kHz. The file ""train.csv"" gives the labels for the train set. Candidates that contain a right whale call have label=1, otherwise label=0.
Example of a Clip Containing Right Whale Call
These clips contain any mixture of right whale calls, non-biological noise, or other whale calls. Transforming the time series into the frequency domain via FFT might be useful. For a quick exploration of the clips, you may find Cornell's RavenLite software useful.",kaggle competitions download -c whale-detection-challenge,"['https://www.kaggle.com/code/diegoasuarezg/whale-detection-through-sound-analysis-w-cnn', 'https://www.kaggle.com/code/diegoasuarezg/transforming-2khz-aiff-whale-audio-to-png', 'https://www.kaggle.com/code/diegoasuarezg/wdc-255x255-tfrecord-files-generation']"
501,"There are many symptoms and features of Parkinson’s disease which can be objectively measured and monitored using simple technology devices we carry every day. Mobile phones are some of the most pervasive forms of monitoring devices, with many smartphones carrying basic sensors that can be used to give a window into a patient’s life. We have taken the initial steps with such a device, developing a basic collection application, and collecting data from a group of Parkinson’s patients and control subjects.
Now the challenge is on you to determine the best way to use it!
The challenge is to develop a way to help patients and clinicians using objective, passively collected data points. The goal is to use the provided data to distinguish PD patients from control subjects and/or to quantify PD symptoms in a way that could enable the measurement of disease progression.
You are invited to submit an entry showing the way you would use the data to describe a solution that addresses the objectives of the contest, including next steps to be done if more data were available. 
ENTRY DEADLINE: March 26, 2013, 11:59 PM EST
Share your participation and progress online: #PDdata","Entries:
Submissions to the contest will include an entry describing all data/inputs, use of the data, preliminary findings/evidence to support the use of objective, passively collected data points to address the contest objectives and the potential utility if used in a broader way.
All entries must include a narrative explaining what you did with the data (no more than 5 pages). 
Additional information (images, charts, etc) can be included in the submission as an appendix.
There should be an additional page describing your team and contact info.
All entries must be submitted in electronic format via the web based submission tool – no paper entries will be permitted.
Final entries must be received on time and in the appropriate format.
Submission Deadline: March 26, 2013, 11:59pm EST 
Entrants may register for the challenge up to the final submission deadline.
The decision of the Challenge Organizing Team regarding late or incorrect submission of entries is final.
Entry Review:
The judging team includes:
Alexandra Carmichael - CoFounder of CureTogether and Director of Quantified Self
Karl E. Case - Professor of Economics Emeritus at Wellesley College
Maurizio Facheris - Associate Director, Research Programs at The Michael J. Fox Foundation
Ken Kubota- Director of Kinetics Foundation
Daniel Vannoni - Entrepreneur and Managing Director of Gecko Ventures
Selection Process:
Judges will review all the submissions received based on the above criteria. Judges will then meet to discuss the applications and determine the best project.
Applicants will be notified via email about the results.
The winner will be invited to present their findings at an MJFF-sponsored event. MJFF will cover travel and event costs for up to two people from the winning team.
Evaluation Criteria:
Researchers should use the data set as a means for monitoring and measuring the progression, stage, and/or behavior of patients with Parkinson’s disease and to distinguish them from control subjects. Entries do not have to exclusively use the provided data; we encourage creative, innovative, collaborative approaches that will help patients.
Judges will be looking for winning submissions that can best answer the following questions:
Can the data help distinguish PD patients from control subjects?
Can the data help to measure the progression, change and/or variability of symptoms in PD subjects?
Can the data be used in other creative ways to inform patient treatment, care and/or quality of life?
Do the analyses and proposed uses of the data use innovative approaches and methods?","Big data warning - READ ME FIRST
The data is available as binary files or as text file .csvs (where the binary data has already been converted for you). You do not need both versions. If you don't want to write your own binary readers, download the text version. If you are unsure, download the text version. We strongly encourage looking at the data samples to get an idea of how the data is formatted. 
If using text files, you'll want to download:
mjff_text_files.zip
HDL Data Documentation.pdf    
Study Overview.pdf
Participant Codes and Description.pdf
UPDRS Part 1 Questionaire 2.xlsx
UPDRS Part 1 Questionaire-initialscore.xls
UPDRS_Questionaire_Blank.docx
text_sample.tar.bz2
If using binary files, you'll want to download:
mjff_binary_files.zip
HDL.zip    (java code which writes the binaries)
HumDyn.R (R script to convert the binaries to text)
HDL Data Documentation.pdf  
Study Overview.pdf
Participant Codes and Description.pdf
UPDRS Part 1 Questionaire 2.xlsx
UPDRS Part 1 Questionaire-initialscore.xls
UPDRS_Questionaire_Blank.docx
binary_sample.tar.bz2 2.03 KB ",kaggle competitions download -c predicting-parkinson-s-disease-progression-with-smartphone-data,[]
502,"We (the competition hosts) are excited to sponsor the Event Recommendation Engine Challenge, which asks you to predict what events our users will be interested in based on events they’ve responded to in the past, user demographic information, and what events they’ve seen and clicked on in our app. The insights you discover from this data, and the algorithms the winners create, will allow us to improve our event recommendation algorithm, a core part of our applications and a key element in improving user experience.
This is the first competition launching under the Kaggle Startup Program!","For each user in the test set, you are submitting a list of events. These events should be ordered from the ones in which you predict the user will be most interested to those in which the user will be least be interested.
The evaluation metric for this competition is Mean Average Precision at 200 (the maximum number of events for any one user is 116, so 200 doesn't create a meaningful limit). You can find more information about this evaluation metric on the Kaggle wiki.
Sample submission files can be downloaded from the data page. Submission files should be formatted as follows:
Have a header: ""User,Events""
Contain two columns
User: user ids in sorted order
Events: space-delimited list of recommended events, from those the user would be most-interested in to those the user would be least-interested in
First couple lines of a sample submission:
User,Events
1776192,2877501688 1024025121 4078218285 2972428928 3025444328 1823369186 2514143386
3044012,2529072432 1532377761 1390707377 1502284248 3072478280 1918771225
4236494,152418051 4203627753 2790605371 799782433 823015621 2352676247 110357109","  Benchmark Code
There are six files in all: train.csv, test.csv, users.csv, user_friends.csv, events.csv, and event_attendees.csv.
train.csv has six columns:  user, event, invited, timestamp, interested, and not_interested.  Test.csv contains the same columns as train.csv, except for interested and not_interested. Each row corresponds to an event that was shown to a user in our application.  event is an id identifying an event in a our system.  user is an id representing a user in our system.  invited is a binary variable indicated whether the user has been invited to the event. timestamp is a ISO-8601 UTC time string representing the approximate time (+/- 2 hours) when the user saw the event in our application. interested is a binary variable indicating whether a user clicked on the ""Interested"" button for this event; it is 1 if the user clicked Interested and 0 if the user did not click the button.  Similarly, not_interested is a binary variable indicating whether a user clicked on the ""Not Interested"" button for this event; it is 1 if the user clicked the button and 0 if not.  It is possible that the user saw an event and clicked neither Interested nor Not Interested, and hence there are rows that contain 0,0 as values for interested,not_interested.
users.csv contains demographic data about our some of our users (including all of the users appearing in the train and test files), and it has the following columns: user_id, locale, birthyear, gender, joinedAt, location, and timezone. user_id is the id of the user in our system.  locale is a string representing the user's locale, which should be of the form language_territory. birthyear is a 4-digit integer representing the year when the user was born. gender is either male or female, depending on the user's gender.  joinedAt is an ISO-8601 UTC time string representing when the user first used our application.   is a string representing the user's location (if known).  timezone is a signed integer representing the user's UTC offset (in minutes).",kaggle competitions download -c event-recommendation-engine-challenge,['https://www.kaggle.com/code/tombalu/event-recommendation-mldm-eda']
503,"The leaderboard is a central fixture of the Kaggle experience. It provides context to the incredible work accomplished by the Kaggle data science community. To a competitor, the leaderboard is a dynamic, living, action-filled battle. Tactics come to life. Individuals leapfrog over each other.  Teams merge and blend submissions.  Some submit early and often, attempting to build up insurmountable leads. Others bide time, waiting to pounce minutes before the buzzer with their finest of forests.  We see the joys of regularization and the agony of overfitting.  It's raw. It's beautiful. It's thousands of hours of collective human toil.
It's boring.
To an observer, the leaderboard is a spreadsheet.  They see funny team names, numbers with too many decimals, strange column titles, and none of the history behind the battle. We run a veritable nerd olympics, but instead of smashing the 100m world record, we're elbowing for a few decimal places of some esoteric quantity called a capped binomial deviance. It's faceless. It's cold. It fails to tell the story of the battle. And you know what that means?
This means war.
We're calling on you to bring the leaderboard to life.  Break out the D3. Sacrifice an old PC to the javascript gods. Abandon all text, ye who enter here.  We're bootstrapping our own community to do what they do best, and that is doing things better.
What kinds of submissions do we hope result from this competition?
Maybe you know an API or two and can create a motion chart?
Maybe you know the hot, new HTML5 canvas tricks?
Maybe you know of an R package that styles plots like The Economist or XKCD?
Maybe you know Edward Tufte and can call in a favor?
Be creative. Scrape profile photos. Examine team formation. Examine relative scores. Watch for edge cases, cluttered text, and all the gotchas that crop up when you juggle a leaderboard of 10 vs. 1000 teams.  We're looking for entries that convey the storyline behind the leaderboard.  Style and substance counts, as does reproducibility (sorry to the Bob Rosses of the world who want to hand draw their submission).  Web-readiness is appreciated, but we know better than to put such constraints on the Kaggle community.  Use whatever brush you wish to paint this masterpiece.
Credits:
We'd like to acknowledge Chris Mulligan at Columbia University for providing the impetus that put this prospect in motion. You can see his blog post or even check out a git repository of the code he used to do it.
Image: Grimaldi's Leap Frog in the Comic Pantomime of the Golden Fish, 1812 (coloured engraving), Heath, William (1795-1840) / Victoria & Albert Museum, London, UK / The Bridgeman Art Library","Since this is Kaggle, we are offering an objective evaluation. The top two submissions by votes will each recieve a prize.  Submit early and stuff that ballot box with sock puppet accounts! We're kidding. Please don't.
We are also interested in productionalizing these visualizations and giving them back to the community. To this end, we are offering two additional prizes to submissions that we pick subjectively. Why subjectively and how are we evaluating? Well, we're a growing community and need code that acknowledges this fact.  This means we have to pay attention to development, performance, and style. We're looking for such things as:
Ease of implementation on the web
Handling all the strange use cases that happen on a live website (what happens if a team name is the max length? what happens when there are 5,000 teams? what happens when a team name is '.'?!)
Style, coherence, elegance, simplicity
Actual code (as opposed to demo mockups)
Q: It's your website, why aren't you building this?
A: We truly believe in our community.  If our analytics competitions are any indication, somebody out there will do this better than we would (or even more interestingly, they will do it differently than we would). Besides, it's fun and (we hope) will lead to real, live visualizations during competitions. There is nobody more qualified than our competitors to build something like this.","This data has always been publicly available. We've taken the liberty to scrape the public leaderboards from our contests and provide them here.  We have also scraped all our user profile photos (at least, those which aren't the default goose) and provided them.
The leaderboard files contain a TeamId, TeamName, SubmissionDate, and the best public score at the time of submission.  We can not expose every submission score because we want to restrict this to public information (which is the number of submissions and the best public score). Your visualization method should assume this file format as its input.
TeamId   TeamName   SubmissionDate   Score  
12341  The Overfitters 2/10/12 22:59 0.74376
12340  Mungy McDatamungington 2/11/12 16:55 0.67884
12340  Mungy McDatamungington 2/11/12 18:41 0.68326
12344  All 0's All the Time 2/11/12 21:52 0.72021
12370  Santa's little helper 2/12/12 0:43 0.65689",kaggle competitions download -c leapfrogging-leaderboards,[]
504,"This competition will launch at midnight UTC on Saturday, December 15.
Santa Claus was excited to learn about the Kaggle competition platform, and wanted to use it for a slightly different purpose. Rather than a predictive modeling problem, he has an optimization problem for you: a very, very important optimization problem.
Santa needs help choosing the route he takes when delivering presents around the globe. Every year, Santa has to visit every boy and girl on his list.  It's a tough challenge, and Santa admits he scored a B- on his combinatorical optimization final. He's hoping you can develop algorithms that will solve his problem year after year.
Santa asked that we give you one particular instance of his TSP (Traveling Santa Problem). However, Santa's dilemma isn't quite the same as the Traveling Salesman Problem with which you may be familiar. Santa likes to see new terrain every year--don't ask, it's a reindeer thing--and doesn't want his route to be predictable.
You're looking for shortest-distance paths through a set of chimneys, but instead of providing one path, Santa asks you to provide two disjoint paths. If one of your paths contains an edge from A to B, your other path must not contain an edge from A to B or from B to A (either order still counts as using that edge). Your score is the larger of the two distances.  Santa asks competition winners to publish and open source the algorithms they use (for his future use, of course).
Rudolph was very adament about minimizing his workload. Trust us, you don't want to be on Rudolph's bad side.
Important note about prizes: We believe that Kaggle's public leaderboard is very important for both the fun of the competition and achieving great results, and we want to provide an incentive for everyone to submit to the public leaderboard all along the way (even though you can easily determine your submission's score all by yourself). So the competition will have two sets of prizes, one based on the scores at the end of the competition, and one based on the scores at the end of a randomly chosen day (UTC) between December 23 and January 17.  The day will not be revealed (or even chosen) until after the competition ends. (The competition will end at the end of the day UTC on January 18.)
  Attributions:
Data generation and lots of help framing the problem (including coming up with this TSP variant): Robert Bosch of Oberlin College Math Department
Santa photo: AurélienS
Sleigh photo: Creative Tools
Globe: William Cook
   ","Submissions consist of two paths, which must have disjoint edges. So if one of your paths contains an edge from A to B, your other path must not contain an edge from A to B or from B to A (either order still counts as using that edge). Your score is the larger of the two distances. Submissions that do not satisfy this condition will result in an error and no score.
Submissions that do satisfy this condition will be scored as follows: Compute the length of each path by summing the Euclidean distance for each each, and your score is the longer of the two distances.
Ties will be broken by earliest submitter.","The cities are specified in a CSV file with 3 columns: id, x, y.",kaggle competitions download -c traveling-santa-problem,['https://www.kaggle.com/code/javiabellan/starting-kernel-plotting-nearest-neighbor']
505,"About Us
Colorado School Grades was created by a coalition of non-profit, community organizations that believe all children deserve access to a high-performing school. Our mission is to provide community members, parents, students, and educators with school performance information that is both accessible and easy-to-understand. Our hope is that this will help families and students make more informed decisions about the school they choose based in some part on academic performance information. We also aim to inspire and equip community members, parents, students, and educators with the information and resources they need to effectively engage in local school improvement efforts.  We provide resources for community stakeholders to improve their chosen schools.
Colorado School Grades receives over 300,000 parents annually. Please use the site as a reference for any additional questions that you may have. And please be sure to include the Colorado School Grades link and logo in your visualization.
Competition is organized and administered by Ryan Wilson at FiveFifty.  
  Data Visualization Interests
We believe that information is power and our work translates the state of Colorado’s school performance labels into easier-to-understand letter grades. We make these grades public on an intuitive, user-friendly platform at www.ColoradoSchoolGrades.com. Now that we have three years of letter grades for the schools, we are interested in seeing what trends and insights visualisation experts can identify and explain in compelling data visualizations. Here is a list of some the questions we find intriguing:
How have grades changed over time across the state (or perhaps more importantly how have they remained the same)?
Where are the A schools primarily located? Our vision is that all kids have access to a high-performing school – how does the Colorado deliver against that promise of equity? 
Are there correlations between A schools and student demographics (free/reduced lunch is a proxy for poverty or by race) – Do poor and minority kids have access to A schools?
Academic growth is an indicator used in the grading system. It is described in more detail in the data description page and on the Colorado School Grades website, but is perhaps the greatest indicator of how much teaching and learning is actually occurring in the school. That said, where are the schools that have the best sub-grades for student growth? Are there particular schools that have high percentages of low income students AND high grades for student growth. Some may say those schools are doing more to close Colorado’s achievement gap between the wealthy and the poor than any other.
What percentage of Colorado’s student’s are ready for college and career by school or by school district?
Which districts have the most A schools, F schools, or improving schools?
Which schools have improved their letter grades the most?
How do these grades, graduation rates, and college/career readiness metrics compare to labor market and economic data / needs?
What have we missed? Please use your creativity to identify interesting trends or insights that the data tells us.","Winners will be determined using the following criteria:
Accurate portrayal of the data and analysis, using the visuals you choose.  
Displayed distillation of complex data into easily digestible visual.
Awareness of things like the hierarchy of importance of visual properties and best labeling practices. 
Use of words to sell, tell, propel readers to action, sound omniscient, keep things simple, speak authoritatively and distinctively, attract, invite, intrigue, encourage, evoke, promote, promise, inform...Simultaneously and Concisely.
Use of accurate context.
Demonstrated understanding on the subject matter
Visual.ly is our unbiased judge of the competition.  They are not involved with Colorado Succeeds or Colorado School Grades outside of this competition.  Their interests are in identifying the best work from the standpoint of mastery of data visualization and design principles.","NOTE: The file 'ColoradoSchoolGrades_20121210.zip' contains all the other data files.  If you download it, you do not need to download any of the other files.
Colorado School Grades
Summary of Grading Methodology
While most other rating systems are based solely on student academic achievement, or one snapshot in time of student performance, Colorado School Grades believes in the importance of using student academic growth to calculate overall school performance. For this reason, Colorado School Grades worked with the Center for Education Policy Analysis at the University of Colorado at Denver to calculate the grades using the exact same variables and weights as the Colorado Department of Education’s School Performance Framework. The input data for calculating the overall grades includes:
  For Elementary and Middle Schools
For High Schools
Key Performance Indicator
Weighting
Key Performance Indicator
Weighting",kaggle competitions download -c visualize-the-state-of-education-in-colorado,[]
506,"This is a private, invitation-only competition. The relevant information is provided only to contestants.","This is a private, invitation-only competition. The relevant information is provided only to contestants.",,,
507,"For this challenge, potential Facebook recruits will be exploring the map of the entire internet. Unlike the map of a city, where best routes are relatively fixed except for the occasional construction or parade detour, the paths that information travels over the web are constantly changing. There is no centralized system of stop-lights or traffic cops.  Instead, there are tens of thousands of autonomous systems using a common protocol to advertise the next available hops, updated depending on service-agreements, capacity, and load. This will be a test of both the candidates engineering know-how and their ability to statistically learn on complex, dynamic graph structures.
The Task: you will be given a path which, at one point in the training time period, was an optimal path from node A to B. The question is then to make a probalistic prediction, for each of the 5 test graphs, whether the given path is STILL an optimal path.  This is a much more difficult task than link prediction alone. The global structure of the graph may affect many optimal routes, paths can have varying lengths (and thus varying a priori probabilities of being optimal), and there may be multiple optimal routes for a given source & destination.
The Prize: Facebook is seeking data-savvy software engineers (Data Engineers) to build the next generation of systems that will transform the online experience of over a billion users. Appropriate candidates should have experience with multiple components across the big data stack (check out the Visualization track for another way to highlight your skills).  There are many teams that they could be a fit for depending on their backgrounds . Positions are available in Menlo Park, Seattle, New York City, and London; candidates must have, or be eligible to obtain, authorization to work in the US or UK.
    An example visualization of Internet topology (round-trip times) produced by Walrus 
(courtesy of Young Hyun and inverted for display purposes)","Your predictions will be evaluated by the Area Under the ROC Curve (AUC) metric.  While the ground-truth is binary, real-valued submissions are allowed. See the Data Description page for instructions on formatting your submission.","There are 15 graphs in the training set and 5 in the test set. These are directed, dynamic, real-world Internet topology graphs with nodes that correspond to Autonomous Systems (AS).  Each graph is sampled at a fixed time interval, \\(\Delta t\\), apart.  The 5 test graphs represent the status of the network at the 5 time points after the final training graph, but are not supplied to participants. 
Graph Time Supplied?
train1  \\( t_0 \\)  Y
train2  \\( t_0 +\Delta t \\)  Y
train3  \\( t_0 + 2\Delta t \\)  Y
train4  \\( t_0 + 3\Delta t \\)  Y
train5  \\( t_0 + 4\Delta t \\)  Y
train6  \\( t_0 + 5\Delta t \\)  Y",kaggle competitions download -c facebook-ii,[]
508,"There is more to the Universe than meets the eye. Out in the cosmos exists a form of matter that outnumbers the stuff we can see by almost 7 to 1, and we don't know what it is. What we do know is that it does not emit or absorb light, so we call it Dark Matter.
Such a vast amount of aggregated matter does not go unnoticed. In fact we observe that this stuff aggregates and forms massive structures called Dark Matter Halos.
Although dark, it warps and bends spacetime such that any light from a background galaxy which passes close to the Dark Matter will have its path altered and changed. This bending causes the galaxy to appear as an ellipse in the sky. 
  Figure 1: Dark Matter bending the light from a background galaxy. In extreme cases the galaxy here is seen as the two arcs surrounding it. (Credit: NASA, ESA, and Johan Richard (Caltech, USA))
Since there are many galaxies behind a Dark Matter halo, their shapes will correlate with its position.
Figure 2: The effect of Dark Matter on the sky 
  What’s The Problem?
Detecting these Dark Matter halos is hard, but possible using this data. If we can accurately estimate the positions of these halos, we can then understand the function they play in the Universe. There are various methods to attack the problem (we have given you some examples), however we have not been able to reach the level of precision required to understand exactly where this Dark Matter is for all Dark Matter halos.
We challenge YOU to detect the most elusive, mysterious and yet most abundant matter in all existence.

Figure 3: Dark Matter in Action. If you look closely at this real world example, you can see the warped and elliptical galaxies. (Credit NASA; ESA; L. Bradley (Johns Hopkins University); R. Bouwens (University of California, Santa Cruz); H. Ford (Johns Hopkins University); and G. Illingworth (University of California, Santa Cruz)
Challenge Organisers: David Harvey (Astrophysics PhD Student, Institute for Astronomy, University of Edinburgh), Dr. Tom Kitching (Royal Society Post Doctorial Fellow, Institute for Astronomy, University of Edinburgh)","The challenge is to predict the centers (x and y coordinates) of the dark matter halos in the test set of 120 skies.
We have provided DarkWorldsMetric.py that you can use to calculate the metric of your estimates to the training data. All you need are the python packages numpy, math, csv, c, getopt, sys and argparse and to run it type
python DarkWorldsMetric.py PATH_TO_PREDICTIONS PATH_TO_Training_halos.csv
and it shold output the metric. Here is how it works...
In order to test your algorithm for Accuracy and Precision there will be two parts to the metric. The first part will be how close you get to the true position of the halo, and the second will be to make sure there is no preferred direction to your estimates (positional bias).
$$m=F/1000+G$$
Where F will be the average radial distance from the user estimate to the true position of the halo. In the case of more than one halo we will match up your estimates to the true estimates in the configuration that gives the best average radial distance in that particular sky so you will get the best score possible.
G is calculated by firstly finding the angle of the predicted position with respect to the centre of the true halo position, where the angle is measured from the estimated position to the line connecting the true halo position and a given reference point as shown in the figure. Then each angle is converted into a vector and then the average vector is calculated. If the algorithm has no preferred direction this average vector should be 0. We calculate the vector using the following equation.
$$G=\sqrt{\left(\frac{1}{N}\sum_{i=1}^N\cos(\phi_i)\right)^2+\left(\frac{1}{N}\sum_{j=1}^N\sin(\phi_j)\right)^2} $$
We will estimate the vector your distribution of phi over all the halos in all the simulations.
The reference point is provided in the Training_halos.csv file for you to evaluate your predictions locally.","Training Data
The training data consists of 300 simulated skies similar to the final panel in Figure 2 of the description page. Each sky contains between 300 and 740 galaxies. Each galaxy will have an x and y position ranging from 0 to 4200 (units are pixels), and a measure of ellipticity: e1 and e2 (see An Introduction to Ellipticity). 
Training galaxy data is provided in a series of 300 files, one file for each Sky (e.g, Training_Sky27.csv or Training_Sky123.csv). These files have 4 columns:
galaxy id
x-coordinate
y-coordinate
e1
e2
Dark matter halo locations in each sky are provided in the file Training_halos.csv. This file contains 10 columns, namely:
Sky Id
number of halos (1, 2 or 3)",kaggle competitions download -c DarkWorlds,['https://www.kaggle.com/code/mpwolke/darkworlds-skies-galaxys-zip-tar']
509,"Visualization Track now open >>
  Do you think you can take on the crudest, meanest trolls on the internet? Okay then, game on!
The challenge is to detect when a comment from a conversation would be considered insulting to another participant in the conversation. Samples could be drawn from conversation streams like news commenting sites, magazine comments, message boards, blogs, text messages, etc.
The idea is to create a generalizable single-class classifier which could operate in a near real-time mode, scrubbing the filth of the internet away in one pass.
Prizes you say?
Besides the prize money, eternal fame and glory, monuments in your honor, and the admiration of friend and foe alike, you get a chance for an interview at Impermium for the Principal Data Engineer role.
In addition, there will be a Visualization prospect attached to the contest.  Slice and dice the data to show us the most amazing, informative and thought-provoking infographics, diagrams or plots! Submission open 1 week before the contest ends. 
(Please note that referenced fame, glory, monuments and admiration are to be bestowed only in the imagination of the contestants)","This is a single-class classification problems.  Your predictions should be a number in the range [0,1]  where 0 indicates 0% probability of comment being an insult, and 1 represents 100% insult.
 All predictions should be in the first column of your submission file. Please see 'sample_submissions_null.csv"" for the correct format.
The evaluation metric for this competition is the Area under the Receiver Operating Curve (AUC).   This evaluation metric penalizes wrong predictions made with high probability.  For more about this metric, check out the AUC page on the Kaggle Wiki.
  Please note, winners for the competition will be determined by their performance on an additional verfication set that will be released at the end of the competition.  See Timeline tab for details.","Data
The data consists of a label column followed by two attribute fields. 
This is a single-class classification problem. The label is either 0 meaning a neutral comment, or 1 meaning an insulting comment (neutral can be considered as not belonging to the insult class.  Your predictions must be a real number in the range [0,1] where 1 indicates 100% confident prediction that comment is an insult.
The first attribute is the time at which the comment was made. It is sometimes blank, meaning an accurate timestamp is not possible. It is in the form ""YYYYMMDDhhmmss"" and then the Z character. It is on a 24 hour clock and corresponds to the localtime at which the comment was originally made.
The second attribute is the unicode-escaped text of the content, surrounded by double-quotes. The content is mostly english language comments, with some occasional formatting.
Guidelines
We are looking for comments that are intended to be insulting to a person who is a part of the larger blog/forum conversation. 
We are NOT looking for insults directed to non-participants (such as celebrities, public figures etc.). 
Insults could contain profanity, racial slurs, or other offensive language. But often times, they do not. ",kaggle competitions download -c detecting-insults-in-social-commentary,"['https://www.kaggle.com/code/rishabhgarg1023/detecting-insults-from-social-commentary', 'https://www.kaggle.com/code/suraj520/cyberbully-detection-basics-to-advanced', 'https://www.kaggle.com/code/jozefkond/silvo-sematicky']"
510,"An important part of succeeding as an insurance company is having a good understanding of which of the company’s current customers will be with the company into the future.  Every customer comes with a different risk profile and it is critical to plan appropriately for that future risk.  The goal of this competition is to predict which current customers will still be with the company in 6 months, given many of the customer’s characteristics. ","The training data contains observations from 2008 to 2010.  Observations from the first half of 2011 make up the test data used to score the public leaderboard (during the competition).  Observations from the second half of 2011 make up the test data used for the private leaderboard (for final scoring).
The metric (for both leaderboard and final winners) used to score entries will be Bernoulli log likelihood.
Contestants will provide predictions (probabilities) for a binary event: probability of existence in 6 months.  For each observation, the bernoulli log likelihood will be calculated and summed across all observation.  Larger likelihood represents a better model. 
Bernoulli log-likelihood = y*log(P) + (1-y)*log(1-P)
     where:
y = actual response (0,1)
P = predicted response
https://www.kaggle.com/wiki/LogarithmicLoss         ",,,
511,"Flight Quest Phase 1 Winners
1. Xavier Conort
1. Hong Cao
1. Clifton Phua
1. Ghim-Eng Yap
1. Kenny Chua
Team Gxav &* used a mixture of gradient boosting and random forest models to predict gate and runway arrival times. With average errors of 4.2 and 3.2 minutes for gate and runway arrivals, respectively, this translates to 40% and 45% improvements over the standard industry benchmark estimates. Key to their success was careful feature selection with their final models using only 58 and 84 features for gate and runway arrivals, respectively, from the total 258 features they painstakingly constructed and optimized.
2. Jonathan Peters
2. Pawel Jankiewicz
Team As High As Honor used a two-step approach that combined the results of a generalized linear model that encoded intuition about important variables with refinements derived from a random forest model. The team capitalized on the success of the linear model to add the effects of multiple variables and cleanly resolve issues of missing data.
3. Gabor Takacs
Team Taki used a six layer model relying on successive ridge regressions and gradient boosting machines to model both gate and runway arrival times. This approach used 56 features extracted from the raw data, with all but two coming from the test day data.
4. Sergey Kozub
Team Sun’s approach to predicting gate and runway arrival times relied on creating a derived data set with new variables encoding information about the aircraft, airport, airway, gate, hour, and flight path times. Important features used in this model include aircraft GPS position, ASDI flight plans the direction from which airplanes approached airport runways.
5. Jacques Kvam
Jacques Kvam’s approach for predicting runway and gate arrival times used gradient boosting for a model using 10,000 trees and a whopping 1,102 features trained on 260,000 flights. Most significant among these included the distance between the final waypoint and the arrival airport. Many weather features were important as well including temporary vertical visibility and wind speed at the arrival airport.
Honorable Mention: Matt Berseth
Team __mtb__ used random forest and gradient boosted models to estimate runway and gate arrival times. The final solution included over 100 different individual models, each focused on a narrow set of features (i.e. wind/weather, flight plan, aircraft's current location, etc.). These individual models were blended together to generate the final estimates. The training data was created by randomly selecting eight cutoff times for each day in the training period. A separate cross validation data set was used to select hyper-parameters.
Think you can change the future of flight?
Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays.
There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with “real time business intelligence,”—information available in the cockpit that would allow them to make adjustments to their flight patterns. 
Your challenge, should you decide to accept it: 
Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time.
Tweet open_in_newhttps://www.facebook.com/plugins/like.php?href=http%3A%2F%2Fwww.gequest.com%2Fc%2Fflight&send=false&layout=button_count&width=450&show_faces=false&font&colorscheme=light&action=like&height=21","You are predicting both the runway arrival time and the gate arrival time, so you have two predictions for each airplane.
Your predictions will be evaluated based on the root mean squared error (RMSE) between the predicteds and the truth:
$$ \textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$
We will separately compute the RMSE for gate arrival times and runway arrival times, and use a weighted average of the two:
$$ \textrm{RMSE_Final} = (0.75 \times \textrm{RMSE_Gate}) + (0.25 \times \textrm{RMSE_Runway}) $$
 "," The Flight Quest data is temporally split into five datasets that will be released over the course of the competition. The names and dates of the flight data contained in each dataset are as follows:
Initial Training Set: November 12, 2012 - November 25, 2012 (14 days)
Public Leaderboard Set: November 26, 2012 - December 9, 2012 (14 days)
Augmented Training Set, Part 1: December 10, 2012 - January 2, 2012 (24 days)
Augmented Training Set, Part 2: January 3, 2013 - February 6, 2013 (35 days)
Final Evaluation Set: February 15, 2013 - February 28, 2013 (14 days)
The initial and augmented training sets contain all the flight and weather information for domestic US flights during the corresponding days. The public leaderboard set and final evaluation sets have this information for each day up to a predetermined cutoff time.
For each day in the test period (first for the public leaderboard, and later for the final evaluation), we will select a random time (uniformly chosen between 9am EST and 9pm EST) and select all of the flights in the air at that cutoff time. You will be provided with relevant data for each day that would be available at the chosen cutoff time.
Your model must be structured so that it makes each test day's final test data set predictions based on no information in the final evaluation test data other than the information from that day, which will be in an appropriately named folder. (Reworded for clarification on 11/30/2012. See forum for explanation.",kaggle competitions download -c flight,[]
512,"The most expensive political campaign season in US history is well underway, with experts predicting that spending from both the Republican and Democratic camps will exceed $5.8 billion before races for president and Congress are decided in November.
Needless to say, campaign finance is a big business. And where big money leads the way, big data is never too far behind.
That's why the Center for Investigative Reporting, the nation's largest non-profit investigative reporting organization, is teaming up with Investigative Reporters and Editors, Inc. -- the world's leading investigative and data journalism trade group -- to offer this Prospect challenge in the hopes of answering one simple question: What can some of the world's most brilliant data scientists teach us about finding hidden patterns, interesting connections and ultimately compelling stories in a treasure trove of data about federal campaign contributions?
Data journalists have been examining federal campaign finance records for decades, finding patterns and trends that have forced resignations and reforms. But despite many noteworthy successes, journalists' imaginations are limited by our skills. We're not mathematicians and machine learning experts! We might learn from a source, for instance, that a particular congressman has begun receiving campaign contributions from a new and unusual donor. But a carefully tuned anomaly detection system might reveal those patterns before our sources would ever notice them.
What kinds of ideas are we looking for with this Prospect challenge? We want to see how sophisticated clustering algorithms can help spot donors that coordinate their operations, giving to the same candidates at the same times. We want to see how classification or anomaly detection systems can find donations that are particularly interesting or unusual. We want to know how you would mix up campaign contribution data with other sources -- lobbying records, congressional votes, federal contracts -- to find patterns that journalists are missing.
Novel approaches to analysis, ideas for useful tools, and data visualizations will all be considered.
What we're looking for is new ideas and approaches. To give you a sense of what journalists have done so far, we've included several examples of some of the most interesting campaign finance reporting around, along with some tipsheets that explain how journalists approach campaign finance data and what to look for.","Winners will be chosen by a panel of judges, including:
Chase Davis, director of technology at the Center for Investigative Reporting
Stephen Doig, a Pulitzer Prize-winning data journalist now at Arizona State University
Mark Horvit, executive director of Investigative Reporters and Editors, Inc.
Several other award-winning journalists with experience covering politics and campaigns
[Participants will also be able to vote on submissions at the end of the contest.  This feedback will be taken into account by the judging panel.]","FEC Itemized campaign finance records for the 2012 election cycle
Fetched on Monday, September 3rd, 2012 at approximately 02:45:25 AM
The Federal Elections Commission itemized contributions and transactions database has been an invaluable tool for reporters wanting to ""follow the money"" in federal elections campaigns.
The data we've included here covers every campaign contribution given to federal candidates so far in the 2012 election cycle (January 2011 through early September 2012). It includes data about contributions given to every federal candidate nationwide -- for president, House and Senate -- by political action committees, individuals and Super PACs. It also contains information about contributors to those committees, and assorted details about each contribution: amount, date, location and occupation of the donor, etc.
Use of outside data is allowed in this competition.
An important disclaimer: This dataset does not include the electronic disclosure data, which is available on the FEC's Web site, and comes directly from campaigns. Disclosure data is much more complexed than the itemized set, but contains more information.
  Tables
: List of all FEC licensed campaign committees for the election cycle. This table can be used as a reference when using the tables describing money changing hands. From the FEC Web site: ""The committee master file contains one record for each committee registered with the Federal Election Commission. This includes federal political action committees and party committees, campaign committees for presidential, house and senate candidates, as well as groups or organizations who are spending money for or against candidates for federal office. The file contains basic information about the committees. The ID number the Commission assigned to the committee is first, along with the name of the committee, the sponsor, where appropriate, the treasurer's name and the committee's address. The file also includes information about what type of committee is being described, along with the candidate's ID number if it is a campaign committee."" Contains 13,065 records.",kaggle competitions download -c cir-prospect,[]
513,"This is the Wind Forecasting track of Global Energy Forecasting Competition 2012 (GEFCom2012).This competition will bring together state-of-the-art techniques for energy forecasting, serve as the bridge to connect academic research and industry practice, promote analytics in power engineering education, and prepare the industry to overcome forecasting challenges in the smart grid world.
The total prize pool for the wind forecasting track is $7,500. GEFCom is not a paper contest. Instead, this is a competition that requires participants to develop models and submit forecasts based on a given data set. Accuracy of the forecasts will be one of the evaluation criteria. In addition to accuracy, the participants are also required to submit a report describing the methodology, findings and models. Selected entries will be invited to IEEE PES General Meeting 2013 in Vancouver, Canada to present their methodologies and results. The team that finishes at the top of the leaderboard will win a cash prize. However overall winners of the competition will be determined by the GEFCom Award Committee after the presentations based on forecasting accuracy, clarity of documentation, rigors of the approach, interpretability of the models and practicality to the industry. A few winning entries will be invited to submit the report in scientific paper format to prestigious scholarly journals, such as International Journal of Forecasting and IEEE Transactions on Smart Grid.
The topic for the wind forecasting track is focused on mimicking the operation 48-hour ahead prediction of hourly power generation at 7 wind farms, based on historical measurements and additional wind forecast information (48-hour ahead predictions of wind speed and direction at the sites). The data is available for period ranging from the 1st hour of 2009/7/1 to the 12th hour of 2012/6/28.
The period between 2009/7/1 and 2010/12/31 is a model identification and training period, while the remainder of the dataset, that is, from 2011/1/1 to 2012/6/28, is there for the evaluation. The training period is there to be used for designing and estimating models permiting to predicting wind power generation at lead times from 1 to 48 hours ahead, based on past power observations and/or available meteorological wind forecasts for that period. Over the evaluation part, it is aimed at mimicking real operational conditions. For that, a number of 48-hour periods with missing power observations where defined. All these power observations are to be predicted. These periods are defined as following. The first period with missing observations is that from 2011/1/1 at 01:00 until 2011/1/3 at 00:00. The second period with missing observations is that from 2011/1/4 at 13:00 until 2011/1/6 at 12:00. Note that to be consistent, only the meteorological forecasts for that period that would actually be available in practice are given. These two periods then repeats every 7 days until the end of the dataset. Inbetween periods with missing data, power observations are available for updating the models.
 ",The forecasting accuracy will be evaluated with a Root Mean Square Error (RMSE) criterion. The RMSE is calculated as the square root of the average of square forecast errors. All forecasts will be equally weighted in the score calculation.,"""train.csv"" contains the training data:
- the first column (""date"") is a timestamp giving date and time of the hourly wind power measurements in following columns. For instance ""2009070812"" is for the 8th of July 2009 at 12:00;
- the following 7 columns (""wp1"" to ""wp7"") gather the normalized wind power measurements for the 7 wind farms. They are normalized so as to take values between 0 and 1 in order for the wind farms not to be recognizable.
In parallel, files with explanatory variables (wind forecasts) are also provided for those who may want to use them. For example, the file ""windforecasts_wf1"" contains the wind forecasts for the wind farm 1. In these files:
- the first column (""date"") is a timestamp giving date and time at which the forecasts are issued. For instance ""2009070812"" is for the 8th of July 2009 at 12:00;
- the second column (""hors"") is for the lead time of the forecast. For instance if ""date"" = 2009070812 and ""hors"" = 1, the forecast is for the 8th of July 2009 at 13:00
- the following 4 columns (""u"", ""v"", ""ws"" and ""wd"") are the forecasts themselves, the first two being the zonal and meridional wind components, while the following two are corresponding wind speed and direction.
Finally, the file ""benchmark.csv"" provide example forecast results from the persistence forecast method (""what you see is what you get""). This file also gives a template for submission of results that should be strictly followed.
Please notice that the first column of ""benchmark.csv"" is called ""id"" and contains unique identifier for each row. The other 8 columns are the same as ""train.csv"".",kaggle competitions download -c GEF2012-wind-forecasting,"['https://www.kaggle.com/code/kwheng2014/eda-train', 'https://www.kaggle.com/code/gmudit/tsa-wind-power-forecasting', 'https://www.kaggle.com/code/mohitkhurana21/mk-wind-forecast', 'https://www.kaggle.com/code/shashwat5139/wind-forecasting', 'https://www.kaggle.com/code/chen527/wind-power-preidct']"
514,"Note: The prediction phase of this competition has ended. Please join the visualization competition which ends on Nov. 11, 2012.
--
This challenge is to develop a statistical model to predict census mail return rates at the Census block group level of geography. The Census Bureau will use this model for planning purposes for the decennial census and for demographic sample surveys. The model-based estimates of predicted mail return will be publicly released in a later version of the Census ""planning database"" containing updated demographic data.
Participants are encouraged to develop and evaluate different statistical approaches to proposing the best predictive model for geographic units. The intent is to improve our current predictive analytics.
Please note also that as described in the rules, only US citizens and residents are eligible for prizes.","Mean absolute error (weighted by Tot_Population_CEN_2010), described here on the Kaggle wiki.",,,
515,"This competition is now complete. Congratulations to the winners!
Millions of programmers use Stack Overflow to get high quality answers to their programming questions every day.  We take quality very seriously, and have evolved an effective culture of moderation to safe-guard it.
With more than six thousand new questions asked on Stack Overflow every weekday we're looking to add more sophisticated software solutions to our moderation toolbox.
Closing Questions
Currently about 6% of all new questions end up ""closed"".  Questions can be closed as off topic, not constructive, not a real question, or too localized.  More in depth descriptions of each reason can be found in the Stack Overflow FAQ.  The exact duplicate close reason has been excluded from this contest, since it depends on previous questions.
Your goal is to build a classifier that predicts whether or not a question will be closed given the question as submitted, along with the reason that the question was closed.  Additional data about the user at question creation time is also available.","Your solution should produce a probability (between 0 and 1) of a given question not being closed and the probabilities of being closed as each of the four close reasons.  The evaluation metric is Multiclass Logarithmic Loss.
Refer to Submission Instructions for instructions on how to format your submissions file.
While live data is available from Stack Overflow, be aware that the final evaluation will use data gathered after all solutions are submitted.","The code for the benchmarks is on Github.
The training data contains data through July 31st UTC, and the public leaderboard data goes from August 1 UTC to August 14 UTC.
The train.csv file contains post text and associated metadata at the time of post creation which will serve as inputs to your solution.  The state of the post as of July 31st is also included. It contains the following fields (not in this order):
Input
PostCreationDate
OwnerUserId
OwnerCreationDate
ReputationAtPostCreation
OwnerUndeletedAnswerCountAtPostTime
Title
BodyMarkdown
Tag1
Tag2",kaggle competitions download -c predict-closed-questions-on-stack-overflow,"['https://www.kaggle.com/code/aravindanr22052001/stackoverflowrun', 'https://www.kaggle.com/code/imabhilash/stackoverflowproject', 'https://www.kaggle.com/code/aneeshtickoo/stackoverflowrun', 'https://www.kaggle.com/code/hemanthhari/stack-hemanth', 'https://www.kaggle.com/code/akhisreelibra/stack-overflow-basic-setup']"
516,"This is the Load Forecasting track of Global Energy Forecasting Competition 2012 (GEFCom2012).This competition will bring together the state-of-the-art techniques for energy forecasting, serve as the bridge to connect academic research and industry practice, promote analytics in power engineering education, and prepare the industry to overcome forecasting challenges in the smart grid world.
The prize pool for the load forecasting track is $7,500. GEFCom is not a paper contest. Instead, this is a competition that requires participants to develop models and submit forecasts based on a given data set. Accuracy of the forecasts will be one evaluation criteria. In addition to accuracy, the participants are also required to submit a report describing the methodology, findings and models. Selected entries will be invited to IEEE PES General Meeting 2013 at Vancouver, Canada to present their methodologies and results. The team that finishes top of the leaderboard will win a cash prize. However an overall winner of the competition will be determined by the GEFCom Award Committee after the presentations based on forecasting accuracy, clarity of documentation, rigors of the approach, interpretability of the models and practicality to the industry. A few winning entries will be invited to submit the report in scientific paper format to prestigious scholarly journals, such as International Journal of Forecasting and IEEE Transactions on Smart Grid.
The topic for the load forecasting track is a hierarchical load forecasting problem: backcasting and forecasting hourly loads (in kW) for a US utility with 20 zones. The participants are required to backcast and forecast at both zonal level (20 series) and system (sum of the 20 zonal level series) level, totally 21 series.
Data (loads of 20 zones and temperature of 11 stations) history ranges from the 1st hour of 2004/1/1 to the 6th hour of 2008/6/30.
Given actual temperature history, the 8 weeks below in the load history are set to be missing and are required to be backcasted. It's OK to use the entire history to backcast these 8 weeks.
2005/3/6 - 2005/3/12;
2005/6/20 - 2005/6/26;
2005/9/10 - 2005/9/16;
2005/12/25 - 2005/12/31;
2006/2/13 - 2006/2/19;
2006/5/25 - 2006/5/31;
2006/8/2 - 2006/8/8;
2006/11/22 - 2006/11/28;
In addition, the particpants need to forecast hourly loads from 2008/7/1 to 2008/7/7. No actual temperatures are given for this week. ","The forecasting accuracy will be evaluated by weighted root mean square error.
The weights are assigned as following:
Each hour of the 8 backcasted weeks at zonal level: 1;
Each hour of the 8 backcasted weeks at system level: 20;
Each hour of the 1 forecasted week at zonal level: 8;
Each hour of the 1 forecasted week at system level: 160;
Details of weight assignment are shown in data file: weights.csv.","In each of the 5 data files, there is a header row. Three columns of calendar variables: year, month of the year and day of the month. The last 24 columns are the 24 hours of the day.
In ""Load_history.csv"", Column A is zone_id ranging from 1 to 20. 
In ""Temperature_history.csv"", Column A is station_id ranging from 1 to 11.
In ""submission_template.csv"", ""weights.csv"", and ""Benchmark.csv"", Column A is id, the identifier for each row; Column B is zone_id ranging from 1 to 21, where the 21st ""zone"" represents system level, which is the sum of the other 20 zones.
""Benchmark.csv"" shows the results from a benchmark model.
Please make sure the submission strictly follow the format as indicated in ""submission_template.csv"", where the year was sorted in smallest to largest order first, then month, then day, and then zone_id.",kaggle competitions download -c global-energy-forecasting-competition-2012-load-forecasting,[]
517,"Two years of mobile behavior, 67 million clicks, 27 million searches, 8 million users, 1 million products
You can enter one or both tracks of the competition:
Cloud computing sized problem
PC sized problem (a sample of the cloud sized problem) - You are here
There will also be a Visualization Contest that can be entered from either track.
For more details on the event, go to:
http://www.sfbayacm.org/DM-Hackathon-2012-10
Data Provided by:
  Cloud Compute Sponsors:    
                    ","The evaluation metric for this competition is MAP@5.
This metric is defined and explained here.","The main data for this competition is in the train.csv and test.csv files. These files contain information on what items users clicked on after making a search.
Each line of train.csv describes a user's click on a single item. It contains the following fields:
user: A user ID
sku: The stock-keeping unit (item) that the user clicked on
category: The category the sku belongs to
query: The search terms that the user entered
click_time: Time the sku was clicked on
query_time: Time the query was run
test.csv contains all of the same fields as train.csv except for sku. It is your job to estimate which sku's were clicked on in these test queries.
Due to the internal structure of BestBuy's databases, there is no guarantee that the user clicks resulted from a search with the given query. What we do know is that the user made a query at query_time, and then, at click_time, they clicked on the sku, but we don't know that the click came from the search results. The click_time is never more than five minutes after the query_time.",kaggle competitions download -c acm-sf-chapter-hackathon-small,"['https://www.kaggle.com/code/arsenismagilov/arsen-second', 'https://www.kaggle.com/code/voonadhanvanth/arsen-second', 'https://www.kaggle.com/code/huanguyen/random-forest', 'https://www.kaggle.com/code/koushikl/kaggle']"
518,"View the Winning Entry in the HBR >>
Data-mine the progress of almost a century's worth of the most influential management concepts and ideas.  The Harvard Business Review is asking you to turn your data-vision on the archival history of the HBR. The goal of this prospect to to generate analysis and visualizations from the metadata and abstracts of every article they have published over the last 90 years. Winning entries will be featured in the Vision Statement feature of the upcoming 90th anniversary issue.
What makes a great entry?  Check out the past 'Vision Statement' features scattered throughout the contest page, and available for download.  The HBR wants you to find the story behind the data. Don't just build a latent topic model... show how the important topics have trended over the last 90 years.  Once you quantify the impact of an article, can you pick out the most seminal case-studies of the 20th century?
Entries must contain not just an idea, but actual analysis and visualization. You don't have to be a professional graphic designer, but you should keep in mind how your work will make its point to a professional, but possibly non-technical, audience.  This is a contest for every analyst who has struggled to explain the value of data to his or her boss.   Well, now is your chance to show what you can do to your boss's boss's boss.","The entries will be submitted and evaluationed through the Kaggle Prospect platform.
How Kaggle Prospect works:
HBR has made the data available for download here.
Kaggle members can analyze the data, visualize interesting patterns and relationships, and submit their discoveries that they think would have the most impact on HBR's readers
Kaggle members can see all of the proposals, comment, and upvote their favorites (think of it as peer-review on steriods)
At the end of the contest, HBR will select the prize winning entries from among the top 10 submissions with the most votes.","There are two files available for download.  The pdf contains more examples of past Vision Statements.  The csv is the data for the contest itself (the updated version which fixes a bug in the abstracts that was found by Kagglers on the forum )
  Articles should have publication dates betwen 1922 and 2012 ( so if your date parser is giving you articles from the future, remember to subtract 100 years )
 ",kaggle competitions download -c harvard-business-review-vision-statement-prospect,[]
519,"3 months of real-world mobile behavior, 1.8 million clicks, 1.2 million users.
You can enter one or both tracks of the competition:
Cloud computing sized problem - You are here
PC sized problem (a sample of the cloud sized problem)
There will also be a Visualization Contest that can be entered from either track.
For more details on the event, go to:
http://www.sfbayacm.org/DM-Hackathon-2012-10  (links to presentations were added 8/19/2012)
Data Provided by:
  Cloud Compute Sponsors:    
                    ","The evaluation metric for this competition is MAP@5.
This metric is defined and explained here.","The main data for this competition is in the train.csv and test.csv files. These files contain information on what items users clicked on after making a search.
Each line of train.csv describes a user's click on a single item. It contains the following fields:
user: A user ID
sku: The stock-keeping unit (item) that the user clicked on
category: The category the sku belongs to
query: The search terms that the user entered
click_time: Time the sku was clicked on
query_time: Time the query was run
test.csv contains all of the same fields as train.csv except for sku. It is your job to estimate which sku's were clicked on in these test queries.
Due to the internal structure of BestBuy's databases, there is no guarantee that the user clicks resulted from a search with the given query. What we do know is that the user made a query at query_time, and then, at click_time, they clicked on the sku, but we don't know that the click came from the search results. The click_time is never more than five minutes after the query_time.",kaggle competitions download -c acm-sf-chapter-hackathon-big,['https://www.kaggle.com/code/mpwolke/best-buy-mobile-web-site-hackathon']
520,"Start here if...
You have some experience with R or Python and machine learning basics, but you’re new to computer vision. This competition is the perfect introduction to techniques like neural networks using a classic dataset including pre-extracted features.
Competition Description
MNIST (""Modified National Institute of Standards and Technology"") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.
In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.
Practice Skills
Computer vision fundamentals including simple neural networks
Classification methods such as SVM and K-nearest neighbors
Acknowledgements 
More details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.","Goal
The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.
For every in the test set, you should predict the correct label.
Metric
This competition is evaluated on the categorization accuracy of your predictions (the percentage of images you get correct).
Submission File Format
The file should contain a header and have the following format:
ImageId,Label
1,0
2,0
3,0
etc.","The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.
Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.
The training data set, (train.csv), has 785 columns. The first column, called ""label"", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.
Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).
For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.
Visually, if we omit the ""pixel"" prefix, the pixels make up the image like this:
000 001 002 003 ... 026 027
028 029 030 031 ... 054 055
056 057 058 059 ... 082 083
 |   |   |   |  ...  |   |
728 729 730 731 ... 754 755
756 757 758 759 ... 782 783 ",kaggle competitions download -c digit-recognizer,"['https://www.kaggle.com/code/yassineghouzam/introduction-to-cnn-keras-0-997-top-6', 'https://www.kaggle.com/code/kanncaa1/pytorch-tutorial-for-deep-learning-lovers', 'https://www.kaggle.com/code/poonaml/deep-neural-network-keras-way', 'https://www.kaggle.com/code/kanncaa1/convolutional-neural-network-cnn-tutorial', 'https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk']"
521,"This competition has completed. Congratulations to the winners along with all the other participants!
CareerBuilder.com is proud to sponsor the Job Recommendation Engine Challenge, which asks you to predict what jobs its users will apply to based on their previous applications, demographic information, and work history. The insights you discover from this data, and the algorithms the winners create, will allow CareerBuilder to improve its job recommendation algorithm, a core part of its website and a key element in improving user experience.   

There will also be a data visualization prospect towards the end of this contest.","The evaluation metric for this competition is MAP@150.
The details of this evaluation metric can be found here.
The public leaderboard is calculated on only a portion of the Test users, and the private leaderboard is calculated with the rest of the Test users.","Outline
In order to understand the content of the data files, you need to understand the structure of this contest.
In outline, we give you data on users, job postings, and job applications that users have made to job postings. In total, the applications span 13 weeks. We have split the applications into 7 groups, each group representing a 13-day window. Each 13-day window is split into two parts: The first 9 days are the training period, and the last 4 days are the test period.  These splits are illustrated below.
Each user and each job posting is randomly assigned to exactly one window.
Each job is assigned to a window with probability proportional to the time it was live on the site in that window.  Each user is assigned to a window with probabilty proportional to the number of applications they made to jobs in that window, during that window.  In the above image, User1 only made submissions to jobs in Window 1, and so was assigned to Window 1 with probability 100%.  User2, however, made submissions to jobs in both Window 1 and Window 2, and so may have been assigned to either Window1 or Window2.
In each window, we give you all the job applications that users in that window made to jobs in that window during the 9-day training period. This data can be found in apps.tsv.",kaggle competitions download -c job-recommendation,"['https://www.kaggle.com/code/augustodenevreze/users-jobs-exploration', 'https://www.kaggle.com/code/swofde/keras-starter-code', 'https://www.kaggle.com/code/sehandev/keras-starter-code']"
522,"Help enable the development of safe, effective medicines.
When developing new medicines it is important to identify molecules that are highly active toward their intended targets but not toward other targets that might cause side effects. The objective of this competition is to identify the best statistical techniques for predicting biological activities of different molecules, both on- and off-target, given numerical descriptors generated from their chemical structures.
The challenge is based on 15 molecular activity data sets, each for a biologically relevant target. Each row corresponds to a molecule and contains descriptors derived from that molecule's chemical structure.
In addition to the prediction competition, Merck is also hosting a visualization challenge with a $2,000 prize for the most insightful and elegant graphical representations of the data.
Prizes total $40,000.","        Predictions for activity will be evaluated using the correlation coefficient \\(R^2\\), averaged over the 15 data sets.
$$ R^2 = \frac{1}{15}\sum_{s=1}^{15} r^2_s $$
where
$$ r^2_s =\frac{ [\sum_{i=1}^{N_s} (x_i-\bar x)(y_i-\bar y) ]^2}{ \sum_{i=1}^{N_s} (x_i-\bar x)^2 \sum_{i=1}^{N_s}(y_i-\bar y)^2  }$$
where \\(x\\) is the known activity, \\(\bar x\\) is the mean of the known activity, \\(y\\) is the predicted activity, \\(\bar y\\) is the mean of the predicted activity, and \\(N_s\\) is the number of molecules in data set \\(s\\).
Sample code has been provided to calculate r-squared.","The Training and Test Sets each consist of 15 biological activity data sets in comma separated value (CSV) format. Each row of data corresponds to a chemical structure represented by molecular descriptors.
Training and Test Files
The training files are of the form
Column 1: Molecule ID
Column 2: Activity. Note that these are raw activity values and different data sets can have activity measured in different units.
Column 3-end: Molecular descriptors/features
The test files are in the same format with Column 2 removed.
Molecule IDs and descriptor names are global to all data sets. Thus some molecules will appear in multiple data sets, as will some descriptors.
The challenge is to predict the activity value for each molecule/data set combination in the test set. To keep predictions for molecules unique to each data set, a data set identifier has been prepended to each molecule ID (e.g., ""ACT1_"" or ""ACT8_"").
Data Set Creation",kaggle competitions download -c MerckActivity,[]
523,"In the first phase of this prediction challenge Practice Fusion invited anyone with an interest in using electronic medical record data to improve public health to submit and vote on ideas for prediction problems based on a new dataset of 10,000 de-identified medical records. The votes are in and Shea Parkes' top voted submission has won.
Practice Fusion is now sponsoring the second and final phase of the challenge inspired by the winning problem: Identify patients diagnosed with Type 2 Diabetes Mellitus.
Over 25 million people, or nearly 8.3% of the entire United States population, have diabetes. Diabetes is also associated with a wide range of complications from heart disease and stroke to blindness and kidney disease. Predicting who has diabetes will lead to a better understanding of these complications and the common comorbidities that diabetics suffer.
The Challenge: Given a de-identified data set of patient electronic health records, build a model to determine who has a diabetes diagnosis, as defined by ICD9 codes 250, 250.0, 250.*0 or 250.*2 (e.g., 250, 250.0, 250.00, 250.10, 250.52, etc).","Predict the probability that each person has a diagnosis of Type 2 Diabetes Mellitus. Predictions are evaluated using the log loss metric.
Log loss is defined as:
$$\text{log loss}=-\frac{1}{N}\sum_{i=1}^Ny_i\log\left(\hat{y_i}\right)+\left(1-y_i\right)\log\left(1-\hat{y_i}\right),$$
where \\(N\\) is the number of patients, \\(\text{log}\\) is the natural logarithm, \\(\hat{y_i}\\) is the posterior probability that the \\(i^{th}\\) patient has diabetes, and \\(y_i\\) is the ground truth (\\(y_i=1\\) means the patient has diabetes, \\(y_i=0\\) means that he does not).",,,
524,"Competition closes at 1pm London ( UTC+1) July 22nd
“Soulful” ... “Catchy” ... “Cool"" ... ""Cheesy"" ... ""Edgy”
How do people connect to and describe the music they have just heard?
EMI Insight performs extensive market research about their artists by interviewing thousands of people around the world. This research has produced EMI One Million Interview Dataset; one of the largest music preference datasets in the world today, that connects data about people--who they are, where they live, how they engage with music in their daily lives-- with their opinions about EMI’s artists.

This Data Science London hackathon will focus on one key subset of this data: understanding what it is about people and artists that predicts how much people are going to like a particular track. We have taken a sample of the data from the United Kingdom that provides a granular mixture of profile, word-association, and rating data.
The goal of this weekend hackathon is to design an algorithm that combines users’ (a) demographics, (b) artist and track ratings, (c) answers to questions about their preferences for music, and (d) words that they use to describe EMI artists in order to predict how much they like tracks they have just heard.
There is also a Visualization thread where you can submit your most amazing Music-Data Viz and view and vote on other contestants' entries.  Go to 'Prospect' at the top of this page.  Submissision will open at the same time as the competition.
(Data will be made available 24 hours prior to the start of the contest)
For more info http://musicdatascience.com/
hashtag #musicdata #ds_ldn #DSGhack
  Proudly brought to you by
 ",Root Mean Square Error ( RMSE ),"You are provided with five files:
Train/test. This csv file contains data that relate to how people rate EMI artists, during the market research interviews, right after hearing a sample of an artist’s song. The 6 columns are:
Artist. An anonymised identifier for the EMI artist.
Track. An anonymised identifier for the artist’s track.
User. An anonymised identifier for the market research respondent, who will have just heard a sample from the track.
Rating. A number between X-100 which answers the question: How much do you like or dislike the music?  (Train only, you're predicting this for the test set)
Time. The time the market research was completed: It is the anonymised research date indicating which month the research was conducted in. It can help you understand which other artists/tracks were researched in the same wave. Note it is not in chronological order

Words. This csv file contains data that shows how people describe the EMI artists whose music they have just heard.
Artist. An anonymised identifier for the EMI artist.
User. An anonymised identifier for the market research respondent, who will have just heard one or more samples from the artist.",kaggle competitions download -c MusicHackathon,['https://www.kaggle.com/code/mpwolke/emi-music-ds-hackathon']
525,"Many organizations prospect for loyal supporters and donors by sending direct mail appeals. This is an effective way to build a large base, but can be very expensive and have a low efficiency. Eliminating likely non-donors is the key to running an efficient Prospecting program and ultimately to pursuing the mission of the organization. Help us help these organizations to target the best prospective donors and fund organizational goals! 
Please note: This competition has rules that we have not used previously restricting what kinds of models are acceptable. Please see the rules page for more information.","Note: The maximum possible public leaderboard score is: 0.87190
The response variable is ""Amount2"" (donation amount raised to the 1.15 power), and the metric used is ""AverageAmongTopP"" (with P=75%). This response variable is not included in the training dataset, but you can construct it (as in the sample code).
The Solution Data is composed of many individual mailings. We are not trying to achieve a result that recommends ""Mail all of package 1 and none of package 2."" Each mailing will be mailed - the goal of this contest is to mail the best prospects for EACH mailing. Therefore, prospects in each mailing will be rank-ordered based on the model output. The top 75% of each mailing, according to the model results, will be categorized as ""good"" prospects. The bottom 25% will be categorized as ""bad"" prospects. Category variables: ""ListID,"" ""Package,"" and ""Agency"" may be used as variables in the scoring algorithm, but using them to identify superior overall mailings (should be mailed 100%) and inferior mailings (should not be mailed) will not achieve the goal of maximizing model performance for each mailing. 
The Training Data will be from Nov 2011 and earlier. The Solution Data will be from Dec 2011 and into 2012.","You will predict ""Amount2"", which is a transformation of the donation amount (donation amount raised to the 1.15 power).
Training data: kaggle_training_dataset_formatted2
Testing data: test.
Please see ""Kaggle FAQ"" (downloadable with the data files) for other questions.
Note: Category variables (""ListID,"" ""Package,"" and ""Agency"") may be used as variables in the scoring algorithm, but using them to identify superior overall mailings (should be mailed 100%) and inferior mailings (should not be mailed) will not achieve the goal of maximizing model performance for each mailing. This is because we will take the top 75% of prospects in *each mailing* when evaluating performance.
  TABLE OVERVIEW
Kaggle_training_dataset_formatted2: Full mail history for the 11 months leading up to the Solution data. 
Kaggle_donation_dataset_formatted2: Entire donation history pre-Training dataset for all organizations in Agency 1, 2 and 3
Kaggle_mail_dataset_formatted1a: Part of mail history before Training_dataset for Agency 1",kaggle competitions download -c Raising-Money-to-Fund-an-Organizational-Mission,['https://www.kaggle.com/code/mpwolke/donate-to-organizational-mission']
526,"This competition has completed, congratulations to the preliminary winners and the other participants!
The William and Flora Hewlett Foundation (Hewlett Foundation) is sponsoring the Automated Student Assessment Prize (ASAP) in hopes of discovering new tools to support schools and teachers. The competition aspires to solve the problem of the high cost and the slow turnaround of hand scoring thousands of written responses in standardized tests.  As a result many schools exclude written responses in favor of multiple-choice questions, which are less able to assess students’ critical reasoning and writing skills.  ASAP has been designed to help determine whether computerized systems are capable of grading written content accurately for schools and teachers to adopt those solutions.  ASAP aspires to inform key decision makers, who are already considering adopting these systems, by delivering a fair, impartial and open series of trials to test current capabilities and to drive greater awareness when outcomes warrant further consideration.
Critical reasoning is one of a suite of skills that experts believe students must be taught to succeed in the new century. The Hewlett Foundation makes grants to educators and nonprofit organizations in support of these skills, which it calls “deeper learning.” They include the mastery of core academic content, critical reasoning and problem solving, working collaboratively, communicating effectively, and learning how to learn independently. With ASAP, Hewlett is appealing to data scientists to help solve an important problem in the field of educational testing. 
Hewlett is sponsoring the following prizes as part of Phase Two:
$50,000:  1st place
$25,000:  2nd place
$15,000:  3rd place
$  7,500:  4th place
$  2,500:  5th place
In May of this year, $100,000 in prizes was rewarded for ASAP, Phase One, and we have launched ASAP, Phase Two, with the same intentions.  During Phase One, we focused on systems to support the grading of student written essays. This time, we’re offering a similar competition, only focused on short answer responses.  We welcome you to learn more about our previous phase at www.kaggle.com/c/asap-aes
During Phase Two, you are provided access to graded short answer responses and their corresponding prompts, so that you can build, train and test your scoring engines against a wide field of competitors. Your success depends upon how closely you can align your scores to those of human expert graders.  While we believe that a pool of $100,000 in potential financial incentives are important, we also intend to secure and distribute your solutions to the public, in hopes of elevating the field of automated assessment through your contributions.  We want you to induce a breakthrough that is both personally satisfying and game-changing for improving public education.
We have already learned that automated assessment systems can yield fast, effective and affordable solutions that would allow states to introduce new testing tools capable of assessing deeper measures of learning.  We believe that you can help us pave the way towards better student assessment. 
ASAP is designed to achieve the following goals:
Challenge developers of student assessment systems to demonstrate their current capabilities.
Reveal the efficacy and cost of alternative scoring systems to support teachers.
Promote the capabilities of effective scoring systems to state departments of education and other key decision makers, when those advantages have been proven to support student and teacher interests.
The Phase Two graded content is selected according to specific characteristics.  On average, each answer is approximately 50 words in length.  Some are more dependent upon source materials than others, and the answers cover a broad range of disciplines (from English Language Arts to Science).  The range of answer types is provided so that we can better understand the strengths of your solution.  It is our intent to showcase quality and reliability, based on how well you can align with expert human graders for each response.
You will be provided with training data for each prompt.  Most training sets will consist of about 1,800 responses that have been randomly selected from a sample of approximately 3,000.  The number of training data may vary.  The data will contain ASCII formatted text for each response followed by two hand scores.  The first score is the final score and the one that you are trying to predict. The second score was used to determine reliability of the first score. The second score did not in any way influence the first (final) score. You are provided with both scores, so that you may evaluate the reliability of the hand scoring.  Further instruction and clarification regarding the data is available on the DATA tab.
Following a period of 2.5 months to train your scoring engine, you will be provided with test data that will contain approximately 6,000 new responses (600 per data set), randomly selected for blind evaluation.  However, you will notice that the score columns will be blank.  You will be asked to supply, based on your engine's predictions for each response, your score for each response and to submit your new scored data set on this site.
As part of the ZIP file that you will submit with your predictive scores, you will be asked to submit a technical METHODS PAPER. We would like to understand your specific approach to developing your scorig engine, along with any known limitations. Basically, you will have the opportunity to present your scoring engine to the world, so that others may build upon it.  Your technical METHODS PAPER will not be used to determine any prize rewards, but it is a required component of your final submission.
Also, please note that it is our intention to continue staging other follow-on ASAP phases in the months ahead.  We have started with graded essays (Phase 1), and we are now focusing on short answers (Phase 2); we are developing plans for a third phase, and we’re planning to launch a phase to demonstrate efficacy of systems capable of offering formative feedback as part of classroom applications:
Phase 1:  Demonstration for long-form constructed response (essays); 
Phase 2:  Demonstration for short-form constructed response (short answers);
Phase 3:  Demonstration for symbolic mathematical/logic reasoning (charts/graphs).
In every instance, we seek to drive innovation for new solutions to student assessment, to support teachers in evaluating critical reasoning skills.  We hope that you will enjoy this process.  May the best model win!","The code to calculate Quadratic Weighted Kappa, along with test cases, can be found at this github repo.
Score predictions are evaluated based on objective criteria, and specifically using the quadratic weighted kappa error metric, which measures the agreement between two raters.  This metric typically varies from 0 (only random agreement between raters) to 1 (complete agreement between raters).  In the event that there is less agreement between the raters than expected by chance, this metric may go below 0.  The quadratic weighted kappa is calculated between the automated scores for the responses and the resolved score for human raters on each set of responses.  The mean of the quadratic weighted kappa is then taken across all sets of responses.
A set of essay responses E has N possible ratings, 1,2,…,N, and two raters, Rater A and Rater B.  Each essay response e is characterized by a tuple (ea,eb), which corresponds to its scores by Rater A (resolved human score) and Rater B (automated score).  The quadratic weighted kappa is calculated as follows.  First, and N-by-N histogram matrix O is constructed over the essay ratings, such that Oi,j corresponds to the number of essays that received a rating i by Rater A and a rating j by Rater B.
An N-by-N matrix of weights, w, is calculated based on the difference between raters’ scores:
$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$
An N-by-N histogram matrix of expected ratings, E, is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between each rater’s histogram vector of ratings, normalized such that E and O have the same sum.
From these three matrices, the quadratic weighted kappa is calculated:
$$\kappa=1-\frac{\sum_{i,j}w_{i,j}O_{i,j}}{\sum_{i,j}w_{i,j}E_{i,j}}$$
The Fisher Transformation is approximately a variance-stabilizing transformation and is defined:
$$z = \frac{1}{2} \ln \frac{1+\kappa}{1-\kappa}$$
Since this transformation approaches infinity as kappa approaches 1, the maximum kappa value is capped at 0.999.  Next the mean of the transformed kappa values is calculated in the z-space.  Finally, the reverse transformation is applied to get the average kappa value:
$$\kappa = \frac{e^{2z}-1}{e^{2z}+1}$$
If you have questions regarding the evaluation criteria, please refer to the help page.","Code for benchmarks
For this competition, there are ten data sets. Each of the data sets was generated from a single prompt. Selected respones have an average length of 50 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students primarily in Grade 10. All responses were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.
The training data is provided in a tab-separated value (TSV) file containing the following columns:
Id: A unique identifier for each individual student essay.
EssaySet: 1-10, an id for each set of essays.
Score1: The human rater's score for the answer. This is the final score for the answer and the score that you are trying to predict.
Score2: A second human rater's score for the answer. This is provided as a measure of reliability, but had no bearing on the score the essay received.
EssayText: The ascii text of a student's response.
The private leaderboard set will not be released until August 30, 2012. The public leaderboard and private leaderboard files each have the following columns:",kaggle competitions download -c asap-sas,[]
527,"Get the data »
Practice Fusion is America's fastest growing Electronic Health Record (EHR) community, with more than 170,000 medical professional users treating 34 million patients in all 50 states. Practice Fusion’s EHR-driven research dataset is used to detect disease outbreaks, identify dangerous drug interactions and compare the effectiveness of competing treatments.
In partnership with Kaggle, Practice Fusion is releasing 10,000 de-identified, HIPAA-compliant medical records to spur innovation into new uses of clinical data to improve public health and patient care. This dataset is one of the largest and richest sources of medical record data ever released and includes information on diagnoses, lab results, medications, allergies, immunizations, vital signs, and health behavior.
Practice Fusion’s past data challenges have spawned a range of creative visualizations, applications, analyses, and even a few start-ups. In this year’s Analyze This!, Practice Fusion and Kaggle call on an ever-growing community of developers, designers, and data scientists interested in solving our nation’s most stubborn healthcare problems by tackling our two data challenges: the Prediction Challenge and the Open Challenge.
Open Challenge
Get the dataset and show us what you can find!
Combine the Analyze This! dataset with one or more datasets from www.data.gov. Use the mash-up however you like: map chronic disease across the country, create a personal health app, or a tool for running clinical trials. Share your results and analyses online!
Submissions for the Open Challenge will be accepted from the launch of Analyze This! to Monday, September 10.",,,,
528,"The EMC source code classification challenge requires you to classify source code files according to the projects they belong to.
Given a set of source code files collected from various open source projects, how well can unseen source code files from the same set of open source projects can be classified?
Possible real-world applications:
Protecting intellectual property
Data Loss Protection (DLP)
Automatic categorization of source code repositories",The evaluation metric is Multi-Class Log Loss.,"1st:  Israeli Shekel equivalent of $6,000 (see rules)
2st:  Israeli Shekel equivalent of $3,000 (see rules)
3st:  Israeli Shekel equivalent of $1,000 (see rules)",kaggle competitions download -c emc-data-science,['https://www.kaggle.com/code/mpwolke/emc-israel-data-science']
529,"Splunk Innovation Prize now OPEN »
  Announced at the GigaOM Structure Conference, powered by Splunk, and using data from WordPress.com, this competition is about predicting which people will ""like"" which blog posts from across 90k active blogs on WordPress.com.  WordPress.com hosts about half of the 74 million WordPress sites in the world (over 16% of all domains on the web). The winning solutions may be used by WordPress.com in a recommendation engine, but winning solutions must be open-sourced, so they could be used by anyone to solve a similar problem using similar data in a similar domain.
Competition winners will be announced in September at GigaOM Mobilize.
About Splunk: Splunk® Inc. provides the engine for machine data™. Splunk software enables organizations to monitor, search, analyze, visualize and act on massive streams of real-time and historical machine data. 
Splunk has donated access to a Splunk server containing the entire WordPress dataset for you to explore, visualize and experiment. When you accept the rules for the competition, you will automatically be sent a personal login to the Splunk server.
There is also a 5K companion competition to the predictive modeling challenge. The Splunk Innovation Prize will be awarded for the most innovative use of Splunk for data science (using the competition dataset).
Never used Splunk before?  Here are some tutorials to get you started.
Getting started videos
Splunk tutorial
Search Reference guide (explains the Splunk search language)
About Gigaom: GigaOM is one of the most credible and insightful voices at the intersection of business and technology, with an online audience of more than 5.5 million monthly unique visitors, industry-leading events, and a pioneering research service and digital community, GigaOM Pro, which provides expert analysis on emerging technology markets.
   ","The evaluation metric for this competition is Mean Average Precision at 100 (MAP@100). For more information, please refer to the wiki page.","Data for the competition is also available for immediate exploration through SocialSplunk.  A login code will be sent to you when you register for the competition.  Haven't received your login code yet?  Email support@kaggle.com and we'll get you sorted out.
The evaluation data currently only drive the public leaderboard. The private leaderboard is not yet active. To avoid a situation where people could look up the answers on the web, we have a two-phase data release.
First Data Release
The data for the first release are drawn from a 6-week period of blog posts and ""likes"" of those blog posts. The training data consist of the first 5 weeks of posts and ""likes"" that occurred during those 5 weeks. This data files provided (some of which contain redundant information in different forms):
trainUsers.json: One JSON dictionary per line, where each line corresponds to one WordPress.com user, and the fields are:
""uid"": ID for user
""inTestSet"" : is this user in the test set (one of the users you're required to make predictions about)
""likes"" : a list of dictionaries, one for each training like by this user, only containing like by this user during the training period:
""blog"": blog liked",kaggle competitions download -c predict-wordpress-likes,[]
530,"Get the data »
Practice Fusion is America's fastest growing Electronic Health Record (EHR) community, with more than 170,000 medical professional users treating 34 million patients in all 50 states. Practice Fusion’s EHR-driven research dataset is used to detect disease outbreaks, identify dangerous drug interactions and compare the effectiveness of competing treatments.
In partnership with Kaggle, Practice Fusion is releasing 10,000 de-identified, HIPAA-compliant medical records to spur innovation into new uses of clinical data to improve public health and patient care. This dataset is one of the largest and richest sources of medical record data ever released and includes information on diagnoses, lab results, medications, allergies, immunizations, vital signs, and health behavior.
Practice Fusion’s past data challenges have spawned a range of creative visualizations, applications, analyses, and even a few start-ups. In this year’s Analyze This!, Practice Fusion and Kaggle call on an ever-growing community of developers, designers, and data scientists interested in solving our nation’s most stubborn healthcare problems by tackling our two data challenges: the Prediction Challenge and the Open Challenge.
Prediction Challenge
The first phase of the Prediction Challenge is Kaggle Prospect in which you submit ideas for the best prediction problem! The Kaggle community is invited to vote on the most interesting and promising ideas. A panel of judges will select a winner from the top-voted 10 ideas for the second phase, a predictive modeling competition based on the winning idea.
Kaggle Prospect submissions and voting will be available from the launch of Analyze This! to June 30. The predictive modeling phase of the Prediction Challenge will start no later than July 9, and end on Monday, September 10.
The predictive modeling competition is now underway and can be found here.
Open Challenge
Get the dataset and show us what you can find!
Combine the Analyze This! dataset with one or more datasets from www.data.gov. Use the mash-up however you like: map chronic disease across the country, create a personal health app, or a tool for running clinical trials. Share your results and analyses online!
Submissions for the Open Challenge will be accepted from the launch of Analyze This! to Monday, September 10.",Insert your evaluation here,,,
531,"The  aim of the competition is to determine to what degree it's possible to predict people with a sufficiently high degree of Psychopathy based on Twitter usage and Linguistic Inquiry.
The organizers provide all interested participants an anonymised dataset of users self assessed psychopathy scores together with 337 variables derived from functions of Twitter information, useage and lingusitc analysis. Psychopathy scores are based on a checklist developed by Professor Del Paulhus at the University of British Columbia.
The model should aim to identify people scoring high in Psychopathy, for the purpose of this competition, defined as 2 SD's above a mean of 1.98. This accounts for roughly 3% of the entire sample and therefore the challenge with this dataset is developing a model to work with a highly imbalanced dataset.

The best performing model(s) will be formally cited in a future paper/papers. The authors of the winning model may also be invited to attend future conferences to discuss their model.
  The intention of this research is to seperate fact from fiction and examine just what can be predicted by social media use and how this information might be used, both for good and bad. As an organization, the Online Privacy Foundation works to raise awareness of online privacy issues and empower people to make informed choices about what they do online. We hope you'll support our mission and take part in this competition. 
 ","This competition uses ""average precision"", a metric which is more common for binary prediction but makes sense in this context as well. The true scores are sorted (descending) according to the order of the submission (only the order of the submission matters). In each row, we then compute the cumulative (from the top up to that row) ""True Scores Ordered by Submission"" divided by the cumulative ""True Scores Ordered By True Scores"", where that quotient is called the precision at row n. The final score is the average of the precision at row n (over all n).
True Scores Submission True Scores Ordered by Submission True Scores Ordered by True Scores Precision at n
3 8 3 3 1
2.3 1 1.6 2.3 0.868
1.6 4 2.3 1.6 1
Averaged precision across all rows: 0.956
  Note that the train/test and public/private leaderboard splits for this competition are the same as for ""Personality Prediction Based on Twitter Usage"".","Three files are provided
1. DescriptiveStats.pdf - Descriptive Statistics of each personality dimension
2. Pyschopath_Trainingset_v1.csv - Training set providing personality self assessment results(cols 2 - 8)  together with 337 variables derived from a users Twitter activity (as follows)
3. Pyschopath_Testset_v1.csv - Accompanying test set
Label Description
MyID - User Identifier
    psychopathy  - Self report psychopathy score based on http://www2.psych.ubc.ca/~dpaulhus/research/DARK_TRIAD/MEASURES/SD3.FINAL28.doc 
   ",kaggle competitions download -c twitter-psychopathy-prediction,"['https://www.kaggle.com/code/annamalkova88/psychopathy-prediction-my-vers', 'https://www.kaggle.com/code/abhigyaverma/psychopathy-prediction-using-regression-techniques']"
532,"The aim of this competition is to determine the best models to predict the personality traits of Machiavellianism, Narcissism, Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism based on Twitter usage and linguistic inquiry.
The organizers provide all interested participants an anonymised dataset of users self assessed personality scores (based checklists developed by Prof Del Paulhus at the University of British Columbia and Prof Sam Gosling and the University of Texas) together with 337 variables derived from functions of Twitter information and lingusitc analysis.
The best performing model(s) will be formally cited in a future paper and any presentations.
  The intention of this research is to seperate fact from fiction and examine just what can be predicted by social media use and how this information might be used, both for good and bad. As an organization, the Online Privacy Foundation works to raise awareness of online privacy issues and empower people to make informed choices about what they do online. We hope you'll support our mission and take part in this competition. 
  (Twitter is a trademark of Twitter, Inc.)","This competition uses ""average precision"", a metric which is more common for binary prediction but makes sense in this context as well. The true scores are sorted (descending) according to the order of the submission (only the order of the submission matters). In each row, we then compute the cumulative (from the top up to that row) ""True Scores Ordered by Submission"" divided by the cumulative ""True Scores Ordered By True Scores"", where that quotient is called the precision at row n. The final score for one column is the average of the precision at row n (over all n).
True Scores Submission True Scores Ordered by Submission True Scores Ordered by True Scores Precision at n
3 8 3 3 1
2.3 1 1.6 2.3 0.868
1.6 4 2.3 1.6 1
Averaged precision across all rows: 0.956.
The score of your entire submission is the average (across columns) of the average precision (across rows).","Three files are provided
1. DescriptiveStats.pdf - Descriptive Statistics of each personality dimension
2. Personality_Traits_Trainingset_v1.csv - Training set providing personality self assessment results(cols 2 - 8)  together with 337 variables derived from a users Twitter activity (as follows)
3. Personality_Traits_Testset_v1.csv - Accompanying test set
  Label Description
myID - User Identifier
        machiavellianism  - Self report machiavellianism score based on  ",kaggle competitions download -c twitter-personality-prediction,[]
533,"This is Round 2 - Check out the results of the first round!
NEW: data annotations and examples of algorithms (in Matlab).
This competition is identitical to the first round of the CHALEARN gesture challenge, the only difference is that is will be judged on new fresh final evaluation data. Keep informed of new data releases and new events, sign up to the gesturechallenge group.
Similarly, there will be a development phase (May 7, 2012 to September 6, 2012) and a final evaluation phase (September 7, 2012 to September 10. 2012): 
Development phase: Create a learning system capable of learning from a single training example a gesture classification problem. Practice with development data (a large database of 50,000 labeled gestures is available) and submit predictions on-line on validation data to get immediate feed-back on the leaderboard. Recommended: towards the end of the development phase, submit your code for verification purpose. See the evaluation section.
Final evaluation phase: Make predictions on the new final evaluation data revealed at the end of the development phase. The participants will have 4 days to train their systems and upload their predictions. 
Sponsors
This challenge is organized by CHALEARN and is sponsored in part by Microsoft (Kinect for Xbox 360). Other sponsors include Texas Instrument. This effort was initiated by the DARPA Deep Learning program and is supported by the US National Science Foundation (NSF) under grants ECCS 1128436 and ECCS 1128296 , the EU Pascal2 network of excellence. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.","The evaluation procedure is the same as for round 1.
Because of the success of the verification procedure using code uploaded by the participants, we hightly recommend that the participants take advantage of this opportunity and upload regularly updated versions of their code during the development period. Their last code submission before September 7 12:59 UTC deadline will be used for the verification.
What you need to predict
Each video contains the recording of 1 to 5 gestures from a vocabulary of 8 to 15 gesture tokens. For instance a gesture vocabulary may consist of the signs to referee volleyball games or the signs to represent small animals in the sign language for the deaf.
You need to predict the identity of those gestures, represented by a numeric label (from 1 to 15). The data are divided into data batches, each having a different vocabulary of gestures. So the numeric labels represent different gestures in every batch.
In the data used for evaluation (called validation data and final evaluation data), you get one video clip for each gesture token as a training example in every batch. You must predict the labels of the gestures played in the other unlabeled videos.
  Levenshtein distance
For each video, you provide an ordered list of labels R corresponding to the recognized gestures. We compare this list to the corresponding list of labels T in the prescribed list of gestures that the user had to play. These are the ""true"" gesture labels (provided that the users did not make mistakes). We compute the so-called Levenshtein distance L(R, T), that is the minimum number of edit operations (substitution, insertion, or deletion) that one has to perform to go from R to T (or vice versa). The Levenhstein distance is also known as ""edit distance"".
For example:
L([1 2 4], [3 2]) = 2
L([1], [2]) = 1
L([2 2 2], [2]) = 2
We provide the Matab(R) code for the Levenshtein distance in our sample code.
  Score
The overall score we compute is the sum of the Levenshtein distances for all the lines of the result file compared to the corresponding lines in the truth value file, divided by the total number of gestures in the truth value file. This score is analogous to an error rate. However, it can exceed one.
Public score means the score that appears on the leaderboard during the development period and is based on the validation data.
Final score means the score that will be computed on the final evaluation data released at the end of the development period, which will not be revealed until the challenge is over. The final score will be used to rank the participants and determine the prizes.
  Verification procedure
To verify that the participants complied with the rule that there should be no manual labeling of the test data, the top ranking participants eligible to win prizes will be asked to cooperate with the organizers to reproduce their results.
Preferred procedure
During the development period of round 2 (from May 7 until September 6, 2012) the participants can upload executable code reproducing their results together with their submissions. The organizers will evaluate requests to support particular platforms, but do not commit to support all platforms. The sooner a version of the code is uploaded, the highest the chances that the organizers will succeed in running it on their platform. The burden of proof will rest on the participants, see our backup procedure. The code will be kept in confidence and used only for verification purpose after the challenge is over. The code submitted will need to be standalone and in particular it will not be allowed to access the Internet. It will need to be capable of training models from the final evaluation data training examples, for each data batch, and making label predictions on the test examples of that batch. Detailed instructions are found with the submission instructions.
Backup procedure
If for some reason a participant elects not to submit executable code before the September 6, 2012 deadline, he/she will have the option of bringing a full system to the site of the workshop at ICPR 2012, or another location mutually agreed upon, to let the organizers perform a live test. The organizers may also decide to run this backup procedure if, for a technical reason, the executable code provided by the participants cannot be run on their computers. The verification will be carried out using verification data similar to the final evaluation data. Statistically significant discrepancies in performance between the final evaluation data and the verification data may be a cause of disqualification. The results of the verifications will be published by the organizers."," Same data as round 1
Download from Kaggle | Download from other mirrors 
  Setting
We are portraying a single user in front of a fixed Kinect (TM) camera, interacting with a computer by performing gestures:
- playing a game,
- remotely controlling an appliance or a robot, or
- learning to perform gestures from an educational software.
 We recorded both an RGB and Depth images (representing the distance of the objects to the camera).
RGB image Depth image
     View more examples...",kaggle competitions download -c GestureChallenge2,[]
534,"The objective of the competition is to help us build as good a model as possible to predict monthly online sales of a product. Imagine the products are online  self-help programs following an initial advertising campaign.
We have shared the data in the comma separated values (CSV) format.  Each row in this data set represents a different consumer product.
The first 12 columns (Outcome_M1 through Outcome_M12) contains the monthly online sales for the first 12 months after the product launches.  
Date_1 is the day number the major advertising campaign began and the product launched.  
Date_2 is the day number the product was announced and a pre-release advertising campaign began.
Other columns in the data set are features of the product and the advertising campaign.  Quan_x are quantitative variables and Cat_x are categorical variables. Binary categorical variables are measured as (1) if the product had the feature and (0) if it did not.","Predict the first 12 months of online sales for a product based on product features.
The evaluation metric is RMSLE (root mean square logarithmic error) across all 12 prediction columns.
The RMSLE is calculated as
\[ \epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }\]
Where:
\\(\epsilon\\) is the RMSLE value (score)
\\(n\\) is the total sales in the (public/private) data set
\\(p_i\\) is your prediction 
\\(a_i\\) is the actual sales for  \\(i\\) 
\\(\log(x)\\) is the natural logarithim of \\(x\\)","We have shared the data in the comma separated values (CSV) format.  Each row in this data set represents a different consumer product.
The first 12 columns (Outcome_M1 through Outcome_M12) contains the monthly online sales for the first 12 months after the product launches.  
Date_1 is the day number the major advertising campaign began and the product launched.  
Date_2 is the day number the product was announced and a pre-release advertising campaign began.
Other columns in the data set are features of the product and the advertising campaign.  Quan_x are quantitative variables and Cat_x are categorical variables. Binary categorical variables are measured as (1) if the product had the feature and (0) if it did not.",kaggle competitions download -c online-sales,[]
535,"Hosted by Data Science London and Data Science Global as part of a Big Data Week event, and organised by Kaggle, the first ever global data science hackathon will take place at the same time in several cities around the world spanning a 24-hour period. During this time the data scientists will compete with each other for cash prizes using a large dataset provided by the Cook County, Illinois, local government.
The challenge for the hackathon is to build with better more accurate predictive models of metropolitan air pollution. The EPA’s Air Quality Index is used daily by people suffering from asthma and other respiratory diseases to avoid dangerous levels of outdoor air pollutants, which can trigger attacks. According to the World Health Organisation there are now estimated to be 235 million people suffering from asthma. Globally, it is now the most common chronic disease among children, with incidence in the US doubling since 1980.  The model we build could be used as the basis for an early warning system that is capable of accurately predicting dangerous levels of air pollutants on an hourly basis.
Data Science Global is a non-profit organization dedicated to bringing together the world’s communities of data scientists, artists, technologists and visionaries.  For our inaugural event, we are hosting a global data science hackathon.   It will be taking place simultaneously in cities around the world: London, New York, Boston, Chicago, San Francisco, Melbourne, Canberra, Sydney and Turku, Finland, as well as remote participants competing directly through Kaggle.  You can join in the live webcast from the participating venues at datascienceglobal.org","Submissions will be scored by mean absolute error (MAE) across all values in the evaluation data.
Both the training and evaluation data sets have missing values, and our intention is to ignore these in calculating MAE. To do this, we have (in the solution file) transformed all NA's to ""-1,000,000"" and shown you where these occur by providing you with a sample submission that has these values in all of the correct places, and 0's everywhere else. ","The data consist of hourly measurements of various quantities (mostly are pollutants), where each row contains the measurements for one hour. Time slices (""chunks"") of 11 days have been created, with the first 8 days of each chunk available in the training data. You are asked to make pdictions about various points within the following 3 days (1, 2 ,3, 4, 5, 10, 17, 24, 48, and 72 hours after the end of the 8-day training data).

Within the training data, you are provided the following:
rowID
chunkID
position_within_chunk (starts at 1 for each chunk of data, increments by hour)
month_most_common (most common month within chunk of data--a number from 1 to 12) weekday (day of the week, as a string)
hour (a number from 0 to 23, local time)
Solar.radiation_64 
WindDirection..Resultant_1 (direction the wind is blowing from given as an angle, e.g. a wind from the east is ""90"")
WindDirection..Resultant_1018 (direction the wind is blowing from given as an angle, e.g. a wind from the east is ""90"")
WindSpeed..Resultant_1 (""1"" is site number)
WindSpeed..Resultant_1018 (""1018"" is site number)",kaggle competitions download -c dsg-hackathon,"['https://www.kaggle.com/code/bennheinz/air-quality-forecasting', 'https://www.kaggle.com/code/sanikamal/air-quality-prediction-eda']"
536,"The objective of the competition is to help us build as good a model as possible so that we can, as optimally as this data allows, relate molecular information, to an actual biological response.
We have shared the data in the comma separated values (CSV) format. Each row in this data set represents a molecule. The first column contains experimental data describing an actual biological response; the molecule was seen to elicit this response (1), or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are calculated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The descriptor matrix has been normalized.","Predicted probabilities that a molecule elicits a response are evaluated using the log loss metric.
Log loss is defined as:
$$\text{log loss}=-\frac{1}{N}\sum_{i=1}^Ny_i\log\left(\hat{y_i}\right)+\left(1-y_i\right)\log\left(1-\hat{y_i}\right),$$
where \\(N\\) is the number of samples, \\(\text{log}\\) is the natural logarithm, \\(\hat{y_i}\\) is the posterior probability that the \\(i^{th}\\) sample elicited a response, and \\(y_i\\) is the ground truth (\\(y_i=1\\) means the molecule elicited a response, \\(y_i=0\\) means that it did not).","Code for benchmarks
The data is in the comma separated values (CSV) format. Each row in this data set represents a molecule. The first column contains experimental data describing a real biological response; the molecule was seen to elicit this response (1), or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are caclulated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The descriptor matrix has been normalized.",kaggle competitions download -c bioresponse,"['https://www.kaggle.com/code/alexvolodin/ml-7-ht', 'https://www.kaggle.com/code/zmey56/my-learning-kernel', 'https://www.kaggle.com/code/vernondsouza123/predict-biological-response-through-lightgbm', 'https://www.kaggle.com/code/sergeyexp/ml-7-in-kaggle', 'https://www.kaggle.com/code/ahmedashrafahmed/predicting-a-biologicalresponse-using-randomforest']"
537,"The Million Song Dataset Challenge aims at being the best possible offline evaluation of a music recommendation system.  Any type of algorithm can be used: collaborative filtering, content-based methods, web crawling, even human oracles!* By relying on the Million Song Dataset, the data for the competition is completely open: almost everything is known and possibly available.
What is the task in a few words? You have: 1) the full listening history for 1M users, 2) half of the listening history for 110K users (10K validation set, 100K test set), and you must predict the missing half. How much easier can it get?
The most straightforward approach to this task is pure collaborative filtering, but remember that there is a wealth of information available to you through the Million Song Dataset. Go ahead, explore!  If you have questions, we recommend that you consult the MSD Mailing List.
Ready to start recommending?  Read through our Getting Started tutorial. You can also look at this open-source solution offered by a contestant.
For a more technical introduction to the MSD Challenge, see our AdMIRe paper. (Please use this following citation when referring to the contest in an academic setting.)
* This contest is for computer models, but if you manage to get recommendations from humans for 110K listeners, we'd like to know how!
  The Million Song Dataset Challenge is a joint effort between the Computer Audition Lab at UC San Diego and LabROSA at Columbia University. The user data for the challenge, like much of the data in the Million Song Dataset, was generously donated by The Echo Nest, with additional data contributed by SecondHandSongs, musiXmatch, and Last.fm. Follow-up evaluations will be conducted by IMIRSEL at the Graduate School of Library Information Science at UIUC as part of the Music Information Retrieval Evaluation eXchange (MIREX).
       ","We use mean average precision (truncated at 500).
Other measures will be used after the contest for analysis purposes (e.g. average rank, precision at K, AUC, etc). They will be computed from the best submissions of the top teams. In particular, we hope to have a human evaluation looking at the top predictions for some users.
All post-analysis will be taken care of by the great MIREX team at IMIRSEL [1,2].
Downie, J. Stephen (2008).  The Music Information Retrieval Evaluation Exchange (2005-2007): A window into music information retrieval research. Acoustical Science and Technology 29 (4): 247-255. (pdf)
Downie, J. Stephen, Andreas F. Ehmann, Mert Bay and M. Cameron Jones (2010). The Music Information Retrieval Evaluation eXchange: Some Observations and Insights. Advances in Music Information Retrieval Vol. 274, pp. 93-115. (pdf)","The files above contain:
the official indexing of songs (note that indexing starts at 1);
the official ordering of user IDs for your Kaggle submission;
the visible half of the listening histories of the 110K evaluation users;
the mapping from songs to tracks, more details below.
The half listening histories provided here are enough to get you started, but to leverage all the data available (in particular full listening histories for 1M users), you need to visit the Million Song Dataset (MSD) website, details below.
The core data is the Taste Profile Subset released by The Echo Nest as part of the Million Song Dataset. It consists of triplets (user ID, song ID, play count). The data is split in two:
The train set contains a little over a million users, full history released (available on the MSD website).
The validation and test sets combined contain 110k users, half of their history released (available here on Kaggle).
Needless to say, the test set and the train set users are not overlapping.
The metadata and audio features (among other things) for all songs are available through the Million Song Dataset. It is difficult to summarize the amount of information accessible to you, but here are a few pointers:",kaggle competitions download -c msdchallenge,"['https://www.kaggle.com/code/mpwolke/million-songs-zip-txt', 'https://www.kaggle.com/code/mayankmehra25/music-recommeder-system']"
538,"The aim of the contest is to determine how people may be identified based on their eye movement characteristic. The organizers provide all interested participants dataset of eye movements' recordings in CSV format. After downloading the training dataset, participants may analyze it to prepare their own classification models and try to classify sapmles in test dataset.
This in an official competition for BTAS 2012 (The Fifth IEEE International Conference on Biometrics: Theory, Applications and Systems, September 23-27, Washington DC, USA) and all results will be published during that conference (and of course on this web page as well).
To become a participant you don't need to have any special eye tracking equipment. All data needed is ready to download! You only need to have some experience in data classification and... take your chance.","Predicted probabilities that a subject generated the eye movement sample are evaluated using the log loss metric.
Log loss is defined as:
$$\text{log loss}=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^My_{i,j}\log\left(\hat{y_{i,j}}\right),$$
where \\(N\\) is the number of samples, \\(M\\) is the number of subjects, \\(\text{log}\\) is the natural logarithm, \\(\hat{y_{i,j}}\\) is the posterior probability that the \\(j^{th}\\) subject generated the \\(i^{th}\\) sample, and \\(y_{i,j}\\) is the ground truth (\\(y_{i,j}=1\\) means that the \\(j^{th}\\) subject generated the \\(i^{th}\\) sample, \\(y_{i,j}=0\\) means that they did not).","Code for benchmarks
Dataset is stored in simple CSV format where first column is classification (0 or 1) and all other columns are values obtained from eye tracker. 
The dataset consists of 978 samples from 37 subjects. Every sample is labeled with 1 (it belongs to one chosen specific person) or 0 (it belongs to someone else). Samples were taken with 250Hz frequency using Ober2 eye tracker. As it was 2048 measures taken, the whole measurements lasted 8192 ms. There was a jumping point on 3x3 matrix used as stimulus. The stimulus consists of eleven point position changes giving twelve consecutive point positions. First point appears in the middle of the screen and the person should look at it with eyes positioned directly ahead. After 1600 ms the point in the middle disappears and for 20 ms a screen is blank. In that time eyes are in instable state waiting for another point of interest. Then the point appears in the upper right corner. The flashing point on the blank screen attracts eyes attention even without the person's will. The 'jumps' of the point continue until the last point position in the middle of the screen is reached. 
Datasets downloadable for competition are available in CSV format. It is a text file with one line for every sample. Every line is a list of comma separated elements as follows:
class lx rx ly ry",kaggle competitions download -c emvic,['https://www.kaggle.com/code/mpwolke/eye-movements-verification-emvic']
539,"TASK 2 DESCRIPTION
Search advertising has been one of the major revenue sources of the Internet industry for years. A key technology behind search advertising is to predict the click-through rate (pCTR) of ads, as the economic model behind search advertising requires pCTR values to rank ads and to price clicks. In this task, given the training instances derived from session logs of the Tencent proprietary search engine, soso.com, participants are expected to accurately predict the pCTR of ads in the testing instances.
TRAINING DATA FILE   
The training data file is a text file, where each line is a training instance derived from search session log messages. To understand the training data, let us begin with a description of search sessions.   
A search session refers to an interaction between a user and the search engine. It contains the following ingredients: the user, the query issued by the user, some ads returned by the search engine and thus impressed (displayed) to the user, and zero or more ads that were clicked by the user. For clarity, we introduce a terminology here. The number of ads impressed in a session is known as the ’depth’. The order of an ad in the impression list is known as the ‘position’ of that ad. An Ad, when impressed, would be displayed as a short text known as ’title’, followed by a slightly longer text known as the ’description’, and a URL (usually shortened to save screen space) known as ’display URL’.   
We divide each session into multiple instances, where each instance describes an impressed ad under a certain setting  (i.e., with certain depth and position values).  We aggregate instances with the same user id, ad id, query, and setting in order to reduce the dataset size. Therefore, schematically, each instance contains at least the following information:
UserID 
AdID 
Query 
Depth 
Position 
Impression 
the number of search sessions in which the ad (AdID) was impressed by the user (UserID) who issued the query (Query).
Click 
the number of times, among the above impressions, the user (UserID) clicked the ad (AdID).   
Moreover, the training, validation and testing data contain more information than the above list, because each ad and each user have some additional properties. We include some of these properties into the training, validation  and the testing instances, and put other properties in separate data files that can be indexed using ids in the instances. For more information about these data files, please refer to the section ADDITIONAL DATA FILES. 
Finally, after including additional features, each training instance is a line consisting of fields delimited by the TAB character: 
1. Click: as described in the above list. 
2. Impression: as described in the above list. 
3. DisplayURL: a property of the ad. 
The URL is shown together with the title and description of an ad. It is usually the shortened landing page URL of the ad, but not always. In the data file,  this URL is hashed for anonymity. 
4. AdID: as described in the above list. 
5. AdvertiserID: a property of the ad. 
Some advertisers consistently optimize their ads, so the title and description of their ads are more attractive than those of others’ ads. 
6. Depth: a property of the session, as described above.   
7. Position: a property of an ad in a session, as described above. 
8. QueryID:  id of the query. 
This id is a zero‐based integer value. It is the key of the data file 'queryid_tokensid.txt'.
9. KeywordID: a property of ads. 
This is the key of  'purchasedkeyword_tokensid.txt'. 
10. TitleID: a property of ads. 
This is the key of 'titleid_tokensid.txt'. 
11. DescriptionID: a property of ads. 
 This is the key of 'descriptionid_tokensid.txt'. 
12. UserID 
This is the key of 'userid_profile.txt'.  When we cannot identify the user, this field has a special value of 0.
 ADDITIONAL DATA FILES
There are five additional data files, as mentioned in the above section: 
1. queryid_tokensid.txt 
2. purchasedkeywordid_tokensid.txt 
3. titleid_tokensid.txt 
4. descriptionid_tokensid.txt 
5. userid_profile.txt 
Each line of the first four files maps an id to a list of tokens, corresponding to the query, keyword, ad title, and ad description, respectively. In each line, a TAB character separates the id and the token set.  A token can basically be a word in a natural language. For anonymity, each token is represented by its hash value.  Tokens are delimited by the character ‘|’. 
Each line of ‘userid_profile.txt’ is composed of UserID, Gender, and Age, delimited by the TAB character. Note that not every UserID in the training and the testing set will be present in ‘userid_profile.txt’. Each field is described below: 
1. Gender: 
'1'  for male, '2' for female,  and '0'  for unknown. 
2. Age: 
'1'  for (0, 12],  '2' for (12, 18], '3' for (18, 24], '4'  for  (24, 30], '5' for (30,  40], and '6' for greater than 40. 
TESTING DATASET
The testing dataset shares the same format as the training dataset, except for the counts of ad impressions and ad clicks that are needed for computing the empirical CTR. A subset of the testing dataset is used to consistently rank submitted/updated results on the leaderboard. The testing dataset is used for picking the final winners.
The log for forming the training dataset corresponds to earlier time than that of the testing dataset.
EVALUATION
Teams are expected to submit their result file in text format, in which each line corresponds to a line in the downloaded file with the same order, and there is only one field in each line: the predicted CTR. In the result file, the lines corresponding to the lines from validation dataset will be used to score for the ranking on the leaderboard during the competition except the last day (June 1, 2012), and the lines corresponding to the lines from testing dataset will be used for the ranking on the leaderboard on the day of June 1, 2012, and for picking the final winners.
The performance of the prediction will be scored in terms of the AUC (for more details about AUC, please see ‘ROC graphs: Notes and practical considerations for researchers‘ by Tom Fawcett). For a detailed definition of the metric, please refer to the tab ‘Evalaution’.
PRIZES
Teams with the best performance scores will be the winners. The prizes for the 1st, 2nd and 3rd winners for task 2 are US Dollars $5000, $2000, and $1000, respectively.
 ","The Evaluation Metric for Task 2 (Advertising)
 The evaluation metric for task 2 is the Area Under Curve (AUC), which is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. Although the definition of AUC is apparently simple, there are some common misconceptions and pitfalls when using them in practice as summarized by the paper:
http://home.comcast.net/~tom.fawcett/public_html/papers/ROC101.pdf.  
For this competition, we adopt Algorithm 3 in above paper.
 ","track2.7z and track2.zip contain the same files - you only need to download one format

track2.7z contains the following compressed files:

Filename                            Available Format
--------------------------------------------------------------
training                                    .txt (9.9Gb)
queryid_tokensid                    .txt (704Mb)
purchasedkeywordid_tokensid    .txt (26Mb)
titleid_tokensid                            .txt (172Mb)
descriptionid_tokensid                 .txt (268Mb)
userid_profile                               .txt (284Mb)",kaggle competitions download -c kddcup2012-track2,"['https://www.kaggle.com/code/shivashi11/ad-click-prediction', 'https://www.kaggle.com/code/santanukundu/kdd-cup-data-analysis-1']"
540,"The William and Flora Hewlett Foundation (Hewlett) is sponsoring the Automated Student Assessment Prize (ASAP).  Hewlett is appealing to data scientists and machine learning specialists to help solve an important social problem.  We need fast, effective and affordable solutions for automated grading of student-written essays.
Hewlett is sponsoring the following prizes:
$60,000:  1st place
$30,000:  2nd place
$10,000:  3rd place
You are provided access to hand scored essays, so that you can build, train and test scoring engines against a wide field of competitors.  Your success depends upon how closely you can deliver scores to those of human expert graders.  While we believe that these financial incentives are important, we also intend to introduce top performers both to leading vendors in the industry and/or an established base of interested buyers.  Hewlett is opening the field of automated student assessment to you.  We want to induce a breakthrough that is both personally satisfying and game-changing for improving public education.
Today, state departments of education are developing new forms of testing and grading methods, to assess the new common core standards.  In this environment the need for more sophisticated and affordable options is vital.  For example, we know that essays are an important expression of academic achievement, but they are expensive and time consuming for states to grade them by hand.  So, we are frequently limited to multiple-choice standardized tests.  We believe that automated scoring systems can yield fast, effective and affordable solutions that would allow states to introduce essays and other sophisticated testing tools.  We believe that you can help us pave the way towards a breakthrough.  ASAP is designed to achieve the following goals:
Challenge developers of automated student assessment systems to demonstrate their current capabilities.
Compare the efficacy and cost of automated scoring to that of human graders.
Reveal product capabilities to state departments of education and other key decision makers interested in adopting them.
The graded essays are selected according to specific data characteristics.  On average, each essay is approximately 150 to 550 words in length.  Some are more dependent upon source materials than others.  This range of essay type is provided so that we can better understand the strengths of your solution.  It is our intent to showcase quality and reliability, based on how well you can match expert human graders for each essay.
You will be provided with training data for each essay prompt.  The number of training essays does vary.  For example, the lowest amount of training data is 1,190 essays, randomly selected from a total of 1,982.  The data will contain ASCII formatted text for each essay followed by one or more human scores, and (where necessary) a final resolved human score.  Where it is relevant, you are provided with more than one human score, so that you may evaluate the reliability of the human scorers, but - keep in mind - that you will be predicting to the resolved score.  Also, please note that most essays are scored using a holistic scoring rubric.  However, one data set uses a trait scoring rubric.  The variability is intended to test the limits of your scoring engine’s capabilities.
Following a period of 3 months to build and/or train your engine, you will be provided with test data that will contain new essays, randomly selected for blind evaluation.  However, you will notice that the rater and resolved score columns will be blank.  You will be asked to supply, based on your engine's predictions for each essay, your score in the resolved score column and then submit your new data set on this site.
As part of the file that you will submit with your predictive scores, you will be asked to submit additional information.  We would like to understand both the time and capital that you’ve spent developing your engine, the profile of your team (or you as an individual if you are working alone) and the projected cost to implement your solution on a larger scale, along with any known limitations.  Basically, you will have the opportunity to present your case for who you are, why your model is commercially viable and to what extent you can use your model to satisfy the interests of potential buyers.  This other information will not be used to determine any prize rewards, and it is optional.  But, if you provide it, it will be used to evaluate whether or not your model should be presented to state departments of education and others who stand to benefit from your work.
Also, please note that it is our intention to stage other follow-on ASAP phases in the months ahead.  We are starting with graded essays and will follow with new data:
Phase 1: Demonstration for long-form constructed response (essays);
Phase 2: Demonstration for short-form constructed response (short answers);
Phase 3: Demonstration for symbolic mathematical/logic reasoning (charts/graphs).
In every instance, we seek to drive innovation for new solutions to automated student assessment.  We hope that you will enjoy this process.  May the best model win!","Essay score predictions are evaluated using objective criteria.
Specifically, your performance will be evaluated with the quadratic weighted kappa error metric, which measures the agreement between two raters.  This metric typically varies from 0 (only random agreement between raters) to 1 (complete agreement between raters).  In the event that there is less agreement between the raters than expected by chance, this metric may go below 0.  The quadratic weighted kappa is calculated between the automated scores for the essays and the resolved score for human raters on each set of essays.  The mean of the quadratic weighted kappa is then taken across all sets of essays.  This mean is calculated after applying the Fisher Transformation to the kappa values.
A set of essay responses E has N possible ratings, 1,2,…,N, and two raters, Rater A and Rater B.  Each essay response e is characterized by a tuple (ea,eb), which corresponds to its scores by Rater A (resolved human score) and Rater B (automated score).  The quadratic weighted kappa is calculated as follows.  First, and N-by-N histogram matrix O is constructed over the essay ratings, such that Oi,j corresponds to the number of essays that received a rating i by Rater A and a rating j by Rater B.
An N-by-N matrix of weights, w, is calculated based on the difference between raters’ scores:
$$w_{i,j} = \frac{\left(i-j\right)^2}{\left(N-1\right)^2}$$
An N-by-N histogram matrix of expected ratings, E, is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between each rater’s histogram vector of ratings, normalized such that E and O have the same sum.
From these three matrices, the quadratic weighted kappa is calculated: 
$$\kappa=1-\frac{\sum_{i,j}w_{i,j}O_{i,j}}{\sum_{i,j}w_{i,j}E_{i,j}}$$
The Fisher Transformation is approximately a variance-stabilizing transformation and is defined:
$$z = \frac{1}{2} \ln \frac{1+\kappa}{1-\kappa}$$
Since this transformation approaches infinity as kappa approaches 1, the maximum kappa value is capped at 0.999.  Next the mean of the transformed kappa values is calculated in the z-space.  For Essay Set #2, which has scores in two different domains, each transformed kappa is weighted by 0.5.  This means that each dataset has an equally weighted contribution to the final score.  Finally, the reverse transformation is applied to get the average kappa value:
$$\kappa = \frac{e^{2z}-1}{e^{2z}+1}$$
If you have questions regarding the evaluation criteria, please refer to the help page.","Code for evaluation metric and benchmarks
For this competition, there are eight essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.
The training data is provided in three formats: a tab-separated value (TSV) file, a Microsoft Excel 2010 spreadsheet, and a Microsoft Excel 2003 spreadsheet.  The current release of the training data contains essay sets 1-6.  Sets 7-8 will be released on February 10, 2012.  Each of these files contains 28 columns:
essay_id: A unique identifier for each individual student essay
essay_set: 1-8, an id for each set of essays
essay: The ascii text of a student's response
rater1_domain1: Rater 1's domain 1 score; all essays have this
rater2_domain1: Rater 2's domain 1 score; all essays have this
rater3_domain1: Rater 3's domain 1 score; only some essays in set 8 have this.
domain1_score: Resolved score between the raters; all essays have this",kaggle competitions download -c asap-aes,"['https://www.kaggle.com/code/soumya9977/autograding-using-lstm-tf-keras', 'https://www.kaggle.com/code/irfanmansuri/the-havelett-foundation-automated-scoring', 'https://www.kaggle.com/code/mpwolke/hewlett-foundation-essay-scoring', 'https://www.kaggle.com/code/dodger23/automated-content-grading', 'https://www.kaggle.com/code/ktgiahieu/llms-chatgpt-grade-aes']"
541,"Writer identification is a very active research field. It is of a primordial importance in forensic document examination when it helps experts in delibirating on the authenticity of a certain document.
This is a follow-up contest of the last year' Arabic Writer Identification Contest.
As we mentioned in the last edition, writer identification generally requires two steps. The first one is an image-processing step, where features are extracted from the images. The second step is a classification step, where the document is assigned to the “closest” document in the dataset according to the “difference” between their features.
With regards to the last edition, this contest has several improvments:
we have significantly augmented the number of writers (we have more than 200 writers in this new database).
we will not be providing any side-information (eg. number of documents per writer), as this is not necessarly known in a real forensic casework.
we will only provide binary images, as color and gray-level images might transform this into a pen identification task.
This competition is organized in conjunction with the International Conference of Frontiers in Handwriting Recognition ICFHR2012 which will be held in Bari, Italy in September 18-20.",,"Additional 1-Nearest-Neighbor benchmark in Python
In this contest, more than 200 writers were asked to write three different paragraphs in Arabic language. The first two paragraphs are used for training and the third one for testing. For some writers, the first two paragraphs have been removed from the training set to test the ability of systems to detect unknown writers. Also, some writers have written the third paragraph more than once and some other participants did not writer the third paragraph at all.
Images are provided in PNG binary format. The binarization has been performed using Otsu's method. The images are provided in two subfolders “train” and “test”. The folder “train” contains images having the following format XXX_Y.png where XXX represents the ID of the writer and Y represents the number of the paragraph.
The folder “test” contains images of the third paragraph, they all have the following format ZZ.png, where ZZ is the ID of the image in the test set.

Participants are asked to provide for each ZZ image, the ID of the most probable writer (among those which are in the training set). Like mentioned before, some ZZ images do not actually correspond to any writer in the training set. Participants are supposed to produce an ID eqal to 0 for those images.
The competition is judged using the caterogization accuracy, which corresponds to the percentage of correctly identified writers.
In the event there is a tie on the private leaderboard, the tie will be broken by the submission time (the prize money will go to the first submission).
For participants who are not familiar with image processing, some geometric features are provided. These features are given in the form of histograms. The list below shows the name and the number of values in each of these features:",kaggle competitions download -c awic2012,[]
542," BACKGROUND
Online social networking services have become tremendously popular in recent years, with popular social networking sites like Facebook, Twitter, and Tencent Weibo adding thousands of enthusiastic new users each day to their existing billions of actively engaged users. Since its launch in April 2010, Tencent Weibo, one of the largest micro-blogging websites in China, has become a major platform for building friendship and sharing interests online. Currently, there are more than 200 million registered users on Tencent Weibo, generating over 40 million messages each day. This scale benefits the Tencent Weibo users but it can also flood users with huge volumes of information and hence puts them at risk of information overload. Reducing the risk of information overload is a priority for improving the user experience and it also presents opportunities for novel data mining solutions. Thus, capturing users’ interests and accordingly serving them with potentially interesting items (e.g. news, games, advertisements, products), is a fundamental and crucial feature social networking websites like Tencent Weibo. 
TASK 1 DESCRIPTION 
The prediction task involves predicting whether or not a user will follow an item that has been recommended to the user. Items can be persons, organizations, or groups and will be defined more thoroughly below. 
DATASETS
First, we define some notations as follows:
“Item”: An item is a specific user in Tencent Weibo, which can be a person, an organization, or a group, that was selected and recommended to other users. Typically, celebrities, famous organizations, or some well-known groups were selected to form the ‘items set’ for recommendation. The size of this is about 6K items in the dataset. 
Items are organized in categories; each category belongs to another category, and all together they form a hierarchy. For example, an item, a vip user Dr. Kaifu LEE,
vip user: http://t.qq.com/kaifulee (wikipedia: http://en.wikipedia.org/wiki/Kai-Fu_Lee)
represented as
science-and-technology.internet.mobile
We can see that categories in different levels are separated by a dot ‘.’, and the category information about an item can help enhance your model prediction. For example, if a user Peter follows kaifulee, he may be interested in the other items of the category that kaifulee belongs to, and might also be interested in the items of the parent category of kaifulee’s category.
“Tweet”: a “tweet” is the action of a user posting a message to the microblog system, or the posted message itself. So when one user is “tweeting“, his/her followers will see the “tweet”.
“Retweet”: a user can repost a tweet and append some comments (or do nothing), to share it with more people (my followers).
“Comment”: a user can add some comments to a tweet. The contents of the comments  will not be automatically pushed to his/her followers as ‘tweeting’ or ‘retweeting’,but will appear at the ‘comment history’ of the commented tweet.
“Followee/follower”: If User B is followed by User A, B is a followee to A, and A is a follower to B.
We describe the datasets as follows:
The dataset represents a sampled snapshot of Tencent Weibo users’ preferences for various items –– the recommendation of items to users and the history of users’ ‘following’ history. It is of a larger scale compared to other publicly available datasets ever released. Also it provides richer information in multiple domains such as user profiles, social graph, item category, which may hopefully evoke deeply thoughtful ideas and methodology.
The users in the dataset, numbered in millions, are provided with rich information (demographics, profile keywords, follow history, etc.) for generating a good prediction model. To protect the privacy of the users, the IDs of both the users and the recommended items are anonymized as random numbers such that no identification is revealed. Furthermore, their information, when in Chinese, will be encoded as random strings or numbers, thus no contestant who understands Chinese would get advantages. Timestamps for recommendation are given for performing session analysis.
Two datasets in 7 text files, downloadable:
a) Training dataset : some fields are in the file rec_log_train.txt 
b) Testing dataset: some fields are in the file rec_log_test.txt
Format of the above 2 files:
(UserId)\t(ItemId)\t(Result)\t(Unix-timestamp)
Result: values are 1 or -1, where 1 represents the user UserId accepts the recommendation of item ItemId and follows it (i.e., adds it to his/her social network), and -1 represents the user rejects the recommended item.
We provide the true values of the ‘Result’ field in rec_log_train.txt, whereas in  rec_log_test.txt, the true values of the ‘Result’ field are withheld (for simplicity, in the file they are always 0). Another difference from rec_log_test.txt to rec_log_train.txt is that repeated recommended (UserId,ItemId) pairs were removed.
c)      More fields of the training and the testing datasets about the user and the item are in the following 5 files:
          i.              User profile data: user_profile.txt
Each line contains the following information of a user: the year of birth, the gender, the number of tweets and the tag-Ids. It is important to note that information about the users to be recommended is also in this file.
Format:
(UserId)\t(Year-of-birth)\t(Gender)\t(Number-of-tweet)\t(Tag-Ids)
Year of birth is selected by user when he/she registered.
Gender has an integer value of 0, 1, or 2, which represents “unknown”, “male”, or “female”, respectively.
Number-of-tweet is an integer that represents the amount of tweets the user has posted.
Tags are selected by users to represent their interests. If a user likes mountain climbing and swimming, he/she may select ""mountain climbing"" or ""swimming"" to be his/her tag. There are some users who select nothing. The original tags in natural languages are not used here, each unique tag is encoded as an unique integer.
Tag-Ids are in the form “tag-id1;tag-id2;...;tag-idN”. If a user doesn’t have tags, Tag-Ids will be ""0"".
        ii.              Item data: item.txt
Each line contains the following information of an item: its category and keywords.
Format:
(ItemId)\t(Item-Category)\t(Item-Keyword)
Item-Category is a string “a.b.c.d”, where the categories in the hierarchy are delimited by the character “.”, ordered in top-down fashion (i.e., category ‘a’ is a parent category of ‘b’, and category ‘b’ is a parent category of ‘c’, and so on.
Item-Keyword contains the keywords extracted from the corresponding Weibo profile of the person, organization, or group. The format is a string “id1;id2;…;idN”, where each unique keyword is encoded as an unique integer such that no real term is revealed.
      iii.              User action data: user_action.txt
The file user_action.txt contains the statistics about the ‘at’ (@) actions between the users in a certain number of recent days.
Format:
(UserId)\t(Action-Destination-UserId)\t(Number-of-at-action)\t(Number-of-retweet )\t(Number-of-comment)
If user A wants to notify another user about his/her tweet/retweet/comment, he/she would use an ‘at’ (@) action to notify the other user, such as ‘@tiger’ (here the user to be notified is ‘tiger’)..
For example, user A has retweeted user B 5 times, has “at” B 3 times, and has commented user B 6 times, then there is one line “A   B     3     5     6” in user_action.txt.
       iv.              User sns data: user_sns.txt
The file user_sns.txt contains each user’s follow history (i.e., the history of following another user). Note that the following relationship can be reciprocal.
Format:
(Follower-userid)\t(Followee-userid)
         v.              User key word data: user_key_word.txt
The file user_key_word.txt contains the keywords extracted from the tweet/retweet/comment by each user.
Format:
(UserId)\t(Keywords)
Keywords is in the form “kw1:weight1;kw2:weight2;…kw3:weight3”.
Keywords are extracted from the tweet/retweet/comment of a user, and can be used as features to better represent the user in your prediction model. The greater the weight, the more interested the user is with regards to the keyword.
Every keyword is encoded as a unique integer, and the keywords of the users are from the same vocabulary as the Item-Keyword. 
EVALUATION 
Teams’ scores and ranks on the leaderboard are based on a metric calculated from the predicted results in submitted result file and the held out ground truth of a validation dataset whose instances were a fixed set sampled from the testing dataset in the beginning and, until the last day of the competition (June 1, 2012) by then the scores and associated ranks on leaderboard are based on the predicted results and that of the rest of the testing dataset. This entails that the top-3 ranked teams at the time when the competition ends are the winners. The log for forming the training dataset corresponds to earlier time than that of the testing dataset.
The evaluation metric is average precision. For a detailed definition of the metric, please refer to the tab ‘Evaluation’. 
PRIZES 
The prizes for the 1st, 2nd and 3rd winners for task 1 are US Dollars $5000, $2000, and $1000, respectively.
 ","Evaluation Metric for Track 1 (Weibo)
Suppose there are m items in an ordered list is recommended to one user, who may click 1 or more or none of them to follow, then, by adapting the definition of average precision in IR (http://en.wikipedia.org/wiki/Information_retrieval, http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf ), the average precision at n for this user is
  ap@n = Σ k=1,...,n P(k) / (number of items clicked in m items)
  where if the denominator is zero, the result is set zero; P(k) means the precision at cut-off k in the item list, i.e., the ratio of number of clicked items up to the position k over the number k, and P(k) equals 0 when k -th item is not followed upon recommendation; n = 3 as this is the default number of items recommended to each user in our recommender system.
For example,
(1)     If among the 5 items recommended to the user, the user clicked #1, #3, #4, then ap@3 = (1/1 + 2/3)/3 ≈ 0.56
(2)     If among the 4 items recommended to the user, the user clicked #1, #2, #4, then ap@3 = (1/1 + 2/2)/3 ≈ 0.67
(3)     If among the 3 items recommended to the user, the user clicked #1, #3, then ap@3 = (1/1 + 2/3)/2 ≈ 0.83
The average precision for N users at position n is the average of the average precision of each user, i.e.,
  AP@n = Σ i=1,...,N ap@ni / N
  which is exactly the metric for the result file that the teams submit for evaluation of their models.
 ","track1.7z and track1.zip contain the same files - you only need to download one format

track1.7z contains the following compressed files:
  Filename                            Available Format
--------------------------------------------------------------
rec_log_train                    .txt (1.99Gb)
user_profile                     .txt (55.8Mb)
item                                   .txt (1.18Mb)
user_action                     .txt (217Mb)
user_sns                          .txt (740Mb)
user_key_word               .txt (182Mb)
 ",kaggle competitions download -c kddcup2012-track1,[]
543,"The Benchmark Bond Trade Price Challenge is a competition to predict the next price that a US corporate bond might trade at. Contestants are given information on the bond including current coupon, time to maturity and a reference price computed by Benchmark Solutions.  Details of the previous 10 trades are also provided.  ","Performance evaluation will be conducted using mean absolute error.  Each observation will be weighted as indicated by the weight column.  This weight is calculated as the square root of the time since the last observation, scaled so that the mean weight is 1.","NOTE: The compressed files contain .csv versions of the training and test data.  The .mat files are provided for MATLAB users as a convenience.
US corporate bond trade data is provided.  Each row includes trade details, some basic information about the traded bond, and information about the previous 10 trades.  Contestants are asked to predict trade price.
Column details:
id: The row id.
bond_id: The unique id of a bond to aid in timeseries reconstruction. (This column is only present in the train data)
trade_price: The price at which the trade occured.  (This is the column to predict in the test data)
weight: The weight of the row for evaluation purposes. This is calculated as the square root of the time since the last trade and then scaled so the mean is 1.
current_coupon: The coupon of the bond at the time of the trade.
time_to_maturity: The number of years until the bond matures at the time of the trade.
is_callable: A binary value indicating whether or not the bond is callable by the issuer.
reporting_delay: The number of seconds after the trade occured that it was reported.
trade_size: The notional amount of the trade.",kaggle competitions download -c benchmark-bond-trade-price-challenge,[]
544,"One-shot-learning Gesture Recognition
Humans are capable of recognizing patterns like hand gestures after seeing just ONE example. Can machines do that too?
This challenge is one of four CHALEARN Gesture Challenge events
   This challenge round is over, round 2 has started follow that link to enter.
Motivations
You will never need a remote controller anymore, you will never need a light switch. Lying in bed in the dark, you will point to the ceiling to turn on the light, you will wave your hand to increase the temperature, you will make a T with your hands to turn on the TV set. You and your loved ones will feel safer at home, in parking lots, in airports: nobody will be watching, but computers will detect distressed people and suspicious activities. Computers will teach you how to effectively use gestures to enhance speech, to communicate with people who do not speak your language, to speak with deaf people, and you will easily learn many other sign languages to comminicate under water, to referee sports, etc. All that thanks to gesture recognition!
  Kinect (TM)
This is a challenge on gesture and sign language recognition using a Kinect camera. Kinect is revolutionizing the field of gesture recognition by providing an affordable 3D camera, which records both an RGB image and a depth image (using an infrared sensor). The challenge focuses on hand gestures. Applications include man-machine communication, translating sign languages for the deaf, video surveillance, and computer gaming. Check out some examples.
  One-shot-learning
Every application needs a specialized gesture vocabulary. If we want gesture recognition to become part of everyday life, we need gesture recognition machines, which easily get tailored to new gesture vocabularies. This is why the focus of the challenge is on “one-shot-learning” of gestures, which means learning to recognize new categories of gestures from a single video clip of each gesture. The gestures will be drawn from a small vocabulary of gestures, generally related to a particular task, for instance, hand signals used by divers, finger codes to represent numerals, signals used by referees, or marchalling signals to guide vehicles or aircrafts.
  Protocol sketch
This competition consists of two main components: a development phase (December 7, 2011 to April 6, 2012) and a final evaluation phase (April 7, 2012 to April 10. 2012): 
During the development phase of the competition, the participants build a learning system capable of learning from a single training example a gesture classification problem. To that end, they get development data consisting of several batches of gestures, each split into a training set (of one example for each gesture) and a test set of short sequences of one to 5 gestures. Each batch contains gestures from a different small vocabulary of 8 to 15 gestures, for instance diving signals, signs of American Sign Language representing small animals, Italian gestures, etc. The test data labels are provided for such development data so the participants can self-evaluate their systems. To evaluate their progress and compare themselves with others, they can use provided validation data, for which one training example is provided for each gesture token in each batch, but the test data labels are withheld. The prediction results on validation test data can be submitted on-line to get immediate feed-back. A real-time leaderboard shows participants their current standing based on their validation set predictions.
During the final evaluation phase of the competition, the participants get to perform similar tasks as those of the validation data on new final evaluation data revealed at the end of the development phase. The participants have a few days to train their systems and upload their predictions. Prior to the end of the development phase, Kaggle will make available a software vault so the participants can upload executable code for their best learning system, which they will then use to train their models and make predictions on the final evaluation test data. This will allow the competition organizers to check their results and ensure the fairness of the competition. Note that participation is NOT conditiioned on submitting code or disclosing methods. See the rules for details. If any of the top ranking participants opted not to submit their learning system for verification, he/she will have to demonstrate the performance of his/her system in person on another verification set.  Any statistically significant deviation between the performance on the final evaluation set and the verification set may result in disqualification.
  Sponsors
This challenge is organized by CHALEARN and is sponsored in part by Microsoft (Kinect for Xbox 360). Other sponsors include Texas Instrument. This effort was initiated by the DARPA Deep Learning program and is supported by the US National Science Foundation (NSF) under grants ECCS 1128436 and ECCS 1128296 , the EU Pascal2 network of excellence. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.","What you need to predict
Each video contains the recording of 1 to 5 gestures from a vocabulary of 8 to 15 gesture tokens. For instance a gesture vocabulary may consist of the signs to referee volleyball games or the signs to represent small animals in the sign language for the deaf.
You need to predict the identity of those gestures, represented by a numeric label (from 1 to 15). The data are divided into data batches, each having a different vocabulary of gestures. So the numeric labels represent different gestures in every batch.
In the data used for evaluation (called validation data and final evaluation data), you get one video clip for each gesture token as a training example in every batch. You must predict the labels of the gestures played in the other unlabeled videos.
  Levenshtein distance
For each video, you provide an ordered list of labels R corresponding to the recognized gestures. We compare this list to the corresponding list of labels T in the prescribed list of gestures that the user had to play. These are the ""true"" gesture labels (provided that the users did not make mistakes). We compute the so-called Levenshtein distance L(R, T), that is the minimum number of edit operations (substitution, insertion, or deletion) that one has to perform to go from R to T (or vice versa). The Levenhstein distance is also known as ""edit distance"".
For example:
L([1 2 4], [3 2]) = 2
L([1], [2]) = 1
L([2 2 2], [2]) = 2
We provide the Matab(R) code for the Levenshtein distance in our sample code.
  Score
The overall score we compute is the sum of the Levenshtein distances for all the lines of the result file compared to the corresponding lines in the truth value file, divided by the total number of gestures in the truth value file. This score is analogous to an error rate. However, it can exceed one.
Public score means the score that appears on the leaderboard during the development period and is based on the validation data.
Final score means the score that will be computed on the final evaluation data released at the end of the development period, which will not be revealed until the challenge is over. The final score will be used to rank the participants and determine the prizes.
  Verification procedure
To verify that the participants complied with the rule that there should be no manual labeling of the test data, the top ranking participants eligible to win prizes will be asked to cooperate with the organizers to reproduce their results. 
Preferred procedure
At the end of the development period, from March 7 until April 6, 2012, the participants will be offered by Kaggle the possibility of uploading executable code for a standard platform to a software vault. The organizers will evaluate requests to support particular platforms, but do not commit to support all platforms. The burden of proof will rest on the participants, see our backup procedure. The code will be kept in confidence and used only for verification purpose after the challenge is over. The code submitted will need to be standalone and in particular it will not be allowed to access the Internet. It will need to be capable of training models from the final evaluation data training examples, for each data batch, and making label predictions on the test examples of that batch. Instructions on how to prepare the code will be given in February 2012.
Backup procedure
If for some reason a participant elects not to submit executable code before the April 6, 2012 deadline, he/she will have the option of bringing a full system to the site of the workshop at CVPR 2012, or another location mutually agreed upon, to let the organizers perform a live test. The organizers may also decide to run this backup procedure if, for a technical reason, the executable code provided by the participants cannot be run on their computers. The verification will be carried out using verification data similar to the final evaluation data. Statistically significant discrepancies in performance between the final evaluation data and the verification data may be a cause of disqualification. The results of the verifications will be published by the organizers.","  Data releases
December 7, 2011
Sample data released. See the README file for release notes. You may download all the batches develxx.zip xx=1 to 20 and validxx.zip xx=1 to 20 individually or download the two large archives devel_all_files_combined.7z and valid_all_files_combined.7z. The videos are in a quasi-lossless AVI format. See also our data mirrors if you have download problems.
January 7. 2012
Entire dataset released. See the README file for release notes. You may download all the batches develxx, xx=1 to 480 in grouped archives devel01-40, devel41-80 ... devel441-480. The videos are in a quasi-lossless AVI format. We also provide videos compressed in a lossy AVI format.
March 7, 2012
Final evaluation data for round 1 available from various mirrors in encrypted zip archives (the decryption key will be made available on April 7). The data are available is lossy compressed AVI format (final-1-20) and in quasi-lossless AVi format (final01, final02, ... final20).
The depth normalization information may be downloaded from Depth_info_final.txt",kaggle competitions download -c GestureChallenge,[]
545,"When studying for a test, you want to know how well you're going to do.  More specifically, you want to know what areas you need to study more.  In order to help students answer this question, we are attempting to predict their probability of answering questions correctly.  The data in this competition comes from students studying for three tests: the GMAT, SAT, and ACT.
You are attempting to predict, for each question attempted in the test set, whether the student will answer the question correctly.  To succeed, you will need to improve on the state-of-the-art in student evaluation.  While the questions included labels indicating their specified test area, there may be structure which helps better organize the areas of knowledge involved in each question.  In the short term, this will help students figure out what areas they are weak in; but ultimately, this will help create tests to better measure what a student actually knows.
The prize pool is $5,000 ($3,000 for first, $1,500 for second and $500 for third), with entries judged using Capped Binomial Deviance.",,"Data description
The fields of the data are as follows (the mappings from category fields to numeric values are in category_labels.csv):
correct: 0 or 1, indicating whether the student answered the question correctly.  This is the field to be predicted in the test set.
outcome: a numeric code representing 'correct' (1), 'incorrect' (2), 'skipped' (3), or 'timeout' (4): a more detailed indicator of the outcome.  Not present in test data.
user_id: an anonymized numeric identifier for the user answering the question
question_id: a numeric identifier for the question being answered
question_type: a numeric code representing the type of question; either 'MultipleChoiceOneCorrect' (0) for multiple choice, or 'SPR' (1) for free response questions
group_name: a numeric code representing the group of the question ('act' (0). 'gmat' (1), 'sat' (2))
track_name: the numeric code for the track within the test this question is associated with (mappings from category fields to numeric values are in category_labels.csv)
subtrack_name: the numeric code for the subtrack within the track this question is associated with (mappings from category fields to numeric values are in category_labels.csv)
tag_string: a space-separated list of tag ids this question has been tagged with (mappings from category fields to numeric values are in category_labels.csv)",kaggle competitions download -c WhatDoYouKnow,[]
546,"The Algorithmic Trading Challenge is a forecasting competition which aims to encourage the development of new models to predict the stock market's short-term response following large trades. Contestants are asked to derive empirical models to predict the behaviour of bid and ask prices following such ""liquidity shocks"".
Modelling market resiliency will improve trading strategy evaluation methods by increasing the realism of backtesting simulations, which currently assume zero market resiliency.","Performance evaluation will be conducted using root mean square error. For each prediction, RMSE will be separately calculated for the bid and ask at each time step following a liquidity shock. The winning model will be the one with the lowest cumulative RMSE across the entire prediction set.","NOTE: The data files were all updated on November 11, 2011. Please download them again if you downloaded them before that date.
Recent trade and quote data from the London Stock Exchange (LSE) is provided. The preprocessed dataset comprises observations of the limit order book before and after a liquidity shock (a trade that results in widening of the bid-ask spread).
Table 1 shows the data schema provided for each liquidity shock.
Table 2 shows information about each data field.
  Contestants are asked to submit predictions based on a test dataset containing 50,000 rows. A larger dataset is provided for training purposes.",kaggle competitions download -c AlgorithmicTradingChallenge,[]
547,"One of the biggest challenges of an auto dealership purchasing a used car at an auto auction is the risk of that the vehicle might have serious issues that prevent it from being sold to customers. The auto community calls these unfortunate purchases ""kicks"".
Kicked cars often result when there are tampered odometers, mechanical issues the dealer is not able to address, issues with getting the vehicle title from the seller, or some other unforeseen problem. Kick cars can be very costly to dealers after transportation cost, throw-away repair work, and market losses in reselling the vehicle.
Modelers who can figure out which cars have a higher risk of being kick can provide real value to dealerships trying to provide the best inventory selection possible to their customers.
The challenge of this competition is to predict if the car purchased at the Auction is a Kick (bad buy).",,"The challenge of this competition is to predict if the car purchased at the Auction is a good / bad buy.
All the variables in the data set are defined in the file Carvana_Data_Dictionary.txt 
The data contains missing values 
The dependent variable (IsBadBuy) is binary (C2)
There are 32 Independent variables (C3-C34)
The data set is split to 60% training and 40% testing.",kaggle competitions download -c DontGetKicked,"['https://www.kaggle.com/code/funxexcel/don-t-get-kicked-pipeline-improved', 'https://www.kaggle.com/code/funxexcel/dgk-base-solution-rf', 'https://www.kaggle.com/code/funxexcel/starter-code-don-t-get-kicked-rf-model', 'https://www.kaggle.com/code/funxexcel/don-t-get-kicked-pipeline-feat-engineering', 'https://www.kaggle.com/code/balajinagappan/don-t-get-kicked-sklearn-from-scratch']"
548,"Banks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit. 
Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years.
The goal of this competition is to build a model that borrowers can use to help make the best financial decisions.
Historical data are provided on 250,000 borrowers and the prize pool is $5,000 ($3,000 for first, $1,500 for second and $500 for third).",AUC,"Training, Test, Sample Entry and Submission Files are provided. Please check the format of the submission file.",kaggle competitions download -c GiveMeSomeCredit,"['https://www.kaggle.com/code/simonpfish/comp-stats-group-data-project-final', 'https://www.kaggle.com/code/caesarlupum/modeling-give-me-some-credit', 'https://www.kaggle.com/code/orange90/credit-scorecard-example', 'https://www.kaggle.com/code/nicholasgah/eda-credit-scoring-top-100-on-leaderboard', 'https://www.kaggle.com/code/riteshrhyme/starter-credit-card-scoring-bbe98584-0']"
549,"Background
We have a large collection of user-generated photos. We want to automatically pick out particularly enjoyable or impressive ones to highlight, especially travel-related, using only the meta-data associated with the images such as caption text, image dimensions and approximate location in the world. We know from our preliminary experiments and intuition that certain words and places are correlated with good photos, and others are indicators of less enjoyable pictures, but we would like to develop an algorithm to tie together these multiple signals.
Objective
Given anonymized information on thousands of photo albums, predict whether a human evaluator would mark them as 'good'.",This competition uses a capped binomial deviance method as described on the Chess Ratings 2 competition page and in the forums.,"Data format
For anonymity reasons, the caption, title and description texts have been broken down into word tokens, and the most common (excluding stop words) have been encoded as numbers. The fields are:
latitude: The integrally-rounded latitude of the location of the album
longitude: The integrally-rounded longitude of the location of the album
width: The width of the images in the album, in pixels
height: The height of the images in the album, in pixels
size: The number of photos in the album
name: The common tokens in the name of the album
description: The common tokens in the description of the album
caption: The common tokens in all the captions of the photos within the album
good: 1 means the human reviewer liked the album, 0 means they didn't. This is the variable we want to predict.
Notes
From our own experiments, we know there's obvious correlations that make sense, such as areas in Africa rich in wildlife having a high proportion of good photos, and certain words that are associated with poor albums. Our goal is to turn some of those informal correlations into a more rigorous model we can reuse on much larger data sets.",kaggle competitions download -c PhotoQualityPrediction,[]
550,"There's recently been a lot of work done in unsupervised feature learning for classification, with great advances made by approaches such as deep belief nets, graphical models, and transfer learning. Meanwhile, there are a ton of older methods that also work well, including matrix factorization, random projections, and clustering methods. The purpose of this competition is to find out which of these methods work the best, on relatively large-scale high dimensional learning tasks.
The Short Version
In this task, you'll do the following:
Learn a feature representation of at most 100 features, using a small amount of labeled data and a large amount of unlabeled data.  The orginial data is very sparse.
Transform training and test data using your learned feature representation.
Train a standard linear classifier on the transformed training data.
Measure the AUC of the classifier on transformed test data.
We have public data to be used for the leaderboard evaluations, and a separate private data set used for final evaluation through a special submission process.  We have scripts to simplify the training and evaluation; how you do the feature transformation is up to you.
  The Long Version
The task that we're evaluating on is a binary-class classification problem, drawn from web classification.  (The data has been cleaned heavily and anonymized.)  The data itself is sparse, high dimensional data with about a million features.  A few features have non-zero values in many or even all examples; most features have non-zero values in very few examples.
Your task is to transform the data from a high dimensional space to a lower dimensional space of at most 100 features.  The goal is to make this new feature space so rich and informative that it allows a new classifier to be trained with the best possible predictive performance.  Any method of producing a condensed representation is fair game: deep learning graphical models, transfer learning, supervised learning, semi-supervised learning, matrix factorization, random projection, clustering, feature selection, or anything else you can invent.
In addition to a small amount of labeled data, you will also be given a large amount of unlabeled data.  Both the labeled and the unlabeled data can be used to learn good ways to transform the feature space.
The final evaluation will be done by using your method to transform training and test sets whose labels are not known.  These transformed data sets will be sent to tne organizers, who will apply the hidden labels and use these new data sets to train and test a standard supervised classifier.  The data set that produces the best classification performance on test data (using AUC as the evaluation measure) will be declared the winner.  We also provide versions of our evaluation scripts to be used on public versions of our private evaluation data sets, and these results can be used to update the leaderboard.  Please see the ""Evaluation"" page for full details.
Although there is a modest cash prize, the main goal of this competition is to encourage research and share ideas.  The results of this competition will be included in a paper submitted to the 2011 NIPS workshop on deep learning and unsupervised feature learning.  Contestants will be acknowledged by name in this paper for noteworthy performance, including results that do especially well or which are especially interesting.","The purpose of this task is to learn features that enable improved classification performance, so the evaluation is set up to test how well a classifier learns from your 100 features.  The evaluation metric is AUC of a standard linear classifier trained on a labeled data set that uses only your features, and applied on a test set using your features.
  Preparing a Submission for Evaluation
To prepare a submission file, please follow these steps.  Note that we have provided a script runLeaderboardEval.pl that automates steps 3, 4, and 5 assuming you have downloaded and compiled libsvm.
1.  Learn a way to transform the data from its sparse representation with a million features to a dense representation of at most 100 features.  You may use any or all of the unlabeled and labeled data that has been made available to help learn this representation.
2.  Use this representation to transform the data in the files public_train_data.svmlight.dat and public_test_data.svmlight.dat to your new feature space.  
3.  Train a Linear SVM with C=1.0 on your transformed training data, using the labels in public_train.labels.dat as ground truth for training.  You can use libsvm, SVM-light, or other similar packages.
4.  Use your linear model to predict the class label for each example in your transformed test data.
5.  Prepare a submission file in CSV format.  The first value in each line should be the predicted value for that example using your learned model in step 4.  The next 100 values in that line should be your 100 feature values.  Each submission file should have 50,000 rows (one per test example, in order), and 101 comma-seaparated values.  You can use the provided verifyFormat.pl script to check the format of your submission file.
6.  Submit!  You may make up to 2 submissions per day.
  Leaderboard Evaluations and Final Evaluation
The leaderboard evaluation is done by computing AUC using 30% of the test data, and is to be used only for informational purposes.
The final evaluation is done by computing AUC on the remaining 70% of the test data.
Note that because the evaluation is intended to evaluate the performance of a linear classifier trained on your features, we reserve the right to replicate the results of top-performers by performing 10-fold cross validation on the submitted transformed data, using libsvm with a linear SVM with C=1.0.  This will allow us to confirm that a simple linear model can indeed be trained on the feature transformation provided.  Submissions that cannot be shown to generate similar cross-validation results with the specified linear modeling scheme will be ruled ineligible.
  Describing Your Methods
Because this is a research oriented competition, we ask all participants to fully describe their methods so that others may learn about the techniques used and the results can be used to make meaningful comparisons.
The description should include some level of technical detail about your approaches.  It should also include a rough wall-clock estimate of computation time needed to learn your feature representation, the time needed to apply your feature representation, and a description of the computation platform that was used.  Finally, the full names of each contributor to your team should be included, along with contact info for payment should your team be the winner.
Please send the description of your methods to semisupervisedfeatures@gmail.com no later than 11:59 PM (UTC) on Monday, Oct. 17th.  Only teams that have submited informative descriptions will be eligible for the cash prize or for acknowledgement in the formal writeup of these results.
We also encourage participants to share methods and ideas freely on the forum.
  Leaderboard Evaluation Script
We have provided a script that can help to prepare submission files.  See the ""Data"" page for instructions and to download.",,kaggle competitions download -c SemiSupervisedFeatureLearning,"['https://www.kaggle.com/code/mpwolke/semi-supervised-feature-learning-tgz', 'https://www.kaggle.com/code/mathurinache/semi-supervised-feature-learning-tgz']"
551,"Going grocery shopping, we all have to do it, some even enjoy it, but can you predict it?
Dunnhumby is looking to build a model to better predict when supermarket shoppers will next visit the store and how much they will spend.
The modelling data set consists of details of every visit made by 100,000 customers over a year from April 2010 to March 31st 2011.
Each visit is stamped with the date and the customer’s spend in that visit.
We also have details of their next visit after March 31st.
Your challenge is to predict the visit_date and visit_spend of this next visit for each customer_id in the modelling test set. For more details on how you're scored, click here","For each test customer in the dataset your Model needs to provide:
a prediction of the first date on or after 1 April 2011 that the test customer will next go shopping
AND
a prediction of the test customer’s dollar spend on that shopping visit.
The prediction will be classified as correct if the date AND the dollar spend is correct (a “Correct Visit”).
Dollar spend will be deemed to be correct if it is within $10 of the actual spend, an under or over prediction of equal to or more than $10.01 being incorrect.
For each test customer the Model can either be ""correct"" or ""incorrect"". There is no concept of accuracy for a Correct Visit i.e. prediction of dollar spend within $0.01 is no better than within $10.00
The Winner of the Competition will be the Entry that predicts the highest number of Correct Visits. There will be prizes for 1st $6,000; 2nd $3,000 and 3rd $1,000.
In the unlikely event of a tie, the award for the relevant winning places shall be combined and split equally between each Winner (e.g. if two Entries tie in  2nd place, $3,000 and $1,000 will be added together and each Winner will receive $2,000 each). In the even more unlikely unlikely event that more than three Contestants qualify as winners by virtue of predicting the same number of Correct Visits, time of entry shall be used as the secondary basis for determining a Winner (e.g. if two Models tie in 3rd place but one is entered on the tenth day of the Competition and the other is entered on the fifteenth day of the Competition, the former would take third place).",,,
552,"Risk varies widely from customer to customer, and a deep understanding of different risk factors helps predict the likelihood and cost of insurance claims. The goal of this competition is to better predict Bodily Injury Liability Insurance claim payments based on the characteristics of the insured customer’s vehicle.
Many factors contribute to the frequency and severity of car accidents including how, where and under what conditions people drive, as well as what they are driving. 
Bodily Injury Liability Insurance covers other people’s bodily injury or death for which the insured is responsible.   The goal of this competition is to predict Bodily Injury Liability Insurance claim payments based on the characteristics of the insured’s vehicle.   ","The training data consists of observations from 2005 to 2007. Observations from 2008 make up the test data used to score the public leaderboard (during the competition), and observations from 2009 make up the test data used for the private leaderboard (for final scoring).
The metric (for both the leaderboard and final winners) used to score entries will be ""normalized Gini coefficient"" (named for the similar Gini coefficient/index used in Economics).
When you submit an entry, the observations are sorted from ""largest prediction"" to ""smallest prediction"". This is the only step where your predictions come into play, so only the order determined by your predictions matters. Visualize the observations arranged from left to right, with the largest predictions on the left. We then move from left to right, asking ""In the leftmost x% of the data, how much of the actual observed loss have you accumulated?"" With no model, you can expect to accumulate 10% of the loss in 10% of the predictions, so no model (or a ""null"" model) achieves a straight line. We call the area between your curve and this straight line the Gini coefficient.
There is a maximum achievable area for a ""perfect"" model. We will use the normalized Gini coefficient by dividing the Gini coefficient of your model by the Gini coefficient of the perfect model.","Each row contains one year’s worth information for insured vehicles.  Since the goal of this competition is to improve the ability to use vehicle characteristics to accurately predict insurance claim payments, the response variable (dollar amount of claims experienced for that vehicle in that year) has been adjusted to control for known non-vehicle effects. Some non-vehicle characteristics (labeled as such in the data dictionary) are included in the set of independent variables.  It is expected that no “main effects” corresponding will be found for these non-vehicle variables, but there may be interesting interactions with the vehicle variables. 
Calendar_Year is the year that the vehicle was insured.  Household_ID is a household identification number that allows year-to-year tracking of each household. Since a customer may insure multiple vehicles in one household, there may be multiple vehicles associated with each household identification number. ""Vehicle"" identifies these vehicles (but the same ""Vehicle"" number may not apply to the same vehicle from year to year). You also have the vehicle’s model year and a coded form of make (manufacturer), model, and submodel.  The remaining columns contain miscellaneous vehicle characteristics, as well as other characteristics associated with the insurance policy.  See the ""data dictionary"" (data_dictionary.txt) for additional information.
Our dataset naturally contained some missing values. Records containing missing values have been removed from the test data set but not from the training dataset. You can make use of the records with missing values, or completely ignore them if you wish. They are coded as ""?"".
There are two datasets to download: training data and test data. You will use the training dataset to build your model, and will submit predictions for the test dataset. The training data has information from 2005-2007, while the test data has information from 2008 and 2009. Submissions should consist of a CSV file. Records from 2008 will be used to score the leaderboard, and records from 2009 will be used to determine the final winner.",kaggle competitions download -c ClaimPredictionChallenge,[]
553,"   The universe isn't behaving. Or at least, that's the view of many of the world's leading scientists: the universe behaves as if there is far more matter than we can observe. And that's important, because it means either that vital scientific theories are wrong, or that there are whole new types of stuff that we haven't yet discovered.
Mapping Dark Matter is a image analysis competition whose aim is to encourage the development of new algorithms that can be applied to challenge of measuring the tiny distortions in galaxy images caused by dark matter.
The aim is to measure the shapes of galaxies to reconstruct the gravitational lensing signal in the presence of noise and a known Point Spread Function. The signal is a very small change in the galaxies’ ellipticity, an exactly circular galaxy image would be changed into an ellipse; however real galaxies are not circular.
The challenge is to measure the ellipticity of 100,000 simulated galaxies.
The data consists of :
Galaxy images, that are very noisy images of elliptical objects with a simple brightness profile. The galaxy images are convolved or smoothed with a kernel that would act to turn a single point into a blurry image. Part of the challenge is to attempt to remove or account for that blurring effect.
      To help account for the blurring effect each galaxy image has a star image where we provide a pixelised version of the kernel that with which the galaxy image was convolved.
Participants are provided with 100,000 galaxy and star pairs. A participant should provide an estimate for the ellipticity for each galaxy.
        * NASA Support : Jet Propulsion Laboratory, operated by the California Institute of Technology, under contract with the National Aeronautics and Space Administration (NASA). 
* RAS Support : Through funding for the PI (Kitching)","The evaluation will be calculated by taking the RMS of the true e1 e2 values in comparison to the submitted values.
Note that the mean ellipticity should not be assumed to be zero. 
See the Submission Instructions page for more details.","The data consists of training data and test data.
Training Data
Galaxy images (galaxy+convolution kernel): 40,000 postage stamp png files, each file contains a simulated galaxy image.
Star images (convolution kernel only): 40,000 postage stamp png files, each file contains a pixelised version of the convolution kernel.
Test Data
Galaxy images (galaxy+convolution kernel): 60,000 postage stamp png files, each file contains a simulated galaxy image.
Star images (convolution kernel only): 60,000 postage stamp png files, each file contains a pixelised version of the convolution kernel.
For each galaxy postage stamp numbered mdm_galaxy_nnnn.png the convolution kernel is provided n the star image mdm_star_nnnn.png .
We also provide a short PDF showing the main effects that happen to astronomical images : blurring (convolution kernel), pixelisation and noise. The challenge is to measure the galaxy ellipticity in presence of these effects. 
Files ",kaggle competitions download -c mdm,"['https://www.kaggle.com/code/mpwolke/galaxy-images', 'https://www.kaggle.com/code/mpwolke/mapping-dark-matter-image-zip']"
554,"This competition challenges data-mining experts to build a predictive model that predicts the number of edits an editor will make in the five months after the end date of the training dataset. The dataset is randomly sampled from the English Wikipedia dataset from the period January 2001 - August 2010.
The objective of this competition is to quantitively understand what factors determine editing behavior. We hope to be able to answer questions, using these predictive models, why people stop editing or increase their pace of editing.
Contestants are expected to build a predictive model that can be reused by the Wikimedia Foundation to forecast long term trends in the number of edits that we can expect.","A contestant’s model should predict, for each editor from the dataset, the number of edits made in the first 6 namespaces of the English Wikipedia between September 1st, 2010 and February 1st, 2011. The dataset contains a sample set of editors and their full set of edits. The winning submission will be used by the Wikimedia Foundation in their analytics portfolio. Models that are both accurate and run in a reasonable time are more useful to the Wikimedia Foundation. We will use the Root Mean Squared Logarithmic Error (“RMSLE”) to measure the accuracy of an algorithm.
The RMSLE is calculated as
\[ \epsilon = \sqrt{\frac{1}{n} \sum_{i=1}^n (\log(p_i + 1) - \log(a_i+1))^2 }\]
Where:
\\(\epsilon\\) is the RMSLE value (score)
\\(n\\) is the total number of editors in the (public/private) data set
\\(p_i\\) is your predicted edits value for editor \\(i\\) in the 5 month period
\\(a_i\\) is the actual edits for editor \\(i\\) in the 5 month period
\\(\log(x)\\) is the natural logarithim of \\(x\\)
Our own internally developed prediction model scores an RMSLE of 1.47708. Your submission should at a minimum beat this prediction to be eligible (see our Rules as well).
Quick reminder of how we calculate the RSMLE for your prediction (this is purely for illustrative purposes) 
Actual number
of Edits
Predicted number
of Edits
Delta
Squared Logarithmic
Error
0
1
-1
0.480453014
0
0.5
-0.5
0.164401954
1
1.5
-0.5
0.049793044
1
2
-1
0.164401954
2
3
-1
0.082760975
0
2
-2
1.206948961
3
5
-2
0.164401954
5
1
4
1.206948961
5
0
5
3.210401996
5
10
-5
0.367400612
90
100
-10
0.010870358
1000
2000
-1000
0.479760636
10000
5000
5000
0.480314415
   RMSLE
  0.787833389","During the period January 2001 - August 2010, 4.012.171 non-deleted registered editors made in total 272.213.427 edits on the English Wikipedia. A registered editor is a person who has created a useraccount on the English Wikipedia and has used this account to edit Wikipedia.
The datafile consists of 44.514 sampled editors who contributed a total of 22.126.031 edits. For each editor all edits are included as long as they were made in the namespace range 0 to 5. The cumulative number of edits made by editors is highly skewed, most editors will have made less than 10 edits while the maximum number of edits made by a single editor in this dataset is 334.173.
user_id (INT):  id of the editor who made the revision. This has been randomly recoded and does not match an editor id from the Wikimedia website. This variable can be stored as an integer.
article_id (INT):  id of the article to which the revision belongs. This variable can be stored as an integer.
revision_id (INT): id of the revision. This variable can be stored as an integer.
namespace (TINYINT):  the name space of the article: Main (0), Talk (1), User (2), User Talk (3), Wikipedia (4), Wikipedia Talk (5). This means that some namespaces are missing from the trainingset, including: 
6    File
7    File talk
8    MediaWiki",kaggle competitions download -c wikichallenge,[]
555,"Please note: This competition is over! The leaderboard now displays the final results.
This means that the only people can download the data or make submissions are people who accepted the competition rules prior to 06:59:59 UTC on October 4, 2012. Individuals who had accepted to rules but not yet formed a team at that date may join a team or create their own team (consisting of them only). No teams may merge at this point.
------------------
More than 71 million individuals in the United States are admitted to hospitals each year, according to the latest survey from the American Hospital Association. Studies have concluded that in 2006 well over $30 billion was spent on unnecessary hospital admissions. Is there a better way? Can we identify earlier those most at risk and ensure they get the treatment they need? The Heritage Provider Network (HPN) believes that the answer is ""yes”.
To achieve its goal of developing a breakthrough algorithm that uses available patient data to predict and prevent unnecessary hospitalizations, HPN is sponsoring the Heritage Health Prize Competition (the “Competition”). HPN believes that incentivized competition is the best way to achieve the radical breakthroughs necessary to begin fixing America’s health care system.
The winning team will create an algorithm that predicts how many days a patient will spend in a hospital in the next year. Once known, health care providers can develop new care plans and strategies to reach patients before emergencies occur, thereby reducing the number of unnecessary hospitalizations. This will result in increasing the health of patients while decreasing the cost of care. In short, a winning solution will change health care delivery as we know it – from an emphasis on caring for the individual after they get sick to a true health care system.
The Competition runs for two years and offers a US $3 million Grand Prize, as well as six Milestone Prizes totaling $230,000, which are awarded in varying amounts at three designated intervals during the Competition.","IMPORTANT NOTE: The information provided below is intended only to provide general guidance to participants in the Heritage Health Prize Competition and is subject to the Competition Official Rules. Any capitalized term not defined below is defined in the Competition Official Rules. Please consult the Competition Official Rules for complete details.
Entries will be judged based on the degree of accuracy of their predictions of DaysInHospital for Y4 (or if applicable Y5), carried to six (6) decimal places. An entry’s predictive accuracy will be judged by comparing (i) the predicted number of days a member will spend in the hospital reflected in the entry and (ii) the actual number of days a member spent in the hospital as reflected in the Scoring Data Set. Prediction accuracy will be evaluated based on the following metric:
Where:
1. i is a member;
2. n is the total number of members;
3. p is the predicted number of days spent in hospital for member i in the test period;
4. a is the actual number of days spent in hospital for member i in the test period.
Note: the metric is calculated using the natural log.
Entrants may submit one (1) entry during each calendar day (UTC) beginning on May 4, 2011 and ending at 06:59:59 UTC on April 3, 2013. Eligible Milestone Prize Entries judged to have the two lowest prediction scores as of the applicable Milestone Prize Deadline will win Milestone Prizes. The eligible Grand Prize Entry that the judges determine produces the lowest prediction score that passes the Accuracy Threshold as of the end of the Competition (06:59:59 UTC on April 4, 2013) will win the Grand Prize of US $3 million. The Accuracy Threshold is the accuracy of predictions required to win the Grand Prize. That maximum accuracy is 0.4.","Note: This competition is NOW CLOSED. DATA IS NO LONGER AVAILABLE FOR ANY REASON.

IMPORTANT NOTE: The information provided below is intended only to provide general guidance to participants in the Heritage Health Prize Competition and is subject to the Competition Official Rules. Any capitalized term not defined below is defined in the Competition Official Rules. Please consult the Competition Official Rules for complete details.
Heritage Provider Network is providing Competition Entrants with deidentified member data collected during a forty-eight month period that is allocated among three data sets (the ""Data Sets""). Competition Entrants will use the Data Sets to develop and test their algorithms for accurately predicting the number of days that the members will spend in a hospital (inpatient or emergency room visit) during the 12-month period following the Data Set cut-off date.
HHP_release3.zip contains the latest files, so you can ignore HHP_release2.zip. SampleEntry.CSV shows you how an entry should look.
Data Sets will be released to Entrants after registration on the Website according to the following schedule:
April 4, 2011 Claims Table - Y1 and DaysInHospital Table - Y2
May 4, 2011 All other Data Sets except Labs Table and Rx Table",kaggle competitions download -c hhp,[]
556,"Driving while distracted, fatigued or drowsy may lead to accidents. Activities that divert the driver's attention from the road ahead, such as engaging in a conversation with other passengers in the car, making or receiving phone calls, sending or receiving text messages, eating while driving or events outside the car may cause driver distraction. Fatigue and drowsiness can result from driving long hours or from lack of sleep.

The objective of this challenge is to design a detector/classifier that will detect whether the driver is alert or not alert, employing any combination of vehicular, environmental and driver physiological data that are acquired while driving.

This competition requires you to choose five entries that count towards the final result. To choose five entries visit your submissions page and click the star next to the relevant entry to select it. If you do not choose any entries, your last five entries will be chosen by default.

The winner receives free registration to the 2011 International Joint Conference on Neural Networks (San Jose, California July 31 - August 5, 2011), which is valued at $950. The winner will also be invited to present their solution at the conference.","Entries will be evaluated using the area under the receiver operator curve (AUC). AUC was first used by the American army after the attack on Pearl Harbour, to detect Japanese aircraft from radar signals.

Today, it is a commonly used evaluation method for binary choose problems, which involve classifying an instance as either positive or negative. Its main advantages over other evaluation methods, such as the simpler misclassification error, are: 

1. It's insensitive to unbalanced datasets (datasets that have more installeds than not-installeds or vice versa).

2. For other evaluation methods, a user has to choose a cut-off point above which the target variable is part of the positive class (e.g. a logistic regression model returns any real number between 0 and 1 - the modeler might decide that predictions greater than 0.5 mean a positive class prediction while a prediction of less than 0.5 mean a negative class prediction). AUC evaluates entries at all cut-off points, giving better insight into how well the classifier is able to separate the two classes.

Understanding AUC

To understand the calculation of AUC, a few basic concepts must be introduced. For a binary choice prediction, there are four possible outcomes:
true positive - a positive instance that is correctly classified as positive;
false positive - a negative instance that is incorrectly classified as positive;
true negative - a negative instance that is correctly classified as negative;
false negative - a positive instance that is incorrectly classified as negative);
These possibilities can be neatly displayed in a confusion matrix:


actual class

  P N
predicted class p true positive false positive
n false negative true negative

The true positive rate, or recall, is çalculated as the number of true positives divided by the total number of positives. When identifying aircraft from radar signals, it is proportion that are correctly identified.

The false positive rate is çalculated as the number of false positives divided by the total number of negatives. When identifying aircraft from radar signals, it is the rate of false alarms.

If somebody makes random guesses, the ROC curve will be a diagonal line stretching from (0,0) to (1,1) - see the blue line in the figure below. To understand this consider:
 Somebody who randomly guesses that 10 per cent of all radar signals point to planes. The false positive rate and the false alarm rate will be 10 per cent.
 Somebody who randomly guesses that 90 per cent of all radar signals point to planes. The false positive rate and the false alarm rate will be 90 per cent.
Meanwhile a perfect model will achieve a true positive rate of 1 and a false positive rate of 0.



While ROC is a two-dimensional representation of a model's performance, the AUC distils this information into a single scalar. As the name implies, it is calculated as the area under the ROC curve. A perfect model will score an AUC of 1, while random guessing will score an AUC of around of 0.5. In practice, almost all models will fit somewhere in between","The data for this challenge shows the results of a number of ""trials"", each one representing about 2 minutes of sequential data that are recorded every 100 ms during a driving session on the road or in a driving simulator.  The trials are samples from some 100 drivers of both genders, and of different ages and ethnic backgrounds. The files are structured as follows:

The first column is the Trial ID - each period of around 2 minutes of sequential data has a unique trial ID. For instance, the first 1210 observations represent sequential observations every 100ms, and therefore all have the same trial ID
The second column is the observation number - this is a sequentially increasing number within one trial ID
The third column has a value X for each row where
               X = 1     if the driver is alert
               X = 0     if the driver is not alert
The next 8 columns with headers P1, P2 , …….., P8  represent physiological data;
The next 11 columns with headers E1, E2, …….., E11  represent environmental data;
The next 11 columns with headers V1, V2, …….., V11  represent vehicular  data;

The third column values are hidden in the test set ('fordTest.csv').

The file 'example_submission.csv' is an example of a submission file - your submission files should be in exactly the same format, with only values in the last column ('Prediction') different. Predictions are expected to be real numbers between 0 and 1 inclusive.
  Note:  The actual names and measurement units of the physiological, environmental and vehicular data are not disclosed in this challenge. Models which use fewer physiological variables (columns with names starting with 'P') are of particular interest, therefore competitors are encouraged to consider models which require fewer of these variables.",kaggle competitions download -c stayalert,"['https://www.kaggle.com/code/nikhilroeewal/stay-alert-logistic-dt-and-randomforest', 'https://www.kaggle.com/code/ukawaladhruv/lg-rf-and-dt', 'https://www.kaggle.com/code/manjupoovathingal/fordalert-1', 'https://www.kaggle.com/code/ashokkumar071994/fordstayalert', 'https://www.kaggle.com/code/naokiyokoyama/ford-notebook']"
557,"One of the main objectives of predictive modelling is to build a model that will give accurate predictions on unseen data.

A necessary step in the building of models is to ensure that they have not overfit the training data, which leads to sub optimal predictions on new data.

The purpose of this challenge is to stimulate research and highlight existing algorithms, techniques or strategies that can be used to guard against overfitting.

In order to achieve this we have created a simulated data set with 200 variables and 20,000 cases. An ‘equation’ based on this data was created in order to generate a Target to be predicted. Given the all 20,000 cases, the problem is very easy to solve – but you only get given the Target value of 250 cases – the task is to build a model that gives the best predictions on the remaining 19,750 cases.

This competition is of particular relevance to medical data analysis, where often the number of cases is severely restricted.","This is a classification problem. The AUC on the unseen portion of the Leaderboard model will be used to determine those competitors that qualify for the final shootout.

All competitors who beat a 'Benchmark' AUC with any of their submissions will qualify to submit ONE set of predictions for the Evaluation model. These must be returned via email within 24 hrs of the competition finishing.

The winner of Part A will be the competitor with the best AUC on this Evaluation model.

All qualifying entrants will also be asked to submit a list of all the variables (1-200) and say whether or not they are in the 'equation' that generated the Evaluation model. The winner of Part B will be the competitor that scores the best variable selection score, based on the following formula:

score 1 point if a variable is correctly identified
score -1 point if a variable is incorrectly identified

The 'Benchmark' AUC might vary through the course of the competition. The methodology used to set the benchmark will be described in the forum.

The top three entrants in each part will be asked to describe their techniques on the Kaggle blog, within 1 week of the Evaluation sets being submitted. Once the blog entries have been made, the winners will be announced.

It is recommended the Evaluation predictions are developed in tandem with the Leaderboard, so the Evaluation submissions can be made immediately the competition finishes.
 ","The data file contains 200 randomly generated variables, var_1 to var_200.

There are 20,000 rows of data, of which you are only given the 'Target' for the first 250. The 'Target' is either 1 or 0, so this is a classification problem.

There are also 5 other fields,

case_id - 1 to 20,000, a unique identifier for each row

train - 1/0, this is a flag for the first 250 rows which are the training dataset

Target_Practice - we have provided all 20,000 Targets for this model, so you can develop your method completely off line.

Target_Leaderboard - only 250 Targets are provided. You submit your predictions for the remaining 19,750 to the Kaggle leaderboard.

Target_Evaluate - again only 250 Targets are provided. Those competitors who beat the 'benchmark' on the Leaderboard will be asked to make one further submission for the Evaluation model.

The three models (Practice, Leaderboard & Evaluate) are all based on the same underlying data, but the generated 'equation' is different for each. The equations are of a similar form, but the underlying model parameters differ.

The values to be predicted are represented as '-99' in the downloaded data.",kaggle competitions download -c overfitting,['https://www.kaggle.com/code/mpwolke/overfitting-12-years-ago']
558,"Writer identification is important for forensic analysis, helping experts to deliberate on the authenticity of documents. This competition aims to further the science of writer identification. It requires participants develop algorithms that can identify handwriting. This is a difficult problem because a writer never reproduces exactly the same characters. 

Writer identification generally requires two steps. The first is an image-processing step, where features are extracted from the images. The second step is a classification step, where the document is assigned to the “closest” document in the dataset according to the “difference” between their features.

In this contest, a previously unpublished data has been made available, containing the writings of more than 50 writers. Participants are asked to provide a similarity score, showing how probable it is that two documents are written by the same person. For participants who are not familiar with image-processing, a set of geometrical features extracted have been provided.

This contest is organized in conjunction with the International Conference on Document Analysis and Recognition (ICDAR2011). The winner of this contest will win $1,000 and will be acknowledged in a special session at the ICDAR2011, which will be held in Beijng in September 18-21. The prize money is paid on condition that the winning entrant shares their methodology. ",,"In this contest, more than 50 writers were asked to write three different paragraphs in Arabic language. The first two paragraphs are used for training and the third one for testing. (For some writers, the first two paragraphs have been removed from the training set to test the ability of systems to detect unknown writers.)

Images are provided in PNG color, gray and binary format. The binarization has been performed using Otsu's method. Each folder contains two subfolders “train” and “test”. The folder “train” contains images having the following format XXX_Y.png where XXX represents the ID of the writer and Y represents the number of the paragraph.
The folder “test” contains images of the third paragraph, they all have the following format ZZ.png, where ZZ is a randomly generated string.

Participants are asked to provide the probability that a ZZ images corresponds to a certain XXX writer. Also, like mentioned before, some ZZ images do not actually correspond to any writer in the training set. Participants are asked to provide the probability that the writer is unknown.

The competition is judged using the mean absolute error evaluation metrics. 

For participants who are not familiar with image processing, more than 70 features are provided. Some of these features are real values, others are histograms. The list below shows the name and the number of values in each of these features.
NumberOfConnectedComponents_1 (1 value)
We are particularly interested in methods that uses only those features but usage of other features is most welcome.
The submissions must be in the following format: The first line contains ranked XXX names and then “unknown”. First column contains ranked ZZ images. Each cell contains the probability that a ZZ images corresponds to a certain XXX writer. Cells of the last column contain the probability that a ZZ image corresponds to an unknown writer. The sum of probabilities does not have to be 1, neither by line, nor by column and not even for the whole file! Please see for an example.",kaggle competitions download -c WIC2011,[]
559,"The datasets for this contest include real historical data provided by the world chess federation (FIDE).  Contest participants will train their rating systems using a training dataset of over 1.84 million game results for more than 54,000 chess players across a recent eleven year period.  Participants then use their method to predict the outcome of a further 100,000 games played among those same players during the following three months.  Contest entries will be scored automatically by the website, based on the accuracy of their predictions.  Two entries per day can be submitted by each team, and prizes will be determined according to each team's best-scoring single submission.

The contest's sponsor, Deloitte Australia, has provided the $10,000 prize to be awarded to the team that submits the most accurate predictions.  Deloitte is a preeminent provider of analytics globally and helps companies capture, manage and analyze their data as part of their overall business strategy.

Chessbase has donated chess software with signatures by famous players for the three teams finishing in 2nd/3rd/4th place.

In addition, FIDE representatives will award a special ""FIDE prize"" to what they consider to be the most promising approach, out of the ten most accurate entries that meet a restrictive definition of a ""practical chess rating system"".  This restrictive definition is specified within the rules of the contest.  For the winner, FIDE will provide air fare for a round trip flight to Athens, Greece, and full board for three nights in Athens, and payment toward other expenses, for one person to present and discuss their system during a special FIDE meeting of chess rating experts in Athens.","Contest submissions will contain the predicted White score in each of the 100,000 games in the test set.  The predictions for individual games will be scored separately and the aggregate ""deviance"" calculated as the average deviance across all games.  Also note that several thousand of the games are spurious (fake) matchups, included in order to discourage participants from ""mining"" the test set for additional information about each player's strength.  These games are ignored by the evaluation function.

The evaluation function used for scoring submissions will be the ""Binomial Deviance"" (or ""Log Likelihood"") statistic suggested by Mark Glickman, namely the mean of:

-[Y*LOG10(E) + (1-Y)*LOG10(1-E)]

per game, where Y is the game outcome (0.0 for a black win, 0.5 for a draw, or 1.0 for a white win) and E is the expected/predicted score for White, and LOG10() is the base-10 logarithm function.

The winning single submission will be the one having the minimum Binomial Deviance.  It is easy to see that predicting an expected score of 0% or 100% has an undefined Binomial Deviance, since LOG10(0) is undefined.  Further, even if White wins, the marginal benefit for a prediction of slightly above 99% is minimal compared to a 99% prediction.  And similarly, even if Black wins, the marginal benefit for a prediction of slightly below 1% is minimal compared to a 1% prediction.  Therefore there is really no good reason to predict an expected score above 99% or below 1%.  So for purposes of scoring, all predictions of a White score above 99% will be treated as 99%, and all predictions of a White score below 1% will be treated as 1%.  You can ""cap"" your predictions at 1% and 99% yourself, or you can let the scoring function do it for you; either way your score will be the same.

Here is an example of the calculation of the evaluation function for five sample games in a test set, where the participant was asked to predict the outcome for five games, one of which was actually a spurious (fake) matchup:

Game #1 (predicted White score = 0.650, actual White score = 0.50)
Game #2 (predicted White score = 0.220, actual White score = 0.00)
Game #3 (predicted White score = 0.000, actual White score = 0.50)
Game #4 (predicted White score = 0.330, game is spurious and therefore not scored)
Game #5 (predicted White score = 0.999, actual White score = 1.00)

The individual binomial deviances for each game would be:

Deviance for #1 = -[0.50*LOG10(0.65) + (1-0.50)*LOG10(1-0.65)] = 0.321509
Deviance for #2 = -[0.00*LOG10(0.22) + (1-0.00)*LOG10(1-0.22)] = 0.107905
Deviance for #3 = -[0.50*LOG10(0.01) + (1-0.50)*LOG10(1-0.01)] = 1.002182
Deviance for #4 = (ignored)
Deviance for #5 = -[1.00*LOG10(0.99) + (1-1.00)*LOG10(1-0.99)] = 0.004365

And the overall binomial deviance score for that submission would be the average of 0.321509, 0.107905, 1.002182, and 0.004365, or 0.358990.  Therefore the overall score of this submission (across these five rows from the test set) would be 0.358990.  Please notice how the predictions in games #3 and #5 were ""capped"" at 0.01 and 0.99 for purposes of the binomial deviance calculation, and how game #4 was not scored because it was spurious.

This graph provides a visual indication of the relative deviance contributions for each combination of actual outcomes and predicted scores:","The data files cover a recent 135-month period of professional chess game outcomes, extracted from the database of the world chess federation (FIDE).  The training period spans 132 months, followed immediately by a three-month test period (months 133-135).  The specific dates are not provided; all games simply have an integer Month ID# ranging from 1 to 135.  There are 54,205 different chess players included in the data; those players are uniquely identified by an ID ranging from 1 to 54,205.

There are eight different files provided as part of the contest:

(1-3) The primary training dataset (including 1,840,124 games) can be used to train your prediction system.  It is split up into three separate files: part 1 (months 1-96), part 2 (months 97-118), and part 3 (months 119-132).

(4-5) A secondary training dataset (including 312,511 games) and tertiary training dataset (including 265,577 games) provide optional additional training data that may be useful in validating your system or predicting game results.  The contest does not require you to use these additional training datasets, but most systems will benefit significantly from using the secondary and tertiary training datasets. More details can be found on the Additional Training Datasets page.

(6) A test dataset (including 100,000 games) identifies the chess games during the test period that must be predicted.  Also note that an example submission file is provided on the Submission Instructions page.

An initial rating list (including 14,118 players) provides an initial list of players' FIDE ratings going into the start of the training period.  This represents approximately 25% of the pool of players; initial ratings for the remaining 75% of players must be calculated based on their initial games during the training period.  Remember that anyone wishing to qualify for the FIDE prize category must use this list to determine the initial ratings of these 14,118 players.

An example submission file (including 100,001 rows) is formatted appropriately for submission to the contest, containing a prediction of an expected score of 50% for all 100,000 games in the test set.  This is identical to the All Draws Benchmark.  It includes a header row followed by predictions for test games #1, #2, #3, …, #99,998, #99,999, #100,000, for a total of 100,001 rows.  Your actual submissions should be exactly like this file except that it will contain your actual predictions for every game, instead of a universal 50%.



There are 1,840,124 games in the primary training set, spanning a recent eleven-year period (known as Month #1 through Month #132).  They represent 35-40% of the almost five million game results that were used by FIDE (the world chess federation) to calculate player ratings during that time.  Many of those five million game results are not individually available in computer files, and so they had to be partially reconstructed from other sources for this contest; this is why there are ""only"" 1.84 million games in the primary training set.  For much of the training period, the primary training set contains about 25% of the full set of FIDE games used for official rating calculations.  The percentage is much higher in the final two or three years of the training period, thanks to recent changes in FIDE's regulations for how tournament organizers must submit their tournament's results to FIDE.  Recent data is much more readily available.

There are three possible outcomes to a completed chess game - either White wins, or the game is drawn, or Black wins.  These correspond to a ""score"" for White of 1.0, 0.5, or 0.0 points, and conversely a score for Black of 0.0, 0.5, or 1.0 points.  Tournament scoring typically adds up a player's total score, so someone who wins three games, draws five games, and loses one game would have a total score of 5.5 out of 9 games, and (despite having fewer wins) would finish ahead of someone else who wins five games, draws zero games and loses four games (total score of 5.0 out of 9).  Thus it is important to consider not just how many games a player manages to win, but also how many losses they manage to avoid.  White always gets to move first and so there is a slight advantage to having the white pieces (more pronounced among the strongest players).  For instance, in the primary training set, there are 125,000 games where the absolute difference in FIDE ratings between the players was fewer than 20 points, and the average score for White in those games was 53.7%.

The primary training dataset includes 1,840,124 total games, each of which is assigned a unique Primary Training Game ID# (called PTID) between 1 and 1,840,124.  For each game, the PTID, and the Month ID (between 1 and 132), and the player ID #'s for the White and Black players, are provided, as well as the outcome of the game from White's perspective (a score of either 1.0, 0.5, or 0.0).  The FIDE ratings of the players are not provided in the primary training dataset; part of the challenge is for you to make your own assessments of the players' changing strengths over time.

Finally, the primary training dataset provides two redundant values (named WhitePlayerPrev and BlackPlayerPrev) for each game result, indicating the quantity of games (in the primary training set) played by each of the two players within the previous 24 months.  These values could actually be calculated by reviewing the data from earlier months in the primary training dataset, rather than being explicitly provided, but for your convenience they have been explicitly provided for each training game.

Why are WhitePlayerPrev and BlackPlayerPrev relevant?  Well, the pool of active players is always increasing in size.  Thus in any given month, many of the games that are played involve players who have previously played a very small number of games (or none at all).  It seemed unfair to include many of those games in the test set, since the predictions would necessarily be quite uncertain.  So in the creation of the test set for months 133-135, there was a filter applied.  Instead of including all games played during months 133-135, the test dataset only includes games in which both players had at least 12 games played during the final 24 months of the primary training dataset (i.e. months 109-132).  Participants may wish to perform cross-validation, drawing samples from the primary training set that would have similar characteristics to the test dataset, in order to develop a model or optimize parameters in the model.  In order to create your own validation sets from the training data, you would presumably want to apply the same sampling filter that was used to create the test set (requiring WhitePlayerPrev > 12 and BlackPlayerPrev > 12).  Including the values of WhitePlayerPrev and BlackPlayerPrev in the training games allows you to do this.

For instance, if your validation test set was drawn from the games of month 130 only, and you only selected games from that month having WhitePlayerPrev > 12 and BlackPlayerPrev > 12, then you would only be getting games in which both players had at least 12 games played in the previous 24 months of the primary training set (i.e. months 106-129).  And this filter is analogous to what was done to filter the games for the test set.



The test dataset (covering 100,000 games between months 133-135) is very similar to the primary training dataset, although it intentionally omits the WhiteScore column.  Of course the point of the contest is to have participants accurately predict the WhiteScore for all games in the test dataset, so the test dataset only indicates the identities of the players, and the month in which the game was played, and does not tell you the game outcomes.  The test dataset includes all known games played in months 133-135 where both players had played 12 or more games in the final 24 months of the primary training dataset.

The test dataset includes 100,000 total games, each of which is assigned a unique Test Game ID# (called TEID) between 1 and 100,000.  For each game, the TEID, and the Month ID (between 133 and 135), and the player ID's for the White and Black players, are provided.  The TEID is particularly important because you will use it to uniquely identify each test game when submitting predictions, and to sort the rows in your submission file.

Please note that you should NOT use the test dataset as an additional source of clues about a player's strength.  The predictions for months 133-135 should be based upon the players' estimated playing abilities at the end of month 132, and these predictions must be completely prospective, as though you made the predictions right at the end of month 132.  We could have asked for a complete cross product (54,205 x 54,205 x 3) of predictions of all possible games, instead of just the 100,000 games from the test set, but this would have been too large a file to submit.  Instead, in order to discourage participants from ""mining"" the test dataset for additional information, the test dataset of 100,000 games includes many thousands of spurious matchups.  The predictions for those particular games will be discarded and not used when scoring submissions.  The method for generating the spurious matchups is not random, and is not being revealed.  The spurious games are intentionally being included, in order to encourage people to only predict prospectively.



The contest datasets do not include games for all players in the FIDE rating pool.  There are many isolated pockets of chess players around the world, playing only within their geographical region or age group, who are not well-connected with the rest of the pool of players.  We wanted to focus on a large pool of reasonably well-connected, reasonably active players in this contest, spanning the whole range of playing strength from weakest to strongest.  Therefore an iterative selection process was performed, starting with the four most prominent world champions (Viswanathan Anand, Garry Kasparov, Vladimir Kramnik, and Veselin Topalov) during that span.  They formed the initial pool of four players, and then additional players were iteratively added to the existing pool if they were well-connected to it.  The final pool represents the complete population of players whose games are included in the contest.  For these players, all known games during the appropriate time periods are included; no data was omitted intentionally for them.

So first there were four players in the pool, and then we looked to see how many other players were ""well-connected"" to those four initial players.  There are 19 players who played against all four of those opponents, having at least 15 games total against those four opponents as a group, during the eleven-year period.  Those 19 players, who can be thought of as ""one degree removed from a world champion"", were added to the pool, bringing it up to 23 total players.  Then, another iteration was performed against the pool of 23 players, identifying an additional 141 players who faced at least four different opponents in the pool of 23 players, and also played at least 15 games total against that pool, during the eleven-year period.  Those 141 players, ""two degrees removed from a world champion"", were added to the pool, bringing it up to a total of 164 players.  This process was performed eight additional times, always pulling in additional players who had played at least four different opponents in the existing pool, and at least 15 games total against existing pool members.  The final iteration brought in 2,007 new players (all of whom can be thought of as being ""ten degrees separated from a world champion"") resulting in a final pool of 54,205 players: exactly the players who are included in the contest datasets.  Each player was randomly assigned an ID# from 1 to 54,205, and players in the contest are always referenced by this ID# rather than by name.

Please note that the source games included in the secondary training dataset and tertiary training dataset were not used during this iterative process of forming the pool of chess players.  The secondary and tertiary training datasets are optional and therefore it was desirable to make sure the pool of players was well-connected within the primary training dataset only.  Analysis of the primary training dataset will confirm that all players in the contest have at least 15 games in the primary training dataset, including at least 4 different opponents.



Most rating systems require an initial rating list in order to start the system.  In these systems, players can only receive new ratings through playing games against opponents who already have ratings themselves, and therefore the system cannot start from nothing, as nobody would ever get the first rating.  A notable exception is the Chessmetrics rating algorithm, which does not use prior rating lists and always generates a rating list using only game results.

In the previous contest we did not provide an initial rating list; participants were required to generate their own ratings using their own approach.  This essentially created a ""blended"" contest, where you had to develop a good algorithm to generate initial ratings and also a separate algorithm to update existing ratings.  Some people even implemented the Chessmetrics algorithm just to create their initial ratings.  In order to avoid the ""blended"" competition, and to focus mainly on optimizing the process for updating players' existing ratings, we decided to include initial ratings for everyone who actually had a FIDE rating going into month #1.  Almost 75% of the 54,205 players in the contest did not yet have a rating at that point, so the initial rating list only contains 14,118 players, but it still should suffice to seed the rating pool sufficiently so that it can increase to include all (or almost all) of the test players by the end of the training period.  In addition, the initial ratings have been randomly adjusted up or down by several rating points in order to slightly anonymize the list and discourage contest participants from decoding the list and determining the identities of each player.  Note that this behavior is explicitly forbidden by the rules of the contest.",kaggle competitions download -c ChessRatings2,[]
560,"Around the world, the pool of funds available for research grants is steadily shrinking (in a relative sense). In Australia, success rates have fallen to 20-25 per cent, meaning that most academics are spending valuable time making applications that end up being rejected.

With this problem in mind, the University of Melbourne is hosting a competition to predict the success of grant applications. The winning model will be used by the university to predict which grant applications are likely to be successful, so that less time is wasted on applications that are unlikely to succeed. The university hopes the competition will also shed some light on what factors are important in determining whether an application will succeed.

The university has provided a dataset containing 249 features, including variables that represent the size of the grant, the general area of study and de-identified information on the investigators who are applying for the grant. Participants train their models on 8,707 grant applications made between 2004 and 2008. They then make predictions on a further 2,176 applications made in 2009 and the first half of 2010.

The winner of this competition will receive US$5,000. To be eligible for the prize, the winning method must be implementable by the University of Melbourne.  ","Towards the end of the competition, teams will have the opportunity to nominate five entries. It is the best of these five entries that counts toward a team's final position. A team's last five entries will be chosen by default if they don't nominate any entries.

Entries will be evaluated using the area under the receiver operator curve (AUC). AUC was first used by the American army after the attack on Pearl Harbour, to detect Japanese aircraft from radar signals.

Today, it is a commonly used evaluation method for binary choice problems, which involve classifying an instance as either positive or negative (success or not in this competition). Its main advantages over other evaluation methods, such as the simpler misclassification error, are: 
It's insensitive to unbalanced datasets (datasets that have more installeds than not-installeds or vice versa).
For other evaluation methods, a user has to choose a cut-off point above which the target variable is part of the positive class (e.g. a logistic regression model returns any real number between 0 and 1 - the modeler might decide that predictions greater than 0.5 mean a positive class prediction while a prediction of less than 0.5 mean a negative class prediction). AUC evaluates entries at all cut-off points, giving better insight into how well the classifier is able to separate the two classes.

Understanding AUC

To understand the calculation of AUC, a few basic concepts must be introduced. For a binary choice prediction, there are four possible outcomes:
true positive - a positive instance that is correctly classified as positive;
false positive - a negative instance that is incorrectly classified as positive;
true negative - a negative instance that is correctly classified as negative;
false negative - a positive instance that is incorrectly classified as negative;
These possibilities can be neatly displayed in a confusion matrix:


actual class

  P N
predicted class p true positive false positive
n false negative true negative

The true positive rate, or recall, is calculated as the number of true positives divided by the total number of positives. When identifying aircraft from radar signals, it is the proportion that are correctly identified.

The false positive rate is calculated as the number of false positives divided by the total number of negatives. When identifying aircraft from radar signals, it is the rate of false alarms.

If somebody makes random guesses, the ROC curve will be a diagonal line stretching from (0,0) to (1,1) - see the blue line in the figure below. To understand this consider:
 Somebody who randomly guesses that 10 per cent of all radar signals point to planes. The false positive rate and the false alarm rate will be 10 per cent.
 Somebody who randomly guesses that 90 per cent of all radar signals point to planes. The false positive rate and the false alarm rate will be 90 per cent.
Meanwhile a perfect model will achieve a true positive rate of 1 and a false positive rate of 0.



While ROC is a two-dimensional representation of a model's performance, the AUC distils this information into a single scalar. As the name implies, it is calculated as the area under the ROC curve. A perfect model will score an AUC of 1, while random guessing will score an AUC of around of 0.5. In practice, almost all models will fit somewhere in between.","This dataset includes 249 features (or predictors). Participants should use these variables to predict the target variable (or outcome), ""Grant Status"". A grant status of 1 represents a successful grant application, while a grant status of 0 represents an unsuccessful application.

The training dataset, which participants use to build their models, is unimelb_training.csv. It contains 8,707 grant applications from late 2005 to 2008. The test dataset, unimelb_test.csv, contains 2,176 grant applications from 2009 to mid 2010. The grant status variable is withheld from the test dataset.

Predictions should take the same format as unimelb_example.csv (a CSV file with 2,176 rows, a grant application ID in the first column and a probability of success - between 0 and 1 - in the second column). 

The university has provided the following features:
Sponsor Code: an ID used to represent different sponsors
Grant Category Code: categorization of the sponsor (e.g. Australian competitive grants, cooperative research centre, industry)
Contract Value Band: the grant's value (see key below)
Start Date: the date the grant application was submitted
RFCD Code: research fields, courses and disciplines classification (see )
RFCD Percentage: if there are several RFCD codes that are relevant to a project
SEO Code: socio economic objective classification (see )
SEO Percentage: if there are several SEO codes that are relevant to a project
Person ID: the investigator's unique ID
Role: the investigator's role in the study
Year of Birth: the investigator's year of birth (rounded to the nearst five year interval)
Country of birth: the investigator's country of birth (often aggregated to by-continent)
Home Language: the investigator's native language (classified into English and Other)
Dept No: the investigator's department
Faculty No: the investigator's faculty
Grade Level: the investigator's level of seniority
No. of years in Uni at time of grant: the number of years the investigator had been at the University of Melbourne when the grant application was made
Number of Successful Grant: the number of successful grant applications the investigator had made
Number of Unsuccessful Grant: the number of unsuccessful grant applications the investigator had made
A journal articles
A: number of A journal articles
B: number of B journal articles
C: number of C journal articles",kaggle competitions download -c unimelb,"['https://www.kaggle.com/code/tiwaris436/predict-grant-applications', 'https://www.kaggle.com/code/irinana/predict-graint', 'https://www.kaggle.com/code/akarachaisaeteaw/predict-graint-gradient', 'https://www.kaggle.com/code/arpitsinha/predict-grant-applications', 'https://www.kaggle.com/code/hari141091/predict-grant-applications-1121']"
561,"Forecasting travel times helps improve road safety and efficiency. Accurate predictions help commuters make informed decisions about when to travel and on what routes. This helps to lower intensity on problem arterials by encouraging motorists to use underutilised parts of the grid, and where possible, by having them select alternative times and modes of travel. 

This competition requires participants to predict travel time on Sydney's M4 freeway from past travel time observations. In addition to better informing network managers and Australian motorists, insights from the competition will improve the general efficiency of the road transport system in Sydney and increase functionality on the government's live traffic website. 

Participants in this competition are required to forecast the travel time on the M4 freeway for 15 mins, 30 mins, 45 mins, one hour, 90mins, two hours, six hours, 12 hours, 18 hours and 24 hours ahead. The NSW Roads and Traffic Authority has made 2 years' worth of historical data on road use between 2008 and 2010  available for this competition. 

$10,000 is being offered for the winning model.

The competition is being hosted by Australia's NSW Roads and Traffic Authority and is being sponsored by the NSW Department of Premier and Cabinet.",,"The NSW Roads and Traffic Authority (RTA) has made available several years' worth of historical travel time data for Sydney's M4 freeway. The data are collected from loops on the road at three minute intervals.




The file is formated as follows
                         40010    40015    40020    …
1/03/10 15:01    804        209       804      248
1/03/10 15:04    892        212       801      237
1/03/10 15:07    857        214       821      243
…                       849        222       834      252

The header row shows the route IDs and the first column has timestamps. The cells show the travel time in deciseconds.

Route IDs are ordered sequentially. Routes 40010-40150 are westbound travel times and 41010-41160 are eastbound travel times. See m4-map.pdf for an approximate guide to the freeway's layout.

RTAData.csv holds the travel time dataset. Complete data is provided from March 2010 to July 2010. For the data from August 2010 to mid-November 2010, the letter ""x"" appears whenever a prediction is required.

29 cut-off times have been selected. After those cut-off times, predictions must be made for the next 15 minutes, 30 minutes, 45 minutes, 1 hour, 90 minutes, 2 hours, 6 hours, 12 hours, 18 hours and 24 hours. To prevent particicipants from meaningfully using the future to predict the present, data are not revealved again until 18 hours after the last forecast has been made (or 42 hours after the cut-off point).

Participants are required to provide 290 forecasts (10 for each cut-off point) for the 61 routes. The route IDs should appear in a header row, and the timestamps in the first column. The file sampleEntry.csv is an example entry, showing how entries should be formatted.

The RTA has also provided some additional historical data, RTAHistorical.csv (collected using their old system), which covers November 2008 to February 2010. The data are consistent with the newer RTA data except that:
- they don't cover weekends; and
- there are no data for routes 40092 and 41140.

The file RTAError.csv shows where loop readings are suspected to be inaccurate. Loop readings can be inaccurate because the loop is behaving erratically or because the loop is not responding at all (in which case the average travel time from adjacent loop combinations have been used).

The file RTAError.csv shows the proportion of loops in a route that have failed (a route is made up of many loops). A reading of 0 means that all loops are functioning properly, a reading of 0.5 means that 50 per cent of the loops are functioning and a reading of 1 means that no loops are functioning.

The file RouteLengthApprox.csv shows approximate route lengths. It is calculated as the number of loops in the route multiplied by 500m (the approximate distance between loops).",kaggle competitions download -c RTA,['https://www.kaggle.com/code/mpwolke/rtadata-time-prediction']
562,"Internet, telco, bank and other commercial databases are filled with information on the connections between millions of individuals who share, influence and learn from each other. It is fast becoming apparent that these relationships hold significant commercial value. So much so that Gartner, a market-research firm, ranks network analysis software at number two in its list of strategic business operations meriting significant investment this year. 

Social network analysis views relationships in terms of nodes (people) and edges (links or connections - the relationship between the people). This competition requires participants to predict edges in an online social network. The algorithms developed could be used to power friend suggestions for an online social network. Good friend suggestion algorithms are extremely valuable because they encourage connections (and the strength of an online social network increases dramatically as the number of edges increase). 

For this competition, participants are given 7,237,983 contacts/edges from an online social network. Using this data, participants must predict whether the connections among a further 8,960 edges are true or false.

The winner receives free registration to the 2011 International Joint Conference on Neural Networks (San Jose, California July 31 - August 5, 2011). The winner will also be invited to present their solution at the conference.","Entries will be evaluated using the area under the receiver operator curve (AUC). AUC was first used by the American army after the attack on Pearl Harbour, to detect Japanese aircraft from radar signals.

Today, it is a commonly used evaluation method for binary choose problems, which involve classifying an instance as either positive or negative. Its main advantages over other evaluation methods, such as the simpler misclassification error, are: 

1. It's insensitive to unbalanced datasets (datasets that have more installeds than not-installeds or vice versa).

2. For other evaluation methods, a user has to choose a cut-off point above which the target variable is part of the positive class (e.g. a logistic regression model returns any real number between 0 and 1 - the modeler might decide that predictions greater than 0.5 mean a positive class prediction while a prediction of less than 0.5 mean a negative class prediction). AUC evaluates entries at all cut-off points, giving better insight into how well the classifier is able to separate the two classes.

Understanding AUC

To understand the calculation of AUC, a few basic concepts must be introduced. For a binary choice prediction, there are four possible outcomes:
true positive - a positive instance that is correctly classified as positive;
false positive - a negative instance that is incorrectly classified as positive;
true negative - a negative instance that is correctly classified as negative;
false negative - a positive instance that is incorrectly classified as negative);
These possibilities can be neatly displayed in a confusion matrix:


actual class

  P N
predicted class p true positive false positive
n false negative true negative

The true positive rate, or recall, is çalculated as the number of true positives divided by the total number of positives. When identifying aircraft from radar signals, it is proportion that are correctly identified.

The false positive rate is çalculated as the number of false positives divided by the total number of negatives. When identifying aircraft from radar signals, it is the rate of false alarms.

If somebody makes random guesses, the ROC curve will be a diagonal line stretching from (0,0) to (1,1) - see the blue line in the figure below. To understand this consider:
 Somebody who randomly guesses that 10 per cent of all radar signals point to planes. The false positive rate and the false alarm rate will be 10 per cent.
 Somebody who randomly guesses that 90 per cent of all radar signals point to planes. The false positive rate and the false alarm rate will be 90 per cent.
Meanwhile a perfect model will achieve a true positive rate of 1 and a false positive rate of 0.



While ROC is a two-dimensional representation of a model's performance, the AUC distils this information into a single scalar. As the name implies, it is calculated as the area under the ROC curve. A perfect model will score an AUC of 1, while random guessing will score an AUC of around of 0.5. In practice, almost all models will fit somewhere in between","The data has been downloaded using the API of a social network. There are 7.2m contacts/edges of 38k users/nodes. These have been drawn randomly ensuring a certain level of closedness.

You are given 7,237,983 contacts/edges from a social network (social_train.zip). The first column is the outbound node and the second column is the inbound node. The ids have been encoded so that the users are anonymous. Ids reach from 1 to 1,133,547.

There are 37,689 outbound nodes and 1,133,518 inbound nodes. Most outbound nodes are also inbound nodes so that the total number of unique nodes is 1,133,547.

The way the contacts were sampled makes sure that the universe is roughly closed. Note that not every relationship is mutual.

The test dataset contains 8,960 edges from 8,960 unique outbound nodes (social_test.csv). Of those 4,480 are true and 4,480 are false edges. You are tasked to predict which are true (1) and which are false (0). You need to supply back a file with outbound node id,inbound node id,[0,1] in each row. This means you can assign a probability of being true to an edge. You are being scored on the AUC. A random model will have an AUC of 0.5, so you need to try to do better than that (ie have a higher AUC). Your entry should conform to the format in sample_submission.csv.

You are encouraged to explore techniques which explain the social network/graph. The best entrant should try to explain his approach/method to other users.

Don’t despair if your first couple of solutions score low, this is an explorative process.",kaggle competitions download -c socialNetwork,['https://www.kaggle.com/code/mpwolke/ijcnn-social-network-txt']
